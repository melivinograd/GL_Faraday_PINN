{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importamos las librerías necesarias para esto\n",
        "import numpy as np\n",
        "from scipy.integrate import ode\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "uapQtkQxtRU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-MciIn_tN3f"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Multilayer perceptron (MLP) // Perceptríon Multicapa .\n",
        "\n",
        "    Esta clase define una red neuronal feedforward con múltiples capas ocultas)\n",
        "    lineales, funciones de activación tangente hiperbólica en  las capas ocultas\n",
        "    y una salida lineal.\n",
        "\n",
        "    Args:\n",
        "        sizes (lista): Lista de enteros que especifica el número de neuronas en\n",
        "        cada capa. El primer elemento debe coincidir con la dimensión de entrada\n",
        "        y el último con la dimensión de salida.\n",
        "\n",
        "    Atributos:\n",
        "        capas (torch.nn.ModuleList): Lista que contiene las capas lineales del MLP.\n",
        "\n",
        "    Métodos:\n",
        "        forward(x): Realiza una pasada hacia adelante a través de la red MLP.\n",
        "\n",
        "    Ejemplo:\n",
        "        tamaños = [entrada_dim, oculta1_dim, oculta2_dim, salida_dim]\n",
        "        mlp = MLP(tamaños)\n",
        "        tensor_entrada = torch.tensor([...])\n",
        "        salida = mlp(tensor_entrada)\n",
        "    \"\"\"\n",
        "    def __init__(self,sizes):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        self.w0 = torch.nn.Parameter(data=torch.Tensor([10]), requires_grad=True)\n",
        "        self.d = torch.nn.Parameter(data=torch.Tensor([1]), requires_grad=True)\n",
        "        for i in range(len(sizes)-1):\n",
        "            self.layers.append(torch.nn.Linear(sizes[i],sizes[i+1]))\n",
        "    def forward(self,x):\n",
        "        h = x\n",
        "        for hidden in self.layers[:-1]:\n",
        "            h = torch.tanh(hidden(h))\n",
        "        output = self.layers[-1]\n",
        "        y = output(h)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d  = 2\n",
        "w0 = 18\n",
        "time = np.arange(0,5,0.01)\n",
        "def oscilador(d,w0,t):\n",
        "    l1 = -d + 1j * (w0**2-d**2)**0.5  # l+\n",
        "    l2 = -d - 1j * (w0**2-d**2)**0.5  # l-\n",
        "    A = -l2/(l1-l2)\n",
        "    B = l1/(l1-l2)\n",
        "    return np.real(A*np.exp(l1*t)+B*np.exp(l2*t))\n",
        "\n",
        "def der_oscilador(d,w0,t): # derivada\n",
        "    l1 = -d + 1j * (w0**2-d**2)**0.5  # l+\n",
        "    l2 = -d - 1j * (w0**2-d**2)**0.5  # l-\n",
        "    A = -l2/(l1-l2)\n",
        "    B = l1/(l1-l2)\n",
        "    return np.real(A*l1*np.exp(l1*t)+B*l2*np.exp(l2*t))"
      ],
      "metadata": {
        "id": "Zj_Ip5fDtgty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_data_pinn    = torch.linspace(0,1,200).view(-1,1) #Tiempos de los datos\n",
        "y_data_pinn    = oscilador(d ,w0 ,t_data_pinn).view(-1,1) #Valores de los datos 'medidos'\n",
        "t_physics = torch.linspace(0,1,200).view(-1,1).requires_grad_(True)"
      ],
      "metadata": {
        "id": "6CvExuuCtmkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pinn = MLP([1,32,32,32,32,32,1])\n",
        "optimizer_pinn = torch.optim.Adam(pinn.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "NGrM9N9ztpp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinn.w0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO0lEUzmvYDa",
        "outputId": "6f4561e5-f873-4710-85b4-d416df7daa70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 25000\n",
        "l = 1e-4  #Lambda (cambié de 1 a 5 para mejor resultado en [0,1])\n",
        "loss1_list = []\n",
        "loss2_list = []\n",
        "\n",
        "for epoch in range(iterations):\n",
        "    optimizer_pinn.zero_grad()\n",
        "    yh_pinn = pinn(t_data_pinn)\n",
        "    loss1 = torch.mean((yh_pinn-y_data_pinn)**2)\n",
        "    yhp_pinn = pinn(t_physics)\n",
        "    #Computo de derivada\n",
        "    dx  = torch.autograd.grad(yhp_pinn, t_physics, torch.ones_like(yhp_pinn), create_graph=True)[0]# computamos dy/dx\n",
        "    dx2 = torch.autograd.grad(dx,  t_physics, torch.ones_like(dx),  create_graph=True)[0]# computamos d^2y/dx^2\n",
        "    physics = dx2 + 2*pinn.d*dx + yhp_pinn*(pinn.w0**2) #Complete con la ecuación diferencial\n",
        "    loss2 = l*torch.mean(physics**2) #Calculo el error cuadrático medio para la física\n",
        "\n",
        "    loss = loss1 + loss2 #Se suma el error de la física con el de los datos\n",
        "    loss.backward()\n",
        "    optimizer_pinn.step()\n",
        "\n",
        "    with torch.autograd.no_grad():\n",
        "      print(epoch,loss1.item(), loss2.item(),\"Traning Loss:\",loss.data.item(), pinn.w0.item(), pinn.d.item())\n",
        "      #loss1_list.append(loss1.detach().numpy())\n",
        "      #loss2_list.append(loss2.detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IdKoRRYtr2M",
        "outputId": "b3d39920-0796-430c-cdcf-4e1ec87a2dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "20000 7.375512723228894e-06 7.852327689761296e-05 Traning Loss: 8.589879143983126e-05 17.975122451782227 2.0199058055877686\n",
            "20001 5.915834208281012e-06 8.134949166560546e-05 Traning Loss: 8.726532541913912e-05 17.97534942626953 2.0199203491210938\n",
            "20002 7.630936124769505e-06 8.186462946468964e-05 Traning Loss: 8.949556649895385e-05 17.97555160522461 2.0199079513549805\n",
            "20003 6.51306663712603e-06 8.711763075552881e-05 Traning Loss: 9.363069693790749e-05 17.975770950317383 2.019918203353882\n",
            "20004 8.53865913086338e-06 9.176698222290725e-05 Traning Loss: 0.00010030563862528652 17.97595977783203 2.019904851913452\n",
            "20005 7.899621778051369e-06 0.00010175212082685903 Traning Loss: 0.0001096517444238998 17.976173400878906 2.0199129581451416\n",
            "20006 1.0665898116712924e-05 0.00011150733917020261 Traning Loss: 0.00012217323819641024 17.976350784301758 2.0198941230773926\n",
            "20007 1.036519188346574e-05 0.00012828430044464767 Traning Loss: 0.00013864949869457632 17.976564407348633 2.019904851913452\n",
            "20008 1.4805301361775491e-05 0.00014670378004666418 Traning Loss: 0.00016150908777490258 17.976728439331055 2.0198752880096436\n",
            "20009 1.5186427845037542e-05 0.00017823824600782245 Traning Loss: 0.00019342466839589179 17.976940155029297 2.0198943614959717\n",
            "20010 2.3069917006068863e-05 0.00021665879467036575 Traning Loss: 0.00023972871713340282 17.977081298828125 2.0198512077331543\n",
            "20011 2.5403745894436724e-05 0.0002798593486659229 Traning Loss: 0.0003052630927413702 17.977285385131836 2.0198841094970703\n",
            "20012 3.962466144002974e-05 0.00036081887083128095 Traning Loss: 0.0004004435322713107 17.977388381958008 2.0198254585266113\n",
            "20013 4.6756780648138374e-05 0.0004871261480730027 Traning Loss: 0.0005338829359970987 17.977577209472656 2.0198774337768555\n",
            "20014 7.255614036694169e-05 0.0006545776850543916 Traning Loss: 0.0007271338254213333 17.97762107849121 2.0198001861572266\n",
            "20015 9.01616585906595e-05 0.0009026463958434761 Traning Loss: 0.000992808025330305 17.977779388427734 2.019876718521118\n",
            "20016 0.00013668513565789908 0.0012349160388112068 Traning Loss: 0.0013716011308133602 17.97772216796875 2.019777774810791\n",
            "20017 0.00017303833737969398 0.0016906976234167814 Traning Loss: 0.0018637359607964754 17.97780990600586 2.0198872089385986\n",
            "20018 0.0002506681776139885 0.0022740366403013468 Traning Loss: 0.0025247049052268267 17.977584838867188 2.019765853881836\n",
            "20019 0.00030669989064335823 0.002958037657663226 Traning Loss: 0.0032647375483065844 17.977529525756836 2.019916534423828\n",
            "20020 0.0004059762868564576 0.0036912085488438606 Traning Loss: 0.004097184631973505 17.977060317993164 2.0197834968566895\n",
            "20021 0.0004414880240801722 0.004236571956425905 Traning Loss: 0.004678059834986925 17.976787567138672 2.0199711322784424\n",
            "20022 0.0004896505852229893 0.004437984898686409 Traning Loss: 0.004927635658532381 17.9760799407959 2.019862413406372\n",
            "20023 0.0004122949030715972 0.003959658555686474 Traning Loss: 0.004371953662484884 17.97560691833496 2.020057201385498\n",
            "20024 0.0003269138978794217 0.002899379003793001 Traning Loss: 0.003226292785257101 17.97486114501953 2.0200343132019043\n",
            "20025 0.0001620905240997672 0.0015398761024698615 Traning Loss: 0.0017019666265696287 17.974353790283203 2.0201916694641113\n",
            "20026 6.276699423324317e-05 0.00045097197289578617 Traning Loss: 0.0005137389525771141 17.973838806152344 2.0202999114990234\n",
            "20027 2.231635880889371e-05 7.452738645952195e-05 Traning Loss: 9.684374526841566e-05 17.973403930664062 2.020406484603882\n",
            "20028 5.1904775318689644e-05 0.0004065829562023282 Traning Loss: 0.0004584877169691026 17.973100662231445 2.0206329822540283\n",
            "20029 0.00013714414671994746 0.001027375808916986 Traning Loss: 0.001164519926533103 17.972639083862305 2.0207297801971436\n",
            "20030 0.0001627997262403369 0.001473813084885478 Traning Loss: 0.001636612811125815 17.972318649291992 2.021010637283325\n",
            "20031 0.00017794799350667745 0.0013998473295941949 Traning Loss: 0.001577795366756618 17.971786499023438 2.0211551189422607\n",
            "20032 0.00010935642785625532 0.0009193033911287785 Traning Loss: 0.001028659869916737 17.97138023376465 2.0214149951934814\n",
            "20033 6.022205707267858e-05 0.0003556754672899842 Traning Loss: 0.000415897520724684 17.970914840698242 2.0216310024261475\n",
            "20034 2.946807217085734e-05 8.11294448794797e-05 Traning Loss: 0.00011059751705033705 17.970495223999023 2.021834373474121\n",
            "20035 3.615529931266792e-05 0.0001881568314274773 Traning Loss: 0.00022431212710216641 17.97015953063965 2.022104501724243\n",
            "20036 7.562850805697963e-05 0.00048439635429531336 Traning Loss: 0.0005600248696282506 17.96973991394043 2.0222787857055664\n",
            "20037 8.89820876182057e-05 0.0007175382925197482 Traning Loss: 0.0008065204019658267 17.969451904296875 2.022557497024536\n",
            "20038 9.409340418642387e-05 0.000696453673299402 Traning Loss: 0.000790547055657953 17.969064712524414 2.0227444171905518\n",
            "20039 6.277313514146954e-05 0.00047395232832059264 Traning Loss: 0.000536725448910147 17.96880340576172 2.022981643676758\n",
            "20040 3.6722354707308114e-05 0.00021175308211240917 Traning Loss: 0.0002484754368197173 17.96854019165039 2.023190975189209\n",
            "20041 2.215828499174677e-05 7.882552017690614e-05 Traning Loss: 0.0001009838015306741 17.96833610534668 2.023369073867798\n",
            "20042 2.2792904928792268e-05 0.0001229423505719751 Traning Loss: 0.00014573524822480977 17.968210220336914 2.023585796356201\n",
            "20043 4.061028448631987e-05 0.0002574327518232167 Traning Loss: 0.00029804304358549416 17.96804428100586 2.023725748062134\n",
            "20044 4.652224379242398e-05 0.0003770996700040996 Traning Loss: 0.0004236219101585448 17.967975616455078 2.0239179134368896\n",
            "20045 5.1358128985157236e-05 0.00038902927190065384 Traning Loss: 0.0004403874045237899 17.967838287353516 2.0240418910980225\n",
            "20046 3.7801142752869055e-05 0.00030741735827177763 Traning Loss: 0.00034521849011071026 17.967782974243164 2.0241799354553223\n",
            "20047 2.5791796360863373e-05 0.00018507352797314525 Traning Loss: 0.00021086532797198743 17.967702865600586 2.024292469024658\n",
            "20048 1.5777015505591407e-05 9.600855992175639e-05 Traning Loss: 0.00011178557178936899 17.967662811279297 2.024373769760132\n",
            "20049 1.126507322624093e-05 7.797437137924135e-05 Traning Loss: 8.923944551497698e-05 17.9676570892334 2.0244715213775635\n",
            "20050 1.778620935510844e-05 0.00011468731099739671 Traning Loss: 0.00013247352035250515 17.967634201049805 2.0245161056518555\n",
            "20051 1.9792259990936145e-05 0.0001789217785699293 Traning Loss: 0.00019871404219884425 17.967676162719727 2.024587392807007\n",
            "20052 2.7095531550003216e-05 0.00021730469597969204 Traning Loss: 0.00024440023116767406 17.967676162719727 2.0246145725250244\n",
            "20053 2.3448674255632795e-05 0.00022134851315058768 Traning Loss: 0.00024479717831127346 17.967742919921875 2.024651288986206\n",
            "20054 2.1389285393524915e-05 0.0001846705563366413 Traning Loss: 0.0002060598344542086 17.967784881591797 2.0246708393096924\n",
            "20055 1.4747423847438768e-05 0.0001344147021882236 Traning Loss: 0.00014916212239768356 17.967878341674805 2.02467679977417\n",
            "20056 9.975305147236213e-06 9.170079283649102e-05 Traning Loss: 0.00010167609434574842 17.967981338500977 2.0246894359588623\n",
            "20057 8.842701390676666e-06 7.111309241736308e-05 Traning Loss: 7.995579653652385e-05 17.968103408813477 2.0246739387512207\n",
            "20058 7.806295798218343e-06 7.840720354579389e-05 Traning Loss: 8.621349843451753e-05 17.968259811401367 2.024672746658325\n",
            "20059 1.174986482510576e-05 9.832712385104969e-05 Traning Loss: 0.00011007698776666075 17.968402862548828 2.0246469974517822\n",
            "20060 1.227731081598904e-05 0.0001235257659573108 Traning Loss: 0.00013580307131633162 17.968584060668945 2.024629831314087\n",
            "20061 1.5024194908619393e-05 0.00013553777534980327 Traning Loss: 0.00015056197298690677 17.96873664855957 2.024601936340332\n",
            "20062 1.3554783436120488e-05 0.00013435460277833045 Traning Loss: 0.00014790939167141914 17.968923568725586 2.024571657180786\n",
            "20063 1.255872211913811e-05 0.00011834594624815509 Traning Loss: 0.0001309046638198197 17.96908950805664 2.0245420932769775\n",
            "20064 9.975930879591033e-06 9.694339678389952e-05 Traning Loss: 0.00010691932402551174 17.96927833557129 2.0245015621185303\n",
            "20065 7.90693775343243e-06 7.779186125844717e-05 Traning Loss: 8.56987971928902e-05 17.969465255737305 2.0244665145874023\n",
            "20066 6.987595497776056e-06 6.67935746605508e-05 Traning Loss: 7.378117152256891e-05 17.969661712646484 2.0244216918945312\n",
            "20067 6.478866453107912e-06 6.667226261924952e-05 Traning Loss: 7.315113180084154e-05 17.969873428344727 2.024380683898926\n",
            "20068 7.592753263452323e-06 7.348810322582722e-05 Traning Loss: 8.108085603453219e-05 17.97007942199707 2.0243349075317383\n",
            "20069 8.080511179286987e-06 8.407206041738391e-05 Traning Loss: 9.21525715966709e-05 17.970308303833008 2.0242879390716553\n",
            "20070 9.282615792471915e-06 9.166080417344347e-05 Traning Loss: 0.00010094341996591538 17.97052574157715 2.024240732192993\n",
            "20071 9.053960638993885e-06 9.478397259954363e-05 Traning Loss: 0.00010383793414803222 17.970766067504883 2.0241892337799072\n",
            "20072 9.096412213693839e-06 9.135016443906352e-05 Traning Loss: 0.00010044657392427325 17.970993041992188 2.024139165878296\n",
            "20073 7.943524906295352e-06 8.454061025986448e-05 Traning Loss: 9.248413698514923e-05 17.971237182617188 2.0240864753723145\n",
            "20074 7.4168824539810885e-06 7.583059050375596e-05 Traning Loss: 8.32474761409685e-05 17.971471786499023 2.0240328311920166\n",
            "20075 6.366675279423362e-06 6.926257628947496e-05 Traning Loss: 7.562925020465627e-05 17.971715927124023 2.023979663848877\n",
            "20076 6.2294698182085995e-06 6.523506453959271e-05 Traning Loss: 7.146453572204337e-05 17.971956253051758 2.0239222049713135\n",
            "20077 5.93146887695184e-06 6.508587102871388e-05 Traning Loss: 7.101734081516042e-05 17.97219467163086 2.0238680839538574\n",
            "20078 6.2207859627960715e-06 6.710293382639065e-05 Traning Loss: 7.332371751544997e-05 17.972436904907227 2.023808717727661\n",
            "20079 6.467134880949743e-06 7.034950976958498e-05 Traning Loss: 7.681664283154532e-05 17.972671508789062 2.0237529277801514\n",
            "20080 6.614152880501933e-06 7.334359543165192e-05 Traning Loss: 7.995774649316445e-05 17.97291374206543 2.0236940383911133\n",
            "20081 6.948152986296918e-06 7.481043576262891e-05 Traning Loss: 8.175858965842053e-05 17.97314453125 2.0236356258392334\n",
            "20082 6.5922354224312585e-06 7.523305248469114e-05 Traning Loss: 8.182528836186975e-05 17.973386764526367 2.02357816696167\n",
            "20083 6.841006324975751e-06 7.357472350122407e-05 Traning Loss: 8.041573164518923e-05 17.973615646362305 2.0235178470611572\n",
            "20084 6.111479706305545e-06 7.193889177870005e-05 Traning Loss: 7.805036875652149e-05 17.973854064941406 2.0234625339508057\n",
            "20085 6.333385954349069e-06 6.907667557243258e-05 Traning Loss: 7.5410061981529e-05 17.97408103942871 2.0234017372131348\n",
            "20086 5.598438292508945e-06 6.743831181665882e-05 Traning Loss: 7.303674647118896e-05 17.974313735961914 2.023347854614258\n",
            "20087 5.835188403580105e-06 6.54642135486938e-05 Traning Loss: 7.129940058803186e-05 17.974538803100586 2.023287773132324\n",
            "20088 5.367654466681415e-06 6.496791320387274e-05 Traning Loss: 7.033556903479621e-05 17.974763870239258 2.02323317527771\n",
            "20089 5.551293725147843e-06 6.455354014178738e-05 Traning Loss: 7.010483386693522e-05 17.974985122680664 2.023174285888672\n",
            "20090 5.432089892565273e-06 6.50091387797147e-05 Traning Loss: 7.044123049126938e-05 17.975202560424805 2.023118257522583\n",
            "20091 5.450882781588007e-06 6.567047967109829e-05 Traning Loss: 7.11213651811704e-05 17.975418090820312 2.023061513900757\n",
            "20092 5.614002475340385e-06 6.629592826357111e-05 Traning Loss: 7.19099334673956e-05 17.975624084472656 2.0230038166046143\n",
            "20093 5.405007414083229e-06 6.720629608025774e-05 Traning Loss: 7.261130667757243e-05 17.975833892822266 2.022948741912842\n",
            "20094 5.7150186876242515e-06 6.737215881003067e-05 Traning Loss: 7.308717613341287e-05 17.976032257080078 2.0228893756866455\n",
            "20095 5.3069343266543e-06 6.795123044867069e-05 Traning Loss: 7.325816841330379e-05 17.976234436035156 2.0228350162506104\n",
            "20096 5.644657449010992e-06 6.747695442754775e-05 Traning Loss: 7.31216132408008e-05 17.976425170898438 2.0227746963500977\n",
            "20097 5.146304829395376e-06 6.757504161214456e-05 Traning Loss: 7.272134826052934e-05 17.97662353515625 2.0227200984954834\n",
            "20098 5.4347747209249064e-06 6.670253787888214e-05 Traning Loss: 7.213731441879645e-05 17.976810455322266 2.0226595401763916\n",
            "20099 4.972635451849783e-06 6.648776616202667e-05 Traning Loss: 7.146040297811851e-05 17.97700309753418 2.0226035118103027\n",
            "20100 5.17214084538864e-06 6.561067857546732e-05 Traning Loss: 7.078282214934006e-05 17.977188110351562 2.022542953491211\n",
            "20101 4.832375452679116e-06 6.533657142426819e-05 Traning Loss: 7.016894960543141e-05 17.97737693786621 2.0224850177764893\n",
            "20102 4.929548595100641e-06 6.473511166404933e-05 Traning Loss: 6.966466025914997e-05 17.97756004333496 2.0224249362945557\n",
            "20103 4.748045284941327e-06 6.45383115625009e-05 Traning Loss: 6.928635411895812e-05 17.97774314880371 2.0223653316497803\n",
            "20104 4.748286301037297e-06 6.428173946915194e-05 Traning Loss: 6.903002213221043e-05 17.977922439575195 2.0223052501678467\n",
            "20105 4.708748747361824e-06 6.416987889679149e-05 Traning Loss: 6.88786240061745e-05 17.978099822998047 2.0222437381744385\n",
            "20106 4.628672741091577e-06 6.417791882995516e-05 Traning Loss: 6.880659202579409e-05 17.978275299072266 2.022183418273926\n",
            "20107 4.6852915147610474e-06 6.410522473743185e-05 Traning Loss: 6.879051943542436e-05 17.97844696044922 2.022120475769043\n",
            "20108 4.548367996903835e-06 6.426086474675685e-05 Traning Loss: 6.880923319840804e-05 17.97861671447754 2.022059917449951\n",
            "20109 4.6538616516045295e-06 6.419048440875486e-05 Traning Loss: 6.884434696985409e-05 17.978782653808594 2.02199649810791\n",
            "20110 4.490350420383038e-06 6.44012980046682e-05 Traning Loss: 6.889164797030389e-05 17.97894859313965 2.02193546295166\n",
            "20111 4.608742528944276e-06 6.433545786421746e-05 Traning Loss: 6.894420221215114e-05 17.979108810424805 2.02187180519104\n",
            "20112 4.445195827429416e-06 6.455718539655209e-05 Traning Loss: 6.900238076923415e-05 17.97926902770996 2.021810293197632\n",
            "20113 4.559502031042939e-06 6.450656655943021e-05 Traning Loss: 6.90660672262311e-05 17.97942543029785 2.02174711227417\n",
            "20114 4.411931968206773e-06 6.472846143878996e-05 Traning Loss: 6.914039113325998e-05 17.979581832885742 2.0216856002807617\n",
            "20115 4.518603418546263e-06 6.470470543717965e-05 Traning Loss: 6.922330794623122e-05 17.979732513427734 2.021622896194458\n",
            "20116 4.391002221382223e-06 6.493293767562136e-05 Traning Loss: 6.932394171599299e-05 17.97988510131836 2.021561622619629\n",
            "20117 4.493840606301092e-06 6.495159323094413e-05 Traning Loss: 6.944543565623462e-05 17.980031967163086 2.0214998722076416\n",
            "20118 4.378444373287493e-06 6.521754403365776e-05 Traning Loss: 6.95959897711873e-05 17.980180740356445 2.0214390754699707\n",
            "20119 4.4889543460158166e-06 6.529654638143256e-05 Traning Loss: 6.978549936320633e-05 17.980321884155273 2.0213780403137207\n",
            "20120 4.376439392217435e-06 6.56502743368037e-05 Traning Loss: 7.002671191003174e-05 17.980466842651367 2.021318197250366\n",
            "20121 4.512463419814594e-06 6.582865898963064e-05 Traning Loss: 7.034112059045583e-05 17.98060417175293 2.0212578773498535\n",
            "20122 4.394714324007509e-06 6.635468162130564e-05 Traning Loss: 7.07493964000605e-05 17.980745315551758 2.0211989879608154\n",
            "20123 4.581409939419245e-06 6.671067967545241e-05 Traning Loss: 7.12920882506296e-05 17.980876922607422 2.02113938331604\n",
            "20124 4.4536795940075535e-06 6.755716458428651e-05 Traning Loss: 7.201084372354671e-05 17.98101234436035 2.0210816860198975\n",
            "20125 4.725820417661453e-06 6.82557511026971e-05 Traning Loss: 7.298157288460061e-05 17.981138229370117 2.0210225582122803\n",
            "20126 4.595602604240412e-06 6.969364767428488e-05 Traning Loss: 7.428925164276734e-05 17.981271743774414 2.0209665298461914\n",
            "20127 5.01155955134891e-06 7.10695530869998e-05 Traning Loss: 7.60811090003699e-05 17.98138999938965 2.0209078788757324\n",
            "20128 4.9086556828115135e-06 7.3621777119115e-05 Traning Loss: 7.85304291639477e-05 17.98151969909668 2.0208535194396973\n",
            "20129 5.570031134993769e-06 7.637374073965475e-05 Traning Loss: 8.194377005565912e-05 17.981632232666016 2.0207951068878174\n",
            "20130 5.572230293182656e-06 8.110596536425874e-05 Traning Loss: 8.667819201946259e-05 17.981760025024414 2.020742654800415\n",
            "20131 6.671812116110232e-06 8.671868999954313e-05 Traning Loss: 9.339050302514806e-05 17.98186492919922 2.020684242248535\n",
            "20132 6.96277902534348e-06 9.587541717337444e-05 Traning Loss: 0.00010283819574397057 17.981990814208984 2.0206339359283447\n",
            "20133 8.89260172698414e-06 0.00010758575808722526 Traning Loss: 0.0001164783607237041 17.982084274291992 2.0205750465393066\n",
            "20134 9.901841622195207e-06 0.0001260536228073761 Traning Loss: 0.00013595545897260308 17.982208251953125 2.0205276012420654\n",
            "20135 1.34928714032867e-05 0.00015108491061255336 Traning Loss: 0.00016457778110634536 17.98228645324707 2.020467758178711\n",
            "20136 1.622119998501148e-05 0.0001896716858027503 Traning Loss: 0.00020589288033079356 17.982406616210938 2.020423650741577\n",
            "20137 2.3287231670110486e-05 0.0002443186822347343 Traning Loss: 0.00026760590844787657 17.98246192932129 2.0203616619110107\n",
            "20138 3.0040126148378477e-05 0.0003271028690505773 Traning Loss: 0.00035714299883693457 17.982574462890625 2.020322322845459\n",
            "20139 4.4567528675543144e-05 0.0004480092029552907 Traning Loss: 0.0004925767425447702 17.982587814331055 2.020256757736206\n",
            "20140 6.0463426052592695e-05 0.0006273873150348663 Traning Loss: 0.0006878507556393743 17.98267936706543 2.020224094390869\n",
            "20141 9.088729711947963e-05 0.000893151096533984 Traning Loss: 0.0009840383427217603 17.982616424560547 2.0201528072357178\n",
            "20142 0.00012611952843144536 0.0012722769752144814 Traning Loss: 0.0013983964454382658 17.982656478881836 2.020129442214966\n",
            "20143 0.0001877651666291058 0.0018261211225762963 Traning Loss: 0.002013886347413063 17.982450485229492 2.020049571990967\n",
            "20144 0.00025648679002188146 0.002548370510339737 Traning Loss: 0.002804857213050127 17.98236846923828 2.0200388431549072\n",
            "20145 0.0003641909279394895 0.003523994470015168 Traning Loss: 0.0038881853688508272 17.98191261291504 2.0199472904205322\n",
            "20146 0.0004598785890266299 0.00453179469332099 Traning Loss: 0.004991673398762941 17.981590270996094 2.0199527740478516\n",
            "20147 0.000577897357288748 0.005563115701079369 Traning Loss: 0.006141013000160456 17.980783462524414 2.0198516845703125\n",
            "20148 0.0006025556358508766 0.0059027643874287605 Traning Loss: 0.006505319848656654 17.98014259338379 2.019876003265381\n",
            "20149 0.0005792109877802432 0.005500965751707554 Traning Loss: 0.006080176681280136 17.979082107543945 2.0197832584381104\n",
            "20150 0.0004038970801047981 0.003903166390955448 Traning Loss: 0.0043070632964372635 17.978281021118164 2.0198285579681396\n",
            "20151 0.00021948992798570544 0.001925452146679163 Traning Loss: 0.002144942060112953 17.977365493774414 2.0197858810424805\n",
            "20152 5.9230500482954085e-05 0.00044606055598706007 Traning Loss: 0.0005052910419180989 17.9766845703125 2.0198559761047363\n",
            "20153 3.928762089344673e-05 0.00016477933968417346 Traning Loss: 0.000204066964215599 17.97612762451172 2.019904851913452\n",
            "20154 0.00011786751565523446 0.0009195223683491349 Traning Loss: 0.001037389854900539 17.97551155090332 2.0199997425079346\n",
            "20155 0.0002075526863336563 0.0018194785807281733 Traning Loss: 0.0020270312670618296 17.97503089904785 2.0201430320739746\n",
            "20156 0.0002449003513902426 0.0020673710387200117 Traning Loss: 0.0023122713901102543 17.974313735961914 2.020259141921997\n",
            "20157 0.0001730985241010785 0.0014478502562269568 Traning Loss: 0.0016209487803280354 17.97373390197754 2.02046799659729\n",
            "20158 9.518794104224071e-05 0.0005641675088554621 Traning Loss: 0.0006593554280698299 17.973066329956055 2.0206127166748047\n",
            "20159 4.1925213736249134e-05 0.00014306105731520802 Traning Loss: 0.00018498627468943596 17.97247886657715 2.020850419998169\n",
            "20160 7.238412217702717e-05 0.0003660189686343074 Traning Loss: 0.0004384031053632498 17.97195816040039 2.0210344791412354\n",
            "20161 0.00011574062227737159 0.000868026225361973 Traning Loss: 0.0009837668621912599 17.97136116027832 2.021261692047119\n",
            "20162 0.00013910017150919884 0.0010596929350867867 Traning Loss: 0.00119879306294024 17.97088050842285 2.021481513977051\n",
            "20163 0.00011090452608186752 0.0008074237848632038 Traning Loss: 0.000918328296393156 17.97031021118164 2.02168345451355\n",
            "20164 6.129951361799613e-05 0.0003538448363542557 Traning Loss: 0.0004151443426962942 17.969858169555664 2.021934747695923\n",
            "20165 3.840260978904553e-05 0.000108477303001564 Traning Loss: 0.00014687991642858833 17.969423294067383 2.022132158279419\n",
            "20166 4.099547004443593e-05 0.00022370867372956127 Traning Loss: 0.0002647041401360184 17.969018936157227 2.022397518157959\n",
            "20167 7.202236156444997e-05 0.0004839272878598422 Traning Loss: 0.0005559496348723769 17.96869659423828 2.0225989818573\n",
            "20168 7.88521210779436e-05 0.0006331976037472486 Traning Loss: 0.0007120497175492346 17.96833038330078 2.022836208343506\n",
            "20169 6.940495950402692e-05 0.0005130467470735312 Traning Loss: 0.0005824516993016005 17.968061447143555 2.023031711578369\n",
            "20170 4.085702312295325e-05 0.0002680105098988861 Traning Loss: 0.00030886754393577576 17.967777252197266 2.02321720123291\n",
            "20171 2.0735278667416424e-05 9.027412306750193e-05 Traning Loss: 0.00011100940173491836 17.96755599975586 2.0234036445617676\n",
            "20172 2.124966158589814e-05 9.43971099331975e-05 Traning Loss: 0.00011564676970010623 17.96737289428711 2.023540496826172\n",
            "20173 3.065574128413573e-05 0.00023432853049598634 Traning Loss: 0.00026498426450416446 17.967187881469727 2.023702383041382\n",
            "20174 4.4863278162665665e-05 0.00035655873944051564 Traning Loss: 0.00040142203215509653 17.967063903808594 2.0237932205200195\n",
            "20175 4.212188650853932e-05 0.00037429764051921666 Traning Loss: 0.000416419527027756 17.966909408569336 2.023909091949463\n",
            "20176 3.298894807812758e-05 0.00027044981834478676 Traning Loss: 0.00030343877733685076 17.966812133789062 2.0239686965942383\n",
            "20177 1.7333857613266446e-05 0.00014285455108620226 Traning Loss: 0.00016018841415643692 17.966716766357422 2.0240399837493896\n",
            "20178 1.0125617336598225e-05 6.961545295780525e-05 Traning Loss: 7.974106847541407e-05 17.966651916503906 2.02408766746521\n",
            "20179 1.0623268281051423e-05 8.564727613702416e-05 Traning Loss: 9.627054532757029e-05 17.966630935668945 2.024118661880493\n",
            "20180 1.7123673387686722e-05 0.00015467187040485442 Traning Loss: 0.00017179554561153054 17.966611862182617 2.0241494178771973\n",
            "20181 2.304654844920151e-05 0.0002144927711924538 Traning Loss: 0.00023753932327963412 17.966657638549805 2.0241401195526123\n",
            "20182 2.316397694812622e-05 0.0002253863785881549 Traning Loss: 0.0002485503500793129 17.96669578552246 2.024146556854248\n",
            "20183 1.9432947738096118e-05 0.00018191058188676834 Traning Loss: 0.00020134352962486446 17.966787338256836 2.02411150932312\n",
            "20184 1.2145871551183518e-05 0.00012178850010968745 Traning Loss: 0.00013393437257036567 17.966886520385742 2.0240962505340576\n",
            "20185 8.92149910214357e-06 7.745066977804527e-05 Traning Loss: 8.637216524221003e-05 17.96700668334961 2.02405047416687\n",
            "20186 7.080194791342365e-06 7.280272984644398e-05 Traning Loss: 7.98829278210178e-05 17.96715545654297 2.024014711380005\n",
            "20187 1.0722579645516817e-05 9.564350330037996e-05 Traning Loss: 0.00010636608203640208 17.9672908782959 2.02396297454834\n",
            "20188 1.262554360437207e-05 0.00012743179104290903 Traning Loss: 0.0001400573382852599 17.967464447021484 2.023911476135254\n",
            "20189 1.5004794477135874e-05 0.0001420118351234123 Traning Loss: 0.0001570166350575164 17.96761131286621 2.0238587856292725\n",
            "20190 1.33870935314917e-05 0.00013381837925408036 Traning Loss: 0.00014720547187607735 17.96779441833496 2.023799180984497\n",
            "20191 1.113857524615014e-05 0.00010813681728905067 Traning Loss: 0.0001192753916257061 17.967966079711914 2.023745059967041\n",
            "20192 8.296914529637434e-06 8.134794916259125e-05 Traning Loss: 8.964486187323928e-05 17.96816062927246 2.0236804485321045\n",
            "20193 6.551666501763975e-06 6.677835335722193e-05 Traning Loss: 7.333001849474385e-05 17.968366622924805 2.023622512817383\n",
            "20194 7.109406851668609e-06 6.791242776671425e-05 Traning Loss: 7.50218314351514e-05 17.96857452392578 2.0235564708709717\n",
            "20195 7.630067557329312e-06 8.1282363680657e-05 Traning Loss: 8.891243487596512e-05 17.96881103515625 2.0234978199005127\n",
            "20196 9.817269528866746e-06 9.414797386853024e-05 Traning Loss: 0.00010396524157840759 17.969032287597656 2.0234336853027344\n",
            "20197 9.442987902730238e-06 0.00010123859101440758 Traning Loss: 0.00011068157618865371 17.969284057617188 2.023375988006592\n",
            "20198 9.967284313461278e-06 9.62391059147194e-05 Traning Loss: 0.00010620638931868598 17.969518661499023 2.0233123302459717\n",
            "20199 7.885259947215673e-06 8.592238009441644e-05 Traning Loss: 9.380764095112681e-05 17.969776153564453 2.0232560634613037\n",
            "20200 7.422298494930146e-06 7.31219188310206e-05 Traning Loss: 8.054421778069809e-05 17.970022201538086 2.0231943130493164\n",
            "20201 6.011749064782634e-06 6.634130841121078e-05 Traning Loss: 7.23530538380146e-05 17.97027587890625 2.023142099380493\n",
            "20202 6.337910690490389e-06 6.521150498883799e-05 Traning Loss: 7.154941704357043e-05 17.970529556274414 2.023083209991455\n",
            "20203 6.4952346292557195e-06 6.984959327382967e-05 Traning Loss: 7.634482608409598e-05 17.97077751159668 2.0230326652526855\n",
            "20204 7.1121335167845245e-06 7.551241287728772e-05 Traning Loss: 8.262454502983019e-05 17.971036911010742 2.022974729537964\n",
            "20205 7.464498139597708e-06 7.905000529717654e-05 Traning Loss: 8.65145048010163e-05 17.971284866333008 2.022923231124878\n",
            "20206 7.239335900521837e-06 7.883295620558783e-05 Traning Loss: 8.607229392509907e-05 17.971546173095703 2.0228676795959473\n",
            "20207 7.021122200967511e-06 7.495348836528137e-05 Traning Loss: 8.197461283998564e-05 17.9717960357666 2.0228161811828613\n",
            "20208 6.25431948719779e-06 7.014263974269852e-05 Traning Loss: 7.639695832040161e-05 17.97205352783203 2.0227653980255127\n",
            "20209 6.041758297214983e-06 6.581486377399415e-05 Traning Loss: 7.185662252595648e-05 17.97230339050293 2.0227138996124268\n",
            "20210 5.6058256632240955e-06 6.426846812246367e-05 Traning Loss: 6.987429514992982e-05 17.97255516052246 2.022667169570923\n",
            "20211 5.839698133058846e-06 6.473132089013234e-05 Traning Loss: 7.057101902319118e-05 17.97280502319336 2.022615671157837\n",
            "20212 5.823809260618873e-06 6.708570435876027e-05 Traning Loss: 7.290951180038974e-05 17.97304916381836 2.0225722789764404\n",
            "20213 6.207366823218763e-06 6.919618317624554e-05 Traning Loss: 7.54035499994643e-05 17.973297119140625 2.0225212574005127\n",
            "20214 6.128594577603508e-06 7.071469008224085e-05 Traning Loss: 7.684328738832846e-05 17.973535537719727 2.022479295730591\n",
            "20215 6.284758001129376e-06 7.040365017019212e-05 Traning Loss: 7.668841135455295e-05 17.97377586364746 2.022427558898926\n",
            "20216 5.905429134145379e-06 6.929073424544185e-05 Traning Loss: 7.519616337958723e-05 17.974008560180664 2.0223848819732666\n",
            "20217 5.900865744479233e-06 6.715110066579655e-05 Traning Loss: 7.305196777451783e-05 17.974239349365234 2.0223329067230225\n",
            "20218 5.401441740104929e-06 6.568066601175815e-05 Traning Loss: 7.108210411388427e-05 17.97446632385254 2.022289514541626\n",
            "20219 5.532879185921047e-06 6.432666123146191e-05 Traning Loss: 6.985953950788826e-05 17.974687576293945 2.022237777709961\n",
            "20220 5.1071915549982805e-06 6.44725514575839e-05 Traning Loss: 6.957974255783483e-05 17.97490882873535 2.022193670272827\n",
            "20221 5.468560630106367e-06 6.460504664573818e-05 Traning Loss: 7.007360545685515e-05 17.975120544433594 2.022141456604004\n",
            "20222 5.082863935967907e-06 6.587318057427183e-05 Traning Loss: 7.095604087226093e-05 17.975337982177734 2.0220959186553955\n",
            "20223 5.5444120334868785e-06 6.62599632050842e-05 Traning Loss: 7.180437387432903e-05 17.975543975830078 2.022042751312256\n",
            "20224 5.082240477349842e-06 6.723483238602057e-05 Traning Loss: 7.231707422761247e-05 17.975757598876953 2.0219967365264893\n",
            "20225 5.527856046683155e-06 6.68520006001927e-05 Traning Loss: 7.237985846586525e-05 17.97595977783203 2.021941900253296\n",
            "20226 4.958249064657139e-06 6.709412264171988e-05 Traning Loss: 7.205237488960847e-05 17.97616958618164 2.021895170211792\n",
            "20227 5.394341314968187e-06 6.61353551549837e-05 Traning Loss: 7.152969919843599e-05 17.976369857788086 2.0218374729156494\n",
            "20228 4.771427938976558e-06 6.626668618991971e-05 Traning Loss: 7.103811367414892e-05 17.97657585144043 2.0217902660369873\n",
            "20229 5.290035460348008e-06 6.549200043082237e-05 Traning Loss: 7.078203634591773e-05 17.97677230834961 2.0217294692993164\n",
            "20230 4.662319042836316e-06 6.62359016132541e-05 Traning Loss: 7.089821883710101e-05 17.976972579956055 2.0216827392578125\n",
            "20231 5.355672783480259e-06 6.609348201891407e-05 Traning Loss: 7.144915434764698e-05 17.977163314819336 2.021618604660034\n",
            "20232 4.698311840911629e-06 6.776367081329226e-05 Traning Loss: 7.246198219945654e-05 17.977357864379883 2.021573066711426\n",
            "20233 5.631601197819691e-06 6.83087419020012e-05 Traning Loss: 7.394034037133679e-05 17.977540969848633 2.0215048789978027\n",
            "20234 4.874026672041509e-06 7.107485726010054e-05 Traning Loss: 7.594888302264735e-05 17.97772789001465 2.0214617252349854\n",
            "20235 6.1584355535160284e-06 7.244110020110384e-05 Traning Loss: 7.859953620936722e-05 17.977903366088867 2.021388530731201\n",
            "20236 5.262334525468759e-06 7.694306259509176e-05 Traning Loss: 8.220539893954992e-05 17.978086471557617 2.0213494300842285\n",
            "20237 7.14454199624015e-06 8.001043170224875e-05 Traning Loss: 8.71549709700048e-05 17.978252410888672 2.0212697982788086\n",
            "20238 6.152281457616482e-06 8.81036467035301e-05 Traning Loss: 9.425592725165188e-05 17.978431701660156 2.0212371349334717\n",
            "20239 9.111316103371792e-06 9.533089178148657e-05 Traning Loss: 0.00010444220970384777 17.978588104248047 2.021148681640625\n",
            "20240 8.243492629844695e-06 0.00011128333426313475 Traning Loss: 0.00011952682689297944 17.9787654876709 2.0211265087127686\n",
            "20241 1.3214093996793963e-05 0.00012836893438361585 Traning Loss: 0.00014158303383737803 17.978906631469727 2.0210256576538086\n",
            "20242 1.312976201006677e-05 0.00016158263315446675 Traning Loss: 0.0001747123897075653 17.979080200195312 2.0210204124450684\n",
            "20243 2.2007961888448335e-05 0.00020148906332906336 Traning Loss: 0.0002234970306744799 17.97920036315918 2.0209012031555176\n",
            "20244 2.4448923795716837e-05 0.0002729256812017411 Traning Loss: 0.00029737461591139436 17.979366302490234 2.0209217071533203\n",
            "20245 4.113930117455311e-05 0.00036472961073741317 Traning Loss: 0.0004058689228259027 17.979448318481445 2.020775079727173\n",
            "20246 5.0286234909435734e-05 0.0005197773571126163 Traning Loss: 0.0005700635956600308 17.979595184326172 2.0208351612091064\n",
            "20247 8.243331831181422e-05 0.0007234788499772549 Traning Loss: 0.0008059121901169419 17.979610443115234 2.0206503868103027\n",
            "20248 0.0001066258701030165 0.0010480424389243126 Traning Loss: 0.0011546682799234986 17.97970962524414 2.020768880844116\n",
            "20249 0.0001659045956330374 0.0014563039876520634 Traning Loss: 0.0016222086269408464 17.979602813720703 2.0205366611480713\n",
            "20250 0.00021430652122944593 0.0020451319869607687 Traning Loss: 0.002259438391774893 17.97959327697754 2.0207316875457764\n",
            "20251 0.000302977830870077 0.002662176499143243 Traning Loss: 0.0029651543591171503 17.97928237915039 2.0204591751098633\n",
            "20252 0.0003567763778846711 0.003351854160428047 Traning Loss: 0.0037086305674165487 17.979068756103516 2.0207290649414062\n",
            "20253 0.0004204616998322308 0.0036762687377631664 Traning Loss: 0.00409673061221838 17.97849464416504 2.0204663276672363\n",
            "20254 0.00038625262095592916 0.003617554437369108 Traning Loss: 0.004003806971013546 17.97803497314453 2.02075457572937\n",
            "20255 0.0003263435501139611 0.002794993110001087 Traning Loss: 0.0031213366892188787 17.97730255126953 2.0205976963043213\n",
            "20256 0.0001741822634357959 0.001667336793616414 Traning Loss: 0.0018415190279483795 17.97673797607422 2.020798444747925\n",
            "20257 7.793308031978086e-05 0.0006103613413870335 Traning Loss: 0.0006882944144308567 17.97611427307129 2.020822048187256\n",
            "20258 2.1409010514616966e-05 0.00018310145242139697 Traning Loss: 0.00020451046293601394 17.975618362426758 2.020864486694336\n",
            "20259 4.222967254463583e-05 0.00041373580461367965 Traning Loss: 0.00045596546260640025 17.975208282470703 2.0210514068603516\n",
            "20260 0.00011625576007645577 0.0009200146305374801 Traning Loss: 0.0010362704051658511 17.974756240844727 2.020987033843994\n",
            "20261 0.0001352552935713902 0.0013007000088691711 Traning Loss: 0.0014359552878886461 17.974409103393555 2.0212419033050537\n",
            "20262 0.00014624878531321883 0.0011532797943800688 Traning Loss: 0.0012995286379009485 17.973934173583984 2.021216869354248\n",
            "20263 7.804469350958243e-05 0.0007230187766253948 Traning Loss: 0.0008010634919628501 17.973556518554688 2.0214247703552246\n",
            "20264 4.31719672633335e-05 0.0002826774143613875 Traning Loss: 0.0003258493961766362 17.973129272460938 2.0215325355529785\n",
            "20265 2.618914732011035e-05 0.00018351177277509123 Traning Loss: 0.0002097009273711592 17.972776412963867 2.0216243267059326\n",
            "20266 4.283218731870875e-05 0.00039358294452540576 Traning Loss: 0.0004364151391200721 17.972469329833984 2.021832227706909\n",
            "20267 8.126388274831697e-05 0.0006279113586060703 Traning Loss: 0.0007091752486303449 17.972164154052734 2.021841287612915\n",
            "20268 7.286979962373152e-05 0.0006976298755034804 Traning Loss: 0.0007704996969550848 17.97193717956543 2.0220561027526855\n",
            "20269 6.467331695603207e-05 0.0004814532585442066 Traning Loss: 0.0005461265682242811 17.971681594848633 2.0220890045166016\n",
            "20270 2.6522273401496932e-05 0.00022101383365225047 Traning Loss: 0.0002475361106917262 17.971485137939453 2.0222277641296387\n",
            "20271 1.527512722532265e-05 8.129940397338942e-05 Traning Loss: 9.657452756073326e-05 17.97129249572754 2.022336721420288\n",
            "20272 2.2349027858581394e-05 0.00014885437849443406 Traning Loss: 0.00017120339907705784 17.971126556396484 2.022376775741577\n",
            "20273 3.44522122759372e-05 0.00032309794914908707 Traning Loss: 0.00035755016142502427 17.970996856689453 2.0225205421447754\n",
            "20274 5.2761064580408856e-05 0.00041809905087575316 Traning Loss: 0.0004708601045422256 17.970869064331055 2.0225019454956055\n",
            "20275 3.941507020499557e-05 0.000390904548112303 Traning Loss: 0.0004303196328692138 17.970787048339844 2.0226075649261475\n",
            "20276 3.107855809503235e-05 0.0002395609044469893 Traning Loss: 0.00027063945890404284 17.970712661743164 2.0225956439971924\n",
            "20277 1.2786273146048188e-05 0.00011247938527958468 Traning Loss: 0.00012526565114967525 17.970651626586914 2.0226147174835205\n",
            "20278 9.093841981666628e-06 7.302327867364511e-05 Traning Loss: 8.211711974581704e-05 17.970617294311523 2.022636651992798\n",
            "20279 1.686182622506749e-05 0.00012216914910823107 Traning Loss: 0.00013903097715228796 17.97056770324707 2.0225813388824463\n",
            "20280 1.9690165572683327e-05 0.00020306112128309906 Traning Loss: 0.00022275128867477179 17.970556259155273 2.022614002227783\n",
            "20281 2.8252317861188203e-05 0.00022873264970257878 Traning Loss: 0.00025698496028780937 17.970535278320312 2.022526264190674\n",
            "20282 1.853863795986399e-05 0.00020492836483754218 Traning Loss: 0.00022346701007336378 17.97056007385254 2.02252197265625\n",
            "20283 1.5757919754832983e-05 0.00013726936595048755 Traning Loss: 0.00015302728570532054 17.970605850219727 2.0224409103393555\n",
            "20284 8.424927727901377e-06 9.281389793613926e-05 Traning Loss: 0.00010123882384505123 17.970666885375977 2.0223729610443115\n",
            "20285 7.729477147222497e-06 8.904468995751813e-05 Traning Loss: 9.677416528575122e-05 17.97077178955078 2.0223186016082764\n",
            "20286 1.3567707355832681e-05 0.00011613902461249381 Traning Loss: 0.0001297067356063053 17.97085189819336 2.022207736968994\n",
            "20287 1.3090280845062807e-05 0.00015221121429931372 Traning Loss: 0.00016530149150639772 17.970979690551758 2.022170066833496\n",
            "20288 1.851233719207812e-05 0.00015483066090382636 Traning Loss: 0.00017334299627691507 17.97107696533203 2.02204966545105\n",
            "20289 1.129617066908395e-05 0.00013725781172979623 Traning Loss: 0.00014855398330837488 17.971221923828125 2.021998167037964\n",
            "20290 1.0886309610214084e-05 9.729611338116229e-05 Traning Loss: 0.00010818242299137637 17.971357345581055 2.021892786026001\n",
            "20291 5.586949555436149e-06 7.172718324000016e-05 Traning Loss: 7.731413643341511e-05 17.97151756286621 2.021808624267578\n",
            "20292 5.422883077699225e-06 6.495300476672128e-05 Traning Loss: 7.03758851159364e-05 17.97168731689453 2.0217323303222656\n",
            "20293 7.975771040946711e-06 7.641108823008835e-05 Traning Loss: 8.438686199951917e-05 17.97184944152832 2.0216264724731445\n",
            "20294 7.765418558847159e-06 9.664155368227512e-05 Traning Loss: 0.00010440697224112228 17.972030639648438 2.0215721130371094\n",
            "20295 1.1757078937080223e-05 0.00010337489220546558 Traning Loss: 0.00011513197387102991 17.97218894958496 2.021465301513672\n",
            "20296 8.215494744945318e-06 0.00010265486343996599 Traning Loss: 0.00011087035818491131 17.972373962402344 2.0214145183563232\n",
            "20297 9.306449101131875e-06 8.698758028913289e-05 Traning Loss: 9.629403211874887e-05 17.972543716430664 2.02132511138916\n",
            "20298 5.834422609041212e-06 7.593753980472684e-05 Traning Loss: 8.17719628685154e-05 17.972740173339844 2.021263360977173\n",
            "20299 6.097293407947291e-06 6.934355769772083e-05 Traning Loss: 7.544085383415222e-05 17.97293472290039 2.0211997032165527\n",
            "20300 6.352905074891169e-06 7.253515650518239e-05 Traning Loss: 7.888806430855766e-05 17.973142623901367 2.0211265087127686\n",
            "20301 6.446456609410234e-06 8.117403922369704e-05 Traning Loss: 8.762049401411787e-05 17.97335433959961 2.021082878112793\n",
            "20302 8.411509043071419e-06 8.634218102088198e-05 Traning Loss: 9.47536900639534e-05 17.973567962646484 2.0210089683532715\n",
            "20303 7.023089438007446e-06 8.884728595148772e-05 Traning Loss: 9.587037493474782e-05 17.973785400390625 2.02097487449646\n",
            "20304 7.874786206230056e-06 8.260595495812595e-05 Traning Loss: 9.04807384358719e-05 17.974000930786133 2.0209124088287354\n",
            "20305 5.942976713413373e-06 7.625765283592045e-05 Traning Loss: 8.220062591135502e-05 17.974218368530273 2.0208752155303955\n",
            "20306 5.864530521648703e-06 6.938031583558768e-05 Traning Loss: 7.524484681198373e-05 17.974443435668945 2.02082896232605\n",
            "20307 5.547566161112627e-06 6.686019332846627e-05 Traning Loss: 7.240776176331565e-05 17.97466278076172 2.020781993865967\n",
            "20308 5.154483005753718e-06 6.851485522929579e-05 Traning Loss: 7.366934005403891e-05 17.974895477294922 2.0207488536834717\n",
            "20309 6.47821889288025e-06 7.044828089419752e-05 Traning Loss: 7.692650251556188e-05 17.975112915039062 2.0206966400146484\n",
            "20310 5.432321813714225e-06 7.407974771922454e-05 Traning Loss: 7.951207226142287e-05 17.975341796875 2.02067232131958\n",
            "20311 6.824269803473726e-06 7.288332562893629e-05 Traning Loss: 7.970759179443121e-05 17.97555160522461 2.020622730255127\n",
            "20312 5.215521468926454e-06 7.221385021694005e-05 Traning Loss: 7.742937305010855e-05 17.975772857666016 2.0205986499786377\n",
            "20313 5.920701823924901e-06 6.797670357627794e-05 Traning Loss: 7.389740494545549e-05 17.975980758666992 2.020556688308716\n",
            "20314 4.746898866869742e-06 6.601073255296797e-05 Traning Loss: 7.075763278407976e-05 17.976198196411133 2.020526170730591\n",
            "20315 4.959679245075677e-06 6.42000522930175e-05 Traning Loss: 6.915973062859848e-05 17.976409912109375 2.0204927921295166\n",
            "20316 4.879825155512663e-06 6.444517202908173e-05 Traning Loss: 6.932499672984704e-05 17.97662353515625 2.0204555988311768\n",
            "20317 4.734361027658451e-06 6.591364945052192e-05 Traning Loss: 7.064801320666447e-05 17.976835250854492 2.0204291343688965\n",
            "20318 5.338838491297793e-06 6.68097854941152e-05 Traning Loss: 7.214862125692889e-05 17.97704315185547 2.020388603210449\n",
            "20319 4.798920599569101e-06 6.821994611527771e-05 Traning Loss: 7.301886944333091e-05 17.977249145507812 2.020364761352539\n",
            "20320 5.379333288146881e-06 6.753165507689118e-05 Traning Loss: 7.291098881978542e-05 17.97745132446289 2.020325183868408\n",
            "20321 4.6956688493082765e-06 6.735208444297314e-05 Traning Loss: 7.204775465652347e-05 17.977649688720703 2.0202994346618652\n",
            "20322 4.9725158532965e-06 6.597259198315442e-05 Traning Loss: 7.094510510796681e-05 17.977846145629883 2.020263433456421\n",
            "20323 4.639865437638946e-06 6.555030995514244e-05 Traning Loss: 7.019017357379198e-05 17.978036880493164 2.020232677459717\n",
            "20324 4.6271102291939314e-06 6.552335253218189e-05 Traning Loss: 7.015046139713377e-05 17.97823143005371 2.020200729370117\n",
            "20325 4.889096089755185e-06 6.6051070461981e-05 Traning Loss: 7.094016473274678e-05 17.978412628173828 2.0201644897460938\n",
            "20326 4.615499619831098e-06 6.781474075978622e-05 Traning Loss: 7.243023719638586e-05 17.978601455688477 2.0201358795166016\n",
            "20327 5.336027697921963e-06 6.909959483891726e-05 Traning Loss: 7.443562208209187e-05 17.97877311706543 2.0200963020324707\n",
            "20328 4.862139576289337e-06 7.196960359578952e-05 Traning Loss: 7.683174044359475e-05 17.97895622253418 2.020069122314453\n",
            "20329 5.840655830979813e-06 7.391049439320341e-05 Traning Loss: 7.975115295266733e-05 17.9791202545166 2.0200283527374268\n",
            "20330 5.384797532315133e-06 7.815848948666826e-05 Traning Loss: 8.354328747373074e-05 17.97930145263672 2.0199997425079346\n",
            "20331 6.623370609304402e-06 8.232357504311949e-05 Traning Loss: 8.894694474292919e-05 17.979459762573242 2.0199592113494873\n",
            "20332 6.573608970938949e-06 9.032608068082482e-05 Traning Loss: 9.689969010651112e-05 17.979637145996094 2.019927740097046\n",
            "20333 8.404077561863232e-06 0.00010055983875645325 Traning Loss: 0.00010896391904680058 17.979785919189453 2.019888401031494\n",
            "20334 9.309576853411272e-06 0.00011764978989958763 Traning Loss: 0.0001269593631150201 17.97995948791504 2.0198545455932617\n",
            "20335 1.2566039913508575e-05 0.0001416813611285761 Traning Loss: 0.00015424740558955818 17.980093002319336 2.0198161602020264\n",
            "20336 1.5466841432498768e-05 0.00017922323604580015 Traning Loss: 0.0001946900738403201 17.980260848999023 2.0197806358337402\n",
            "20337 2.2080857888795435e-05 0.0002343181986361742 Traning Loss: 0.0002563990419730544 17.980369567871094 2.0197417736053467\n",
            "20338 2.9433334930217825e-05 0.00031844127806834877 Traning Loss: 0.0003478746220935136 17.980527877807617 2.0197060108184814\n",
            "20339 4.387723674881272e-05 0.0004452768189366907 Traning Loss: 0.0004891540738753974 17.980592727661133 2.0196642875671387\n",
            "20340 6.147111707832664e-05 0.0006358987302519381 Traning Loss: 0.00069736986188218 17.980728149414062 2.0196304321289062\n",
            "20341 9.364920697407797e-05 0.0009265555418096483 Traning Loss: 0.0010202047415077686 17.980710983276367 2.0195834636688232\n",
            "20342 0.00013369013322517276 0.0013478710316121578 Traning Loss: 0.0014815612230449915 17.980785369873047 2.0195538997650146\n",
            "20343 0.0002024307323154062 0.001978826941922307 Traning Loss: 0.002181257586926222 17.980609893798828 2.019498586654663\n",
            "20344 0.00028277214732952416 0.002811664016917348 Traning Loss: 0.0030944361351430416 17.980539321899414 2.0194761753082275\n",
            "20345 0.0004068920679856092 0.003953647334128618 Traning Loss: 0.004360539373010397 17.980073928833008 2.0194084644317627\n",
            "20346 0.0005186389898881316 0.005117391236126423 Traning Loss: 0.005636030342429876 17.979719161987305 2.0193963050842285\n",
            "20347 0.000649500812869519 0.006272382568567991 Traning Loss: 0.0069218832068145275 17.97884750366211 2.0193142890930176\n",
            "20348 0.0006628261762671173 0.006499393377453089 Traning Loss: 0.007162219379097223 17.978126525878906 2.019317865371704\n",
            "20349 0.0006066777277737856 0.0057637253776192665 Traning Loss: 0.0063704028725624084 17.97698974609375 2.0192346572875977\n",
            "20350 0.0003837190452031791 0.003693836973980069 Traning Loss: 0.004077556077390909 17.976125717163086 2.019263505935669\n",
            "20351 0.0001738612336339429 0.0014637310523539782 Traning Loss: 0.0016375923296436667 17.975210189819336 2.019221544265747\n",
            "20352 3.749883398995735e-05 0.0002036829391727224 Traning Loss: 0.00024118176952470094 17.97451400756836 2.0192806720733643\n",
            "20353 6.816345558036119e-05 0.00043453043326735497 Traning Loss: 0.0005026938742958009 17.973974227905273 2.019329786300659\n",
            "20354 0.00018657170585356653 0.0015419659903272986 Traning Loss: 0.0017285377252846956 17.97330665588379 2.019408702850342\n",
            "20355 0.00025910709518939257 0.002301863394677639 Traning Loss: 0.0025609703734517097 17.9727840423584 2.019557237625122\n",
            "20356 0.00025102950166910887 0.00205211085267365 Traning Loss: 0.002303140237927437 17.972023010253906 2.019651174545288\n",
            "20357 0.00013632187619805336 0.001058472553268075 Traning Loss: 0.0011947944294661283 17.971410751342773 2.019867181777954\n",
            "20358 6.994583964115009e-05 0.0002623128821142018 Traning Loss: 0.0003322587290313095 17.970767974853516 2.020003318786621\n",
            "20359 5.8717028878163546e-05 0.0002774568856693804 Traning Loss: 0.0003361739218235016 17.97015380859375 2.0202322006225586\n",
            "20360 0.00011902462574653327 0.000795329746324569 Traning Loss: 0.0009143543429672718 17.96963882446289 2.0204334259033203\n",
            "20361 0.00015440858260262758 0.0011709673563018441 Traning Loss: 0.0013253759825602174 17.9689998626709 2.0206239223480225\n",
            "20362 0.00013365535414777696 0.0009889573557302356 Traning Loss: 0.001122612738981843 17.968509674072266 2.020879030227661\n",
            "20363 9.004995081340894e-05 0.0005066990852355957 Traning Loss: 0.0005967490142211318 17.96795082092285 2.021045684814453\n",
            "20364 4.6292621846077964e-05 0.00020803358347620815 Traning Loss: 0.0002543262089602649 17.96749496459961 2.0213260650634766\n",
            "20365 5.9688391047529876e-05 0.0002686086518224329 Traning Loss: 0.0003282970283180475 17.967086791992188 2.0215280055999756\n",
            "20366 7.444618677254766e-05 0.0005290356348268688 Traning Loss: 0.0006034818361513317 17.966650009155273 2.021784543991089\n",
            "20367 8.793568122200668e-05 0.0006333960918709636 Traning Loss: 0.0007213318021968007 17.966325759887695 2.02203106880188\n",
            "20368 7.401931361528113e-05 0.0005063803400844336 Traning Loss: 0.0005803996464237571 17.9659423828125 2.0222256183624268\n",
            "20369 4.491124127525836e-05 0.0002867807634174824 Traning Loss: 0.0003316919901408255 17.96567726135254 2.0224862098693848\n",
            "20370 3.798770558205433e-05 0.00016729610797483474 Traning Loss: 0.00020528380991891026 17.965412139892578 2.0226402282714844\n",
            "20371 3.280162127339281e-05 0.00022619855008088052 Traning Loss: 0.0002590001677162945 17.965185165405273 2.0228707790374756\n",
            "20372 4.7507281124126166e-05 0.00032556240330450237 Traning Loss: 0.00037306969170458615 17.965015411376953 2.023016929626465\n",
            "20373 4.60190130979754e-05 0.0003659167850855738 Traning Loss: 0.0004119357909075916 17.964815139770508 2.0231688022613525\n",
            "20374 3.766143345274031e-05 0.0002961992286145687 Traning Loss: 0.000333860662067309 17.964698791503906 2.0233101844787598\n",
            "20375 2.9565275326604024e-05 0.0001919004280352965 Traning Loss: 0.00022146569972392172 17.96455192565918 2.023383140563965\n",
            "20376 1.7126969396485947e-05 0.00014671827375423163 Traning Loss: 0.00016384523769374937 17.964468002319336 2.02351713180542\n",
            "20377 2.468091042828746e-05 0.00016027908714022487 Traning Loss: 0.00018495999393053353 17.964391708374023 2.0235533714294434\n",
            "20378 2.2129715944174677e-05 0.00021537503926083446 Traning Loss: 0.00023750474792905152 17.964336395263672 2.023651361465454\n",
            "20379 2.7560381568036973e-05 0.00022870194516144693 Traning Loss: 0.0002562623121775687 17.964332580566406 2.0236763954162598\n",
            "20380 2.212447543570306e-05 0.00020139066327828914 Traning Loss: 0.000223515133257024 17.964326858520508 2.023707628250122\n",
            "20381 1.576549584569875e-05 0.0001486027758801356 Traning Loss: 0.00016436827718280256 17.96438980102539 2.0237343311309814\n",
            "20382 1.4710580217069946e-05 0.00011042453843401745 Traning Loss: 0.0001251351204700768 17.96445083618164 2.023710012435913\n",
            "20383 1.0579162335488945e-05 0.0001157643273472786 Traning Loss: 0.00012634348240680993 17.96456527709961 2.023738384246826\n",
            "20384 1.7620204744162038e-05 0.0001360158494208008 Traning Loss: 0.00015363605052698404 17.964693069458008 2.023684501647949\n",
            "20385 1.4994169760029763e-05 0.00016108735871966928 Traning Loss: 0.00017608152120374143 17.96484375 2.0236876010894775\n",
            "20386 1.789144334907178e-05 0.00015213387086987495 Traning Loss: 0.00017002531967591494 17.96501922607422 2.0236287117004395\n",
            "20387 1.2834613698942121e-05 0.00012705865083262324 Traning Loss: 0.00013989326544106007 17.965187072753906 2.023595094680786\n",
            "20388 1.0137096069229301e-05 9.548712841933593e-05 Traning Loss: 0.00010562422539805993 17.965381622314453 2.023556709289551\n",
            "20389 9.759835847944487e-06 7.921969518065453e-05 Traning Loss: 8.897953375708312e-05 17.96556282043457 2.02349591255188\n",
            "20390 8.095699740806594e-06 8.767758117755875e-05 Traning Loss: 9.577328455634415e-05 17.965770721435547 2.023474931716919\n",
            "20391 1.2898821296403185e-05 0.00010210716573055834 Traning Loss: 0.00011500598338898271 17.965978622436523 2.0233960151672363\n",
            "20392 1.0869402103708126e-05 0.00011833875760203227 Traning Loss: 0.000129208157886751 17.966205596923828 2.0233709812164307\n",
            "20393 1.3298687917995267e-05 0.00011302757047815248 Traning Loss: 0.00012632625293917954 17.96644401550293 2.02329421043396\n",
            "20394 9.423637493455317e-06 9.928446525009349e-05 Traning Loss: 0.00010870810365304351 17.966690063476562 2.0232551097869873\n",
            "20395 8.526339115633164e-06 7.869000546634197e-05 Traning Loss: 8.721634367248043e-05 17.966951370239258 2.0231990814208984\n",
            "20396 7.161344456108054e-06 6.732919428031892e-05 Traning Loss: 7.449054101016372e-05 17.967214584350586 2.0231423377990723\n",
            "20397 6.488809049187694e-06 6.908211798872799e-05 Traning Loss: 7.557092612842098e-05 17.967493057250977 2.0231049060821533\n",
            "20398 8.952520147431642e-06 7.733349775662646e-05 Traning Loss: 8.62860179040581e-05 17.967769622802734 2.0230350494384766\n",
            "20399 7.947841368149966e-06 8.950578921940178e-05 Traning Loss: 9.745363058755174e-05 17.96805763244629 2.023007869720459\n",
            "20400 1.0370231393608265e-05 9.069756924873218e-05 Traning Loss: 0.00010106780246132985 17.968339920043945 2.0229408740997314\n",
            "20401 7.832450137357228e-06 8.759466436458752e-05 Traning Loss: 9.542711632093415e-05 17.968626022338867 2.0229127407073975\n",
            "20402 8.332007382705342e-06 7.590806490043178e-05 Traning Loss: 8.424006955465302e-05 17.968910217285156 2.022857666015625\n",
            "20403 6.35949254501611e-06 6.777334783691913e-05 Traning Loss: 7.413284038193524e-05 17.969196319580078 2.022817373275757\n",
            "20404 6.256997494347161e-06 6.349702016450465e-05 Traning Loss: 6.975401629460976e-05 17.969484329223633 2.022775650024414\n",
            "20405 6.606413080589846e-06 6.519957969430834e-05 Traning Loss: 7.180598913691938e-05 17.969770431518555 2.022724151611328\n",
            "20406 6.327108167170081e-06 7.105293479980901e-05 Traning Loss: 7.738004205748439e-05 17.97005844116211 2.0226943492889404\n",
            "20407 7.840731996111572e-06 7.445765368174762e-05 Traning Loss: 8.229838567785919e-05 17.97034454345703 2.0226385593414307\n",
            "20408 6.651148851233302e-06 7.708942575845867e-05 Traning Loss: 8.374057506443933e-05 17.97063446044922 2.022611618041992\n",
            "20409 7.699952220718842e-06 7.352368993451819e-05 Traning Loss: 8.122363942675292e-05 17.970918655395508 2.0225579738616943\n",
            "20410 6.054738605598686e-06 7.05114725860767e-05 Traning Loss: 7.656621164642274e-05 17.97120475769043 2.0225274562835693\n",
            "20411 6.515522272820817e-06 6.565646617673337e-05 Traning Loss: 7.217199163278565e-05 17.97148323059082 2.0224826335906982\n",
            "20412 5.659715952788247e-06 6.416832184186205e-05 Traning Loss: 6.982803461141884e-05 17.97176170349121 2.022446632385254\n",
            "20413 5.8756836551765446e-06 6.40107537037693e-05 Traning Loss: 6.98864387231879e-05 17.972034454345703 2.0224106311798096\n",
            "20414 6.0550978560058866e-06 6.542988558067009e-05 Traning Loss: 7.148498116293922e-05 17.972307205200195 2.022366762161255\n",
            "20415 5.839286131958943e-06 6.741750257788226e-05 Traning Loss: 7.32567859813571e-05 17.972576141357422 2.022333860397339\n",
            "20416 6.413479695766e-06 6.770384061383083e-05 Traning Loss: 7.411732076434419e-05 17.972841262817383 2.022284746170044\n",
            "20417 5.685900305252289e-06 6.805112934671342e-05 Traning Loss: 7.373702828772366e-05 17.973102569580078 2.022252082824707\n",
            "20418 6.2067633734841365e-06 6.622660293942317e-05 Traning Loss: 7.243336585815996e-05 17.973356246948242 2.022203207015991\n",
            "20419 5.3548924370261375e-06 6.55532130622305e-05 Traning Loss: 7.090810686349869e-05 17.973608016967773 2.022167682647705\n",
            "20420 5.758975021308288e-06 6.401135033229366e-05 Traning Loss: 6.977032171562314e-05 17.97385025024414 2.022120475769043\n",
            "20421 5.1768961384368595e-06 6.412237416952848e-05 Traning Loss: 6.929926894372329e-05 17.97409439086914 2.0220797061920166\n",
            "20422 5.428850272437558e-06 6.39739228063263e-05 Traning Loss: 6.940276944078505e-05 17.97433090209961 2.0220346450805664\n",
            "20423 5.210802555666305e-06 6.453998503275216e-05 Traning Loss: 6.975078576942906e-05 17.97456932067871 2.021989107131958\n",
            "20424 5.231810519035207e-06 6.476691487478092e-05 Traning Loss: 6.999872857704759e-05 17.974802017211914 2.0219457149505615\n",
            "20425 5.248597972240532e-06 6.468329229392111e-05 Traning Loss: 6.99318916304037e-05 17.97503662109375 2.0218961238861084\n",
            "20426 5.028417035646271e-06 6.452184607042e-05 Traning Loss: 6.955026037758216e-05 17.97526741027832 2.0218517780303955\n",
            "20427 5.156799034011783e-06 6.383493018802255e-05 Traning Loss: 6.899172876728699e-05 17.975496292114258 2.021799325942993\n",
            "20428 4.815122338186484e-06 6.364418368320912e-05 Traning Loss: 6.84593032929115e-05 17.975723266601562 2.0217533111572266\n",
            "20429 5.022628556616837e-06 6.30918875685893e-05 Traning Loss: 6.811451748944819e-05 17.9759464263916 2.021700143814087\n",
            "20430 4.707832886197139e-06 6.330887845251709e-05 Traning Loss: 6.801671406719834e-05 17.97616958618164 2.0216517448425293\n",
            "20431 4.946849003317766e-06 6.316665530903265e-05 Traning Loss: 6.811350613133982e-05 17.97638511657715 2.0215981006622314\n",
            "20432 4.706105301011121e-06 6.35919495834969e-05 Traning Loss: 6.829805352026597e-05 17.976600646972656 2.0215468406677246\n",
            "20433 4.887877366854809e-06 6.355329242069274e-05 Traning Loss: 6.844117160653695e-05 17.976808547973633 2.0214927196502686\n",
            "20434 4.6987729547254276e-06 6.37584671494551e-05 Traning Loss: 6.845723692094907e-05 17.97701644897461 2.021439790725708\n",
            "20435 4.781043571711052e-06 6.353240314638242e-05 Traning Loss: 6.831344944657758e-05 17.977216720581055 2.021385908126831\n",
            "20436 4.629242539522238e-06 6.340883555822074e-05 Traning Loss: 6.803807627875358e-05 17.977418899536133 2.021331787109375\n",
            "20437 4.630017429008149e-06 6.306567956926301e-05 Traning Loss: 6.769569881726056e-05 17.977615356445312 2.0212771892547607\n",
            "20438 4.522686140262522e-06 6.28364214207977e-05 Traning Loss: 6.735910574207082e-05 17.977811813354492 2.021221876144409\n",
            "20439 4.489031653065467e-06 6.260053487494588e-05 Traning Loss: 6.70895678922534e-05 17.978004455566406 2.0211668014526367\n",
            "20440 4.438024461705936e-06 6.247904821066186e-05 Traning Loss: 6.691706948913634e-05 17.978195190429688 2.021111488342285\n",
            "20441 4.402560989547055e-06 6.244114774744958e-05 Traning Loss: 6.684370600851253e-05 17.978382110595703 2.0210564136505127\n",
            "20442 4.399000317789614e-06 6.244827818591148e-05 Traning Loss: 6.684727850370109e-05 17.978565216064453 2.0210013389587402\n",
            "20443 4.355478267825674e-06 6.253892934182659e-05 Traning Loss: 6.689440488116816e-05 17.978748321533203 2.020946502685547\n",
            "20444 4.374160198494792e-06 6.257015775190666e-05 Traning Loss: 6.694431795040146e-05 17.978925704956055 2.0208916664123535\n",
            "20445 4.308083134674234e-06 6.26661567366682e-05 Traning Loss: 6.697423668811098e-05 17.979103088378906 2.0208370685577393\n",
            "20446 4.3405293581599835e-06 6.262639362830669e-05 Traning Loss: 6.696692435070872e-05 17.97927474975586 2.020782232284546\n",
            "20447 4.250363872415619e-06 6.267045682761818e-05 Traning Loss: 6.692082388326526e-05 17.979446411132812 2.020728349685669\n",
            "20448 4.299402462493163e-06 6.254080653889105e-05 Traning Loss: 6.68402062729001e-05 17.979612350463867 2.0206735134124756\n",
            "20449 4.183912551525282e-06 6.255593325477093e-05 Traning Loss: 6.673984898952767e-05 17.979778289794922 2.020620346069336\n",
            "20450 4.250200618116651e-06 6.238576315809041e-05 Traning Loss: 6.663596286671236e-05 17.979938507080078 2.0205655097961426\n",
            "20451 4.1105322452494875e-06 6.242529343580827e-05 Traning Loss: 6.653582386206836e-05 17.980098724365234 2.0205132961273193\n",
            "20452 4.198107944830554e-06 6.225272227311507e-05 Traning Loss: 6.645083340117708e-05 17.980253219604492 2.020458936691284\n",
            "20453 4.0403911043540575e-06 6.234196916921064e-05 Traning Loss: 6.638235936407e-05 17.98040771484375 2.0204081535339355\n",
            "20454 4.152651399635943e-06 6.218039197847247e-05 Traning Loss: 6.633304292336106e-05 17.980558395385742 2.0203542709350586\n",
            "20455 3.98010570279439e-06 6.231872976059094e-05 Traning Loss: 6.629883864661679e-05 17.9807071685791 2.0203044414520264\n",
            "20456 4.115828687645262e-06 6.216053589014336e-05 Traning Loss: 6.627636321354657e-05 17.980852127075195 2.0202507972717285\n",
            "20457 3.928985734091839e-06 6.233308522496372e-05 Traning Loss: 6.626207323279232e-05 17.98099708557129 2.020202398300171\n",
            "20458 4.086514309165068e-06 6.216689507709816e-05 Traning Loss: 6.625340756727383e-05 17.981138229370117 2.0201494693756104\n",
            "20459 3.881433713104343e-06 6.23682644800283e-05 Traning Loss: 6.624969682889059e-05 17.981279373168945 2.0201022624969482\n",
            "20460 4.0600848478788976e-06 6.21904109721072e-05 Traning Loss: 6.625049718422815e-05 17.981414794921875 2.020049810409546\n",
            "20461 3.831386038655182e-06 6.24266394879669e-05 Traning Loss: 6.625802780035883e-05 17.981550216674805 2.0200037956237793\n",
            "20462 4.037367943965364e-06 6.223681702977046e-05 Traning Loss: 6.627418770221993e-05 17.98168182373047 2.019951820373535\n",
            "20463 3.780603492486989e-06 6.252843741094694e-05 Traning Loss: 6.630903953919187e-05 17.981813430786133 2.019907236099243\n",
            "20464 4.027625436719973e-06 6.233913154574111e-05 Traning Loss: 6.636675971094519e-05 17.9819393157959 2.019855499267578\n",
            "20465 3.734763595275581e-06 6.272250175243244e-05 Traning Loss: 6.645726534770802e-05 17.982067108154297 2.0198123455047607\n",
            "20466 4.040359272039495e-06 6.255846528802067e-05 Traning Loss: 6.659882637904957e-05 17.982189178466797 2.0197606086730957\n",
            "20467 3.6978726711822674e-06 6.311118340818211e-05 Traning Loss: 6.680905789835379e-05 17.98231315612793 2.019719362258911\n",
            "20468 4.09218409913592e-06 6.302208930719644e-05 Traning Loss: 6.711427704431117e-05 17.98242950439453 2.019667387008667\n",
            "20469 3.684647026602761e-06 6.387607572833076e-05 Traning Loss: 6.756072252755985e-05 17.9825496673584 2.0196282863616943\n",
            "20470 4.22069297201233e-06 6.398114055627957e-05 Traning Loss: 6.8201836256776e-05 17.9826602935791 2.0195751190185547\n",
            "20471 3.7320226056181127e-06 6.539778405567631e-05 Traning Loss: 6.912980461493134e-05 17.982778549194336 2.0195388793945312\n",
            "20472 4.49784101874684e-06 6.597004539798945e-05 Traning Loss: 7.046788959996775e-05 17.982885360717773 2.0194835662841797\n",
            "20473 3.917713002010714e-06 6.849212513770908e-05 Traning Loss: 7.240983541123569e-05 17.983001708984375 2.019451379776001\n",
            "20474 5.069795861345483e-06 7.016780728008598e-05 Traning Loss: 7.523759995820001e-05 17.98310089111328 2.019392728805542\n",
            "20475 4.421414814714808e-06 7.49733517295681e-05 Traning Loss: 7.939476927276701e-05 17.98321533203125 2.0193662643432617\n",
            "20476 6.258953362703323e-06 7.927099068183452e-05 Traning Loss: 8.552994404453784e-05 17.983306884765625 2.0193021297454834\n",
            "20477 5.672659426636528e-06 8.900085231289268e-05 Traning Loss: 9.46735090110451e-05 17.983421325683594 2.0192840099334717\n",
            "20478 8.803415767033584e-06 9.956890426110476e-05 Traning Loss: 0.00010837231820914894 17.983501434326172 2.0192110538482666\n",
            "20479 8.70739677338861e-06 0.00012038064596708864 Traning Loss: 0.00012908804637845606 17.98361587524414 2.0192055702209473\n",
            "20480 1.4435642697208095e-05 0.00014609156642109156 Traning Loss: 0.00016052721184678376 17.983678817749023 2.0191190242767334\n",
            "20481 1.6048325051087886e-05 0.00019260951376054436 Traning Loss: 0.00020865784608758986 17.98379135131836 2.0191333293914795\n",
            "20482 2.7276259061181918e-05 0.00025507030659355223 Traning Loss: 0.00028234656201675534 17.98382568359375 2.0190255641937256\n",
            "20483 3.380426642252132e-05 0.0003620212955866009 Traning Loss: 0.00039582556928507984 17.983932495117188 2.019070625305176\n",
            "20484 5.7014553021872416e-05 0.0005123470327816904 Traning Loss: 0.0005693616112694144 17.983915328979492 2.0189309120178223\n",
            "20485 7.613975321874022e-05 0.00075817626202479 Traning Loss: 0.0008343160152435303 17.983999252319336 2.01902437210083\n",
            "20486 0.00012467637134250253 0.0011049555614590645 Traning Loss: 0.0012296319473534822 17.9838809967041 2.0188403129577637\n",
            "20487 0.00017108148313127458 0.001636026194319129 Traning Loss: 0.001807107706554234 17.983896255493164 2.019005298614502\n",
            "20488 0.00026436513871885836 0.0023357532918453217 Traning Loss: 0.0026001185178756714 17.98358726501465 2.018770456314087\n",
            "20489 0.0003478859434835613 0.0032600245904177427 Traning Loss: 0.003607910592108965 17.98343276977539 2.019026279449463\n",
            "20490 0.00047725296462886035 0.004207502119243145 Traning Loss: 0.004684755112975836 17.982812881469727 2.0187649726867676\n",
            "20491 0.0005352217121981084 0.00498259486630559 Traning Loss: 0.005517816636711359 17.982358932495117 2.0190927982330322\n",
            "20492 0.0005770418792963028 0.0050502680242061615 Traning Loss: 0.005627309903502464 17.981414794921875 2.0188934803009033\n",
            "20493 0.0004490451538003981 0.004222450777888298 Traning Loss: 0.004671495873481035 17.980688095092773 2.0191993713378906\n",
            "20494 0.00030812350451014936 0.0026417020708322525 Traning Loss: 0.0029498254880309105 17.979698181152344 2.0191850662231445\n",
            "20495 0.00012076489656465128 0.0011659839656203985 Traning Loss: 0.001286748913116753 17.97894859313965 2.019346237182617\n",
            "20496 6.117791053839028e-05 0.0005414407351054251 Traning Loss: 0.0006026186747476459 17.97823143005371 2.0195627212524414\n",
            "20497 0.00011567105684662238 0.0008914482896216214 Traning Loss: 0.0010071193100884557 17.97754669189453 2.0195658206939697\n",
            "20498 0.0001758345024427399 0.0016743545420467854 Traning Loss: 0.0018501890590414405 17.97698974609375 2.0199294090270996\n",
            "20499 0.0002421490498818457 0.0018961693858727813 Traning Loss: 0.002138318493962288 17.97625732421875 2.0199365615844727\n",
            "20500 0.00015686414553783834 0.001438968232832849 Traning Loss: 0.0015958324074745178 17.975631713867188 2.0202853679656982\n",
            "20501 8.833452011458576e-05 0.000558456580620259 Traning Loss: 0.0006467910716310143 17.974916458129883 2.020467758178711\n",
            "20502 2.8791486329282634e-05 0.000131562483147718 Traning Loss: 0.00016035397129599005 17.97426986694336 2.020674228668213\n",
            "20503 5.0413924327585846e-05 0.0003949707024730742 Traning Loss: 0.00044538461952470243 17.97368049621582 2.021010637283325\n",
            "20504 0.0001289519714191556 0.0009076662827283144 Traning Loss: 0.00103661825414747 17.97304916381836 2.0210978984832764\n",
            "20505 0.0001263306912733242 0.0011595409596338868 Traning Loss: 0.0012858716072514653 17.972515106201172 2.021461009979248\n",
            "20506 0.00011296905722701922 0.000801611808128655 Traning Loss: 0.000914580887183547 17.97193145751953 2.021592140197754\n",
            "20507 4.2897248931694776e-05 0.00031245622085407376 Traning Loss: 0.0003553534625098109 17.97140884399414 2.0218498706817627\n",
            "20508 2.2805154003435746e-05 8.784179226495326e-05 Traning Loss: 0.0001106469426304102 17.97096061706543 2.0221009254455566\n",
            "20509 4.5727138058282435e-05 0.00025803656899370253 Traning Loss: 0.0003037637216039002 17.970508575439453 2.0222232341766357\n",
            "20510 6.470178050221875e-05 0.0005414987681433558 Traning Loss: 0.000606200541369617 17.97018051147461 2.022507429122925\n",
            "20511 8.034224447328597e-05 0.0005722552305087447 Traning Loss: 0.0006525974604301155 17.969833374023438 2.022582530975342\n",
            "20512 4.4291489757597446e-05 0.0003830368223134428 Traning Loss: 0.00042732831207104027 17.96959114074707 2.0227930545806885\n",
            "20513 2.6270899979863316e-05 0.00015326707216445357 Traning Loss: 0.0001795379794202745 17.969390869140625 2.022913694381714\n",
            "20514 2.103490987792611e-05 0.00012586860975716263 Traning Loss: 0.00014690351963508874 17.969192504882812 2.0229930877685547\n",
            "20515 3.180419298587367e-05 0.00026980406255461276 Traning Loss: 0.00030160826281644404 17.969083786010742 2.023146152496338\n",
            "20516 5.3037441830383614e-05 0.00038377652526833117 Traning Loss: 0.0004368139780126512 17.96889877319336 2.0231504440307617\n",
            "20517 3.9172853576019406e-05 0.00036223375354893506 Traning Loss: 0.00040140660712495446 17.968795776367188 2.023268222808838\n",
            "20518 2.9542123229475692e-05 0.00020602907170541584 Traning Loss: 0.00023557119129691273 17.968660354614258 2.023282051086426\n",
            "20519 1.0217235285381321e-05 8.626910130260512e-05 Traning Loss: 9.648633567849174e-05 17.96858787536621 2.023308515548706\n",
            "20520 8.89289458427811e-06 7.965935947140679e-05 Traning Loss: 8.85522531461902e-05 17.968564987182617 2.023343086242676\n",
            "20521 2.175876397814136e-05 0.00015579583123326302 Traning Loss: 0.00017755459703039378 17.96854019165039 2.023277521133423\n",
            "20522 2.1867565010325052e-05 0.00023047190916258842 Traning Loss: 0.00025233946507796645 17.968589782714844 2.0233044624328613\n",
            "20523 2.7852289349539205e-05 0.00021109735826030374 Traning Loss: 0.00023894965124782175 17.968599319458008 2.0232276916503906\n",
            "20524 1.3464235962601379e-05 0.00014733188436366618 Traning Loss: 0.00016079611668828875 17.968658447265625 2.0232138633728027\n",
            "20525 1.006058846542146e-05 8.518387767253444e-05 Traning Loss: 9.52444679569453e-05 17.968713760375977 2.0231728553771973\n",
            "20526 9.49202603806043e-06 8.512436761520803e-05 Traning Loss: 9.461639274377376e-05 17.968799591064453 2.0230977535247803\n",
            "20527 1.1278905731160194e-05 0.00013279521954245865 Traning Loss: 0.00014407411799766123 17.968917846679688 2.0230743885040283\n",
            "20528 2.040615800069645e-05 0.00016682659042999148 Traning Loss: 0.00018723274115473032 17.96903419494629 2.0229616165161133\n",
            "20529 1.4711753465235233e-05 0.00017105729784816504 Traning Loss: 0.00018576905131340027 17.96918296813965 2.0229310989379883\n",
            "20530 1.5187873941613361e-05 0.00012848472397308797 Traning Loss: 0.00014367260155268013 17.969324111938477 2.0228354930877686\n",
            "20531 8.350958523806185e-06 9.236330515705049e-05 Traning Loss: 0.00010071426368085667 17.969467163085938 2.0227715969085693\n",
            "20532 7.685994205530733e-06 8.012759644770995e-05 Traning Loss: 8.781359065324068e-05 17.969633102416992 2.0227062702178955\n",
            "20533 1.0827701771631837e-05 9.470437362324446e-05 Traning Loss: 0.0001055320753948763 17.96977996826172 2.0226094722747803\n",
            "20534 1.0188118721998762e-05 0.00011879601515829563 Traning Loss: 0.0001289841311518103 17.969968795776367 2.0225601196289062\n",
            "20535 1.3988848877488635e-05 0.00011981413990724832 Traning Loss: 0.00013380298332776874 17.97013282775879 2.0224595069885254\n",
            "20536 8.829614671412855e-06 0.00010673881479306147 Traning Loss: 0.00011556842946447432 17.9703426361084 2.0224053859710693\n",
            "20537 8.604535651102196e-06 8.128124318318442e-05 Traning Loss: 8.98857761058025e-05 17.970542907714844 2.0223259925842285\n",
            "20538 6.180118816700997e-06 6.941070023458451e-05 Traning Loss: 7.559081859653816e-05 17.97076416015625 2.0222482681274414\n",
            "20539 6.2453550526697654e-06 7.279682176886126e-05 Traning Loss: 7.904217636678368e-05 17.97099494934082 2.022193670272827\n",
            "20540 8.84911605680827e-06 8.268404781119898e-05 Traning Loss: 9.153316204901785e-05 17.971223831176758 2.0221056938171387\n",
            "20541 7.5520456448430195e-06 9.192174911731854e-05 Traning Loss: 9.947379294317216e-05 17.971467971801758 2.0220654010772705\n",
            "20542 9.32920283958083e-06 8.615270780865103e-05 Traning Loss: 9.548191155772656e-05 17.97170639038086 2.0219852924346924\n",
            "20543 6.226327968761325e-06 7.727910997346044e-05 Traning Loss: 8.350543794222176e-05 17.971961975097656 2.0219368934631348\n",
            "20544 6.331622898869682e-06 6.636891339439899e-05 Traning Loss: 7.270053902175277e-05 17.97222328186035 2.02187180519104\n",
            "20545 5.831650923937559e-06 6.404425948858261e-05 Traning Loss: 6.987591041252017e-05 17.97248077392578 2.021805763244629\n",
            "20546 5.661569957737811e-06 6.896594277350232e-05 Traning Loss: 7.462751091225073e-05 17.972749710083008 2.021759033203125\n",
            "20547 7.64886226534145e-06 7.345185440499336e-05 Traning Loss: 8.110071939881891e-05 17.97299575805664 2.021690607070923\n",
            "20548 6.183884124766337e-06 7.72618004702963e-05 Traning Loss: 8.344568777829409e-05 17.97325325012207 2.021652936935425\n",
            "20549 7.436443866026821e-06 7.271350477822125e-05 Traning Loss: 8.014994818950072e-05 17.973491668701172 2.021592140197754\n",
            "20550 5.459274234453915e-06 6.87622232362628e-05 Traning Loss: 7.422149792546406e-05 17.973745346069336 2.02154803276062\n",
            "20551 5.653294465446379e-06 6.441574078053236e-05 Traning Loss: 7.006903615547344e-05 17.97399139404297 2.0214996337890625\n",
            "20552 5.507019977812888e-06 6.456968549173325e-05 Traning Loss: 7.007670501479879e-05 17.9742431640625 2.0214459896087646\n",
            "20553 5.301913006405812e-06 6.808095349697396e-05 Traning Loss: 7.338286377489567e-05 17.974496841430664 2.021411180496216\n",
            "20554 6.53769529890269e-06 7.062037184368819e-05 Traning Loss: 7.715806714259088e-05 17.97474479675293 2.0213546752929688\n",
            "20555 5.666871402354445e-06 7.34306886442937e-05 Traning Loss: 7.909756095614284e-05 17.974992752075195 2.0213234424591064\n",
            "20556 6.584373750229133e-06 7.219581311801448e-05 Traning Loss: 7.878018368501216e-05 17.975236892700195 2.021270751953125\n",
            "20557 5.664584477926837e-06 7.222114800242707e-05 Traning Loss: 7.788573566358536e-05 17.975475311279297 2.0212361812591553\n",
            "20558 6.0605611906794365e-06 7.250485941767693e-05 Traning Loss: 7.856541924411431e-05 17.97571563720703 2.0211923122406006\n",
            "20559 6.354871402436402e-06 7.595239003421739e-05 Traning Loss: 8.230726234614849e-05 17.975940704345703 2.0211520195007324\n",
            "20560 6.637186288571684e-06 8.262328628916293e-05 Traning Loss: 8.926047303248197e-05 17.976179122924805 2.0211143493652344\n",
            "20561 8.16133433545474e-06 9.09472510102205e-05 Traning Loss: 9.910858352668583e-05 17.976390838623047 2.0210695266723633\n",
            "20562 8.540268936485518e-06 0.0001032203872455284 Traning Loss: 0.00011176065891049802 17.976621627807617 2.0210304260253906\n",
            "20563 1.0868995559576433e-05 0.00011806100519606844 Traning Loss: 0.00012892999802716076 17.976816177368164 2.0209877490997314\n",
            "20564 1.2347291885816958e-05 0.00014109746553003788 Traning Loss: 0.00015344475104939193 17.977035522460938 2.0209434032440186\n",
            "20565 1.6245465303654782e-05 0.00017483420378994197 Traning Loss: 0.00019107967091258615 17.977210998535156 2.0209076404571533\n",
            "20566 2.105161729559768e-05 0.00022658295347355306 Traning Loss: 0.00024763456895016134 17.977418899536133 2.020853042602539\n",
            "20567 2.912074705818668e-05 0.0003060994204133749 Traning Loss: 0.00033522016019560397 17.977569580078125 2.0208234786987305\n",
            "20568 4.090283619007096e-05 0.0004228871257510036 Traning Loss: 0.0004637899692170322 17.977760314941406 2.0207574367523193\n",
            "20569 5.898828021599911e-05 0.0006029158248566091 Traning Loss: 0.000661904108710587 17.977859497070312 2.020733118057251\n",
            "20570 8.493186760460958e-05 0.000860447296872735 Traning Loss: 0.0009453791426494718 17.978010177612305 2.020658016204834\n",
            "20571 0.00012514898844528943 0.0012543064076453447 Traning Loss: 0.0013794554397463799 17.978004455566406 2.0206377506256104\n",
            "20572 0.00017815669707488269 0.0017867188435047865 Traning Loss: 0.0019648754969239235 17.978059768676758 2.020552396774292\n",
            "20573 0.00025816523702815175 0.0025596830528229475 Traning Loss: 0.0028178482316434383 17.97785758972168 2.0205323696136475\n",
            "20574 0.00034569427953101695 0.003448567818850279 Traning Loss: 0.003794262185692787 17.97771453857422 2.0204355716705322\n",
            "20575 0.0004603508859872818 0.004531558603048325 Traning Loss: 0.004991909489035606 17.977190017700195 2.020408868789673\n",
            "20576 0.0005282124038785696 0.005251171533018351 Traning Loss: 0.0057793837040662766 17.97673225402832 2.020307779312134\n",
            "20577 0.0005698227905668318 0.0055547975935041904 Traning Loss: 0.006124620325863361 17.975868225097656 2.020261764526367\n",
            "20578 0.0004722276935353875 0.004662854131311178 Traning Loss: 0.005135081708431244 17.975154876708984 2.0201821327209473\n",
            "20579 0.0003233302850276232 0.003049128921702504 Traning Loss: 0.0033724592067301273 17.97425079345703 2.0201098918914795\n",
            "20580 0.00013105483958497643 0.0012180641060695052 Traning Loss: 0.0013491190038621426 17.97359275817871 2.020085096359253\n",
            "20581 3.771182309719734e-05 0.00019588998111430556 Traning Loss: 0.0002336018078494817 17.973012924194336 2.0200183391571045\n",
            "20582 5.066016819910146e-05 0.00035716898855753243 Traning Loss: 0.0004078291531186551 17.972509384155273 2.0200417041778564\n",
            "20583 0.0001402039488311857 0.0012014086823910475 Traning Loss: 0.0013416126603260636 17.972137451171875 2.0200328826904297\n",
            "20584 0.00021360849495977163 0.0018794862553477287 Traning Loss: 0.002093094866722822 17.971582412719727 2.0200626850128174\n",
            "20585 0.00019796760170720518 0.0017592433141544461 Traning Loss: 0.0019572109449654818 17.971153259277344 2.0201363563537598\n",
            "20586 0.00013759500870946795 0.0010451760608702898 Traning Loss: 0.0011827710550278425 17.970579147338867 2.0201637744903564\n",
            "20587 5.737848550779745e-05 0.00035835447488352656 Traning Loss: 0.00041573296766728163 17.970123291015625 2.020299196243286\n",
            "20588 4.869624171988107e-05 0.00018031735089607537 Traning Loss: 0.00022901358897797763 17.969696044921875 2.020362377166748\n",
            "20589 7.247630128404126e-05 0.0005012899055145681 Traning Loss: 0.0005737661849707365 17.969253540039062 2.020490884780884\n",
            "20590 0.00011281525803497061 0.0008569939527660608 Traning Loss: 0.0009698091889731586 17.96893310546875 2.0206139087677\n",
            "20591 0.00012005640019197017 0.000918170902878046 Traning Loss: 0.0010382273467257619 17.9685001373291 2.0206947326660156\n",
            "20592 8.641753083793446e-05 0.000648100976832211 Traning Loss: 0.0007345185149461031 17.968204498291016 2.020867347717285\n",
            "20593 5.8641526266001165e-05 0.0003052487154491246 Traning Loss: 0.00036389025626704097 17.96785545349121 2.020951986312866\n",
            "20594 3.1781055440660566e-05 0.00016478842007927597 Traning Loss: 0.00019656948279589415 17.967559814453125 2.0211362838745117\n",
            "20595 4.42870550614316e-05 0.000241485278820619 Traning Loss: 0.000285772344795987 17.967315673828125 2.0212795734405518\n",
            "20596 5.816499833599664e-05 0.0004204459546599537 Traning Loss: 0.00047861094935797155 17.96702003479004 2.021420955657959\n",
            "20597 6.467565253842622e-05 0.0005104399169795215 Traning Loss: 0.0005751155549660325 17.966838836669922 2.021604537963867\n",
            "20598 6.300318636931479e-05 0.00044229987543076277 Traning Loss: 0.0005053030326962471 17.966585159301758 2.021695613861084\n",
            "20599 3.743314300663769e-05 0.0002894165809266269 Traning Loss: 0.0003268497239332646 17.96643829345703 2.0218729972839355\n",
            "20600 2.8486827432061546e-05 0.00014604635362047702 Traning Loss: 0.00017453318287152797 17.96628189086914 2.0219597816467285\n",
            "20601 1.8075701518682763e-05 0.00011793476005550474 Traning Loss: 0.0001360104652121663 17.96615219116211 2.022080898284912\n",
            "20602 2.479106478858739e-05 0.00017965235747396946 Traning Loss: 0.00020444342226255685 17.96608543395996 2.022181749343872\n",
            "20603 3.5484048567013815e-05 0.00026725270436145365 Traning Loss: 0.0003027367638424039 17.96596908569336 2.0222277641296387\n",
            "20604 3.343522985232994e-05 0.0003111626720055938 Traning Loss: 0.00034459790913388133 17.9659423828125 2.0223278999328613\n",
            "20605 3.5705670597963035e-05 0.0002654013514984399 Traning Loss: 0.0003011070075444877 17.96586036682129 2.0223355293273926\n",
            "20606 1.8149834431824274e-05 0.00018396157247480005 Traning Loss: 0.00020211140508763492 17.96585464477539 2.022408962249756\n",
            "20607 1.4498285054287408e-05 0.00010009031393565238 Traning Loss: 0.00011458859808044508 17.965850830078125 2.0224175453186035\n",
            "20608 8.595615327067208e-06 7.929416460683569e-05 Traning Loss: 8.78897772054188e-05 17.96587562561035 2.022432327270508\n",
            "20609 1.1460527275630739e-05 0.00011271109542576596 Traning Loss: 0.0001241716236108914 17.965961456298828 2.0224478244781494\n",
            "20610 2.009941999858711e-05 0.0001643522991798818 Traning Loss: 0.00018445172463543713 17.966022491455078 2.022409200668335\n",
            "20611 1.8861612261389382e-05 0.00020093985949642956 Traning Loss: 0.00021980146993882954 17.966161727905273 2.02242374420166\n",
            "20612 2.272016899951268e-05 0.00018394237849861383 Traning Loss: 0.0002066625456791371 17.966264724731445 2.0223662853240967\n",
            "20613 1.2867345503764227e-05 0.00014240863674785942 Traning Loss: 0.00015527597861364484 17.966434478759766 2.0223567485809326\n",
            "20614 1.0956322512356564e-05 8.929300383897498e-05 Traning Loss: 0.00010024932271335274 17.966598510742188 2.022303581237793\n",
            "20615 6.47321212454699e-06 6.571794074261561e-05 Traning Loss: 7.219114922918379e-05 17.966785430908203 2.0222551822662354\n",
            "20616 6.9921225076541305e-06 7.313923561014235e-05 Traning Loss: 8.013135811779648e-05 17.966997146606445 2.022218942642212\n",
            "20617 1.1917441952391528e-05 9.782217239262536e-05 Traning Loss: 0.00010973961616400629 17.967182159423828 2.0221498012542725\n",
            "20618 1.1405802979425061e-05 0.00012508934014476836 Traning Loss: 0.00013649514585267752 17.9674072265625 2.022128105163574\n",
            "20619 1.5640323908883147e-05 0.0001273463130928576 Traning Loss: 0.00014298663882073015 17.96759605407715 2.022059917449951\n",
            "20620 1.0660733096301556e-05 0.00011641964374575764 Traning Loss: 0.0001270803768420592 17.967830657958984 2.022031545639038\n",
            "20621 1.0496250979485922e-05 9.015040268423036e-05 Traning Loss: 0.00010064665548270568 17.968050003051758 2.0219762325286865\n",
            "20622 6.849018973298371e-06 7.186295988503844e-05 Traning Loss: 7.87119788583368e-05 17.96829605102539 2.0219316482543945\n",
            "20623 6.230769031390082e-06 6.448608473874629e-05 Traning Loss: 7.071685104165226e-05 17.96855354309082 2.021897077560425\n",
            "20624 7.655748959223274e-06 6.860341090941802e-05 Traning Loss: 7.625915895914659e-05 17.968807220458984 2.0218443870544434\n",
            "20625 7.116942015272798e-06 8.081895794020966e-05 Traning Loss: 8.79358995007351e-05 17.96908950805664 2.0218234062194824\n",
            "20626 1.0136795026483014e-05 8.700673060957342e-05 Traning Loss: 9.714352199807763e-05 17.969348907470703 2.02177357673645\n",
            "20627 8.027529474929906e-06 9.082440374186262e-05 Traning Loss: 9.885193139780313e-05 17.969636917114258 2.021754503250122\n",
            "20628 9.287174179917201e-06 8.408926078118384e-05 Traning Loss: 9.337643859907985e-05 17.969900131225586 2.021718978881836\n",
            "20629 7.030427696008701e-06 7.733067468507215e-05 Traning Loss: 8.436110510956496e-05 17.970182418823242 2.0216965675354004\n",
            "20630 6.8235422077123076e-06 6.944068445591256e-05 Traning Loss: 7.626423030160367e-05 17.970455169677734 2.021676778793335\n",
            "20631 6.474419478763593e-06 6.532617408083752e-05 Traning Loss: 7.180059037636966e-05 17.970731735229492 2.021648406982422\n",
            "20632 5.800493909191573e-06 6.561243935720995e-05 Traning Loss: 7.141293463064358e-05 17.97101402282715 2.0216357707977295\n",
            "20633 6.994378509261878e-06 6.673989992123097e-05 Traning Loss: 7.37342779757455e-05 17.971284866333008 2.0216057300567627\n",
            "20634 6.085528184485156e-06 7.070930587360635e-05 Traning Loss: 7.679483678657562e-05 17.971569061279297 2.021594285964966\n",
            "20635 7.314080903597642e-06 7.162043038988486e-05 Traning Loss: 7.89345140219666e-05 17.971834182739258 2.0215699672698975\n",
            "20636 6.36234472040087e-06 7.290899520739913e-05 Traning Loss: 7.92713399278e-05 17.97211456298828 2.021554946899414\n",
            "20637 6.765466423530597e-06 7.103947427822277e-05 Traning Loss: 7.780494343023747e-05 17.972381591796875 2.021536111831665\n",
            "20638 6.156724339234643e-06 6.893765385029837e-05 Traning Loss: 7.509438000852242e-05 17.97265625 2.021515130996704\n",
            "20639 5.822210823680507e-06 6.6249085648451e-05 Traning Loss: 7.207129965536296e-05 17.972923278808594 2.0214998722076416\n",
            "20640 5.7914398894354235e-06 6.382382707670331e-05 Traning Loss: 6.961526378290728e-05 17.973188400268555 2.0214767456054688\n",
            "20641 5.273443093756214e-06 6.306815339485183e-05 Traning Loss: 6.834160012658685e-05 17.973453521728516 2.0214619636535645\n",
            "20642 5.680317826772807e-06 6.27048866590485e-05 Traning Loss: 6.838520494056866e-05 17.973709106445312 2.0214385986328125\n",
            "20643 5.316700480761938e-06 6.409988418454304e-05 Traning Loss: 6.941658648429438e-05 17.973968505859375 2.0214192867279053\n",
            "20644 5.7374204516236205e-06 6.505500641651452e-05 Traning Loss: 7.079242641339079e-05 17.97421646118164 2.021394968032837\n",
            "20645 5.507300556928385e-06 6.630598363699391e-05 Traning Loss: 7.18132869224064e-05 17.974470138549805 2.0213699340820312\n",
            "20646 5.630275154544506e-06 6.639272760367021e-05 Traning Loss: 7.202300184872001e-05 17.974712371826172 2.0213446617126465\n",
            "20647 5.4330403145286255e-06 6.58782955724746e-05 Traning Loss: 7.131133315851912e-05 17.974958419799805 2.021315336227417\n",
            "20648 5.280083769321209e-06 6.467448110925034e-05 Traning Loss: 6.995456351432949e-05 17.975194931030273 2.021287441253662\n",
            "20649 5.1066349442407954e-06 6.331154145300388e-05 Traning Loss: 6.841817958047614e-05 17.97542953491211 2.0212552547454834\n",
            "20650 4.902889486402273e-06 6.22688967268914e-05 Traning Loss: 6.717178621329367e-05 17.975658416748047 2.021224021911621\n",
            "20651 4.812244696950074e-06 6.169355037854984e-05 Traning Loss: 6.650579598499462e-05 17.97588539123535 2.021190643310547\n",
            "20652 4.729407010017894e-06 6.173607835080475e-05 Traning Loss: 6.646548717981204e-05 17.976110458374023 2.021156072616577\n",
            "20653 4.730781256512273e-06 6.214272434590384e-05 Traning Loss: 6.687350833090022e-05 17.97633171081543 2.0211212635040283\n",
            "20654 4.7363473640871234e-06 6.271171878324822e-05 Traning Loss: 6.744806887581944e-05 17.97655487060547 2.021083116531372\n",
            "20655 4.7583162086084485e-06 6.316737562883645e-05 Traning Loss: 6.79256918374449e-05 17.97677230834961 2.021045446395874\n",
            "20656 4.720913693745388e-06 6.340926483971998e-05 Traning Loss: 6.813017535023391e-05 17.976991653442383 2.0210049152374268\n",
            "20657 4.734247795568081e-06 6.329394091153517e-05 Traning Loss: 6.80281882523559e-05 17.977203369140625 2.0209646224975586\n",
            "20658 4.597234692482743e-06 6.308587762759998e-05 Traning Loss: 6.768311141058803e-05 17.9774169921875 2.020923376083374\n",
            "20659 4.635336154024117e-06 6.2570019508712e-05 Traning Loss: 6.720535748172551e-05 17.977622985839844 2.020880699157715\n",
            "20660 4.430099124874687e-06 6.228942220332101e-05 Traning Loss: 6.671951996395364e-05 17.977828979492188 2.020838737487793\n",
            "20661 4.5151373342378065e-06 6.17930490989238e-05 Traning Loss: 6.630818825215101e-05 17.978029251098633 2.0207934379577637\n",
            "20662 4.300890850572614e-06 6.17102996329777e-05 Traning Loss: 6.601119093829766e-05 17.978227615356445 2.0207507610321045\n",
            "20663 4.411665941006504e-06 6.141705671325326e-05 Traning Loss: 6.582872447324917e-05 17.978422164916992 2.020704507827759\n",
            "20664 4.24366453444236e-06 6.149526598164812e-05 Traning Loss: 6.573893188033253e-05 17.978612899780273 2.020660877227783\n",
            "20665 4.3218406062806025e-06 6.13901429460384e-05 Traning Loss: 6.57119817333296e-05 17.978801727294922 2.0206141471862793\n",
            "20666 4.228824309393531e-06 6.149434193503112e-05 Traning Loss: 6.57231648801826e-05 17.978986740112305 2.020568609237671\n",
            "20667 4.2256892811565194e-06 6.152431888040155e-05 Traning Loss: 6.575000588782132e-05 17.979169845581055 2.020521879196167\n",
            "20668 4.220311438984936e-06 6.155854498501867e-05 Traning Loss: 6.577885505976155e-05 17.97934913635254 2.020474672317505\n",
            "20669 4.131311015953543e-06 6.166814273456112e-05 Traning Loss: 6.579945329576731e-05 17.979528427124023 2.020428419113159\n",
            "20670 4.2121191654587165e-06 6.159085023682564e-05 Traning Loss: 6.580296758329496e-05 17.97970199584961 2.0203800201416016\n",
            "20671 4.056294073961908e-06 6.172808934934437e-05 Traning Loss: 6.578438478754833e-05 17.979877471923828 2.020334005355835\n",
            "20672 4.187898412055802e-06 6.154687434900552e-05 Traning Loss: 6.573477003257722e-05 17.980045318603516 2.02028489112854\n",
            "20673 3.994080543634482e-06 6.166155071696267e-05 Traning Loss: 6.565562944160774e-05 17.980215072631836 2.0202388763427734\n",
            "20674 4.1323810364701785e-06 6.141650374047458e-05 Traning Loss: 6.554888386745006e-05 17.980377197265625 2.0201897621154785\n",
            "20675 3.938094778277446e-06 6.148728425614536e-05 Traning Loss: 6.542538176290691e-05 17.980539321899414 2.020143747329712\n",
            "20676 4.0525428630644456e-06 6.123991624917835e-05 Traning Loss: 6.529245729325339e-05 17.980695724487305 2.020094871520996\n",
            "20677 3.892407221428584e-06 6.126780499471352e-05 Traning Loss: 6.5160209487658e-05 17.980854034423828 2.020048141479492\n",
            "20678 3.964883944718167e-06 6.107247463660315e-05 Traning Loss: 6.503735494334251e-05 17.981006622314453 2.0199995040893555\n",
            "20679 3.856470357277431e-06 6.107112858444452e-05 Traning Loss: 6.492759712273255e-05 17.981159210205078 2.0199522972106934\n",
            "20680 3.883000772475498e-06 6.095198114053346e-05 Traning Loss: 6.483498145826161e-05 17.981307983398438 2.019904613494873\n",
            "20681 3.82683447242016e-06 6.092839248594828e-05 Traning Loss: 6.475522968685254e-05 17.981454849243164 2.019857406616211\n",
            "20682 3.811511078311014e-06 6.087969450163655e-05 Traning Loss: 6.469120853580534e-05 17.981597900390625 2.019810676574707\n",
            "20683 3.796008513745619e-06 6.0838370700366795e-05 Traning Loss: 6.463437603088096e-05 17.981739044189453 2.019763708114624\n",
            "20684 3.7500556118175155e-06 6.0831815062556416e-05 Traning Loss: 6.458187272073701e-05 17.98187828063965 2.0197179317474365\n",
            "20685 3.760291065191268e-06 6.077396756154485e-05 Traning Loss: 6.45342588541098e-05 17.98201560974121 2.0196714401245117\n",
            "20686 3.6999927033321e-06 6.078722435631789e-05 Traning Loss: 6.44872197881341e-05 17.98215103149414 2.0196263790130615\n",
            "20687 3.7219765545160044e-06 6.071797906770371e-05 Traning Loss: 6.443995516747236e-05 17.982282638549805 2.019580364227295\n",
            "20688 3.658662535599433e-06 6.0737434978364035e-05 Traning Loss: 6.439609569497406e-05 17.982412338256836 2.019535779953003\n",
            "20689 3.679378096421715e-06 6.0670361563097686e-05 Traning Loss: 6.43497405690141e-05 17.982540130615234 2.0194904804229736\n",
            "20690 3.6193939649820095e-06 6.068704533390701e-05 Traning Loss: 6.43064413452521e-05 17.982666015625 2.019446611404419\n",
            "20691 3.6320986964710755e-06 6.0634934925474226e-05 Traning Loss: 6.426703475881368e-05 17.9827880859375 2.019402265548706\n",
            "20692 3.580084921850357e-06 6.0649970691883937e-05 Traning Loss: 6.42300583422184e-05 17.98291015625 2.0193588733673096\n",
            "20693 3.5858963656210108e-06 6.060654413886368e-05 Traning Loss: 6.419244164135307e-05 17.983028411865234 2.019315481185913\n",
            "20694 3.541755177138839e-06 6.062043030397035e-05 Traning Loss: 6.41621882095933e-05 17.98314666748047 2.019272804260254\n",
            "20695 3.5428333831077907e-06 6.0591104556806386e-05 Traning Loss: 6.413393566617742e-05 17.983261108398438 2.0192301273345947\n",
            "20696 3.5033167478104588e-06 6.060627129045315e-05 Traning Loss: 6.410959031200036e-05 17.983375549316406 2.0191879272460938\n",
            "20697 3.5041819046455203e-06 6.0587826737901196e-05 Traning Loss: 6.40920115984045e-05 17.98348617553711 2.019145965576172\n",
            "20698 3.464334213276743e-06 6.0618964198511094e-05 Traning Loss: 6.40832950011827e-05 17.983596801757812 2.019104480743408\n",
            "20699 3.4717718335741665e-06 6.061041131033562e-05 Traning Loss: 6.408218177966774e-05 17.98370361328125 2.0190629959106445\n",
            "20700 3.426926468819147e-06 6.066949208616279e-05 Traning Loss: 6.409642082871869e-05 17.983810424804688 2.019021987915039\n",
            "20701 3.4496356420277152e-06 6.068339280318469e-05 Traning Loss: 6.413302617147565e-05 17.98391342163086 2.0189807415008545\n",
            "20702 3.3954131595237413e-06 6.0801568906754255e-05 Traning Loss: 6.419698183890432e-05 17.984018325805664 2.0189404487609863\n",
            "20703 3.4451932151569054e-06 6.0859809309476987e-05 Traning Loss: 6.430500070564449e-05 17.98411750793457 2.018899440765381\n",
            "20704 3.3778510442061815e-06 6.109569221735e-05 Traning Loss: 6.447354098781943e-05 17.98421859741211 2.018859624862671\n",
            "20705 3.471712034297525e-06 6.126186781330034e-05 Traning Loss: 6.473357643699273e-05 17.98431396484375 2.0188186168670654\n",
            "20706 3.3904063911904814e-06 6.173983274493366e-05 Traning Loss: 6.513023981824517e-05 17.984413146972656 2.018779754638672\n",
            "20707 3.5583257158577908e-06 6.216903420863673e-05 Traning Loss: 6.572736310772598e-05 17.98450469970703 2.0187387466430664\n",
            "20708 3.4732649965008022e-06 6.314832717180252e-05 Traning Loss: 6.662159285042435e-05 17.984601974487305 2.01870059967041\n",
            "20709 3.774457809413434e-06 6.420023419195786e-05 Traning Loss: 6.797469541197643e-05 17.98468780517578 2.0186591148376465\n",
            "20710 3.723078179973527e-06 6.629635754507035e-05 Traning Loss: 7.001943595241755e-05 17.984783172607422 2.0186221599578857\n",
            "20711 4.2821561692107935e-06 6.886176561238244e-05 Traning Loss: 7.314392132684588e-05 17.98486328125 2.0185799598693848\n",
            "20712 4.37453354606987e-06 7.354356785072014e-05 Traning Loss: 7.791809912305325e-05 17.98495864868164 2.0185446739196777\n",
            "20713 5.473420969792642e-06 7.984590047271922e-05 Traning Loss: 8.531931962352246e-05 17.985031127929688 2.018501043319702\n",
            "20714 6.018594831402879e-06 9.075245907297358e-05 Traning Loss: 9.677105117589235e-05 17.985126495361328 2.018468141555786\n",
            "20715 8.330257514899131e-06 0.00010648478200891986 Traning Loss: 0.0001148150404333137 17.985187530517578 2.0184223651885986\n",
            "20716 1.0177231160923839e-05 0.00013287807814776897 Traning Loss: 0.0001430553093086928 17.98528289794922 2.01839280128479\n",
            "20717 1.5374600479844958e-05 0.00017290114192292094 Traning Loss: 0.00018827573512680829 17.985322952270508 2.018343210220337\n",
            "20718 2.0820871213800274e-05 0.00023876811610534787 Traning Loss: 0.00025958899641409516 17.98541831970215 2.0183191299438477\n",
            "20719 3.317722075735219e-05 0.0003422232111915946 Traning Loss: 0.0003754004428628832 17.98542022705078 2.018263339996338\n",
            "20720 4.833073035115376e-05 0.0005096273962408304 Traning Loss: 0.000557958148419857 17.985504150390625 2.0182480812072754\n",
            "20721 7.870954141253605e-05 0.0007779063889756799 Traning Loss: 0.0008566159522160888 17.985431671142578 2.0181822776794434\n",
            "20722 0.00011878147779498249 0.001199101796373725 Traning Loss: 0.0013178832596167922 17.985469818115234 2.0181808471679688\n",
            "20723 0.00019249707111157477 0.0018705031834542751 Traning Loss: 0.0020630003418773413 17.9852352142334 2.018099069595337\n",
            "20724 0.00028785259928554296 0.0028477103915065527 Traning Loss: 0.003135562874376774 17.985136032104492 2.0181186199188232\n",
            "20725 0.0004456350870896131 0.004302840679883957 Traning Loss: 0.0047484757378697395 17.984567642211914 2.018015146255493\n",
            "20726 0.000612923817243427 0.006008293945342302 Traning Loss: 0.00662121782079339 17.984134674072266 2.0180623531341553\n",
            "20727 0.0008265027427114546 0.007942820899188519 Traning Loss: 0.0087693240493536 17.983030319213867 2.0179402828216553\n",
            "20728 0.0008994296076707542 0.008769755251705647 Traning Loss: 0.009669184684753418 17.982086181640625 2.0180134773254395\n",
            "20729 0.0008584079332649708 0.008138271979987621 Traning Loss: 0.00899668037891388 17.980567932128906 2.0179107189178467\n",
            "20730 0.0005376391927711666 0.0051606809720396996 Traning Loss: 0.005698319990187883 17.979379653930664 2.0179953575134277\n",
            "20731 0.00021471416403073817 0.0017815906321629882 Traning Loss: 0.001996304839849472 17.978151321411133 2.017993927001953\n",
            "20732 3.848366759484634e-05 0.0001191977207781747 Traning Loss: 0.00015768138109706342 17.977169036865234 2.0180859565734863\n",
            "20733 0.00013133556058164686 0.000984149519354105 Traning Loss: 0.001115485094487667 17.976417541503906 2.0182385444641113\n",
            "20734 0.00033972132951021194 0.002832507248967886 Traning Loss: 0.003172228578478098 17.975419998168945 2.0183632373809814\n",
            "20735 0.00037809781497344375 0.0033631050027906895 Traning Loss: 0.003741202875971794 17.974599838256836 2.018629550933838\n",
            "20736 0.00027219511684961617 0.002067777095362544 Traning Loss: 0.0023399721831083298 17.973554611206055 2.018829107284546\n",
            "20737 9.129435056820512e-05 0.0004510439175646752 Traning Loss: 0.0005423382390290499 17.972654342651367 2.019137382507324\n",
            "20738 7.882598583819345e-05 0.00022669653117191046 Traning Loss: 0.0003055225242860615 17.971834182739258 2.0194361209869385\n",
            "20739 0.00018101684690918773 0.0012431483482941985 Traning Loss: 0.0014241652097553015 17.970884323120117 2.0197455883026123\n",
            "20740 0.00024684390518814325 0.0018855914240702987 Traning Loss: 0.002132435329258442 17.970081329345703 2.0201056003570557\n",
            "20741 0.00019580639491323382 0.0013532406883314252 Traning Loss: 0.0015490470686927438 17.96913719177246 2.0204126834869385\n",
            "20742 8.840006194077432e-05 0.000365951971616596 Traning Loss: 0.0004543520335573703 17.96834373474121 2.0207786560058594\n",
            "20743 6.551849946845323e-05 0.00012521198368631303 Traning Loss: 0.00019073048315476626 17.967634201049805 2.021115303039551\n",
            "20744 0.00011545120651135221 0.0007038982002995908 Traning Loss: 0.0008193494286388159 17.966882705688477 2.021474838256836\n",
            "20745 0.00016160303493961692 0.001140690059401095 Traning Loss: 0.001302293036133051 17.966276168823242 2.0218429565429688\n",
            "20746 0.00012885731121059507 0.0008833522442728281 Traning Loss: 0.0010122095700353384 17.965599060058594 2.022191047668457\n",
            "20747 6.492062675533816e-05 0.00027935081743635237 Traning Loss: 0.00034427145146764815 17.965055465698242 2.0225460529327393\n",
            "20748 4.049099879921414e-05 7.009253022260964e-05 Traning Loss: 0.00011058352538384497 17.96459197998047 2.022876501083374\n",
            "20749 6.609519914491102e-05 0.00037801763392053545 Traning Loss: 0.0004441128403414041 17.964115142822266 2.0231997966766357\n",
            "20750 9.753627819009125e-05 0.0006960328901186585 Traning Loss: 0.0007935691392049193 17.963760375976562 2.0235018730163574\n",
            "20751 8.559113484807312e-05 0.0006316315266303718 Traning Loss: 0.0007172226905822754 17.963350296020508 2.023780345916748\n",
            "20752 4.839541361434385e-05 0.00028067725361324847 Traning Loss: 0.00032907267450354993 17.963043212890625 2.0240190029144287\n",
            "20753 2.2281759811448865e-05 6.641092477366328e-05 Traning Loss: 8.869268640410155e-05 17.962783813476562 2.024237871170044\n",
            "20754 3.085881326114759e-05 0.0001631539489608258 Traning Loss: 0.000194012769497931 17.962520599365234 2.0244140625\n",
            "20755 4.910346251563169e-05 0.00038507289718836546 Traning Loss: 0.00043417635606601834 17.96234703063965 2.02458119392395\n",
            "20756 5.573929229285568e-05 0.00045101536670699716 Traning Loss: 0.0005067546735517681 17.962112426757812 2.024719715118408\n",
            "20757 3.7637550121871755e-05 0.0003040379087906331 Traning Loss: 0.0003416754479985684 17.961963653564453 2.024829626083374\n",
            "20758 1.7489663150627166e-05 0.0001206702072522603 Traning Loss: 0.00013815987040288746 17.961833953857422 2.024930477142334\n",
            "20759 1.367704589938512e-05 7.505319808842614e-05 Traning Loss: 8.873024489730597e-05 17.961734771728516 2.024984359741211\n",
            "20760 1.9731633074115962e-05 0.00017662538448348641 Traning Loss: 0.00019635702483356 17.961721420288086 2.025050401687622\n",
            "20761 3.3564527257112786e-05 0.00027679288177751005 Traning Loss: 0.0003103573981206864 17.961679458618164 2.0250728130340576\n",
            "20762 2.8797692721127532e-05 0.00027460732962936163 Traning Loss: 0.00030340501689352095 17.961734771728516 2.0250983238220215\n",
            "20763 2.0835788745898753e-05 0.0001739229919621721 Traning Loss: 0.00019475878798402846 17.9617862701416 2.0250980854034424\n",
            "20764 1.0397558071417734e-05 8.341128705069423e-05 Traning Loss: 9.380884876009077e-05 17.961891174316406 2.0250751972198486\n",
            "20765 8.495205293002073e-06 7.595576607855037e-05 Traning Loss: 8.445097046205774e-05 17.962039947509766 2.0250601768493652\n",
            "20766 1.6961883375188336e-05 0.00013260749983601272 Traning Loss: 0.00014956938684917986 17.96216583251953 2.0250115394592285\n",
            "20767 1.9093527953373268e-05 0.0001886081154225394 Traning Loss: 0.00020770163973793387 17.962350845336914 2.024989604949951\n",
            "20768 2.1851141355000436e-05 0.0001793686969904229 Traning Loss: 0.00020121983834542334 17.962495803833008 2.0249342918395996\n",
            "20769 1.3324565770744812e-05 0.00012705150584224612 Traning Loss: 0.00014037606888450682 17.962692260742188 2.024893045425415\n",
            "20770 9.542653970129322e-06 7.394169369945303e-05 Traning Loss: 8.348434494109824e-05 17.962888717651367 2.024836540222168\n",
            "20771 8.127401088131592e-06 6.563153874594718e-05 Traning Loss: 7.375894347205758e-05 17.96309471130371 2.0247766971588135\n",
            "20772 1.0249079423374496e-05 9.54395582084544e-05 Traning Loss: 0.0001056886394508183 17.963340759277344 2.0247271060943604\n",
            "20773 1.5000438907009084e-05 0.00012433466326911002 Traning Loss: 0.0001393350976286456 17.963565826416016 2.0246591567993164\n",
            "20774 1.312195399805205e-05 0.00012820021947845817 Traning Loss: 0.00014132217620499432 17.963842391967773 2.024608850479126\n",
            "20775 1.2408085240167566e-05 0.00010065361129818484 Traning Loss: 0.000113061694719363 17.9641056060791 2.024536371231079\n",
            "20776 7.741312401776668e-06 7.350346277235076e-05 Traning Loss: 8.124477608362213e-05 17.964399337768555 2.0244789123535156\n",
            "20777 7.571037258458091e-06 6.340161053230986e-05 Traning Loss: 7.09726446075365e-05 17.964702606201172 2.0244123935699463\n",
            "20778 8.553249244869221e-06 7.554621697636321e-05 Traning Loss: 8.409946894971654e-05 17.96500015258789 2.02435302734375\n",
            "20779 9.781136213860009e-06 9.26860811887309e-05 Traning Loss: 0.0001024672164930962 17.965322494506836 2.0242953300476074\n",
            "20780 1.1139092748635449e-05 9.63563725235872e-05 Traning Loss: 0.00010749546345323324 17.965621948242188 2.024233341217041\n",
            "20781 8.9805080278893e-06 8.641327440273017e-05 Traning Loss: 9.539378515910357e-05 17.965946197509766 2.024177074432373\n",
            "20782 8.167398846126162e-06 6.975432188482955e-05 Traning Loss: 7.792172254994512e-05 17.966259002685547 2.0241146087646484\n",
            "20783 6.686890628770925e-06 6.236838817130774e-05 Traning Loss: 6.905527698108926e-05 17.966577529907227 2.024059534072876\n",
            "20784 7.205262591014616e-06 6.600732012884691e-05 Traning Loss: 7.321258453885093e-05 17.966903686523438 2.024000644683838\n",
            "20785 8.217692084144801e-06 7.523986278101802e-05 Traning Loss: 8.345755486516282e-05 17.96721839904785 2.0239455699920654\n",
            "20786 8.411324415646959e-06 8.092694042716175e-05 Traning Loss: 8.933826757129282e-05 17.96755027770996 2.0238869190216064\n",
            "20787 8.398556019528769e-06 7.76929227868095e-05 Traning Loss: 8.609148062532768e-05 17.96786880493164 2.023831367492676\n",
            "20788 7.2530528996139765e-06 6.98007206665352e-05 Traning Loss: 7.705377356614918e-05 17.968196868896484 2.0237743854522705\n",
            "20789 6.561492227774579e-06 6.30315116723068e-05 Traning Loss: 6.959300662856549e-05 17.96851921081543 2.0237233638763428\n",
            "20790 6.590663815586595e-06 6.17688856436871e-05 Traning Loss: 6.835954991402104e-05 17.968835830688477 2.023669958114624\n",
            "20791 6.570148343598703e-06 6.580808258149773e-05 Traning Loss: 7.237822865135968e-05 17.969160079956055 2.0236213207244873\n",
            "20792 7.435229690599954e-06 6.951652903808281e-05 Traning Loss: 7.695175736444071e-05 17.96946907043457 2.023568868637085\n",
            "20793 7.007056410657242e-06 7.095826731529087e-05 Traning Loss: 7.79653200879693e-05 17.96978759765625 2.0235183238983154\n",
            "20794 7.037405339360703e-06 6.786453741369769e-05 Traning Loss: 7.49019454815425e-05 17.970090866088867 2.023467779159546\n",
            "20795 6.372957614075858e-06 6.399564153980464e-05 Traning Loss: 7.03686018823646e-05 17.970394134521484 2.0234155654907227\n",
            "20796 6.035323167452589e-06 6.154504808364436e-05 Traning Loss: 6.758037488907576e-05 17.970691680908203 2.023366689682007\n",
            "20797 6.245677468541544e-06 6.161860801512375e-05 Traning Loss: 6.78642827551812e-05 17.970979690551758 2.023310899734497\n",
            "20798 5.952589617663762e-06 6.415709503926337e-05 Traning Loss: 7.010968693066388e-05 17.971271514892578 2.0232608318328857\n",
            "20799 6.569223387487e-06 6.549921818077564e-05 Traning Loss: 7.206844020402059e-05 17.971548080444336 2.023202896118164\n",
            "20800 5.9178000810788944e-06 6.623649096582085e-05 Traning Loss: 7.215428922791034e-05 17.971830368041992 2.0231516361236572\n",
            "20801 6.172670964588178e-06 6.422949081752449e-05 Traning Loss: 7.040215859888121e-05 17.97210121154785 2.0230939388275146\n",
            "20802 5.482468623085879e-06 6.260476948227733e-05 Traning Loss: 6.808723992435262e-05 17.972375869750977 2.0230395793914795\n",
            "20803 5.5159262046799995e-06 6.111211405368522e-05 Traning Loss: 6.662803934887052e-05 17.972646713256836 2.0229806900024414\n",
            "20804 5.36299876330304e-06 6.123742059571669e-05 Traning Loss: 6.660041981376708e-05 17.97291374206543 2.022921562194824\n",
            "20805 5.3244280024955515e-06 6.223564560059458e-05 Traning Loss: 6.756007496733218e-05 17.973182678222656 2.0228612422943115\n",
            "20806 5.534031515708193e-06 6.299538654275239e-05 Traning Loss: 6.852942169643939e-05 17.97344398498535 2.0227982997894287\n",
            "20807 5.275464445730904e-06 6.346741429297253e-05 Traning Loss: 6.874287646496668e-05 17.973709106445312 2.022736072540283\n",
            "20808 5.4267397899820935e-06 6.265629053814337e-05 Traning Loss: 6.8083027144894e-05 17.973966598510742 2.022669553756714\n",
            "20809 5.065172899776371e-06 6.193566514411941e-05 Traning Loss: 6.700083758914843e-05 17.974224090576172 2.022603750228882\n",
            "20810 5.113898623676505e-06 6.1010909121250734e-05 Traning Loss: 6.612480501644313e-05 17.974475860595703 2.0225343704223633\n",
            "20811 4.944456122757401e-06 6.0896989452885464e-05 Traning Loss: 6.584144284715876e-05 17.97472381591797 2.022465944290161\n",
            "20812 4.967018412571633e-06 6.114171264925972e-05 Traning Loss: 6.61087324260734e-05 17.97496795654297 2.022395372390747\n",
            "20813 4.985584382666275e-06 6.159643817227334e-05 Traning Loss: 6.658202619291842e-05 17.975204467773438 2.022324800491333\n",
            "20814 4.9280897655989975e-06 6.194305024109781e-05 Traning Loss: 6.687114364467561e-05 17.97544288635254 2.022252082824707\n",
            "20815 4.921184881823137e-06 6.185303209349513e-05 Traning Loss: 6.677421333733946e-05 17.97567367553711 2.02217960357666\n",
            "20816 4.809040092368377e-06 6.154170114314184e-05 Traning Loss: 6.635073805227876e-05 17.97590446472168 2.0221049785614014\n",
            "20817 4.711795554612763e-06 6.111771654104814e-05 Traning Loss: 6.58295102766715e-05 17.976131439208984 2.0220322608947754\n",
            "20818 4.697360054706223e-06 6.075450801290572e-05 Traning Loss: 6.545186624862254e-05 17.976354598999023 2.0219566822052\n",
            "20819 4.551073743641609e-06 6.0781556385336444e-05 Traning Loss: 6.53326278552413e-05 17.976577758789062 2.021883964538574\n",
            "20820 4.682457529270323e-06 6.074588600313291e-05 Traning Loss: 6.542834307765588e-05 17.976795196533203 2.0218076705932617\n",
            "20821 4.48367245553527e-06 6.111554830567911e-05 Traning Loss: 6.559921894222498e-05 17.977012634277344 2.021735191345215\n",
            "20822 4.6759869292145595e-06 6.10178685747087e-05 Traning Loss: 6.569385732291266e-05 17.977222442626953 2.0216591358184814\n",
            "20823 4.427515705174301e-06 6.121004116721451e-05 Traning Loss: 6.563755596289411e-05 17.977434158325195 2.021587610244751\n",
            "20824 4.6032905629544985e-06 6.084066626499407e-05 Traning Loss: 6.544395728269592e-05 17.977638244628906 2.021512269973755\n",
            "20825 4.356516456027748e-06 6.08442715019919e-05 Traning Loss: 6.52007875032723e-05 17.977842330932617 2.0214412212371826\n",
            "20826 4.497729605645873e-06 6.0503367421915755e-05 Traning Loss: 6.500109884655103e-05 17.97804069519043 2.0213663578033447\n",
            "20827 4.299769443605328e-06 6.0604077589232475e-05 Traning Loss: 6.490384839707986e-05 17.97823715209961 2.0212960243225098\n",
            "20828 4.420596724230563e-06 6.049083822290413e-05 Traning Loss: 6.491143722087145e-05 17.97842788696289 2.0212221145629883\n",
            "20829 4.263637947587995e-06 6.0722086345776916e-05 Traning Loss: 6.498572474811226e-05 17.97861671447754 2.021153211593628\n",
            "20830 4.381421149446396e-06 6.068849324947223e-05 Traning Loss: 6.506991485366598e-05 17.978801727294922 2.021080255508423\n",
            "20831 4.211713530821726e-06 6.091849718359299e-05 Traning Loss: 6.513021071441472e-05 17.978984832763672 2.021012783050537\n",
            "20832 4.357889338280074e-06 6.079963350202888e-05 Traning Loss: 6.515752465929836e-05 17.979164123535156 2.0209403038024902\n",
            "20833 4.128661203139927e-06 6.104810017859563e-05 Traning Loss: 6.517676229123026e-05 17.979341506958008 2.0208754539489746\n",
            "20834 4.3614973037620075e-06 6.0878050135215744e-05 Traning Loss: 6.523954652948305e-05 17.979515075683594 2.020803451538086\n",
            "20835 4.048491973662749e-06 6.134549039416015e-05 Traning Loss: 6.53939787298441e-05 17.979686737060547 2.0207417011260986\n",
            "20836 4.427970452525187e-06 6.12658986938186e-05 Traning Loss: 6.569387187482789e-05 17.9798526763916 2.0206692218780518\n",
            "20837 4.008423275081441e-06 6.218013004399836e-05 Traning Loss: 6.618855695705861e-05 17.98002052307129 2.020610809326172\n",
            "20838 4.592586719809333e-06 6.234303873497993e-05 Traning Loss: 6.69356231810525e-05 17.980180740356445 2.0205371379852295\n",
            "20839 4.047299626108725e-06 6.398533150786534e-05 Traning Loss: 6.803263386245817e-05 17.980344772338867 2.0204830169677734\n",
            "20840 4.929510851070518e-06 6.469374056905508e-05 Traning Loss: 6.962325278436765e-05 17.980497360229492 2.0204074382781982\n",
            "20841 4.256108240952017e-06 6.771383777959272e-05 Traning Loss: 7.196994556579739e-05 17.98065757751465 2.0203590393066406\n",
            "20842 5.622445769404294e-06 6.98253934388049e-05 Traning Loss: 7.544783875346184e-05 17.980802536010742 2.0202794075012207\n",
            "20843 4.872124918620102e-06 7.583932165289298e-05 Traning Loss: 8.071144839050248e-05 17.98095703125 2.0202388763427734\n",
            "20844 7.118072971934453e-06 8.154801616910845e-05 Traning Loss: 8.86660855030641e-05 17.98109245300293 2.0201525688171387\n",
            "20845 6.509463219117606e-06 9.442662121728063e-05 Traning Loss: 0.00010093608580064029 17.981243133544922 2.0201239585876465\n",
            "20846 1.04908531284309e-05 0.00010916963219642639 Traning Loss: 0.00011966048623435199 17.981367111206055 2.020026445388794\n",
            "20847 1.0713519259297755e-05 0.00013828127703163773 Traning Loss: 0.00014899478992447257 17.98151397705078 2.0200154781341553\n",
            "20848 1.833876194723416e-05 0.00017566666065249592 Traning Loss: 0.00019400542078074068 17.981618881225586 2.019899606704712\n",
            "20849 2.137946466973517e-05 0.00024408800527453423 Traning Loss: 0.0002654674754012376 17.98175621032715 2.019916534423828\n",
            "20850 3.69860244973097e-05 0.0003377556859049946 Traning Loss: 0.0003747417067643255 17.981828689575195 2.019771099090576\n",
            "20851 4.810567043023184e-05 0.0005010873428545892 Traning Loss: 0.0005491930060088634 17.981945037841797 2.019831418991089\n",
            "20852 8.109082409646362e-05 0.000727997743524611 Traning Loss: 0.0008090885821729898 17.981950759887695 2.019641637802124\n",
            "20853 0.00011214049300178885 0.0011044926941394806 Traning Loss: 0.0012166332453489304 17.982011795043945 2.019767999649048\n",
            "20854 0.0001783018815331161 0.0015974611742421985 Traning Loss: 0.0017757629975676537 17.981884002685547 2.019519567489624\n",
            "20855 0.0002449858875479549 0.002338078571483493 Traning Loss: 0.0025830643717199564 17.9818058013916 2.0197341442108154\n",
            "20856 0.00034654594492167234 0.0031064124777913094 Traning Loss: 0.0034529585391283035 17.981433868408203 2.0194334983825684\n",
            "20857 0.00042268549441359937 0.003965401090681553 Traning Loss: 0.004388086497783661 17.981088638305664 2.019728422164917\n",
            "20858 0.00047100934898480773 0.004199810326099396 Traning Loss: 0.004670819733291864 17.980398178100586 2.019441843032837\n",
            "20859 0.0004127194988541305 0.0038529508747160435 Traning Loss: 0.004265670198947191 17.979755401611328 2.0197317600250244\n",
            "20860 0.0002868953160941601 0.0024928362108767033 Traning Loss: 0.0027797315269708633 17.978912353515625 2.0195844173431396\n",
            "20861 0.00010806299542309716 0.0010398229351267219 Traning Loss: 0.0011478859232738614 17.97821044921875 2.019731044769287\n",
            "20862 2.3719312594039366e-05 0.00016913445142563432 Traning Loss: 0.0001928537676576525 17.977542877197266 2.0197958946228027\n",
            "20863 3.9659618778387085e-05 0.0003094771527685225 Traning Loss: 0.0003491367679089308 17.976957321166992 2.0197460651397705\n",
            "20864 0.00010780718002934009 0.0010623759590089321 Traning Loss: 0.0011701831826940179 17.976486206054688 2.0199625492095947\n",
            "20865 0.0001875954185379669 0.001562786870636046 Traning Loss: 0.0017503822455182672 17.975927352905273 2.019840955734253\n",
            "20866 0.00015379600517917424 0.0014612618833780289 Traning Loss: 0.0016150579322129488 17.975461959838867 2.0200648307800293\n",
            "20867 9.889189095702022e-05 0.0007659649709239602 Traning Loss: 0.0008648568764328957 17.974910736083984 2.020066499710083\n",
            "20868 2.7607511583482847e-05 0.00022940420603845268 Traning Loss: 0.0002570117067079991 17.97444725036621 2.020177125930786\n",
            "20869 2.7610238248598762e-05 0.00022154430916998535 Traning Loss: 0.0002491545455995947 17.974008560180664 2.020341634750366\n",
            "20870 7.364863267866895e-05 0.0005850442685186863 Traning Loss: 0.0006586929084733129 17.973608016967773 2.0203311443328857\n",
            "20871 8.970624185167253e-05 0.0008708196692168713 Traning Loss: 0.0009605259401723742 17.97330093383789 2.0205435752868652\n",
            "20872 9.048493666341528e-05 0.0007116345805115998 Traning Loss: 0.0008021194953471422 17.972978591918945 2.020528554916382\n",
            "20873 3.9084367017494515e-05 0.00035760150058194995 Traning Loss: 0.0003966858785133809 17.972747802734375 2.020665407180786\n",
            "20874 1.9723442164831795e-05 0.0001160156971309334 Traning Loss: 0.0001357391447527334 17.972509384155273 2.0207531452178955\n",
            "20875 2.6670328225009143e-05 0.00019420328317210078 Traning Loss: 0.00022087361139710993 17.97229766845703 2.0207812786102295\n",
            "20876 4.5884451537858695e-05 0.0004282789013814181 Traning Loss: 0.0004741633601952344 17.97209358215332 2.020930290222168\n",
            "20877 6.335533544188365e-05 0.0005147900665178895 Traning Loss: 0.0005781453801319003 17.97188949584961 2.020909309387207\n",
            "20878 4.0530358091928065e-05 0.0003978218592237681 Traning Loss: 0.0004383522318676114 17.971729278564453 2.0210163593292236\n",
            "20879 2.3558952307212166e-05 0.00016915991727728397 Traning Loss: 0.00019271887140348554 17.971586227416992 2.021019458770752\n",
            "20880 8.663794687890913e-06 6.574494909727946e-05 Traning Loss: 7.440874469466507e-05 17.971487045288086 2.0210208892822266\n",
            "20881 1.452939432056155e-05 0.00013626556028611958 Traning Loss: 0.00015079494914971292 17.971406936645508 2.021061897277832\n",
            "20882 3.256187483202666e-05 0.0002676446456462145 Traning Loss: 0.0003002065350301564 17.971323013305664 2.020986318588257\n",
            "20883 3.288780862931162e-05 0.0003308287123218179 Traning Loss: 0.00036371650639921427 17.97124481201172 2.0210156440734863\n",
            "20884 2.959046105388552e-05 0.0002536582760512829 Traning Loss: 0.0002832487225532532 17.971172332763672 2.0209381580352783\n",
            "20885 1.2824495570384897e-05 0.00013761651644017547 Traning Loss: 0.00015044101746752858 17.971118927001953 2.0209031105041504\n",
            "20886 6.581086836376926e-06 7.050298154354095e-05 Traning Loss: 7.708407065365463e-05 17.97110939025879 2.0208518505096436\n",
            "20887 1.0372541510150768e-05 9.314707858720794e-05 Traning Loss: 0.00010351961827836931 17.971120834350586 2.0207393169403076\n",
            "20888 1.3634644346893765e-05 0.00016285122546833009 Traning Loss: 0.00017648587527219206 17.971179962158203 2.020697593688965\n",
            "20889 2.1641077182721347e-05 0.00019353556854184717 Traning Loss: 0.0002151766384486109 17.971240997314453 2.020556688308716\n",
            "20890 1.498386700404808e-05 0.00017405912512913346 Traning Loss: 0.00018904299940913916 17.97132110595703 2.020491600036621\n",
            "20891 1.1674734196276404e-05 0.00011458304652478546 Traning Loss: 0.00012625778617803007 17.97141456604004 2.0203778743743896\n",
            "20892 6.65078368911054e-06 7.706111500738189e-05 Traning Loss: 8.371190051548183e-05 17.971508026123047 2.020267963409424\n",
            "20893 6.132014277682174e-06 8.218778384616598e-05 Traning Loss: 8.831980085233226e-05 17.971637725830078 2.020186424255371\n",
            "20894 1.223204981215531e-05 0.00011007746070390567 Traning Loss: 0.0001223095168825239 17.971759796142578 2.020042657852173\n",
            "20895 1.0730309440987185e-05 0.0001361251197522506 Traning Loss: 0.000146855425555259 17.971923828125 2.019973039627075\n",
            "20896 1.3929346096119843e-05 0.00012396794045343995 Traning Loss: 0.00013789728109259158 17.97207260131836 2.019834280014038\n",
            "20897 7.5099296736880206e-06 9.728990698931739e-05 Traning Loss: 0.00010479983757250011 17.97224235534668 2.0197505950927734\n",
            "20898 6.529685379064176e-06 6.794320506742224e-05 Traning Loss: 7.447288953699172e-05 17.972410202026367 2.0196495056152344\n",
            "20899 5.634803073917283e-06 6.221824878593907e-05 Traning Loss: 6.78530486766249e-05 17.972572326660156 2.0195460319519043\n",
            "20900 5.905429134145379e-06 7.777287828503177e-05 Traning Loss: 8.367830741917714e-05 17.972753524780273 2.0194833278656006\n",
            "20901 1.0173351256526075e-05 9.370152110932395e-05 Traning Loss: 0.00010387487418483943 17.972923278808594 2.019376516342163\n",
            "20902 7.8286911957548e-06 0.000102972102467902 Traning Loss: 0.0001108007927541621 17.973127365112305 2.019331455230713\n",
            "20903 9.595262781658676e-06 9.048072388395667e-05 Traning Loss: 0.00010007598757511005 17.97332191467285 2.01924204826355\n",
            "20904 5.559276360145304e-06 7.624751015100628e-05 Traning Loss: 8.180678560165688e-05 17.973543167114258 2.0191919803619385\n",
            "20905 5.631995009025559e-06 6.37838093098253e-05 Traning Loss: 6.941580795682967e-05 17.97376251220703 2.019136428833008\n",
            "20906 5.375281489250483e-06 6.389657937688753e-05 Traning Loss: 6.927186041139066e-05 17.97398567199707 2.0190799236297607\n",
            "20907 5.464435162139125e-06 7.19059826224111e-05 Traning Loss: 7.737041596556082e-05 17.974212646484375 2.0190553665161133\n",
            "20908 7.508348971896339e-06 7.711239595664665e-05 Traning Loss: 8.46207476570271e-05 17.974435806274414 2.019002914428711\n",
            "20909 5.803775820822921e-06 7.89984260336496e-05 Traning Loss: 8.480220276396722e-05 17.97467041015625 2.0189902782440186\n",
            "20910 6.7609635152621195e-06 7.140322122722864e-05 Traning Loss: 7.816418656148016e-05 17.974899291992188 2.0189504623413086\n",
            "20911 4.70884924652637e-06 6.547079829033464e-05 Traning Loss: 7.017964526312426e-05 17.975139617919922 2.0189332962036133\n",
            "20912 4.992813956050668e-06 6.126711377874017e-05 Traning Loss: 6.625992682529613e-05 17.975374221801758 2.018913507461548\n",
            "20913 5.086104010842973e-06 6.29276764811948e-05 Traning Loss: 6.801378185627982e-05 17.975608825683594 2.018889904022217\n",
            "20914 5.066933681519004e-06 6.771089101675898e-05 Traning Loss: 7.277782424353063e-05 17.97583770751953 2.018885850906372\n",
            "20915 6.15333192399703e-06 7.013633148744702e-05 Traning Loss: 7.628966704942286e-05 17.976062774658203 2.018861770629883\n",
            "20916 5.129119472258026e-06 7.088914571795613e-05 Traning Loss: 7.60182665544562e-05 17.976285934448242 2.0188608169555664\n",
            "20917 5.531630449695513e-06 6.683540414087474e-05 Traning Loss: 7.236703822854906e-05 17.97650909423828 2.0188424587249756\n",
            "20918 4.4758212425222155e-06 6.347151793306693e-05 Traning Loss: 6.79473378113471e-05 17.97673225402832 2.0188353061676025\n",
            "20919 4.386200089356862e-06 6.088592272135429e-05 Traning Loss: 6.527212099172175e-05 17.976957321166992 2.0188262462615967\n",
            "20920 4.548065589915495e-06 6.074105476727709e-05 Traning Loss: 6.528911762870848e-05 17.977176666259766 2.0188097953796387\n",
            "20921 4.2122674130951054e-06 6.286050484050065e-05 Traning Loss: 6.707277498207986e-05 17.977399826049805 2.0188071727752686\n",
            "20922 5.049670107837301e-06 6.377357931341976e-05 Traning Loss: 6.882324669277295e-05 17.97761344909668 2.018786907196045\n",
            "20923 4.277832431398565e-06 6.491579551948234e-05 Traning Loss: 6.919362931512296e-05 17.977828979492188 2.018784284591675\n",
            "20924 4.811541202798253e-06 6.319225212791935e-05 Traning Loss: 6.800379196647555e-05 17.978038787841797 2.018765687942505\n",
            "20925 4.056080797454342e-06 6.206921534612775e-05 Traning Loss: 6.61252997815609e-05 17.978248596191406 2.018756628036499\n",
            "20926 4.163761786912801e-06 6.0508922615554184e-05 Traning Loss: 6.467268394771963e-05 17.978456497192383 2.0187416076660156\n",
            "20927 4.105711923330091e-06 6.0209506045794114e-05 Traning Loss: 6.43152161501348e-05 17.97865867614746 2.0187227725982666\n",
            "20928 3.908091912308009e-06 6.10561401117593e-05 Traning Loss: 6.496423156931996e-05 17.978862762451172 2.0187108516693115\n",
            "20929 4.443487796379486e-06 6.153079448267817e-05 Traning Loss: 6.59742800053209e-05 17.97905731201172 2.018686294555664\n",
            "20930 3.916249170288211e-06 6.271389429457486e-05 Traning Loss: 6.663014210062101e-05 17.9792537689209 2.0186750888824463\n",
            "20931 4.485090812522685e-06 6.206659600138664e-05 Traning Loss: 6.655168544966727e-05 17.97943878173828 2.018649101257324\n",
            "20932 3.8099678931757808e-06 6.203980592545122e-05 Traning Loss: 6.5849773818627e-05 17.97962760925293 2.01863431930542\n",
            "20933 4.141474164498504e-06 6.079969898564741e-05 Traning Loss: 6.494117405964062e-05 17.979808807373047 2.0186095237731934\n",
            "20934 3.7282652556314133e-06 6.0543781728483737e-05 Traning Loss: 6.427204789360985e-05 17.97999382019043 2.0185887813568115\n",
            "20935 3.840996214421466e-06 6.022590241627768e-05 Traning Loss: 6.406690226867795e-05 17.980173110961914 2.018566608428955\n",
            "20936 3.83415499527473e-06 6.043528264854103e-05 Traning Loss: 6.426943582482636e-05 17.9803524017334 2.018541097640991\n",
            "20937 3.7443851397256367e-06 6.0882775869686157e-05 Traning Loss: 6.462715828092769e-05 17.980525970458984 2.0185203552246094\n",
            "20938 3.931088940589689e-06 6.093097908888012e-05 Traning Loss: 6.486206984845921e-05 17.980697631835938 2.0184924602508545\n",
            "20939 3.6899828046443872e-06 6.113376002758741e-05 Traning Loss: 6.48237401037477e-05 17.980867385864258 2.0184710025787354\n",
            "20940 3.830754849332152e-06 6.069887240300886e-05 Traning Loss: 6.452962406910956e-05 17.981035232543945 2.0184431076049805\n",
            "20941 3.6273736441216897e-06 6.049720468581654e-05 Traning Loss: 6.412457878468558e-05 17.981199264526367 2.018419027328491\n",
            "20942 3.6408057439984987e-06 6.01371684751939e-05 Traning Loss: 6.377797399181873e-05 17.98136329650879 2.018392562866211\n",
            "20943 3.640383738456876e-06 5.996461186441593e-05 Traning Loss: 6.360499537549913e-05 17.981521606445312 2.018364667892456\n",
            "20944 3.511006070766598e-06 6.01141327933874e-05 Traning Loss: 6.362513522617519e-05 17.98168182373047 2.0183398723602295\n",
            "20945 3.7144920952414395e-06 6.005827526678331e-05 Traning Loss: 6.377276440616697e-05 17.981834411621094 2.0183095932006836\n",
            "20946 3.4508855151216267e-06 6.050004230928607e-05 Traning Loss: 6.395093078026548e-05 17.98198890686035 2.0182857513427734\n",
            "20947 3.7453878576343413e-06 6.033545651007444e-05 Traning Loss: 6.408084300346673e-05 17.982135772705078 2.018254280090332\n",
            "20948 3.411990746826632e-06 6.072315954952501e-05 Traning Loss: 6.413515075109899e-05 17.98228645324707 2.0182299613952637\n",
            "20949 3.7036966205050703e-06 6.043737448635511e-05 Traning Loss: 6.414107338059694e-05 17.98242950439453 2.018198013305664\n",
            "20950 3.3972578421526123e-06 6.076653880882077e-05 Traning Loss: 6.416379619622603e-05 17.982576370239258 2.018171787261963\n",
            "20951 3.6501046452031005e-06 6.062665852368809e-05 Traning Loss: 6.427676271414384e-05 17.98271369934082 2.0181403160095215\n",
            "20952 3.4363579288765322e-06 6.111089169280604e-05 Traning Loss: 6.454724643845111e-05 17.98285484313965 2.0181126594543457\n",
            "20953 3.6564119909598958e-06 6.136618321761489e-05 Traning Loss: 6.502259202534333e-05 17.982986450195312 2.0180819034576416\n",
            "20954 3.5346029108040966e-06 6.220632349140942e-05 Traning Loss: 6.574092549271882e-05 17.983123779296875 2.0180530548095703\n",
            "20955 3.7744666769867763e-06 6.298440712271258e-05 Traning Loss: 6.675887561868876e-05 17.98324966430664 2.018022298812866\n",
            "20956 3.7119600619917037e-06 6.44457177259028e-05 Traning Loss: 6.815767847001553e-05 17.983383178710938 2.017993211746216\n",
            "20957 4.083173735125456e-06 6.602309440495446e-05 Traning Loss: 7.010626723058522e-05 17.983503341674805 2.017961263656616\n",
            "20958 4.071137027494842e-06 6.877761916257441e-05 Traning Loss: 7.28487575543113e-05 17.98363494873047 2.0179331302642822\n",
            "20959 4.764231107401429e-06 7.204997382359579e-05 Traning Loss: 7.681420538574457e-05 17.983747482299805 2.01789927482605\n",
            "20960 4.8702490857976954e-06 7.769610965624452e-05 Traning Loss: 8.256635919678956e-05 17.983877182006836 2.0178732872009277\n",
            "20961 6.210907031345414e-06 8.487251034239307e-05 Traning Loss: 9.108342055696994e-05 17.983980178833008 2.017836809158325\n",
            "20962 6.6985567173105665e-06 9.693258471088484e-05 Traning Loss: 0.00010363114415667951 17.984107971191406 2.0178141593933105\n",
            "20963 9.329034583061002e-06 0.0001131648532464169 Traning Loss: 0.0001224938896484673 17.984195709228516 2.0177738666534424\n",
            "20964 1.0919284250121564e-05 0.0001396466395817697 Traning Loss: 0.00015056593110784888 17.98432159423828 2.0177557468414307\n",
            "20965 1.6258065443253145e-05 0.00017714829300530255 Traning Loss: 0.0001934063620865345 17.984390258789062 2.0177104473114014\n",
            "20966 2.074641452054493e-05 0.00023691954265814275 Traning Loss: 0.0002576659608166665 17.984512329101562 2.0176987648010254\n",
            "20967 3.206648034392856e-05 0.0003250007866881788 Traning Loss: 0.00035706727067008615 17.98454475402832 2.017646551132202\n",
            "20968 4.379386155051179e-05 0.00046242299140430987 Traning Loss: 0.0005062168347649276 17.984655380249023 2.0176446437835693\n",
            "20969 6.862927693873644e-05 0.000669941131491214 Traning Loss: 0.0007385704084299505 17.984622955322266 2.017582416534424\n",
            "20970 9.743045666255057e-05 0.0009837271645665169 Traning Loss: 0.001081157592125237 17.984697341918945 2.017595052719116\n",
            "20971 0.00015167474339250475 0.0014574381057173014 Traning Loss: 0.001609112834557891 17.98453712463379 2.0175182819366455\n",
            "20972 0.0002155736874556169 0.0021275074686855078 Traning Loss: 0.00234308117069304 17.984512329101562 2.017551898956299\n",
            "20973 0.0003232054878026247 0.0030873515643179417 Traning Loss: 0.0034105570521205664 17.984111785888672 2.017456531524658\n",
            "20974 0.0004336038837209344 0.004232850391417742 Traning Loss: 0.004666454158723354 17.98386001586914 2.0175178050994873\n",
            "20975 0.0005861276877112687 0.005579227115958929 Traning Loss: 0.0061653549782931805 17.983070373535156 2.0174074172973633\n",
            "20976 0.0006657306803390384 0.006464878097176552 Traning Loss: 0.007130608893930912 17.982450485229492 2.0174951553344727\n",
            "20977 0.0007058265618979931 0.0066653708927333355 Traning Loss: 0.007371197454631329 17.981290817260742 2.0173985958099365\n",
            "20978 0.0005512585630640388 0.005318591371178627 Traning Loss: 0.005869850050657988 17.98039436340332 2.0174951553344727\n",
            "20979 0.0003404117887839675 0.003072363091632724 Traning Loss: 0.0034127747640013695 17.979290008544922 2.0174763202667236\n",
            "20980 0.00010569000733084977 0.0008956838282756507 Traning Loss: 0.001001373864710331 17.978469848632812 2.0175626277923584\n",
            "20981 2.9662194720003754e-05 6.864037277409807e-05 Traning Loss: 9.830256749410182e-05 17.97779655456543 2.017674684524536\n",
            "20982 0.00011116263340227306 0.0007691900827921927 Traning Loss: 0.0008803526870906353 17.977075576782227 2.01776123046875\n",
            "20983 0.0002269016986247152 0.0019995577167719603 Traning Loss: 0.002226459328085184 17.976530075073242 2.0179855823516846\n",
            "20984 0.0003018937131855637 0.0024975824635475874 Traning Loss: 0.0027994762640446424 17.975690841674805 2.018113613128662\n",
            "20985 0.00021223517251200974 0.0018020618008449674 Traning Loss: 0.0020142970606684685 17.975008010864258 2.018378973007202\n",
            "20986 0.00010776497219922021 0.0006375653319992125 Traning Loss: 0.0007453302969224751 17.974227905273438 2.018588066101074\n",
            "20987 4.287300544092432e-05 7.282237493200228e-05 Traning Loss: 0.00011569538037292659 17.973508834838867 2.018836259841919\n",
            "20988 7.999772060429677e-05 0.0004285787872504443 Traning Loss: 0.0005085765151306987 17.972896575927734 2.0191245079040527\n",
            "20989 0.00015464474563486874 0.0011061779223382473 Traning Loss: 0.0012608226388692856 17.972137451171875 2.019350528717041\n",
            "20990 0.00016750898794271052 0.001302453107200563 Traning Loss: 0.001469962066039443 17.971534729003906 2.0196704864501953\n",
            "20991 0.00012616255844477564 0.0008523981086909771 Traning Loss: 0.0009785606525838375 17.9708251953125 2.019913673400879\n",
            "20992 5.6634795328136533e-05 0.00025578640634194016 Traning Loss: 0.0003124212089460343 17.970247268676758 2.0202152729034424\n",
            "20993 3.811286296695471e-05 7.405188807751983e-05 Traning Loss: 0.00011216475104447454 17.96974754333496 2.0205047130584717\n",
            "20994 6.463481986429542e-05 0.00036419439129531384 Traning Loss: 0.0004288292257115245 17.969228744506836 2.0207719802856445\n",
            "20995 9.622846846468747e-05 0.000705528655089438 Traning Loss: 0.000801757094450295 17.968852996826172 2.021084785461426\n",
            "20996 9.900780423777178e-05 0.0007253335788846016 Traning Loss: 0.0008243413758464158 17.96839141845703 2.021329164505005\n",
            "20997 6.236485933186486e-05 0.0004281227884348482 Traning Loss: 0.0004904876695945859 17.968061447143555 2.021616220474243\n",
            "20998 3.1552979635307565e-05 0.00012475173571147025 Traning Loss: 0.000156304711708799 17.967741012573242 2.0218541622161865\n",
            "20999 2.2314827219815925e-05 7.647650636499748e-05 Traning Loss: 9.879133722279221e-05 17.967456817626953 2.022082567214966\n",
            "21000 3.8172547647263855e-05 0.00025237788213416934 Traning Loss: 0.0002905504370573908 17.967266082763672 2.022306442260742\n",
            "21001 5.6881570344557986e-05 0.00043226912384852767 Traning Loss: 0.0004891506978310645 17.967012405395508 2.0224735736846924\n",
            "21002 5.25928771821782e-05 0.00044012488797307014 Traning Loss: 0.0004927177797071636 17.966861724853516 2.0226571559906006\n",
            "21003 3.855830436805263e-05 0.00028237179503776133 Traning Loss: 0.00032093009212985635 17.96666717529297 2.0227839946746826\n",
            "21004 1.6558498828089796e-05 0.00011564097076188773 Traning Loss: 0.00013219946413300931 17.966537475585938 2.0229153633117676\n",
            "21005 1.1303273822704796e-05 6.260394002310932e-05 Traning Loss: 7.390721293631941e-05 17.966442108154297 2.0230228900909424\n",
            "21006 1.8447917682351544e-05 0.00013665542064700276 Traning Loss: 0.0001551033346913755 17.966341018676758 2.0231051445007324\n",
            "21007 2.6447049094713293e-05 0.000245096132857725 Traning Loss: 0.00027154319104738533 17.966323852539062 2.023191213607788\n",
            "21008 3.3137082937173545e-05 0.00028241833206266165 Traning Loss: 0.0003155554295517504 17.966270446777344 2.023235321044922\n",
            "21009 2.377603595959954e-05 0.00023122006678022444 Traning Loss: 0.00025499609182588756 17.966306686401367 2.0232880115509033\n",
            "21010 1.6032427083700895e-05 0.00013324109022505581 Traning Loss: 0.0001492735173087567 17.966339111328125 2.023310422897339\n",
            "21011 7.693286534049548e-06 6.968865636736155e-05 Traning Loss: 7.738194108242169e-05 17.966419219970703 2.023326873779297\n",
            "21012 7.765926056890748e-06 7.155979983508587e-05 Traning Loss: 7.932572771096602e-05 17.966541290283203 2.02333664894104\n",
            "21013 1.3866013432561886e-05 0.00011933446512557566 Traning Loss: 0.00013320047582965344 17.966651916503906 2.0233232975006104\n",
            "21014 1.6705613234080374e-05 0.00016774976393207908 Traning Loss: 0.00018445537716615945 17.966821670532227 2.0233194828033447\n",
            "21015 1.960208646778483e-05 0.00017330975970253348 Traning Loss: 0.0001929118443513289 17.96695327758789 2.0232911109924316\n",
            "21016 1.4066582480154466e-05 0.00014129829651210457 Traning Loss: 0.00015536487626377493 17.967130661010742 2.0232715606689453\n",
            "21017 1.0456732525199186e-05 9.269996371585876e-05 Traning Loss: 0.00010315669351257384 17.967294692993164 2.023240566253662\n",
            "21018 6.844034487585304e-06 6.376262899721041e-05 Traning Loss: 7.060666393954307e-05 17.967477798461914 2.023204803466797\n",
            "21019 6.6924326347361784e-06 6.594692968064919e-05 Traning Loss: 7.263936277013272e-05 17.967683792114258 2.0231730937957764\n",
            "21020 1.0014281542680692e-05 8.807577978586778e-05 Traning Loss: 9.809005860006437e-05 17.967878341674805 2.023124933242798\n",
            "21021 1.0821933756233193e-05 0.00011176420230185613 Traning Loss: 0.00012258613423909992 17.968116760253906 2.02308988571167\n",
            "21022 1.2762872756866273e-05 0.00011506430746521801 Traning Loss: 0.00012782718113157898 17.968332290649414 2.023038148880005\n",
            "21023 9.781655535334721e-06 0.00010175637726206332 Traning Loss: 0.00011153802915941924 17.96858787536621 2.0229952335357666\n",
            "21024 8.357576916750986e-06 7.839099271222949e-05 Traning Loss: 8.674857235746458e-05 17.968835830688477 2.0229456424713135\n",
            "21025 6.267895969358506e-06 6.29252172075212e-05 Traning Loss: 6.919311272213235e-05 17.969099044799805 2.0228936672210693\n",
            "21026 5.858964868821204e-06 6.133061106083915e-05 Traning Loss: 6.718957592966035e-05 17.969375610351562 2.02284836769104\n",
            "21027 7.551712769782171e-06 7.000775076448917e-05 Traning Loss: 7.755946717225015e-05 17.96963882446289 2.022791862487793\n",
            "21028 7.597972398798447e-06 8.244535274570808e-05 Traning Loss: 9.004332241602242e-05 17.969926834106445 2.0227489471435547\n",
            "21029 9.308822882303502e-06 8.601207809988409e-05 Traning Loss: 9.53209018916823e-05 17.970190048217773 2.0226926803588867\n",
            "21030 7.632774213561788e-06 8.274898573290557e-05 Traning Loss: 9.038175630848855e-05 17.970476150512695 2.02264666557312\n",
            "21031 7.433966857206542e-06 7.196683145593852e-05 Traning Loss: 7.940080104162917e-05 17.970746994018555 2.0225937366485596\n",
            "21032 5.946296823822195e-06 6.338941602734849e-05 Traning Loss: 6.933571421541274e-05 17.971027374267578 2.0225419998168945\n",
            "21033 5.573505859501893e-06 5.977699038339779e-05 Traning Loss: 6.535049760714173e-05 17.971309661865234 2.022493839263916\n",
            "21034 6.138750904938206e-06 6.183653022162616e-05 Traning Loss: 6.797528476454318e-05 17.97158432006836 2.0224382877349854\n",
            "21035 5.977689852443291e-06 6.781422416679561e-05 Traning Loss: 7.379191083600745e-05 17.971874237060547 2.022393226623535\n",
            "21036 7.204610028566094e-06 7.107552664820105e-05 Traning Loss: 7.828013622201979e-05 17.972145080566406 2.0223374366760254\n",
            "21037 6.311080142040737e-06 7.230353367049247e-05 Traning Loss: 7.86146119935438e-05 17.97243309020996 2.0222928524017334\n",
            "21038 6.69645032758126e-06 6.829341145930812e-05 Traning Loss: 7.498986087739468e-05 17.97270393371582 2.0222411155700684\n",
            "21039 5.600378699455177e-06 6.416154792532325e-05 Traning Loss: 6.976192526053637e-05 17.97298240661621 2.0221948623657227\n",
            "21040 5.482912456500344e-06 6.026733899489045e-05 Traning Loss: 6.57502532703802e-05 17.97325325012207 2.0221474170684814\n",
            "21041 5.324171979737002e-06 5.922847776673734e-05 Traning Loss: 6.455265247495845e-05 17.973520278930664 2.022097587585449\n",
            "21042 5.184373094380135e-06 6.0785208916058764e-05 Traning Loss: 6.596957973670214e-05 17.97378921508789 2.022052526473999\n",
            "21043 5.807559773529647e-06 6.268439028644934e-05 Traning Loss: 6.849195051472634e-05 17.974044799804688 2.0219995975494385\n",
            "21044 5.386189059208846e-06 6.498935545096174e-05 Traning Loss: 7.037554314592853e-05 17.974306106567383 2.0219545364379883\n",
            "21045 5.918738224863773e-06 6.467080675065517e-05 Traning Loss: 7.05895436112769e-05 17.974552154541016 2.0219004154205322\n",
            "21046 5.1752176659647375e-06 6.396261596819386e-05 Traning Loss: 6.91378372721374e-05 17.974803924560547 2.021852731704712\n",
            "21047 5.316900114848977e-06 6.15655371802859e-05 Traning Loss: 6.688243593089283e-05 17.97504425048828 2.021798849105835\n",
            "21048 4.768167400470702e-06 6.011293953633867e-05 Traning Loss: 6.488110375357792e-05 17.97528648376465 2.02174711227417\n",
            "21049 4.722425273939734e-06 5.915435394854285e-05 Traning Loss: 6.387678149621934e-05 17.97552490234375 2.0216941833496094\n",
            "21050 4.712582267529797e-06 5.928732571192086e-05 Traning Loss: 6.399990525096655e-05 17.975757598876953 2.0216383934020996\n",
            "21051 4.556464773486368e-06 6.030280565028079e-05 Traning Loss: 6.485926860477775e-05 17.975994110107422 2.021585464477539\n",
            "21052 4.88080877403263e-06 6.0941933043068275e-05 Traning Loss: 6.582274363609031e-05 17.97622299194336 2.0215260982513428\n",
            "21053 4.541699127003085e-06 6.181374919833615e-05 Traning Loss: 6.635544559685513e-05 17.976457595825195 2.0214712619781494\n",
            "21054 4.850136065215338e-06 6.137492891866714e-05 Traning Loss: 6.622506771236658e-05 17.976682662963867 2.021409273147583\n",
            "21055 4.412415364640765e-06 6.112419214332476e-05 Traning Loss: 6.553660932695493e-05 17.976913452148438 2.0213510990142822\n",
            "21056 4.577648724080063e-06 6.0026024584658444e-05 Traning Loss: 6.46036714897491e-05 17.977134704589844 2.021287679672241\n",
            "21057 4.265521511115367e-06 5.951210550847463e-05 Traning Loss: 6.377762474585325e-05 17.97735595703125 2.021225929260254\n",
            "21058 4.311317297833739e-06 5.8987450756831095e-05 Traning Loss: 6.329876487143338e-05 17.97757339477539 2.0211617946624756\n",
            "21059 4.24453673986136e-06 5.897496885154396e-05 Traning Loss: 6.321950786514208e-05 17.977785110473633 2.02109694480896\n",
            "21060 4.1844659790513106e-06 5.92536962358281e-05 Traning Loss: 6.343816494336352e-05 17.977996826171875 2.0210323333740234\n",
            "21061 4.296965926187113e-06 5.947105819359422e-05 Traning Loss: 6.376802048180252e-05 17.978200912475586 2.020965099334717\n",
            "21062 4.128799901081948e-06 5.989229612168856e-05 Traning Loss: 6.402109283953905e-05 17.97840690612793 2.020900011062622\n",
            "21063 4.286813691578573e-06 5.979976413073018e-05 Traning Loss: 6.40865764580667e-05 17.978605270385742 2.02083158493042\n",
            "21064 4.053586962982081e-06 5.988385601085611e-05 Traning Loss: 6.393744115484878e-05 17.978805541992188 2.020765781402588\n",
            "21065 4.171643013251014e-06 5.9458801842993125e-05 Traning Loss: 6.363044667523354e-05 17.9789981842041 2.0206968784332275\n",
            "21066 3.960752110288013e-06 5.9301150031387806e-05 Traning Loss: 6.326190487015992e-05 17.97919273376465 2.020630359649658\n",
            "21067 4.021123913844349e-06 5.8913497923640534e-05 Traning Loss: 6.293462502071634e-05 17.979381561279297 2.020562171936035\n",
            "21068 3.902090611518361e-06 5.880835669813678e-05 Traning Loss: 6.271044549066573e-05 17.979570388793945 2.0204954147338867\n",
            "21069 3.908434791810578e-06 5.8700130466604605e-05 Traning Loss: 6.260856753215194e-05 17.979755401611328 2.020428419113159\n",
            "21070 3.891580945492024e-06 5.8719659136841074e-05 Traning Loss: 6.261123780859634e-05 17.979936599731445 2.0203616619110107\n",
            "21071 3.846834715659497e-06 5.882783807464875e-05 Traning Loss: 6.26746696070768e-05 17.980117797851562 2.0202958583831787\n",
            "21072 3.894482233590679e-06 5.885380960535258e-05 Traning Loss: 6.274829502217472e-05 17.98029327392578 2.0202295780181885\n",
            "21073 3.807503617281327e-06 5.898017843719572e-05 Traning Loss: 6.278767978074029e-05 17.980466842651367 2.020164728164673\n",
            "21074 3.87007958124741e-06 5.8901900047203526e-05 Traning Loss: 6.277197826420888e-05 17.980634689331055 2.02009916305542\n",
            "21075 3.761414518521633e-06 5.89355404372327e-05 Traning Loss: 6.269695586524904e-05 17.980802536010742 2.0200350284576416\n",
            "21076 3.8056889479776146e-06 5.8773945056600496e-05 Traning Loss: 6.257963104872033e-05 17.98096466064453 2.019970417022705\n",
            "21077 3.7064842217660043e-06 5.873356349184178e-05 Traning Loss: 6.244004907784984e-05 17.98112678527832 2.019907236099243\n",
            "21078 3.720602535395301e-06 5.857971336808987e-05 Traning Loss: 6.230031431186944e-05 17.98128318786621 2.0198440551757812\n",
            "21079 3.6564888432621956e-06 5.8518213336355984e-05 Traning Loss: 6.217470217961818e-05 17.9814395904541 2.0197818279266357\n",
            "21080 3.6393334994500037e-06 5.844163752044551e-05 Traning Loss: 6.208097329363227e-05 17.981592178344727 2.0197203159332275\n",
            "21081 3.6188419016980333e-06 5.8396999520482495e-05 Traning Loss: 6.20158389210701e-05 17.98174285888672 2.0196590423583984\n",
            "21082 3.5726991427509347e-06 5.840193625772372e-05 Traning Loss: 6.197463517310098e-05 17.981891632080078 2.019599199295044\n",
            "21083 3.589645984902745e-06 5.836394120706245e-05 Traning Loss: 6.195358582772315e-05 17.982036590576172 2.0195388793945312\n",
            "21084 3.5194327665521996e-06 5.842053724336438e-05 Traning Loss: 6.193997251102701e-05 17.982181549072266 2.0194804668426514\n",
            "21085 3.560414143066737e-06 5.8365589211462066e-05 Traning Loss: 6.192600267240778e-05 17.982322692871094 2.019421339035034\n",
            "21086 3.4729507660813397e-06 5.843243343406357e-05 Traning Loss: 6.190538260852918e-05 17.98246192932129 2.019364356994629\n",
            "21087 3.5242985632066848e-06 5.835107367602177e-05 Traning Loss: 6.187536928337067e-05 17.98259735107422 2.019306182861328\n",
            "21088 3.42730754709919e-06 5.840896119480021e-05 Traning Loss: 6.183626828715205e-05 17.98273277282715 2.0192503929138184\n",
            "21089 3.4798611068254104e-06 5.830721784150228e-05 Traning Loss: 6.178707553772256e-05 17.98286247253418 2.019193410873413\n",
            "21090 3.380246880624327e-06 5.8351455663796514e-05 Traning Loss: 6.173170550027862e-05 17.98299217224121 2.019138813018799\n",
            "21091 3.4287540984223597e-06 5.824468098580837e-05 Traning Loss: 6.167343235574663e-05 17.983118057250977 2.019083023071289\n",
            "21092 3.3315079690510174e-06 5.828053326695226e-05 Traning Loss: 6.161203782539815e-05 17.983243942260742 2.0190296173095703\n",
            "21093 3.376072299943189e-06 5.8179066400043666e-05 Traning Loss: 6.155513983685523e-05 17.983366012573242 2.018974781036377\n",
            "21094 3.2833258956088684e-06 5.821713784825988e-05 Traning Loss: 6.150046101538464e-05 17.98348617553711 2.0189223289489746\n",
            "21095 3.32676154357614e-06 5.812508970848285e-05 Traning Loss: 6.145185034256428e-05 17.983604431152344 2.0188684463500977\n",
            "21096 3.2366945106332423e-06 5.817197961732745e-05 Traning Loss: 6.140867481008172e-05 17.983720779418945 2.018817186355591\n",
            "21097 3.284126478320104e-06 5.80901323701255e-05 Traning Loss: 6.137425953056663e-05 17.983835220336914 2.018764019012451\n",
            "21098 3.1908029995975085e-06 5.815580516355112e-05 Traning Loss: 6.134661089163274e-05 17.983949661254883 2.01871395111084\n",
            "21099 3.250731197113055e-06 5.8079658629139885e-05 Traning Loss: 6.133039278211072e-05 17.984060287475586 2.0186614990234375\n",
            "21100 3.145687742289738e-06 5.8182846260024235e-05 Traning Loss: 6.13285374129191e-05 17.98417091369629 2.0186126232147217\n",
            "21101 3.2310138067259686e-06 5.811679147882387e-05 Traning Loss: 6.134780414868146e-05 17.984277725219727 2.0185604095458984\n",
            "21102 3.1024426334624877e-06 5.82931206736248e-05 Traning Loss: 6.139556353446096e-05 17.984384536743164 2.0185129642486572\n",
            "21103 3.2333693980035605e-06 5.8251309383194894e-05 Traning Loss: 6.148467946331948e-05 17.984487533569336 2.018460750579834\n",
            "21104 3.0663782126794104e-06 5.857469295733608e-05 Traning Loss: 6.164106889627874e-05 17.984590530395508 2.0184152126312256\n",
            "21105 3.277098585385829e-06 5.861952013219707e-05 Traning Loss: 6.189661507960409e-05 17.984689712524414 2.018362283706665\n",
            "21106 3.0548765153071145e-06 5.925298682996072e-05 Traning Loss: 6.230786675587296e-05 17.984790802001953 2.0183191299438477\n",
            "21107 3.4070515084749786e-06 5.9557562053669244e-05 Traning Loss: 6.2964616518002e-05 17.984886169433594 2.0182645320892334\n",
            "21108 3.1174120067589683e-06 6.0899816162418574e-05 Traning Loss: 6.401722930604592e-05 17.984983444213867 2.0182249546051025\n",
            "21109 3.732978029802325e-06 6.195477908477187e-05 Traning Loss: 6.568776007043198e-05 17.985074996948242 2.01816725730896\n",
            "21110 3.3940359571715817e-06 6.4982617914211e-05 Traning Loss: 6.837665569037199e-05 17.985170364379883 2.0181329250335693\n",
            "21111 4.532433649728773e-06 6.815427332185209e-05 Traning Loss: 7.268670742632821e-05 17.985258102416992 2.0180697441101074\n",
            "21112 4.2748260966618545e-06 7.543707761215046e-05 Traning Loss: 7.971190643729642e-05 17.9853515625 2.0180439949035645\n",
            "21113 6.533003670483595e-06 8.454412454739213e-05 Traning Loss: 9.107712685363367e-05 17.985431671142578 2.0179715156555176\n",
            "21114 6.8633153205155395e-06 0.00010302741429768503 Traning Loss: 0.00010989073052769527 17.98552131652832 2.0179593563079834\n",
            "21115 1.1710159924405161e-05 0.00012886319018434733 Traning Loss: 0.0001405733491992578 17.98558807373047 2.0178706645965576\n",
            "21116 1.4305879631137941e-05 0.00017783661314751953 Traning Loss: 0.00019214249914512038 17.985671997070312 2.017881155014038\n",
            "21117 2.55175655183848e-05 0.0002507997560314834 Traning Loss: 0.000276317325187847 17.985715866088867 2.017765522003174\n",
            "21118 3.5513530747266486e-05 0.0003838678530883044 Traning Loss: 0.0004193813947495073 17.985782623291016 2.0178136825561523\n",
            "21119 6.273273174883798e-05 0.000586346723139286 Traning Loss: 0.0006490794476121664 17.985774993896484 2.0176539421081543\n",
            "21120 9.434686944587156e-05 0.0009436437394469976 Traning Loss: 0.001037990557961166 17.985790252685547 2.0177643299102783\n",
            "21121 0.0001590109313838184 0.0014659640146419406 Traning Loss: 0.001624974887818098 17.985660552978516 2.017537832260132\n",
            "21122 0.00024178170133382082 0.002326238201931119 Traning Loss: 0.0025680200196802616 17.985530853271484 2.0177409648895264\n",
            "21123 0.00036632391856983304 0.0033736960031092167 Traning Loss: 0.003740019863471389 17.98513412475586 2.0174365043640137\n",
            "21124 0.0005041756085120142 0.00475298659875989 Traning Loss: 0.005257162265479565 17.984668731689453 2.0177395343780518\n",
            "21125 0.0005976576940156519 0.005499427206814289 Traning Loss: 0.00609708484262228 17.983835220336914 2.0174129009246826\n",
            "21126 0.0005911695770919323 0.005513398442417383 Traning Loss: 0.0061045680195093155 17.98292350769043 2.0177276134490967\n",
            "21127 0.00040739099495112896 0.003702680580317974 Traning Loss: 0.004110071808099747 17.98179817199707 2.0175387859344482\n",
            "21128 0.00015659058408346027 0.0014755154261365533 Traning Loss: 0.0016321060247719288 17.98077392578125 2.0176844596862793\n",
            "21129 1.9396627976675518e-05 0.00014147133333608508 Traning Loss: 0.0001608679594937712 17.97983169555664 2.017752170562744\n",
            "21130 6.688306893920526e-05 0.0005542493890970945 Traning Loss: 0.000621132436208427 17.97899055480957 2.0176570415496826\n",
            "21131 0.00018938710854854435 0.0018508500652387738 Traning Loss: 0.0020402371883392334 17.978252410888672 2.017890214920044\n",
            "21132 0.00026791414711624384 0.002319155726581812 Traning Loss: 0.002587069757282734 17.977453231811523 2.01774001121521\n",
            "21133 0.00016845279606059194 0.001598958158865571 Traning Loss: 0.001767410896718502 17.976715087890625 2.0179243087768555\n",
            "21134 5.48962561879307e-05 0.0004014772712253034 Traning Loss: 0.0004563735274132341 17.975976943969727 2.017975091934204\n",
            "21135 2.4186039809137583e-05 0.00014429436123464257 Traning Loss: 0.00016848040104378015 17.97528648376465 2.017997980117798\n",
            "21136 9.056939597940072e-05 0.0008314974838867784 Traning Loss: 0.0009220668580383062 17.97465705871582 2.018230438232422\n",
            "21137 0.00015795495710335672 0.001321729039773345 Traning Loss: 0.0014796840259805322 17.974042892456055 2.01818585395813\n",
            "21138 0.00010786415805341676 0.0010292730294167995 Traning Loss: 0.0011371371801942587 17.973543167114258 2.0183725357055664\n",
            "21139 4.37125563621521e-05 0.0003019964206032455 Traning Loss: 0.0003457089769653976 17.973119735717773 2.0184333324432373\n",
            "21140 1.8838221876649186e-05 9.31412068894133e-05 Traning Loss: 0.00011197943240404129 17.972755432128906 2.0184578895568848\n",
            "21141 5.445064016385004e-05 0.00048571976367384195 Traning Loss: 0.0005401704111136496 17.972448348999023 2.018646478652954\n",
            "21142 9.865799802355468e-05 0.0007887311512604356 Traning Loss: 0.0008873891783878207 17.972089767456055 2.0186214447021484\n",
            "21143 6.75837800372392e-05 0.0006193080334924161 Traning Loss: 0.0006868918426334858 17.971750259399414 2.018777370452881\n",
            "21144 2.943484651041217e-05 0.00019442984194029123 Traning Loss: 0.0002238646848127246 17.971435546875 2.01884126663208\n",
            "21145 1.4797679796174634e-05 8.006933057913557e-05 Traning Loss: 9.48670131037943e-05 17.971153259277344 2.018860340118408\n",
            "21146 3.3643158531049266e-05 0.00032053812174126506 Traning Loss: 0.0003541812766343355 17.97093963623047 2.0189874172210693\n",
            "21147 6.452824891312048e-05 0.0005031054024584591 Traning Loss: 0.0005676336586475372 17.970714569091797 2.01892352104187\n",
            "21148 4.2658964957809076e-05 0.0004154872731305659 Traning Loss: 0.00045814624172635376 17.970544815063477 2.01899790763855\n",
            "21149 2.231759572168812e-05 0.00015659979544579983 Traning Loss: 0.00017891739844344556 17.97036361694336 2.018975257873535\n",
            "21150 9.566529115545563e-06 6.512364052468911e-05 Traning Loss: 7.469017145922408e-05 17.970191955566406 2.0189335346221924\n",
            "21151 1.918987800308969e-05 0.00019074177544098347 Traning Loss: 0.00020993164798710495 17.970050811767578 2.0189666748046875\n",
            "21152 3.827290493063629e-05 0.0003177098114974797 Traning Loss: 0.00035598271642811596 17.96990966796875 2.018857717514038\n",
            "21153 2.746234167716466e-05 0.00029793265275657177 Traning Loss: 0.00032539499807171524 17.969823837280273 2.0188510417938232\n",
            "21154 1.7026333807734773e-05 0.00014789628039579839 Traning Loss: 0.00016492261784151196 17.969768524169922 2.0187530517578125\n",
            "21155 4.942592568113469e-06 6.481823947979137e-05 Traning Loss: 6.976083386689425e-05 17.969762802124023 2.0186550617218018\n",
            "21156 8.829015314404387e-06 0.00011150931095471606 Traning Loss: 0.00012033832899760455 17.96978759765625 2.0185978412628174\n",
            "21157 2.1896234102314338e-05 0.00019720743875950575 Traning Loss: 0.0002191036764997989 17.969812393188477 2.0184450149536133\n",
            "21158 1.917642293847166e-05 0.00021737605857197195 Traning Loss: 0.00023655248514842242 17.969865798950195 2.018383741378784\n",
            "21159 1.5090480701474007e-05 0.00014107498282101005 Traning Loss: 0.00015616546443197876 17.969928741455078 2.018249750137329\n",
            "21160 5.245896318228915e-06 7.286467007361352e-05 Traning Loss: 7.811056275386363e-05 17.970014572143555 2.0181329250335693\n",
            "21161 4.913328666589223e-06 7.37441805540584e-05 Traning Loss: 7.865751103963703e-05 17.97014045715332 2.018040180206299\n",
            "21162 1.3238019164418802e-05 0.00012140772014390677 Traning Loss: 0.00013464574294630438 17.970273971557617 2.0178816318511963\n",
            "21163 1.2547679034469184e-05 0.00015581975458189845 Traning Loss: 0.00016836743452586234 17.970439910888672 2.0178022384643555\n",
            "21164 1.4251971151679754e-05 0.00012638169573619962 Traning Loss: 0.00014063366688787937 17.970592498779297 2.0176587104797363\n",
            "21165 6.298199423326878e-06 8.17896579974331e-05 Traning Loss: 8.808785787550732e-05 17.970760345458984 2.0175628662109375\n",
            "21166 5.273824172036257e-06 6.0837046476081014e-05 Traning Loss: 6.611087155761197e-05 17.970928192138672 2.017474889755249\n",
            "21167 8.065055226325057e-06 7.984128751559183e-05 Traning Loss: 8.790634456090629e-05 17.971097946166992 2.017362117767334\n",
            "21168 8.57798477227334e-06 0.00010948359704343602 Traning Loss: 0.00011806157999671996 17.971290588378906 2.0173096656799316\n",
            "21169 1.1500529581098817e-05 0.00010761557496152818 Traning Loss: 0.0001191161063616164 17.971492767333984 2.0172107219696045\n",
            "21170 6.4526661844865885e-06 8.608619100414217e-05 Traning Loss: 9.253885946236551e-05 17.971717834472656 2.0171573162078857\n",
            "21171 5.448746378533542e-06 6.261593080125749e-05 Traning Loss: 6.806467717979103e-05 17.971956253051758 2.0171005725860596\n",
            "21172 5.740621872973861e-06 6.253914762055501e-05 Traning Loss: 6.827976903878152e-05 17.972198486328125 2.0170397758483887\n",
            "21173 6.111750735726673e-06 7.971782179083675e-05 Traning Loss: 8.582957525504753e-05 17.97245216369629 2.017025947570801\n",
            "21174 9.325809514848515e-06 8.772546425461769e-05 Traning Loss: 9.705127740744501e-05 17.972694396972656 2.016976833343506\n",
            "21175 6.310643584583886e-06 8.355208410648629e-05 Traning Loss: 8.986272587208077e-05 17.972951889038086 2.016977071762085\n",
            "21176 6.3831057559582405e-06 6.685766129521653e-05 Traning Loss: 7.324076432269067e-05 17.973209381103516 2.0169601440429688\n",
            "21177 4.6738082346564624e-06 5.9648027672665194e-05 Traning Loss: 6.432183727156371e-05 17.973474502563477 2.0169529914855957\n",
            "21178 4.850914137932705e-06 6.428129563573748e-05 Traning Loss: 6.913221295690164e-05 17.973745346069336 2.0169668197631836\n",
            "21179 6.954701802897034e-06 7.204020221251994e-05 Traning Loss: 7.899490447016433e-05 17.97401237487793 2.016958236694336\n",
            "21180 5.778207651019329e-06 7.632983761141077e-05 Traning Loss: 8.210804662667215e-05 17.974279403686523 2.0169854164123535\n",
            "21181 6.4901519181148615e-06 6.910393130965531e-05 Traning Loss: 7.559408550150692e-05 17.974536895751953 2.0169918537139893\n",
            "21182 4.716190687759081e-06 6.207686965353787e-05 Traning Loss: 6.67930580675602e-05 17.97479248046875 2.0170140266418457\n",
            "21183 4.478423306863988e-06 5.9372690884629264e-05 Traning Loss: 6.38511119177565e-05 17.97504997253418 2.0170416831970215\n",
            "21184 5.358147518563783e-06 6.243440293474123e-05 Traning Loss: 6.779254908906296e-05 17.97530174255371 2.0170528888702393\n",
            "21185 4.663316758524161e-06 6.826583557995036e-05 Traning Loss: 7.292914960999042e-05 17.975563049316406 2.017089605331421\n",
            "21186 6.038421815901529e-06 6.759280222468078e-05 Traning Loss: 7.36312213120982e-05 17.975814819335938 2.0170998573303223\n",
            "21187 4.401904789119726e-06 6.529328675242141e-05 Traning Loss: 6.969519017729908e-05 17.976072311401367 2.0171313285827637\n",
            "21188 4.792498202732531e-06 6.052032404113561e-05 Traning Loss: 6.531282269861549e-05 17.976318359375 2.01715087890625\n",
            "21189 4.272931164450711e-06 6.012854282744229e-05 Traning Loss: 6.440147262765095e-05 17.976564407348633 2.0171713829040527\n",
            "21190 4.386351974972058e-06 6.267753633437678e-05 Traning Loss: 6.706389103783295e-05 17.976804733276367 2.017198324203491\n",
            "21191 4.9889345064002555e-06 6.535452121170238e-05 Traning Loss: 7.034345617284998e-05 17.977046966552734 2.0172104835510254\n",
            "21192 4.585303940984886e-06 6.700746598653495e-05 Traning Loss: 7.159276719903573e-05 17.977283477783203 2.017232656478882\n",
            "21193 4.798014288098784e-06 6.600435153814033e-05 Traning Loss: 7.080236537149176e-05 17.97752571105957 2.017244815826416\n",
            "21194 4.745072601508582e-06 6.557908636750653e-05 Traning Loss: 7.032416033325717e-05 17.977754592895508 2.017253875732422\n",
            "21195 4.545502633845899e-06 6.793934880988672e-05 Traning Loss: 7.248485053423792e-05 17.97799301147461 2.017271041870117\n",
            "21196 5.957469056738773e-06 7.20564421499148e-05 Traning Loss: 7.801390893291682e-05 17.978206634521484 2.0172677040100098\n",
            "21197 5.5040763982106e-06 8.072327909758314e-05 Traning Loss: 8.622735913377255e-05 17.978439331054688 2.0172884464263916\n",
            "21198 8.022729161893949e-06 8.890615572454408e-05 Traning Loss: 9.692888124845922e-05 17.978635787963867 2.017277717590332\n",
            "21199 7.68665995565243e-06 0.00010400516475783661 Traning Loss: 0.00011169182835146785 17.978862762451172 2.017296552658081\n",
            "21200 1.1632996574917343e-05 0.00012307502038311213 Traning Loss: 0.00013470801059156656 17.97904396057129 2.017282009124756\n",
            "21201 1.3166183634893969e-05 0.00015844980953261256 Traning Loss: 0.00017161598952952772 17.97926902770996 2.0172972679138184\n",
            "21202 2.0878496798104607e-05 0.00021044757158961147 Traning Loss: 0.00023132606293074787 17.97942543029785 2.017277717590332\n",
            "21203 2.722213503147941e-05 0.0002970539790112525 Traning Loss: 0.0003242761013098061 17.979642868041992 2.017296075820923\n",
            "21204 4.425250517670065e-05 0.00042790849693119526 Traning Loss: 0.0004721609875559807 17.97974967956543 2.0172646045684814\n",
            "21205 6.211142317624763e-05 0.0006374756922014058 Traning Loss: 0.0006995871081016958 17.97994613647461 2.0172972679138184\n",
            "21206 0.00010185220162384212 0.0009619073825888336 Traning Loss: 0.0010637595551088452 17.979955673217773 2.0172438621520996\n",
            "21207 0.000147755112266168 0.0014623897150158882 Traning Loss: 0.0016101448563858867 17.980091094970703 2.017302989959717\n",
            "21208 0.0002376378106418997 0.0022250593174248934 Traning Loss: 0.0024626972153782845 17.979907989501953 2.017218589782715\n",
            "21209 0.0003384448355063796 0.003286659950390458 Traning Loss: 0.003625104669481516 17.979877471923828 2.01731538772583\n",
            "21210 0.0005070996121503413 0.004740559495985508 Traning Loss: 0.005247659049928188 17.97933006286621 2.017200469970703\n",
            "21211 0.0006480491138063371 0.006243629846721888 Traning Loss: 0.006891679018735886 17.978940963745117 2.017338991165161\n",
            "21212 0.0008117223042063415 0.007575689349323511 Traning Loss: 0.008387411944568157 17.97789192199707 2.0172202587127686\n",
            "21213 0.0007820333703421056 0.0075188069604337215 Traning Loss: 0.0083008399233222 17.97705841064453 2.0173778533935547\n",
            "21214 0.0006489899242296815 0.00596456415951252 Traning Loss: 0.0066135539673268795 17.975778579711914 2.0173280239105225\n",
            "21215 0.0003221716615371406 0.0030387432780116796 Traning Loss: 0.003360914997756481 17.974836349487305 2.017456531524658\n",
            "21216 9.138127643382177e-05 0.0006301841931417584 Traning Loss: 0.0007215654477477074 17.973953247070312 2.0175564289093018\n",
            "21217 4.6368488256121054e-05 0.00015576805162709206 Traning Loss: 0.0002021365362452343 17.973182678222656 2.0176477432250977\n",
            "21218 0.00016914393927436322 0.001424747402779758 Traning Loss: 0.0015938912983983755 17.97263526916504 2.017893075942993\n",
            "21219 0.0003422406443860382 0.00278031500056386 Traning Loss: 0.0031225555576384068 17.971784591674805 2.018014430999756\n",
            "21220 0.00031114890589378774 0.0027535802219063044 Traning Loss: 0.0030647290404886007 17.971105575561523 2.018315315246582\n",
            "21221 0.0002004386915359646 0.0014334979932755232 Traning Loss: 0.0016339366557076573 17.97024154663086 2.018540382385254\n",
            "21222 6.495966954389587e-05 0.000230384583119303 Traning Loss: 0.00029534424538724124 17.969484329223633 2.0188112258911133\n",
            "21223 6.661075167357922e-05 0.0002418613585177809 Traning Loss: 0.0003084721101913601 17.96882438659668 2.019139289855957\n",
            "21224 0.00016822390898596495 0.001101682079024613 Traning Loss: 0.0012699059443548322 17.968006134033203 2.019380807876587\n",
            "21225 0.0002072497591143474 0.001617366331629455 Traning Loss: 0.0018246161052957177 17.967342376708984 2.0197441577911377\n",
            "21226 0.00017295032739639282 0.0011843180982396007 Traning Loss: 0.0013572684256359935 17.966527938842773 2.0200226306915283\n",
            "21227 7.857079617679119e-05 0.0003660182701423764 Traning Loss: 0.0004445890663191676 17.965852737426758 2.02034854888916\n",
            "21228 4.61657764390111e-05 6.798699905630201e-05 Traning Loss: 0.00011415277549531311 17.965272903442383 2.020689010620117\n",
            "21229 8.72821910888888e-05 0.00045323753147386014 Traning Loss: 0.0005405197152867913 17.96466636657715 2.0209689140319824\n",
            "21230 0.00012570875696837902 0.0009044334874488413 Traning Loss: 0.0010301421862095594 17.9642333984375 2.021326780319214\n",
            "21231 0.00012317868822719902 0.0008536783279851079 Traning Loss: 0.0009768570307642221 17.96371078491211 2.0216057300567627\n",
            "21232 6.850674981251359e-05 0.000401566008804366 Traning Loss: 0.0004700727586168796 17.96333122253418 2.0219218730926514\n",
            "21233 3.2972013286780566e-05 7.353268301812932e-05 Traning Loss: 0.00010650469630490988 17.963001251220703 2.022221803665161\n",
            "21234 4.098640783922747e-05 0.00016067293472588062 Traning Loss: 0.00020165933528915048 17.96268081665039 2.022470474243164\n",
            "21235 6.546960503328592e-05 0.00045966985635459423 Traning Loss: 0.0005251394468359649 17.962486267089844 2.0227558612823486\n",
            "21236 8.073603385128081e-05 0.0005820792284794152 Traning Loss: 0.0006628152914345264 17.96221160888672 2.0229599475860596\n",
            "21237 5.5754746426828206e-05 0.0004142264660913497 Traning Loss: 0.00046998122707009315 17.96204948425293 2.023183584213257\n",
            "21238 2.922672138083726e-05 0.00015345588326454163 Traning Loss: 0.0001826826046453789 17.961881637573242 2.0233676433563232\n",
            "21239 1.7531101548229344e-05 6.0044538258807734e-05 Traning Loss: 7.757564162602648e-05 17.961746215820312 2.023516893386841\n",
            "21240 2.5002078473335132e-05 0.00017589023627806455 Traning Loss: 0.0002008923183893785 17.961687088012695 2.02367901802063\n",
            "21241 4.445949161890894e-05 0.0003261536476202309 Traning Loss: 0.0003706131246872246 17.961570739746094 2.02377986907959\n",
            "21242 4.0159266063710675e-05 0.0003571707056835294 Traning Loss: 0.00039732997538521886 17.961551666259766 2.0239028930664062\n",
            "21243 3.162295251968317e-05 0.00023947030422277749 Traning Loss: 0.00027109324582852423 17.961503982543945 2.0239858627319336\n",
            "21244 1.4281939911597874e-05 0.00010633347847033292 Traning Loss: 0.0001206154192914255 17.961524963378906 2.024056911468506\n",
            "21245 9.200567546940874e-06 6.597017636522651e-05 Traning Loss: 7.517074118368328e-05 17.961599349975586 2.0241262912750244\n",
            "21246 1.7583879525773227e-05 0.00012551475083455443 Traning Loss: 0.00014309863036032766 17.9616756439209 2.024155616760254\n",
            "21247 2.2049600374884903e-05 0.0002101360441884026 Traning Loss: 0.0002321856445632875 17.961835861206055 2.0242013931274414\n",
            "21248 2.745627898548264e-05 0.00022567340056411922 Traning Loss: 0.0002531296922825277 17.96196174621582 2.0242114067077637\n",
            "21249 1.8535245544626378e-05 0.0001730887161102146 Traning Loss: 0.00019162395619787276 17.962154388427734 2.0242257118225098\n",
            "21250 1.2189261724415701e-05 9.696026245364919e-05 Traning Loss: 0.00010914952144958079 17.96234130859375 2.0242295265197754\n",
            "21251 8.58086787047796e-06 6.24458261881955e-05 Traning Loss: 7.102669042069465e-05 17.96254539489746 2.0242156982421875\n",
            "21252 9.387997124576941e-06 8.472731133224443e-05 Traning Loss: 9.411531209480017e-05 17.962785720825195 2.02421236038208\n",
            "21253 1.589456769579556e-05 0.00012647183029912412 Traning Loss: 0.00014236639253795147 17.962997436523438 2.0241851806640625\n",
            "21254 1.5871974028414115e-05 0.0001508446002844721 Traning Loss: 0.00016671657795086503 17.9632568359375 2.0241684913635254\n",
            "21255 1.585963946126867e-05 0.00013237223902251571 Traning Loss: 0.00014823187666479498 17.963489532470703 2.0241377353668213\n",
            "21256 1.0593736078590155e-05 9.444557508686557e-05 Traning Loss: 0.00010503931116545573 17.96375846862793 2.0241048336029053\n",
            "21257 7.78340927354293e-06 6.476632552221417e-05 Traning Loss: 7.254973752424121e-05 17.964035034179688 2.0240728855133057\n",
            "21258 7.958794412843417e-06 6.282922549871728e-05 Traning Loss: 7.078801718307659e-05 17.96431541442871 2.024029016494751\n",
            "21259 8.89887087396346e-06 8.346443064510822e-05 Traning Loss: 9.236330515705049e-05 17.964630126953125 2.0239930152893066\n",
            "21260 1.2029440767946653e-05 0.0001014926892821677 Traning Loss: 0.00011352213186910376 17.964927673339844 2.0239455699920654\n",
            "21261 1.0894341357925441e-05 0.00010450698755448684 Traning Loss: 0.00011540132982190698 17.96526336669922 2.02390193939209\n",
            "21262 1.0219032446912024e-05 8.797787450021133e-05 Traning Loss: 9.819690603762865e-05 17.96558380126953 2.0238535404205322\n",
            "21263 7.592368547193473e-06 6.889648648211733e-05 Traning Loss: 7.648885366506875e-05 17.96592140197754 2.0238037109375\n",
            "21264 6.688204393867636e-06 5.9274232626194134e-05 Traning Loss: 6.596243474632502e-05 17.96626091003418 2.0237560272216797\n",
            "21265 7.443398772011278e-06 6.345459405565634e-05 Traning Loss: 7.089799328241497e-05 17.966590881347656 2.0237038135528564\n",
            "21266 7.913450644991826e-06 7.537917554145679e-05 Traning Loss: 8.329262345796451e-05 17.966938018798828 2.0236551761627197\n",
            "21267 9.389292245032266e-06 8.193100802600384e-05 Traning Loss: 9.13202966330573e-05 17.96726417541504 2.023601531982422\n",
            "21268 8.295308361994103e-06 8.037881343625486e-05 Traning Loss: 8.867411816027015e-05 17.967609405517578 2.023550033569336\n",
            "21269 7.901242497609928e-06 7.04892590874806e-05 Traning Loss: 7.839049794711173e-05 17.967941284179688 2.023495674133301\n",
            "21270 6.511245828733081e-06 6.190672138473019e-05 Traning Loss: 6.841796857770532e-05 17.968280792236328 2.023442506790161\n",
            "21271 6.26939618086908e-06 5.879246964468621e-05 Traning Loss: 6.506186764454469e-05 17.9686222076416 2.0233895778656006\n",
            "21272 6.693147042824421e-06 6.200164352776483e-05 Traning Loss: 6.869478966109455e-05 17.96895408630371 2.023336410522461\n",
            "21273 6.84991118760081e-06 6.789273174945265e-05 Traning Loss: 7.474264566553757e-05 17.969297409057617 2.0232856273651123\n",
            "21274 7.5766465670312755e-06 7.037991599645466e-05 Traning Loss: 7.795655983500183e-05 17.969623565673828 2.023232936859131\n",
            "21275 6.829141511843773e-06 6.928012589924037e-05 Traning Loss: 7.61092669563368e-05 17.969961166381836 2.023184061050415\n",
            "21276 6.776433565391926e-06 6.419612327590585e-05 Traning Loss: 7.097255729604512e-05 17.970285415649414 2.023132801055908\n",
            "21277 5.93729646425345e-06 6.021439912728965e-05 Traning Loss: 6.615169695578516e-05 17.970611572265625 2.0230844020843506\n",
            "21278 5.908314051339403e-06 5.845025589223951e-05 Traning Loss: 6.43585663056001e-05 17.970932006835938 2.02303409576416\n",
            "21279 5.9944777603959665e-06 5.9792255342472345e-05 Traning Loss: 6.578673492185771e-05 17.971242904663086 2.0229837894439697\n",
            "21280 6.017897248966619e-06 6.253413448575884e-05 Traning Loss: 6.855203537270427e-05 17.971555709838867 2.0229339599609375\n",
            "21281 6.412399670807645e-06 6.393736839527264e-05 Traning Loss: 7.034976442810148e-05 17.971853256225586 2.022881507873535\n",
            "21282 5.922850050410489e-06 6.404889427358285e-05 Traning Loss: 6.997174205025658e-05 17.97215461730957 2.022831678390503\n",
            "21283 6.012566700519528e-06 6.18267513345927e-05 Traning Loss: 6.783931894460693e-05 17.972442626953125 2.022777557373047\n",
            "21284 5.36788274985156e-06 5.997970583848655e-05 Traning Loss: 6.534758722409606e-05 17.972732543945312 2.0227255821228027\n",
            "21285 5.351524578145472e-06 5.848449291079305e-05 Traning Loss: 6.383601430570707e-05 17.9730167388916 2.022670269012451\n",
            "21286 5.182366749068024e-06 5.862296529812738e-05 Traning Loss: 6.380533159244806e-05 17.973297119140625 2.0226151943206787\n",
            "21287 5.143457201484125e-06 5.966899698250927e-05 Traning Loss: 6.48124550934881e-05 17.97357940673828 2.022559404373169\n",
            "21288 5.3554172154690605e-06 6.057378413970582e-05 Traning Loss: 6.592919817194343e-05 17.973852157592773 2.02250075340271\n",
            "21289 5.077643891127082e-06 6.129361281637102e-05 Traning Loss: 6.637125625275075e-05 17.974130630493164 2.0224432945251465\n",
            "21290 5.273229362501297e-06 6.0625916376011446e-05 Traning Loss: 6.589914846699685e-05 17.97439956665039 2.022381067276001\n",
            "21291 4.842679118155502e-06 5.99804989178665e-05 Traning Loss: 6.48231798550114e-05 17.974672317504883 2.0223207473754883\n",
            "21292 4.923614142171573e-06 5.879971286049113e-05 Traning Loss: 6.37233242741786e-05 17.974939346313477 2.0222561359405518\n",
            "21293 4.686358352046227e-06 5.8391808124724776e-05 Traning Loss: 6.307816511252895e-05 17.975204467773438 2.0221920013427734\n",
            "21294 4.690504283644259e-06 5.8352092310087755e-05 Traning Loss: 6.304259295575321e-05 17.975465774536133 2.0221261978149414\n",
            "21295 4.749411800730741e-06 5.8682584494818e-05 Traning Loss: 6.343199493130669e-05 17.975719451904297 2.0220582485198975\n",
            "21296 4.617923423211323e-06 5.927284655626863e-05 Traning Loss: 6.389077316271141e-05 17.97597312927246 2.02199125289917\n",
            "21297 4.7890412133710925e-06 5.931670966674574e-05 Traning Loss: 6.410574860638008e-05 17.97621726989746 2.021920919418335\n",
            "21298 4.521814844338223e-06 5.942760981270112e-05 Traning Loss: 6.394942465703934e-05 17.976463317871094 2.021852493286133\n",
            "21299 4.638121481548296e-06 5.8865098253590986e-05 Traning Loss: 6.350321928039193e-05 17.976699829101562 2.0217809677124023\n",
            "21300 4.390265530673787e-06 5.858292570337653e-05 Traning Loss: 6.297319487202913e-05 17.97693634033203 2.0217108726501465\n",
            "21301 4.418284333951306e-06 5.814623727928847e-05 Traning Loss: 6.256451888475567e-05 17.977169036865234 2.021639347076416\n",
            "21302 4.330684078013292e-06 5.805727050756104e-05 Traning Loss: 6.238795322133228e-05 17.977399826049805 2.0215678215026855\n",
            "21303 4.278684627934126e-06 5.815141776110977e-05 Traning Loss: 6.243010284379125e-05 17.977628707885742 2.0214970111846924\n",
            "21304 4.3436534724605735e-06 5.82403336011339e-05 Traning Loss: 6.258398934733123e-05 17.97785186767578 2.0214250087738037\n",
            "21305 4.216735305817565e-06 5.850085290148854e-05 Traning Loss: 6.271759048104286e-05 17.97807502746582 2.021354913711548\n",
            "21306 4.326775524532422e-06 5.841016536578536e-05 Traning Loss: 6.273694452829659e-05 17.978290557861328 2.0212831497192383\n",
            "21307 4.165202426520409e-06 5.845383202540688e-05 Traning Loss: 6.261903763515875e-05 17.978506088256836 2.0212135314941406\n",
            "21308 4.2360829866083805e-06 5.816922566737048e-05 Traning Loss: 6.240530638024211e-05 17.978715896606445 2.0211429595947266\n",
            "21309 4.11186738347169e-06 5.805346881970763e-05 Traning Loss: 6.216533802216873e-05 17.978923797607422 2.021073818206787\n",
            "21310 4.119373443245422e-06 5.7851957535604015e-05 Traning Loss: 6.197133188834414e-05 17.9791259765625 2.0210049152374268\n",
            "21311 4.076935965713346e-06 5.777596015832387e-05 Traning Loss: 6.185289385030046e-05 17.979324340820312 2.0209360122680664\n",
            "21312 4.026765964226797e-06 5.7790526625467464e-05 Traning Loss: 6.181729258969426e-05 17.979520797729492 2.0208683013916016\n",
            "21313 4.0506442928744946e-06 5.7779190683504567e-05 Traning Loss: 6.182983634062111e-05 17.979711532592773 2.0208001136779785\n",
            "21314 3.962721621064702e-06 5.789205897599459e-05 Traning Loss: 6.185477832332253e-05 17.979902267456055 2.0207338333129883\n",
            "21315 4.007159532193327e-06 5.784822496934794e-05 Traning Loss: 6.185538222780451e-05 17.980087280273438 2.02066707611084\n",
            "21316 3.909367933374597e-06 5.790416616946459e-05 Traning Loss: 6.181353091960773e-05 17.980270385742188 2.020601749420166\n",
            "21317 3.936888333555544e-06 5.779530692961998e-05 Traning Loss: 6.173219298943877e-05 17.980449676513672 2.020536422729492\n",
            "21318 3.8563666748814285e-06 5.7767370890360326e-05 Traning Loss: 6.162373756524175e-05 17.980627059936523 2.020472288131714\n",
            "21319 3.85246357836877e-06 5.765746027464047e-05 Traning Loss: 6.15099270362407e-05 17.98080062866211 2.0204086303710938\n",
            "21320 3.8054165543144336e-06 5.760254862252623e-05 Traning Loss: 6.140796176623553e-05 17.980972290039062 2.020345449447632\n",
            "21321 3.776121957344003e-06 5.755558959208429e-05 Traning Loss: 6.133170973043889e-05 17.98114013671875 2.0202832221984863\n",
            "21322 3.760531626539887e-06 5.751569551648572e-05 Traning Loss: 6.127623055363074e-05 17.981306076049805 2.02022123336792\n",
            "21323 3.7166491893003695e-06 5.752506694989279e-05 Traning Loss: 6.124171341070905e-05 17.981470108032227 2.02016019821167\n",
            "21324 3.7177233025431633e-06 5.749878619099036e-05 Traning Loss: 6.121650949353352e-05 17.981630325317383 2.020099401473999\n",
            "21325 3.6667408949142555e-06 5.752716606366448e-05 Traning Loss: 6.119391036918387e-05 17.981788635253906 2.0200395584106445\n",
            "21326 3.6688097679871134e-06 5.749319097958505e-05 Traning Loss: 6.116199801908806e-05 17.981943130493164 2.019979953765869\n",
            "21327 3.616751882873359e-06 5.7503544667270035e-05 Traning Loss: 6.112029950600117e-05 17.98209571838379 2.019921064376831\n",
            "21328 3.6111973713559564e-06 5.7456749345874414e-05 Traning Loss: 6.106794899096712e-05 17.98224449157715 2.019862651824951\n",
            "21329 3.5628111163532594e-06 5.744420559494756e-05 Traning Loss: 6.1007016483927146e-05 17.982391357421875 2.0198047161102295\n",
            "21330 3.54966346094443e-06 5.739055632147938e-05 Traning Loss: 6.0940219555050135e-05 17.982534408569336 2.019747257232666\n",
            "21331 3.508289864839753e-06 5.737035826314241e-05 Traning Loss: 6.087864676374011e-05 17.982675552368164 2.01969051361084\n",
            "21332 3.4899994716397487e-06 5.732715726480819e-05 Traning Loss: 6.081715764594264e-05 17.98281478881836 2.0196340084075928\n",
            "21333 3.456924559941399e-06 5.730717020924203e-05 Traning Loss: 6.0764094087062404e-05 17.982952117919922 2.019578218460083\n",
            "21334 3.4348165627307026e-06 5.7279881730210036e-05 Traning Loss: 6.071469761081971e-05 17.98308753967285 2.0195226669311523\n",
            "21335 3.409056034797686e-06 5.726376184611581e-05 Traning Loss: 6.067281719879247e-05 17.983219146728516 2.019467830657959\n",
            "21336 3.3833539418992586e-06 5.7251898397225887e-05 Traning Loss: 6.0635251429630443e-05 17.98335075378418 2.0194132328033447\n",
            "21337 3.3643445931375027e-06 5.723427966586314e-05 Traning Loss: 6.0598624259000644e-05 17.983478546142578 2.0193591117858887\n",
            "21338 3.334637995067169e-06 5.722798596252687e-05 Traning Loss: 6.0562622820725664e-05 17.983604431152344 2.019305467605591\n",
            "21339 3.32139052261482e-06 5.720364424632862e-05 Traning Loss: 6.052503522369079e-05 17.983728408813477 2.019252061843872\n",
            "21340 3.2878751881071366e-06 5.7197550631826743e-05 Traning Loss: 6.048542491043918e-05 17.983850479125977 2.0191991329193115\n",
            "21341 3.2783991628093645e-06 5.7168403145624325e-05 Traning Loss: 6.0446800489444286e-05 17.983970642089844 2.01914644241333\n",
            "21342 3.2422010463051265e-06 5.7160872529493645e-05 Traning Loss: 6.040307198418304e-05 17.984088897705078 2.019094467163086\n",
            "21343 3.2348023069062037e-06 5.712490019504912e-05 Traning Loss: 6.03597036388237e-05 17.98420524597168 2.019042491912842\n",
            "21344 3.197813384758774e-06 5.711583435186185e-05 Traning Loss: 6.031364682712592e-05 17.98431968688965 2.018990993499756\n",
            "21345 3.1909626159176696e-06 5.707967284251936e-05 Traning Loss: 6.0270635003689677e-05 17.984432220458984 2.018939733505249\n",
            "21346 3.156077355015441e-06 5.706945376005024e-05 Traning Loss: 6.0225531342439353e-05 17.984542846679688 2.0188889503479004\n",
            "21347 3.148232281091623e-06 5.703507486032322e-05 Traning Loss: 6.018330896040425e-05 17.984651565551758 2.0188381671905518\n",
            "21348 3.116900870736572e-06 5.702155613107607e-05 Traning Loss: 6.013845631969161e-05 17.984758377075195 2.0187878608703613\n",
            "21349 3.106726808255189e-06 5.698984023183584e-05 Traning Loss: 6.009656863170676e-05 17.98486328125 2.01873779296875\n",
            "21350 3.080034275626531e-06 5.697474261978641e-05 Traning Loss: 6.005477553117089e-05 17.984966278076172 2.018688201904297\n",
            "21351 3.066431872866815e-06 5.69474977965001e-05 Traning Loss: 6.0013928305124864e-05 17.98506736755371 2.0186386108398438\n",
            "21352 3.044451204914367e-06 5.6930017308332026e-05 Traning Loss: 5.997446714900434e-05 17.98516845703125 2.018589496612549\n",
            "21353 3.028155560969026e-06 5.690721081919037e-05 Traning Loss: 5.993536615278572e-05 17.985267639160156 2.018540620803833\n",
            "21354 3.0102839900791878e-06 5.688656528946012e-05 Traning Loss: 5.989685087115504e-05 17.98536491394043 2.0184919834136963\n",
            "21355 2.991609790115035e-06 5.68662399018649e-05 Traning Loss: 5.985784810036421e-05 17.98546028137207 2.0184435844421387\n",
            "21356 2.9768918921035947e-06 5.6844568462111056e-05 Traning Loss: 5.982146103633568e-05 17.985553741455078 2.01839542388916\n",
            "21357 2.956523303510039e-06 5.6827346270438284e-05 Traning Loss: 5.9783869801322e-05 17.985647201538086 2.0183475017547607\n",
            "21358 2.944028892670758e-06 5.680289177689701e-05 Traning Loss: 5.974691885057837e-05 17.98573875427246 2.0182998180389404\n",
            "21359 2.9229513529571705e-06 5.6788318033795804e-05 Traning Loss: 5.9711270296247676e-05 17.985828399658203 2.0182526111602783\n",
            "21360 2.9117954909452237e-06 5.676186992786825e-05 Traning Loss: 5.967366450931877e-05 17.985916137695312 2.018205404281616\n",
            "21361 2.8907038540637586e-06 5.674457497661933e-05 Traning Loss: 5.9635280194925144e-05 17.986003875732422 2.0181586742401123\n",
            "21362 2.8808178740291623e-06 5.6719603890087456e-05 Traning Loss: 5.960042108199559e-05 17.9860897064209 2.0181121826171875\n",
            "21363 2.8600948098755907e-06 5.67026472708676e-05 Traning Loss: 5.956274253549054e-05 17.986173629760742 2.018065929412842\n",
            "21364 2.85072201222647e-06 5.6677537941141054e-05 Traning Loss: 5.952826177235693e-05 17.986255645751953 2.018019914627075\n",
            "21365 2.8301128622842953e-06 5.666308425134048e-05 Traning Loss: 5.949319893261418e-05 17.986337661743164 2.017974376678467\n",
            "21366 2.8218692023074254e-06 5.6637334637343884e-05 Traning Loss: 5.945920565864071e-05 17.986417770385742 2.0179288387298584\n",
            "21367 2.8008412300550845e-06 5.662293187924661e-05 Traning Loss: 5.942377174505964e-05 17.986495971679688 2.017883777618408\n",
            "21368 2.7940086511080153e-06 5.6598441005917266e-05 Traning Loss: 5.939244874753058e-05 17.986572265625 2.017838954925537\n",
            "21369 2.772150537566631e-06 5.6587832659715787e-05 Traning Loss: 5.935998342465609e-05 17.986648559570312 2.017794370651245\n",
            "21370 2.7674902867147466e-06 5.656211942550726e-05 Traning Loss: 5.9329609939595684e-05 17.986722946166992 2.0177500247955322\n",
            "21371 2.7441974452813156e-06 5.655564382323064e-05 Traning Loss: 5.929984035901725e-05 17.986797332763672 2.0177061557769775\n",
            "21372 2.7430098725744756e-06 5.653127300320193e-05 Traning Loss: 5.9274283557897434e-05 17.98686981201172 2.017662286758423\n",
            "21373 2.716981953199138e-06 5.653371044900268e-05 Traning Loss: 5.9250691265333444e-05 17.986942291259766 2.0176188945770264\n",
            "21374 2.7211149244976696e-06 5.6513094023102894e-05 Traning Loss: 5.923420758335851e-05 17.987010955810547 2.01757550239563\n",
            "21375 2.6914856334769865e-06 5.6530192523496225e-05 Traning Loss: 5.922167838434689e-05 17.98708152770996 2.0175328254699707\n",
            "21376 2.7037615382141666e-06 5.651728861266747e-05 Traning Loss: 5.922104901401326e-05 17.98714828491211 2.0174899101257324\n",
            "21377 2.669277591849095e-06 5.65649606869556e-05 Traning Loss: 5.923423668718897e-05 17.98721694946289 2.0174477100372314\n",
            "21378 2.6943896500597475e-06 5.6574648624518886e-05 Traning Loss: 5.926903759245761e-05 17.987281799316406 2.0174052715301514\n",
            "21379 2.6546292701823404e-06 5.6681816204218194e-05 Traning Loss: 5.933644570177421e-05 17.987348556518555 2.0173637866973877\n",
            "21380 2.700699269553297e-06 5.6753015087451786e-05 Traning Loss: 5.945371594862081e-05 17.987409591674805 2.0173215866088867\n",
            "21381 2.656448032212211e-06 5.698729000869207e-05 Traning Loss: 5.964373849565163e-05 17.98747444152832 2.0172808170318604\n",
            "21382 2.7400437829783186e-06 5.720805347664282e-05 Traning Loss: 5.994809907861054e-05 17.987533569335938 2.0172388553619385\n",
            "21383 2.6985135264112614e-06 5.7726130762603134e-05 Traning Loss: 6.04246451985091e-05 17.98759651184082 2.0171988010406494\n",
            "21384 2.8537258458527504e-06 5.8320158132119104e-05 Traning Loss: 6.117388693382964e-05 17.987651824951172 2.0171570777893066\n",
            "21385 2.8401213967299554e-06 5.950335616944358e-05 Traning Loss: 6.234347529243678e-05 17.987714767456055 2.017117977142334\n",
            "21386 3.1433319236384705e-06 6.10393108217977e-05 Traning Loss: 6.418264092644677e-05 17.98776626586914 2.017076253890991\n",
            "21387 3.2332745831809007e-06 6.384697189787403e-05 Traning Loss: 6.708024739054963e-05 17.987829208374023 2.017038583755493\n",
            "21388 3.867520263156621e-06 6.782251148251817e-05 Traning Loss: 7.169003220042214e-05 17.98787498474121 2.016996383666992\n",
            "21389 4.279217137082014e-06 7.474258018191904e-05 Traning Loss: 7.902179640950635e-05 17.987939834594727 2.016960382461548\n",
            "21390 5.703878287022235e-06 8.515729132341221e-05 Traning Loss: 9.08611691556871e-05 17.98797607421875 2.0169172286987305\n",
            "21391 7.058418759697815e-06 0.00010284669406246394 Traning Loss: 0.00010990511509589851 17.988040924072266 2.016883611679077\n",
            "21392 1.0479748198122252e-05 0.0001306462218053639 Traning Loss: 0.00014112597273197025 17.98806381225586 2.016838312149048\n",
            "21393 1.4535309674101882e-05 0.00017725257202982903 Traning Loss: 0.0001917878835229203 17.988130569458008 2.016808271408081\n",
            "21394 2.3214704924612306e-05 0.0002528356562834233 Traning Loss: 0.0002760503557510674 17.988122940063477 2.0167593955993652\n",
            "21395 3.4911074180854484e-05 0.0003782327694352716 Traning Loss: 0.0004131438327021897 17.988183975219727 2.016735076904297\n",
            "21396 5.7735236623557284e-05 0.0005856993957422674 Traning Loss: 0.0006434346432797611 17.988117218017578 2.0166797637939453\n",
            "21397 9.041016164701432e-05 0.0009229155257344246 Traning Loss: 0.0010133256437256932 17.988147735595703 2.016664505004883\n",
            "21398 0.00015036809782031924 0.0014813215238973498 Traning Loss: 0.0016316896071657538 17.987943649291992 2.0165977478027344\n",
            "21399 0.0002351382136112079 0.0023390755522996187 Traning Loss: 0.002574213780462742 17.987863540649414 2.016596794128418\n",
            "21400 0.0003789931070059538 0.00369248422794044 Traning Loss: 0.004071477334946394 17.98736000061035 2.0165116786956787\n",
            "21401 0.0005538200493901968 0.005449825897812843 Traning Loss: 0.006003646180033684 17.986976623535156 2.016530990600586\n",
            "21402 0.0007961908704601228 0.00770943658426404 Traning Loss: 0.008505627512931824 17.98592185974121 2.0164225101470947\n",
            "21403 0.0009454083628952503 0.00924812350422144 Traning Loss: 0.010193532332777977 17.984983444213867 2.0164661407470703\n",
            "21404 0.0009963001357391477 0.00953146256506443 Traning Loss: 0.010527762584388256 17.983375549316406 2.016352415084839\n",
            "21405 0.000712644075974822 0.006879652384668589 Traning Loss: 0.007592296227812767 17.982065200805664 2.016422748565674\n",
            "21406 0.0003421850851736963 0.002997540170326829 Traning Loss: 0.003339725313708186 17.98062515258789 2.0163750648498535\n",
            "21407 6.363702414091676e-05 0.00036852058838121593 Traning Loss: 0.00043215759797021747 17.979520797729492 2.0164785385131836\n",
            "21408 0.00011545270535862073 0.0007702645380049944 Traning Loss: 0.0008857172215357423 17.978647232055664 2.016575574874878\n",
            "21409 0.0003416952968109399 0.002878056839108467 Traning Loss: 0.0032197521068155766 17.977569580078125 2.016711711883545\n",
            "21410 0.00042174060945399106 0.003740189364179969 Traning Loss: 0.004161930177360773 17.976665496826172 2.0169482231140137\n",
            "21411 0.00031118965125642717 0.0023975123185664415 Traning Loss: 0.0027087018825113773 17.97551155090332 2.0171256065368652\n",
            "21412 0.00010568755533313379 0.0005553726805374026 Traning Loss: 0.0006610602140426636 17.974517822265625 2.0174508094787598\n",
            "21413 0.00010525694233365357 0.0003704285772982985 Traning Loss: 0.00047568551963195205 17.97357940673828 2.017711639404297\n",
            "21414 0.0002142839366570115 0.001574175083078444 Traning Loss: 0.0017884590197354555 17.972505569458008 2.018059492111206\n",
            "21415 0.0002766790858004242 0.002070067450404167 Traning Loss: 0.0023467466235160828 17.971559524536133 2.0184013843536377\n",
            "21416 0.00018797443772200495 0.0012206403771415353 Traning Loss: 0.001408614800311625 17.970500946044922 2.018710136413574\n",
            "21417 8.16302199382335e-05 0.0002422914985800162 Traning Loss: 0.0003239217330701649 17.96959114074707 2.0191025733947754\n",
            "21418 0.00010346228373236954 0.00038006791146472096 Traning Loss: 0.0004835301951970905 17.96875762939453 2.019411563873291\n",
            "21419 0.0001609919563634321 0.0011529471958056092 Traning Loss: 0.0013139391085132957 17.96786880493164 2.019824743270874\n",
            "21420 0.00018008924962487072 0.0012290774611756206 Traning Loss: 0.0014091667253524065 17.967111587524414 2.020188808441162\n",
            "21421 0.00010133454634342343 0.0005747796385549009 Traning Loss: 0.0006761141703464091 17.966341018676758 2.020557403564453\n",
            "21422 4.835878644371405e-05 8.882375550456345e-05 Traning Loss: 0.0001371825346723199 17.9656925201416 2.0209527015686035\n",
            "21423 7.517435005865991e-05 0.0003339567920193076 Traning Loss: 0.0004091311420779675 17.96514320373535 2.021270751953125\n",
            "21424 0.00010804226621985435 0.0008106057066470385 Traning Loss: 0.0009186479728668928 17.96456527709961 2.021660804748535\n",
            "21425 0.00011019651719834656 0.0007637433591298759 Traning Loss: 0.0008739398908801377 17.964094161987305 2.021960496902466\n",
            "21426 5.486946611199528e-05 0.00032736087450757623 Traning Loss: 0.0003822303260676563 17.963640213012695 2.022273302078247\n",
            "21427 2.701277298911009e-05 5.728896212531254e-05 Traning Loss: 8.430173329543322e-05 17.9632568359375 2.0225443840026855\n",
            "21428 4.2227416997775435e-05 0.0002298118342878297 Traning Loss: 0.00027203926583752036 17.962955474853516 2.0227532386779785\n",
            "21429 6.348585884552449e-05 0.0005179632571525872 Traning Loss: 0.0005814491305500269 17.96261978149414 2.0229902267456055\n",
            "21430 6.495707930298522e-05 0.0005058792303316295 Traning Loss: 0.0005708363023586571 17.962360382080078 2.0231432914733887\n",
            "21431 3.26495137414895e-05 0.0002572241937741637 Traning Loss: 0.00028987371479161084 17.96210289001465 2.0233192443847656\n",
            "21432 1.6107060218928382e-05 7.26130892871879e-05 Traning Loss: 8.872014586813748e-05 17.96189308166504 2.0234358310699463\n",
            "21433 1.92629850062076e-05 0.0001424538786523044 Traning Loss: 0.0001617168600205332 17.961763381958008 2.023533344268799\n",
            "21434 3.58479555870872e-05 0.0003109419485554099 Traning Loss: 0.0003467898932285607 17.961612701416016 2.023625373840332\n",
            "21435 3.870153159368783e-05 0.00034817022969946265 Traning Loss: 0.00038687174674123526 17.961551666259766 2.0236713886260986\n",
            "21436 2.433336885587778e-05 0.00022436164726968855 Traning Loss: 0.00024869502522051334 17.961503982543945 2.023730993270874\n",
            "21437 1.2081914974260144e-05 8.89811126398854e-05 Traning Loss: 0.00010106302943313494 17.961519241333008 2.023739814758301\n",
            "21438 9.471002158534247e-06 8.592754602432251e-05 Traning Loss: 9.539855091134086e-05 17.961605072021484 2.0237460136413574\n",
            "21439 2.0554070943035185e-05 0.00017679775191936642 Traning Loss: 0.0001973518228624016 17.961681365966797 2.0237300395965576\n",
            "21440 2.48340056714369e-05 0.00023822502407711 Traning Loss: 0.00026305901701562107 17.96182632446289 2.0237045288085938\n",
            "21441 2.172141830669716e-05 0.00019643497944343835 Traning Loss: 0.00021815640502609313 17.961944580078125 2.023677110671997\n",
            "21442 1.2026778676954564e-05 0.00010611690959194675 Traning Loss: 0.00011814368917839602 17.96210479736328 2.023634433746338\n",
            "21443 7.423757779179141e-06 6.185677193570882e-05 Traning Loss: 6.928053335286677e-05 17.9622859954834 2.0235886573791504\n",
            "21444 1.1218502550036646e-05 9.279929508920759e-05 Traning Loss: 0.00010401779582025483 17.962459564208984 2.0235331058502197\n",
            "21445 1.562083707540296e-05 0.00014756672317162156 Traning Loss: 0.00016318756388500333 17.96268081665039 2.02347731590271\n",
            "21446 1.7884340195450932e-05 0.00015729112783446908 Traning Loss: 0.0001751754607539624 17.962875366210938 2.0234177112579346\n",
            "21447 1.2431795767042786e-05 0.000117399227747228 Traning Loss: 0.00012983102351427078 17.963117599487305 2.023360252380371\n",
            "21448 8.712341696082149e-06 7.03877376508899e-05 Traning Loss: 7.910007843747735e-05 17.96336555480957 2.023289680480957\n",
            "21449 7.065838417474879e-06 6.276335625443608e-05 Traning Loss: 6.98291914886795e-05 17.963632583618164 2.0232269763946533\n",
            "21450 9.827025678532664e-06 8.848573634168133e-05 Traning Loss: 9.83127611107193e-05 17.963932037353516 2.0231525897979736\n",
            "21451 1.23113532026764e-05 0.0001116798011935316 Traning Loss: 0.0001239911507582292 17.964216232299805 2.0230889320373535\n",
            "21452 1.1161743714183103e-05 0.0001061580260284245 Traning Loss: 0.0001173197670141235 17.964534759521484 2.0230233669281006\n",
            "21453 9.241272891813423e-06 7.915911555755883e-05 Traning Loss: 8.840038935886696e-05 17.964841842651367 2.0229556560516357\n",
            "21454 6.563982424268033e-06 6.121835031080991e-05 Traning Loss: 6.778233364457265e-05 17.965164184570312 2.022897243499756\n",
            "21455 7.599317086715018e-06 6.46116750431247e-05 Traning Loss: 7.221099076559767e-05 17.965492248535156 2.022825241088867\n",
            "21456 8.67494509293465e-06 8.152417285600677e-05 Traning Loss: 9.019911522045732e-05 17.965810775756836 2.022773265838623\n",
            "21457 9.756735380506143e-06 8.914063801057637e-05 Traning Loss: 9.889737702906132e-05 17.966144561767578 2.022707223892212\n",
            "21458 8.702311788510997e-06 8.072880882536992e-05 Traning Loss: 8.943112334236503e-05 17.966468811035156 2.0226542949676514\n",
            "21459 7.15364740244695e-06 6.534651765832677e-05 Traning Loss: 7.250016642501578e-05 17.96680450439453 2.0225956439971924\n",
            "21460 6.408755325537641e-06 5.805328328278847e-05 Traning Loss: 6.446203769883141e-05 17.967144012451172 2.022536516189575\n",
            "21461 6.72518262945232e-06 6.329017924144864e-05 Traning Loss: 7.001536141615361e-05 17.967479705810547 2.022487163543701\n",
            "21462 7.771146556478925e-06 7.227160676848143e-05 Traning Loss: 8.004275150597095e-05 17.967823028564453 2.0224297046661377\n",
            "21463 7.784939953126013e-06 7.533701864304021e-05 Traning Loss: 8.312195859616622e-05 17.968156814575195 2.0223886966705322\n",
            "21464 7.463671408913797e-06 6.92083704052493e-05 Traning Loss: 7.667204044992104e-05 17.968496322631836 2.022336721420288\n",
            "21465 6.283481980062788e-06 6.144544022390619e-05 Traning Loss: 6.772892083972692e-05 17.96883201599121 2.0222959518432617\n",
            "21466 6.250202204682864e-06 5.7927110901800916e-05 Traning Loss: 6.417731492547318e-05 17.969161987304688 2.0222508907318115\n",
            "21467 6.173970177769661e-06 6.122559716459364e-05 Traning Loss: 6.73995673423633e-05 17.96949577331543 2.022209644317627\n",
            "21468 6.842504262749571e-06 6.547581870108843e-05 Traning Loss: 7.23183256923221e-05 17.96981430053711 2.0221710205078125\n",
            "21469 6.7118985498382244e-06 6.667881098110229e-05 Traning Loss: 7.339070725720376e-05 17.970138549804688 2.0221283435821533\n",
            "21470 6.392726845660945e-06 6.338247476378456e-05 Traning Loss: 6.977520388318226e-05 17.970449447631836 2.0220892429351807\n",
            "21471 5.903600140300114e-06 5.912895721849054e-05 Traning Loss: 6.503256008727476e-05 17.970760345458984 2.0220446586608887\n",
            "21472 5.4824258768348955e-06 5.763087028753944e-05 Traning Loss: 6.311329343589023e-05 17.971067428588867 2.022003650665283\n",
            "21473 5.762580713053467e-06 5.8991365222027525e-05 Traning Loss: 6.475394911831245e-05 17.971363067626953 2.0219597816467285\n",
            "21474 5.619646799459588e-06 6.185995880514383e-05 Traning Loss: 6.747960287611932e-05 17.971662521362305 2.0219171047210693\n",
            "21475 5.929933195147896e-06 6.24799940851517e-05 Traning Loss: 6.840992864454165e-05 17.97195053100586 2.021871328353882\n",
            "21476 5.3776707318320405e-06 6.136739102657884e-05 Traning Loss: 6.674505857517943e-05 17.972244262695312 2.0218253135681152\n",
            "21477 5.287109615892405e-06 5.8763074775924906e-05 Traning Loss: 6.405018211808056e-05 17.972530364990234 2.021775960922241\n",
            "21478 4.884283043793403e-06 5.755312304245308e-05 Traning Loss: 6.243740790523589e-05 17.97281837463379 2.0217275619506836\n",
            "21479 4.942194664181443e-06 5.778830382041633e-05 Traning Loss: 6.273049802985042e-05 17.973106384277344 2.021674871444702\n",
            "21480 5.005586899642367e-06 5.905909347347915e-05 Traning Loss: 6.406468310160562e-05 17.973388671875 2.0216224193573\n",
            "21481 4.995540166419232e-06 5.9939360653515905e-05 Traning Loss: 6.493490218417719e-05 17.973674774169922 2.0215649604797363\n",
            "21482 5.008017978980206e-06 5.9531805163715035e-05 Traning Loss: 6.453982496168464e-05 17.97395133972168 2.0215065479278564\n",
            "21483 4.736592472909251e-06 5.8516172430245206e-05 Traning Loss: 6.325276626739651e-05 17.97422981262207 2.0214462280273438\n",
            "21484 4.709758741228143e-06 5.738527761423029e-05 Traning Loss: 6.209503771970049e-05 17.97450065612793 2.021383047103882\n",
            "21485 4.537609129329212e-06 5.7247958466177806e-05 Traning Loss: 6.178556941449642e-05 17.974767684936523 2.0213210582733154\n",
            "21486 4.647977220884059e-06 5.762344517279416e-05 Traning Loss: 6.227142148418352e-05 17.97503089904785 2.0212533473968506\n",
            "21487 4.571263161778916e-06 5.833160685142502e-05 Traning Loss: 6.290287274168804e-05 17.97528648376465 2.021188259124756\n",
            "21488 4.623983386409236e-06 5.8432513469597325e-05 Traning Loss: 6.305649731075391e-05 17.975542068481445 2.021117687225342\n",
            "21489 4.454374447959708e-06 5.814062387798913e-05 Traning Loss: 6.259499787120149e-05 17.97579002380371 2.021050453186035\n",
            "21490 4.4259668356971815e-06 5.742989742429927e-05 Traning Loss: 6.185586244100705e-05 17.976037979125977 2.0209789276123047\n",
            "21491 4.261050889908802e-06 5.705753937945701e-05 Traning Loss: 6.131859117886052e-05 17.976282119750977 2.020909070968628\n",
            "21492 4.297938176023308e-06 5.6923763622762635e-05 Traning Loss: 6.122170452727005e-05 17.97652244567871 2.0208370685577393\n",
            "21493 4.213186457491247e-06 5.7236789871240035e-05 Traning Loss: 6.144997314549983e-05 17.976762771606445 2.0207653045654297\n",
            "21494 4.274255388736492e-06 5.741631321143359e-05 Traning Loss: 6.169056723592803e-05 17.97699737548828 2.020693778991699\n",
            "21495 4.198227088636486e-06 5.7488796301186085e-05 Traning Loss: 6.168702384456992e-05 17.977231979370117 2.0206220149993896\n",
            "21496 4.191109837847762e-06 5.7219815062126145e-05 Traning Loss: 6.14109230809845e-05 17.977458953857422 2.0205509662628174\n",
            "21497 4.119939603697276e-06 5.691058322554454e-05 Traning Loss: 6.1030521464999765e-05 17.977684020996094 2.020479202270508\n",
            "21498 4.07306470151525e-06 5.667738150805235e-05 Traning Loss: 6.0750448028557e-05 17.9779052734375 2.0204083919525146\n",
            "21499 4.0649556467542425e-06 5.661331306328066e-05 Traning Loss: 6.06782705290243e-05 17.978120803833008 2.0203375816345215\n",
            "21500 4.01536863137153e-06 5.674592830473557e-05 Traning Loss: 6.076129648135975e-05 17.978334426879883 2.0202674865722656\n",
            "21501 4.051332325616386e-06 5.680973481503315e-05 Traning Loss: 6.086106805014424e-05 17.97854232788086 2.020197629928589\n",
            "21502 3.971561909565935e-06 5.688595774699934e-05 Traning Loss: 6.0857521020807326e-05 17.978748321533203 2.0201284885406494\n",
            "21503 3.99428472519503e-06 5.673286796081811e-05 Traning Loss: 6.0727154050255194e-05 17.97894859313965 2.020059108734131\n",
            "21504 3.885748355969554e-06 5.664061609422788e-05 Traning Loss: 6.052636308595538e-05 17.979148864746094 2.019991397857666\n",
            "21505 3.903582182829268e-06 5.6445318477926776e-05 Traning Loss: 6.034890247974545e-05 17.97934341430664 2.019923210144043\n",
            "21506 3.809011786870542e-06 5.644602788379416e-05 Traning Loss: 6.0255038988543674e-05 17.979536056518555 2.019857406616211\n",
            "21507 3.842443675239338e-06 5.640153176500462e-05 Traning Loss: 6.024397589499131e-05 17.979724884033203 2.0197904109954834\n",
            "21508 3.7636805245711002e-06 5.650445746141486e-05 Traning Loss: 6.026813935022801e-05 17.97991180419922 2.019726276397705\n",
            "21509 3.7974882616254035e-06 5.6468510592821985e-05 Traning Loss: 6.026600021868944e-05 17.98009490966797 2.019660711288452\n",
            "21510 3.7100392091815593e-06 5.6500408391002566e-05 Traning Loss: 6.021044828230515e-05 17.980274200439453 2.0195982456207275\n",
            "21511 3.7411018638522364e-06 5.636832429445349e-05 Traning Loss: 6.010942524881102e-05 17.980451583862305 2.019534111022949\n",
            "21512 3.6408077903615776e-06 5.635093475575559e-05 Traning Loss: 5.999174391035922e-05 17.980627059936523 2.01947283744812\n",
            "21513 3.6835228911513695e-06 5.621650780085474e-05 Traning Loss: 5.990003046463244e-05 17.980798721313477 2.0194098949432373\n",
            "21514 3.5748130358115304e-06 5.6270080676767975e-05 Traning Loss: 5.9844893257832155e-05 17.980968475341797 2.0193498134613037\n",
            "21515 3.636124802142149e-06 5.619206422124989e-05 Traning Loss: 5.982818765914999e-05 17.98113250732422 2.0192880630493164\n",
            "21516 3.5147700145898852e-06 5.630890154861845e-05 Traning Loss: 5.982367292745039e-05 17.98129653930664 2.0192294120788574\n",
            "21517 3.593228711906704e-06 5.621713717118837e-05 Traning Loss: 5.981036520097405e-05 17.981454849243164 2.0191688537597656\n",
            "21518 3.4504303130233893e-06 5.6328099162783474e-05 Traning Loss: 5.977852924843319e-05 17.981613159179688 2.0191118717193604\n",
            "21519 3.546130074028042e-06 5.6190598115790635e-05 Traning Loss: 5.9736728871939704e-05 17.981765747070312 2.019052028656006\n",
            "21520 3.3781345791794593e-06 5.632042666547932e-05 Traning Loss: 5.9698562836274505e-05 17.981918334960938 2.018996477127075\n",
            "21521 3.5050679798587225e-06 5.6177490478148684e-05 Traning Loss: 5.968255936750211e-05 17.982067108154297 2.0189368724823\n",
            "21522 3.3105338843597565e-06 5.6392967962892726e-05 Traning Loss: 5.9703503211494535e-05 17.982215881347656 2.018883228302002\n",
            "21523 3.4890299502876587e-06 5.627853170153685e-05 Traning Loss: 5.9767560742329806e-05 17.982358932495117 2.0188236236572266\n",
            "21524 3.2540547181270085e-06 5.662986950483173e-05 Traning Loss: 5.988392513245344e-05 17.982501983642578 2.0187718868255615\n",
            "21525 3.5078951441391837e-06 5.6552922615082934e-05 Traning Loss: 6.006081821396947e-05 17.982641220092773 2.018711805343628\n",
            "21526 3.210346676496556e-06 5.71136042708531e-05 Traning Loss: 6.0323949583107606e-05 17.98278045654297 2.018662452697754\n",
            "21527 3.5837697396345902e-06 5.713031714549288e-05 Traning Loss: 6.071408643038012e-05 17.982913970947266 2.018601417541504\n",
            "21528 3.2018244837672682e-06 5.810279253637418e-05 Traning Loss: 6.130461406428367e-05 17.983051300048828 2.018554925918579\n",
            "21529 3.772528543777298e-06 5.842761311214417e-05 Traning Loss: 6.220013892743737e-05 17.983179092407227 2.0184919834136963\n",
            "21530 3.2901275517360773e-06 6.0273014241829515e-05 Traning Loss: 6.356314406730235e-05 17.983312606811523 2.018449544906616\n",
            "21531 4.198937858745921e-06 6.144004146335647e-05 Traning Loss: 6.563898205058649e-05 17.98343276977539 2.018383026123047\n",
            "21532 3.628198101068847e-06 6.519477756228298e-05 Traning Loss: 6.882297748234123e-05 17.983564376831055 2.0183467864990234\n",
            "21533 5.1545302994782105e-06 6.854617095086724e-05 Traning Loss: 7.370069943135604e-05 17.98367691040039 2.0182743072509766\n",
            "21534 4.603694378602086e-06 7.665764132980257e-05 Traning Loss: 8.126133616315201e-05 17.983806610107422 2.018247127532959\n",
            "21535 7.337747774727177e-06 8.563406299799681e-05 Traning Loss: 9.297180804423988e-05 17.98390769958496 2.018164873123169\n",
            "21536 7.216936410259223e-06 0.00010418258898425847 Traning Loss: 0.00011139952403027564 17.984037399291992 2.0181519985198975\n",
            "21537 1.249348497367464e-05 0.00012777144729625434 Traning Loss: 0.00014026493590790778 17.98412322998047 2.0180537700653076\n",
            "21538 1.4092789569986053e-05 0.00017221701273228973 Traning Loss: 0.00018630980048328638 17.984251022338867 2.01806378364563\n",
            "21539 2.5051926058949903e-05 0.00023376652097795159 Traning Loss: 0.0002588184433989227 17.98430824279785 2.0179402828216553\n",
            "21540 3.204722088412382e-05 0.00034341003629378974 Traning Loss: 0.00037545725353993475 17.98442840576172 2.0179874897003174\n",
            "21541 5.609021900454536e-05 0.0005016308859921992 Traning Loss: 0.0005577211268246174 17.984432220458984 2.017824649810791\n",
            "21542 7.792250107740983e-05 0.0007709629717282951 Traning Loss: 0.0008488854509778321 17.98452377319336 2.0179312229156494\n",
            "21543 0.00013080411008559167 0.0011550183407962322 Traning Loss: 0.0012858224799856544 17.984420776367188 2.017712354660034\n",
            "21544 0.0001860939373727888 0.0017643534811213613 Traning Loss: 0.0019504474475979805 17.984426498413086 2.0179078578948975\n",
            "21545 0.00028850685339421034 0.0025407320354133844 Traning Loss: 0.002829238772392273 17.984111785888672 2.0176262855529785\n",
            "21546 0.0003848581400234252 0.0035703869070857763 Traning Loss: 0.003955245018005371 17.983905792236328 2.0179293155670166\n",
            "21547 0.0005052803899161518 0.004429589491337538 Traning Loss: 0.004934869706630707 17.983251571655273 2.0176260471343994\n",
            "21548 0.0005378550849854946 0.004949325229972601 Traning Loss: 0.0054871803149580956 17.98270034790039 2.017991304397583\n",
            "21549 0.0005018914234824479 0.0043325116857886314 Traning Loss: 0.004834403283894062 17.981746673583984 2.017789125442505\n",
            "21550 0.0003098472370766103 0.002892665797844529 Traning Loss: 0.0032025130931288004 17.980972290039062 2.0180766582489014\n",
            "21551 0.00014026123972143978 0.0011365199461579323 Traning Loss: 0.0012767812004312873 17.980087280273438 2.018101930618286\n",
            "21552 2.3982249331311323e-05 0.00021964537154417485 Traning Loss: 0.00024362762633245438 17.979372024536133 2.018186569213867\n",
            "21553 4.717019328381866e-05 0.00045859627425670624 Traning Loss: 0.0005057664820924401 17.978778839111328 2.0184359550476074\n",
            "21554 0.00016076682368293405 0.0012668854324147105 Traning Loss: 0.0014276523143053055 17.97810935974121 2.0183770656585693\n",
            "21555 0.00019350794900674373 0.0018257491756230593 Traning Loss: 0.002019257051870227 17.977590560913086 2.0187089443206787\n",
            "21556 0.00018947181524708867 0.0014852281892672181 Traning Loss: 0.0016747000627219677 17.97688865661621 2.0187289714813232\n",
            "21557 8.346421964233741e-05 0.0007546072592958808 Traning Loss: 0.0008380714571103454 17.976306915283203 2.018969774246216\n",
            "21558 4.006118979305029e-05 0.00026904590777121484 Traning Loss: 0.00030910709756426513 17.97569465637207 2.019181251525879\n",
            "21559 5.583062011282891e-05 0.0003928784281015396 Traning Loss: 0.00044870906276628375 17.97513198852539 2.01926851272583\n",
            "21560 8.531173807568848e-05 0.0007967047276906669 Traning Loss: 0.0008820164948701859 17.97466468811035 2.019575357437134\n",
            "21561 0.00011781261127907783 0.0008797505870461464 Traning Loss: 0.00099756324198097 17.974140167236328 2.019623041152954\n",
            "21562 6.941357423784211e-05 0.0006226104451343417 Traning Loss: 0.0006920240120962262 17.973751068115234 2.0198795795440674\n",
            "21563 4.210208135191351e-05 0.0002705706865526736 Traning Loss: 0.00031267275335267186 17.973325729370117 2.0200345516204834\n",
            "21564 3.140175613225438e-05 0.00021931286028120667 Traning Loss: 0.00025071462732739747 17.972970962524414 2.020153284072876\n",
            "21565 4.804943819181062e-05 0.00042271995334886014 Traning Loss: 0.00047076938790269196 17.972654342651367 2.0203847885131836\n",
            "21566 7.182850822573528e-05 0.0005508583853952587 Traning Loss: 0.0006226868717931211 17.97233772277832 2.0204215049743652\n",
            "21567 5.044331919634715e-05 0.0004616817459464073 Traning Loss: 0.0005121250869706273 17.97209930419922 2.020612955093384\n",
            "21568 3.2587249734206125e-05 0.0002141157165169716 Traning Loss: 0.0002467029553372413 17.971858978271484 2.0206761360168457\n",
            "21569 1.3850867617293261e-05 9.79871183517389e-05 Traning Loss: 0.00011183798778802156 17.97169303894043 2.020747423171997\n",
            "21570 2.146771839761641e-05 0.00018660032947082072 Traning Loss: 0.00020806804241146892 17.971538543701172 2.02085280418396\n",
            "21571 4.2662468331400305e-05 0.0003374738444108516 Traning Loss: 0.0003801363054662943 17.971389770507812 2.0208208560943604\n",
            "21572 3.948829180444591e-05 0.0003860904253087938 Traning Loss: 0.00042557870619930327 17.97126007080078 2.020906925201416\n",
            "21573 3.2745450880611315e-05 0.0002569396165199578 Traning Loss: 0.0002896850637625903 17.971128463745117 2.020869731903076\n",
            "21574 1.1142701623612083e-05 0.0001091036174329929 Traning Loss: 0.00012024631723761559 17.97102928161621 2.0208730697631836\n",
            "21575 6.560692781931721e-06 5.8955502026947215e-05 Traning Loss: 6.551619298988953e-05 17.970972061157227 2.0208652019500732\n",
            "21576 1.5680356227676384e-05 0.0001271009532501921 Traning Loss: 0.0001427813112968579 17.970949172973633 2.020775556564331\n",
            "21577 2.0472340111155063e-05 0.00022454463760368526 Traning Loss: 0.0002450169704388827 17.970966339111328 2.0207653045654297\n",
            "21578 2.725926424318459e-05 0.0002358805068070069 Traning Loss: 0.0002631397801451385 17.97099494934082 2.02064847946167\n",
            "21579 1.5861789506743662e-05 0.00017792540893424302 Traning Loss: 0.0001937872002599761 17.971040725708008 2.0206010341644287\n",
            "21580 1.0051655408460647e-05 9.817972750170156e-05 Traning Loss: 0.00010823138291016221 17.971115112304688 2.0205190181732178\n",
            "21581 7.511097464885097e-06 7.21407777746208e-05 Traning Loss: 7.965187251102179e-05 17.971189498901367 2.0204126834869385\n",
            "21582 8.04977935331408e-06 0.00010258021939080209 Traning Loss: 0.00011062999692512676 17.971311569213867 2.020357131958008\n",
            "21583 1.58911007019924e-05 0.00013391343236435205 Traning Loss: 0.00014980453124735504 17.9714298248291 2.0202226638793945\n",
            "21584 1.154586425400339e-05 0.00014137339894659817 Traning Loss: 0.00015291926683858037 17.971574783325195 2.0201621055603027\n",
            "21585 1.185045675811125e-05 0.00010796300193760544 Traning Loss: 0.00011981345596723258 17.971725463867188 2.0200490951538086\n",
            "21586 7.022264981060289e-06 8.01629139459692e-05 Traning Loss: 8.718518074601889e-05 17.971874237060547 2.0199575424194336\n",
            "21587 6.496652076748433e-06 7.62874842621386e-05 Traning Loss: 8.278413588413969e-05 17.972049713134766 2.019882917404175\n",
            "21588 1.0562038369243965e-05 9.176984895020723e-05 Traning Loss: 0.00010233189095743 17.972198486328125 2.0197668075561523\n",
            "21589 8.94184358912753e-06 0.00011009746231138706 Traning Loss: 0.0001190393086289987 17.97238540649414 2.0197160243988037\n",
            "21590 1.1659951269393787e-05 0.00010119737999048084 Traning Loss: 0.00011285733489785343 17.97255516052246 2.0196118354797363\n",
            "21591 6.220172053872375e-06 8.224364864872769e-05 Traning Loss: 8.846382115734741e-05 17.972755432128906 2.019554853439331\n",
            "21592 5.699203939002473e-06 6.097378718550317e-05 Traning Loss: 6.667299021501094e-05 17.972965240478516 2.019481658935547\n",
            "21593 5.227488600212382e-06 5.853234324604273e-05 Traning Loss: 6.375983502948657e-05 17.97317886352539 2.0194053649902344\n",
            "21594 5.496838639373891e-06 7.206390728242695e-05 Traning Loss: 7.756074774079025e-05 17.97341537475586 2.019364833831787\n",
            "21595 9.060715456143953e-06 8.342794899363071e-05 Traning Loss: 9.248866263078526e-05 17.973630905151367 2.019287347793579\n",
            "21596 6.829115591244772e-06 8.815024921204895e-05 Traning Loss: 9.497936116531491e-05 17.973875045776367 2.019260883331299\n",
            "21597 7.979067959240638e-06 7.622747943969443e-05 Traning Loss: 8.420654921792448e-05 17.974103927612305 2.019200563430786\n",
            "21598 4.8937399697024375e-06 6.527761433972046e-05 Traning Loss: 7.01713579474017e-05 17.974353790283203 2.0191617012023926\n",
            "21599 4.844048362429021e-06 5.8321820688433945e-05 Traning Loss: 6.316586950561032e-05 17.97460174560547 2.0191235542297363\n",
            "21600 5.363769560062792e-06 6.0243928601266816e-05 Traning Loss: 6.56076954328455e-05 17.974849700927734 2.0190725326538086\n",
            "21601 4.9579330152482726e-06 6.701497477479279e-05 Traning Loss: 7.197290688054636e-05 17.975101470947266 2.019052267074585\n",
            "21602 6.769693754904438e-06 6.831058271927759e-05 Traning Loss: 7.508027920266613e-05 17.9753360748291 2.0190024375915527\n",
            "21603 4.924272161588306e-06 6.750371539965272e-05 Traning Loss: 7.242798892548308e-05 17.975582122802734 2.0189828872680664\n",
            "21604 5.549913566937903e-06 6.139242759672925e-05 Traning Loss: 6.69423388899304e-05 17.975814819335938 2.018944501876831\n",
            "21605 4.343311047705356e-06 5.909418177907355e-05 Traning Loss: 6.343749555526301e-05 17.976057052612305 2.018913984298706\n",
            "21606 4.460592208488379e-06 5.964056254015304e-05 Traning Loss: 6.410115747712553e-05 17.976295471191406 2.018890142440796\n",
            "21607 5.098953806736972e-06 6.212016887729988e-05 Traning Loss: 6.721912359353155e-05 17.97653579711914 2.0188512802124023\n",
            "21608 4.559172339213546e-06 6.479841249529272e-05 Traning Loss: 6.935758574400097e-05 17.976776123046875 2.0188333988189697\n",
            "21609 5.303807483869605e-06 6.302588008111343e-05 Traning Loss: 6.832968938397244e-05 17.977012634277344 2.018794298171997\n",
            "21610 4.170750344201224e-06 6.064245098968968e-05 Traning Loss: 6.481320451712236e-05 17.977249145507812 2.018772602081299\n",
            "21611 4.2925616980937775e-06 5.695546860806644e-05 Traning Loss: 6.124802894191816e-05 17.977481842041016 2.018742322921753\n",
            "21612 3.919795290130423e-06 5.58769388590008e-05 Traning Loss: 5.9796733694383875e-05 17.977710723876953 2.0187125205993652\n",
            "21613 3.820272468146868e-06 5.69575531699229e-05 Traning Loss: 6.0777827457059175e-05 17.97793960571289 2.0186898708343506\n",
            "21614 4.4099015212850645e-06 5.8399215049576014e-05 Traning Loss: 6.280911475187168e-05 17.978164672851562 2.0186526775360107\n",
            "21615 3.946220203943085e-06 6.018442945787683e-05 Traning Loss: 6.413064693333581e-05 17.978389739990234 2.018630027770996\n",
            "21616 4.468653514777543e-06 5.9405414503999054e-05 Traning Loss: 6.387406756402925e-05 17.978609085083008 2.0185928344726562\n",
            "21617 3.859665412164759e-06 5.861410318175331e-05 Traning Loss: 6.247376586543396e-05 17.978822708129883 2.018566370010376\n",
            "21618 3.923246822523652e-06 5.708907701773569e-05 Traning Loss: 6.101232429500669e-05 17.979032516479492 2.0185344219207764\n",
            "21619 3.8216003304114565e-06 5.654381311614998e-05 Traning Loss: 6.036541162757203e-05 17.979236602783203 2.018500328063965\n",
            "21620 3.5725815905607305e-06 5.704881914425641e-05 Traning Loss: 6.0621401644311845e-05 17.97944450378418 2.0184719562530518\n",
            "21621 3.9891911001177505e-06 5.722148853237741e-05 Traning Loss: 6.121068145148456e-05 17.979642868041992 2.018432140350342\n",
            "21622 3.4845559184759622e-06 5.7964352890849113e-05 Traning Loss: 6.144891085568815e-05 17.979846954345703 2.0184037685394287\n",
            "21623 3.914970420737518e-06 5.711491030524485e-05 Traning Loss: 6.102988118072972e-05 17.980043411254883 2.0183637142181396\n",
            "21624 3.3981248179770773e-06 5.677712397300638e-05 Traning Loss: 6.017524719936773e-05 17.980239868164062 2.01833176612854\n",
            "21625 3.56544569513062e-06 5.580771903623827e-05 Traning Loss: 5.9373163821874186e-05 17.980432510375977 2.018294334411621\n",
            "21626 3.401161620786297e-06 5.561340367421508e-05 Traning Loss: 5.901456461288035e-05 17.980621337890625 2.018256902694702\n",
            "21627 3.3128089853562415e-06 5.58510837436188e-05 Traning Loss: 5.916389272897504e-05 17.980810165405273 2.0182225704193115\n",
            "21628 3.5379341625230154e-06 5.603983299806714e-05 Traning Loss: 5.957776738796383e-05 17.980993270874023 2.018181085586548\n",
            "21629 3.245024799980456e-06 5.666882134391926e-05 Traning Loss: 5.991384750814177e-05 17.981178283691406 2.0181469917297363\n",
            "21630 3.5801203921437263e-06 5.637446520267986e-05 Traning Loss: 5.9954585594823584e-05 17.98135757446289 2.01810359954834\n",
            "21631 3.2128830298461253e-06 5.649147351505235e-05 Traning Loss: 5.97043581365142e-05 17.981536865234375 2.018066883087158\n",
            "21632 3.423866473895032e-06 5.5913260439410806e-05 Traning Loss: 5.9337126003811136e-05 17.98171043395996 2.0180246829986572\n",
            "21633 3.198302692908328e-06 5.585750113823451e-05 Traning Loss: 5.905580474063754e-05 17.981882095336914 2.0179851055145264\n",
            "21634 3.237559212720953e-06 5.572742156800814e-05 Traning Loss: 5.896497896173969e-05 17.9820499420166 2.0179455280303955\n",
            "21635 3.2403311251982814e-06 5.5794411309761927e-05 Traning Loss: 5.903474084334448e-05 17.982215881347656 2.0179028511047363\n",
            "21636 3.1349159144156147e-06 5.601284283329733e-05 Traning Loss: 5.914775829296559e-05 17.982379913330078 2.0178639888763428\n",
            "21637 3.260801804572111e-06 5.5921718740137294e-05 Traning Loss: 5.918251918046735e-05 17.982542037963867 2.017819881439209\n",
            "21638 3.0851995234115748e-06 5.599984433501959e-05 Traning Loss: 5.908504317631014e-05 17.982702255249023 2.0177805423736572\n",
            "21639 3.188571781720384e-06 5.569408313021995e-05 Traning Loss: 5.888265513931401e-05 17.982858657836914 2.0177371501922607\n",
            "21640 3.0594396775995847e-06 5.55963852093555e-05 Traning Loss: 5.8655823522713035e-05 17.983013153076172 2.0176966190338135\n",
            "21641 3.0669127681903774e-06 5.541509744944051e-05 Traning Loss: 5.8482011809246615e-05 17.983165740966797 2.0176548957824707\n",
            "21642 3.0596513624914223e-06 5.533649164135568e-05 Traning Loss: 5.839614459546283e-05 17.983314514160156 2.0176124572753906\n",
            "21643 2.969978368128068e-06 5.541609425563365e-05 Traning Loss: 5.838607103214599e-05 17.983463287353516 2.0175724029541016\n",
            "21644 3.0717733352503274e-06 5.5333945056190714e-05 Traning Loss: 5.8405719755683094e-05 17.983606338500977 2.017529010772705\n",
            "21645 2.9200878088886384e-06 5.5484841141151264e-05 Traning Loss: 5.8404930314281955e-05 17.983749389648438 2.0174896717071533\n",
            "21646 3.053374257433461e-06 5.5305474234046414e-05 Traning Loss: 5.8358848036732525e-05 17.98388671875 2.017446517944336\n",
            "21647 2.8926767754455796e-06 5.537597826332785e-05 Traning Loss: 5.826865526614711e-05 17.984024047851562 2.017406940460205\n",
            "21648 2.9837633519491646e-06 5.517400859389454e-05 Traning Loss: 5.815777331008576e-05 17.98415756225586 2.017364978790283\n",
            "21649 2.873968242056435e-06 5.518328907783143e-05 Traning Loss: 5.8057255955645815e-05 17.984289169311523 2.0173251628875732\n",
            "21650 2.900045046771993e-06 5.508967660716735e-05 Traning Loss: 5.7989720517070964e-05 17.984418869018555 2.017284870147705\n",
            "21651 2.87049010694318e-06 5.508602043846622e-05 Traning Loss: 5.7956509408541024e-05 17.984546661376953 2.017244338989258\n",
            "21652 2.8389652015903266e-06 5.510810296982527e-05 Traning Loss: 5.794706885353662e-05 17.98467254638672 2.017205238342285\n",
            "21653 2.8692675186903216e-06 5.507408423000015e-05 Traning Loss: 5.794335083919577e-05 17.98479652404785 2.017164707183838\n",
            "21654 2.802017888825503e-06 5.51300763618201e-05 Traning Loss: 5.793209493276663e-05 17.98491668701172 2.0171267986297607\n",
            "21655 2.8457150165195344e-06 5.505956869455986e-05 Traning Loss: 5.7905283028958365e-05 17.985034942626953 2.017086982727051\n",
            "21656 2.7774094633059576e-06 5.509073525900021e-05 Traning Loss: 5.7868142903316766e-05 17.985151290893555 2.017049551010132\n",
            "21657 2.7979515380138764e-06 5.5033277021721005e-05 Traning Loss: 5.7831228332361206e-05 17.985265731811523 2.0170109272003174\n",
            "21658 2.763611746559036e-06 5.503962529473938e-05 Traning Loss: 5.780323772341944e-05 17.98537826538086 2.0169732570648193\n",
            "21659 2.745473466347903e-06 5.505297667696141e-05 Traning Loss: 5.779845014330931e-05 17.985490798950195 2.0169358253479004\n",
            "21660 2.765639010249288e-06 5.505859007826075e-05 Traning Loss: 5.782422886113636e-05 17.985597610473633 2.0168981552124023\n",
            "21661 2.7069681891589426e-06 5.517715180758387e-05 Traning Loss: 5.788412090623751e-05 17.985706329345703 2.016862154006958\n",
            "21662 2.7795558708021417e-06 5.5206815886776894e-05 Traning Loss: 5.798636993858963e-05 17.985809326171875 2.016824722290039\n",
            "21663 2.6894674647337524e-06 5.545133899431676e-05 Traning Loss: 5.814080577692948e-05 17.98591423034668 2.016789674758911\n",
            "21664 2.804681344059645e-06 5.556518954108469e-05 Traning Loss: 5.836987111251801e-05 17.986013412475586 2.0167524814605713\n",
            "21665 2.7053583835368045e-06 5.6002663768595085e-05 Traning Loss: 5.870802124263719e-05 17.986116409301758 2.0167181491851807\n",
            "21666 2.868629053409677e-06 5.634542685584165e-05 Traning Loss: 5.921405681874603e-05 17.9862117767334 2.016681671142578\n",
            "21667 2.7918404157389887e-06 5.718494503526017e-05 Traning Loss: 5.997678454150446e-05 17.986312866210938 2.016648054122925\n",
            "21668 3.034696192116826e-06 5.80956730118487e-05 Traning Loss: 6.11303694313392e-05 17.98640251159668 2.0166120529174805\n",
            "21669 3.0278229132818524e-06 5.985196548863314e-05 Traning Loss: 6.287978612817824e-05 17.986499786376953 2.0165789127349854\n",
            "21670 3.4423455872456543e-06 6.211435538716614e-05 Traning Loss: 6.555669824592769e-05 17.986583709716797 2.016543388366699\n",
            "21671 3.6097021620662417e-06 6.603718065889552e-05 Traning Loss: 6.964688509469852e-05 17.98668098449707 2.0165109634399414\n",
            "21672 4.423593509272905e-06 7.155518687795848e-05 Traning Loss: 7.597877993248403e-05 17.986757278442383 2.0164754390716553\n",
            "21673 5.039074494561646e-06 8.072092896327376e-05 Traning Loss: 8.576000254834071e-05 17.986852645874023 2.016443967819214\n",
            "21674 6.8057111093366984e-06 9.432413935428485e-05 Traning Loss: 0.0001011298518278636 17.986919403076172 2.0164079666137695\n",
            "21675 8.600311048212461e-06 0.00011654293484752998 Traning Loss: 0.00012514324043877423 17.987014770507812 2.016378164291382\n",
            "21676 1.270198936254019e-05 0.000150798266986385 Traning Loss: 0.00016350025543943048 17.987062454223633 2.016340732574463\n",
            "21677 1.7639044017414562e-05 0.000206296012038365 Traning Loss: 0.00022393505787476897 17.98715591430664 2.0163135528564453\n",
            "21678 2.761023279163055e-05 0.0002942182472907007 Traning Loss: 0.0003218284691683948 17.987171173095703 2.0162734985351562\n",
            "21679 4.087449997314252e-05 0.00043524630018509924 Traning Loss: 0.00047612079652026296 17.987253189086914 2.0162501335144043\n",
            "21680 6.573129212483764e-05 0.0006622292567044497 Traning Loss: 0.0007279605488292873 17.98720359802246 2.016205310821533\n",
            "21681 0.00010024038056144491 0.0010175991337746382 Traning Loss: 0.001117839477956295 17.987245559692383 2.0161876678466797\n",
            "21682 0.00016125862020999193 0.0015860775019973516 Traning Loss: 0.0017473361222073436 17.9870548248291 2.01613450050354\n",
            "21683 0.00024343667610082775 0.0024184961803257465 Traning Loss: 0.002661932958289981 17.986974716186523 2.0161256790161133\n",
            "21684 0.00037678968510590494 0.00366956926882267 Traning Loss: 0.004046359099447727 17.986492156982422 2.016059160232544\n",
            "21685 0.000526291667483747 0.0051787663251161575 Traning Loss: 0.005705058109015226 17.986114501953125 2.0160624980926514\n",
            "21686 0.0007192135672084987 0.006959712132811546 Traning Loss: 0.007678925525397062 17.985137939453125 2.0159780979156494\n",
            "21687 0.000809510936960578 0.007919288240373135 Traning Loss: 0.008728799410164356 17.984281539916992 2.015997886657715\n",
            "21688 0.0008067293674685061 0.007703188341110945 Traning Loss: 0.008509918116033077 17.98287010192871 2.015907049179077\n",
            "21689 0.0005495502264238894 0.005296738352626562 Traning Loss: 0.005846288520842791 17.98174285888672 2.0159504413604736\n",
            "21690 0.000254019076237455 0.002198298927396536 Traning Loss: 0.0024523180909454823 17.980527877807617 2.015904426574707\n",
            "21691 4.8048983444459736e-05 0.0002619451261125505 Traning Loss: 0.000309994095005095 17.979595184326172 2.0159759521484375\n",
            "21692 9.152066195383668e-05 0.0006054011755622923 Traning Loss: 0.000696921837516129 17.978862762451172 2.0160393714904785\n",
            "21693 0.00026568648172542453 0.00222883652895689 Traning Loss: 0.0024945230688899755 17.977954864501953 2.016127586364746\n",
            "21694 0.00034294126089662313 0.0030510257929563522 Traning Loss: 0.003393967170268297 17.977209091186523 2.0163071155548096\n",
            "21695 0.0002818818611558527 0.0022358354181051254 Traning Loss: 0.0025177171919494867 17.976205825805664 2.01641583442688\n",
            "21696 0.00011370796710252762 0.0007480527274310589 Traning Loss: 0.0008617606945335865 17.975360870361328 2.016669750213623\n",
            "21697 7.466266106348485e-05 0.00020186578331049532 Traning Loss: 0.00027652844437398016 17.974536895751953 2.016845464706421\n",
            "21698 0.00013086726539768279 0.0008869402809068561 Traning Loss: 0.0010178075172007084 17.97364616394043 2.0171058177948\n",
            "21699 0.00020873926405329257 0.0015610841801390052 Traning Loss: 0.0017698234878480434 17.972881317138672 2.0173654556274414\n",
            "21700 0.00018729656585492194 0.0013572968309745193 Traning Loss: 0.0015445933677256107 17.97197151184082 2.0175790786743164\n",
            "21701 0.0001011266213026829 0.0005775524768978357 Traning Loss: 0.000678679090924561 17.971229553222656 2.01789927482605\n",
            "21702 7.145004929043353e-05 0.00020691165991593152 Traning Loss: 0.0002783617237582803 17.97050666809082 2.018115997314453\n",
            "21703 8.636464190203696e-05 0.0005418290966190398 Traning Loss: 0.0006281937239691615 17.969799041748047 2.0184528827667236\n",
            "21704 0.00013228115858510137 0.0009034044924192131 Traning Loss: 0.0010356856510043144 17.969205856323242 2.018739700317383\n",
            "21705 0.00011278954480076209 0.0008062405395321548 Traning Loss: 0.0009190301061607897 17.96854591369629 2.0190255641937256\n",
            "21706 6.61942976876162e-05 0.00037438716390170157 Traning Loss: 0.0004405814688652754 17.968036651611328 2.019361972808838\n",
            "21707 4.8145415348699316e-05 0.00016137816419359297 Traning Loss: 0.0002095235831802711 17.967552185058594 2.0195958614349365\n",
            "21708 5.2583174692699686e-05 0.0003517771838232875 Traning Loss: 0.00040436035487800837 17.967111587524414 2.0199334621429443\n",
            "21709 8.216969581553712e-05 0.0005697524757124484 Traning Loss: 0.0006519221933558583 17.966745376586914 2.0201644897460938\n",
            "21710 6.812220817664638e-05 0.0005359397036954761 Traning Loss: 0.0006040619336999953 17.966352462768555 2.0204222202301025\n",
            "21711 4.188081584288739e-05 0.00027208655956201255 Traning Loss: 0.00031396737904287875 17.966064453125 2.0206539630889893\n",
            "21712 2.5555920728947967e-05 0.00010578280489426106 Traning Loss: 0.00013133871834725142 17.965784072875977 2.0208048820495605\n",
            "21713 2.4670194761711173e-05 0.00019191093451809138 Traning Loss: 0.00021658113109879196 17.965545654296875 2.0210256576538086\n",
            "21714 4.777783033205196e-05 0.00035350758116692305 Traning Loss: 0.0004012854187749326 17.965347290039062 2.0211310386657715\n",
            "21715 4.270929275662638e-05 0.00040154217276722193 Traning Loss: 0.0004442514618858695 17.965137481689453 2.0212979316711426\n",
            "21716 3.23410022247117e-05 0.0002587774069979787 Traning Loss: 0.00029111839830875397 17.964998245239258 2.0213921070098877\n",
            "21717 1.4263063349062577e-05 0.00010506982653168961 Traning Loss: 0.000119332893518731 17.96487808227539 2.0214664936065674\n",
            "21718 8.89857346919598e-06 8.196589624276385e-05 Traning Loss: 9.086447244044393e-05 17.964811325073242 2.0215635299682617\n",
            "21719 2.2192962205735967e-05 0.0001741470186971128 Traning Loss: 0.00019633998454082757 17.96478843688965 2.0215766429901123\n",
            "21720 2.5708051907713525e-05 0.0002695987932384014 Traning Loss: 0.00029530684696510434 17.964786529541016 2.0216572284698486\n",
            "21721 2.864619455067441e-05 0.00024735592887736857 Traning Loss: 0.0002760021307040006 17.964841842651367 2.0216445922851562\n",
            "21722 1.486809105699649e-05 0.00015390495536848903 Traning Loss: 0.0001687730400590226 17.964921951293945 2.0216610431671143\n",
            "21723 8.159987373801414e-06 7.002928032306954e-05 Traning Loss: 7.818926678737625e-05 17.96504020690918 2.02164888381958\n",
            "21724 8.193825124180876e-06 6.975476571824402e-05 Traning Loss: 7.79485926614143e-05 17.965181350708008 2.0216121673583984\n",
            "21725 1.2288866855669767e-05 0.00013068703992757946 Traning Loss: 0.00014297591405920684 17.96532440185547 2.0216164588928223\n",
            "21726 2.0035600755363703e-05 0.0001730362419039011 Traning Loss: 0.0001930718426592648 17.965490341186523 2.021554470062256\n",
            "21727 1.550130582472775e-05 0.00016469416732434183 Traning Loss: 0.0001801954786060378 17.965660095214844 2.02154278755188\n",
            "21728 1.2925505870953202e-05 0.0001078337081708014 Traning Loss: 0.0001207592140417546 17.965848922729492 2.0214779376983643\n",
            "21729 6.377713361871429e-06 6.653425225522369e-05 Traning Loss: 7.291196379810572e-05 17.966054916381836 2.0214357376098633\n",
            "21730 6.9628918026864994e-06 6.458096322603524e-05 Traning Loss: 7.154385821195319e-05 17.966264724731445 2.021393299102783\n",
            "21731 1.0013242899731267e-05 9.316940122516826e-05 Traning Loss: 0.00010318264685338363 17.966503143310547 2.021329402923584\n",
            "21732 1.1346391147526447e-05 0.00011703919153660536 Traning Loss: 0.0001283855817746371 17.96674346923828 2.0212953090667725\n",
            "21733 1.2143869753344916e-05 0.00010952517914120108 Traning Loss: 0.0001216690507135354 17.967010498046875 2.0212180614471436\n",
            "21734 7.861113772378303e-06 8.515737135894597e-05 Traning Loss: 9.301848331233487e-05 17.967288970947266 2.0211753845214844\n",
            "21735 7.058898518153001e-06 6.270054291235283e-05 Traning Loss: 6.975944415898994e-05 17.96756935119629 2.021111249923706\n",
            "21736 5.909264018555405e-06 6.329912139335647e-05 Traning Loss: 6.920838495716453e-05 17.967866897583008 2.0210607051849365\n",
            "21737 7.922459190012887e-06 7.667953468626365e-05 Traning Loss: 8.460199751425534e-05 17.96815299987793 2.0210142135620117\n",
            "21738 8.895330211089458e-06 8.74080287758261e-05 Traning Loss: 9.630335989641026e-05 17.96845817565918 2.020953416824341\n",
            "21739 8.16484316601418e-06 8.418285142397508e-05 Traning Loss: 9.234769095201045e-05 17.968753814697266 2.020911455154419\n",
            "21740 7.334250312851509e-06 6.999251490924507e-05 Traning Loss: 7.732676749583334e-05 17.969057083129883 2.020852565765381\n",
            "21741 5.4588663260801695e-06 5.994188177282922e-05 Traning Loss: 6.540074537042528e-05 17.969364166259766 2.020812511444092\n",
            "21742 6.173456313263159e-06 5.9113317547598854e-05 Traning Loss: 6.528677477035671e-05 17.969663619995117 2.020763874053955\n",
            "21743 6.3133688854577485e-06 6.75064220558852e-05 Traning Loss: 7.3819792305585e-05 17.969980239868164 2.020718574523926\n",
            "21744 7.371881110884715e-06 7.348856888711452e-05 Traning Loss: 8.086045272648335e-05 17.97028350830078 2.02067494392395\n",
            "21745 6.902154382260051e-06 7.248728798003867e-05 Traning Loss: 7.938944327179343e-05 17.970600128173828 2.0206284523010254\n",
            "21746 6.17083514953265e-06 6.497427239082754e-05 Traning Loss: 7.114510663086548e-05 17.970905303955078 2.0205912590026855\n",
            "21747 5.478993898577755e-06 5.774348392151296e-05 Traning Loss: 6.322247645584866e-05 17.971214294433594 2.020551919937134\n",
            "21748 5.130284534970997e-06 5.621576201519929e-05 Traning Loss: 6.134604336693883e-05 17.97152328491211 2.020516872406006\n",
            "21749 5.7048332564590964e-06 5.958656402071938e-05 Traning Loss: 6.529139500344172e-05 17.97182273864746 2.0204808712005615\n",
            "21750 5.858348231413402e-06 6.43842649878934e-05 Traning Loss: 7.02426114003174e-05 17.972129821777344 2.0204453468322754\n",
            "21751 6.200346433615778e-06 6.540634058183059e-05 Traning Loss: 7.160668610595167e-05 17.97241973876953 2.020411491394043\n",
            "21752 5.5948789849935565e-06 6.288378062890843e-05 Traning Loss: 6.847865734016523e-05 17.97271728515625 2.020378828048706\n",
            "21753 5.353772849048255e-06 5.831240923726e-05 Traning Loss: 6.36661789030768e-05 17.973003387451172 2.0203440189361572\n",
            "21754 4.802538114745403e-06 5.588503336184658e-05 Traning Loss: 6.0687572840834036e-05 17.973289489746094 2.020311117172241\n",
            "21755 4.915922090731328e-06 5.605683691101149e-05 Traning Loss: 6.097275763750076e-05 17.97357177734375 2.0202722549438477\n",
            "21756 4.928211183141684e-06 5.8323472330812365e-05 Traning Loss: 6.3251682149712e-05 17.973846435546875 2.020237922668457\n",
            "21757 5.035604772274382e-06 6.006066541885957e-05 Traning Loss: 6.509626837214455e-05 17.974123001098633 2.0201995372772217\n",
            "21758 5.006509127269965e-06 5.9957150369882584e-05 Traning Loss: 6.496365676866844e-05 17.974388122558594 2.02016282081604\n",
            "21759 4.6361483327928e-06 5.848028013133444e-05 Traning Loss: 6.311642937362194e-05 17.974658966064453 2.020124673843384\n",
            "21760 4.575283583108103e-06 5.645737473969348e-05 Traning Loss: 6.103265695855953e-05 17.97492218017578 2.0200817584991455\n",
            "21761 4.1794278331508394e-06 5.586882616626099e-05 Traning Loss: 6.0048252635169774e-05 17.975187301635742 2.020042896270752\n",
            "21762 4.411805548443226e-06 5.60314510948956e-05 Traning Loss: 6.044325709808618e-05 17.97545051574707 2.019995927810669\n",
            "21763 4.22415041612112e-06 5.7236688007833436e-05 Traning Loss: 6.14608361502178e-05 17.975711822509766 2.0199553966522217\n",
            "21764 4.457780050870497e-06 5.7609693612903357e-05 Traning Loss: 6.206747639225796e-05 17.97597312927246 2.0199055671691895\n",
            "21765 4.24708105128957e-06 5.7498502428643405e-05 Traning Loss: 6.174558075144887e-05 17.976228713989258 2.0198593139648438\n",
            "21766 4.206978701404296e-06 5.650895764119923e-05 Traning Loss: 6.071593816159293e-05 17.976484298706055 2.019807815551758\n",
            "21767 4.065955636178842e-06 5.560302088269964e-05 Traning Loss: 5.966897515463643e-05 17.97673225402832 2.0197560787200928\n",
            "21768 3.9278297663258854e-06 5.525062442757189e-05 Traning Loss: 5.917845555813983e-05 17.97697639465332 2.0197055339813232\n",
            "21769 4.04407546739094e-06 5.5317330406978726e-05 Traning Loss: 5.9361405874369666e-05 17.977216720581055 2.0196495056152344\n",
            "21770 3.887645107170101e-06 5.5998985772021115e-05 Traning Loss: 5.988663178868592e-05 17.977453231811523 2.0195980072021484\n",
            "21771 4.089937647222541e-06 5.618275099550374e-05 Traning Loss: 6.0272686823736876e-05 17.977685928344727 2.019538402557373\n",
            "21772 3.841005764115835e-06 5.6383476476185024e-05 Traning Loss: 6.022448360454291e-05 17.977914810180664 2.0194849967956543\n",
            "21773 3.955286956625059e-06 5.5812415666878223e-05 Traning Loss: 5.976770262350328e-05 17.97814178466797 2.019425392150879\n",
            "21774 3.710905048137647e-06 5.5460248404415324e-05 Traning Loss: 5.9171154134674e-05 17.978364944458008 2.0193698406219482\n",
            "21775 3.7624949982273392e-06 5.497036545420997e-05 Traning Loss: 5.873286136193201e-05 17.978586196899414 2.019310712814331\n",
            "21776 3.6543151509249583e-06 5.494571814779192e-05 Traning Loss: 5.860003147972748e-05 17.978805541992188 2.019252061843872\n",
            "21777 3.665416215881123e-06 5.505163062480278e-05 Traning Loss: 5.871704706805758e-05 17.979021072387695 2.0191941261291504\n",
            "21778 3.679649580590194e-06 5.522336869034916e-05 Traning Loss: 5.89030169066973e-05 17.97923469543457 2.019134283065796\n",
            "21779 3.6203307445248356e-06 5.536051685339771e-05 Traning Loss: 5.898084782529622e-05 17.979442596435547 2.0190775394439697\n",
            "21780 3.6730164083564887e-06 5.520619743037969e-05 Traning Loss: 5.88792136113625e-05 17.97964859008789 2.019017457962036\n",
            "21781 3.5473599382385146e-06 5.509532638825476e-05 Traning Loss: 5.864268678124063e-05 17.97985076904297 2.01896071434021\n",
            "21782 3.5996406495542033e-06 5.478549428516999e-05 Traning Loss: 5.838513607159257e-05 17.98004913330078 2.0189011096954346\n",
            "21783 3.4737015539576532e-06 5.4739830375183374e-05 Traning Loss: 5.821353261126205e-05 17.980243682861328 2.0188443660736084\n",
            "21784 3.531453785399208e-06 5.462543049361557e-05 Traning Loss: 5.815688564325683e-05 17.980432510375977 2.0187861919403076\n",
            "21785 3.4446761674189474e-06 5.4745582019677386e-05 Traning Loss: 5.819025682285428e-05 17.980621337890625 2.0187294483184814\n",
            "21786 3.488624315650668e-06 5.47502568224445e-05 Traning Loss: 5.823888204758987e-05 17.980804443359375 2.018672227859497\n",
            "21787 3.4183258321718313e-06 5.481921471073292e-05 Traning Loss: 5.823753963341005e-05 17.980987548828125 2.018615484237671\n",
            "21788 3.4283052627870347e-06 5.4731797717977315e-05 Traning Loss: 5.81601016165223e-05 17.981164932250977 2.018559217453003\n",
            "21789 3.3618168799876003e-06 5.466348375193775e-05 Traning Loss: 5.802529994980432e-05 17.981340408325195 2.0185039043426514\n",
            "21790 3.3560720567038516e-06 5.4517655371455476e-05 Traning Loss: 5.787372720078565e-05 17.98151206970215 2.018449068069458\n",
            "21791 3.301070591987809e-06 5.444828639156185e-05 Traning Loss: 5.774935561930761e-05 17.98168182373047 2.018394947052002\n",
            "21792 3.2980467494780896e-06 5.4375403124140576e-05 Traning Loss: 5.767344919149764e-05 17.981847763061523 2.018340826034546\n",
            "21793 3.2554055451328168e-06 5.438685911940411e-05 Traning Loss: 5.764226443716325e-05 17.982011795043945 2.0182881355285645\n",
            "21794 3.2546543025091523e-06 5.438175503513776e-05 Traning Loss: 5.763641092926264e-05 17.982173919677734 2.018235206604004\n",
            "21795 3.2209575238084653e-06 5.4405449191108346e-05 Traning Loss: 5.7626406487543136e-05 17.982332229614258 2.018183708190918\n",
            "21796 3.210002887499286e-06 5.438443258753978e-05 Traning Loss: 5.7594435929786414e-05 17.98248863220215 2.018131732940674\n",
            "21797 3.1806171136850026e-06 5.435482307802886e-05 Traning Loss: 5.753543882747181e-05 17.982641220092773 2.018080711364746\n",
            "21798 3.1499555461778073e-06 5.430758392321877e-05 Traning Loss: 5.745753878727555e-05 17.982791900634766 2.0180299282073975\n",
            "21799 3.1313165891333483e-06 5.4245636420091614e-05 Traning Loss: 5.7376953918719664e-05 17.982938766479492 2.017979621887207\n",
            "21800 3.0849860195303336e-06 5.422099638963118e-05 Traning Loss: 5.730598059017211e-05 17.983083724975586 2.017930269241333\n",
            "21801 3.084903028138797e-06 5.416752537712455e-05 Traning Loss: 5.725242954213172e-05 17.983224868774414 2.017880439758301\n",
            "21802 3.0270668958110036e-06 5.418655200628564e-05 Traning Loss: 5.721361958421767e-05 17.98336410522461 2.0178322792053223\n",
            "21803 3.04171612697246e-06 5.4144060413818806e-05 Traning Loss: 5.7185778132406995e-05 17.983501434326172 2.0177829265594482\n",
            "21804 2.974107019326766e-06 5.4186544730328023e-05 Traning Loss: 5.716065061278641e-05 17.9836368560791 2.017735719680786\n",
            "21805 2.9979285045556026e-06 5.413054532255046e-05 Traning Loss: 5.7128472690237686e-05 17.9837703704834 2.0176870822906494\n",
            "21806 2.9246452868392225e-06 5.416480780695565e-05 Traning Loss: 5.708945172955282e-05 17.983901977539062 2.0176405906677246\n",
            "21807 2.9532034204748925e-06 5.408855577115901e-05 Traning Loss: 5.704175782739185e-05 17.98402976989746 2.017592430114746\n",
            "21808 2.8771182769560255e-06 5.4111889767227694e-05 Traning Loss: 5.698900713468902e-05 17.98415756225586 2.0175464153289795\n",
            "21809 2.9080331387376646e-06 5.4029678722145036e-05 Traning Loss: 5.6937711633509025e-05 17.984281539916992 2.0174989700317383\n",
            "21810 2.8315073450357886e-06 5.405959382187575e-05 Traning Loss: 5.6891101849032566e-05 17.984403610229492 2.01745343208313\n",
            "21811 2.8668209779425524e-06 5.398303619585931e-05 Traning Loss: 5.684985808329657e-05 17.98452377319336 2.017406463623047\n",
            "21812 2.7892406251339708e-06 5.4028067097533494e-05 Traning Loss: 5.681730908690952e-05 17.984642028808594 2.0173614025115967\n",
            "21813 2.8311399091762723e-06 5.395691187004559e-05 Traning Loss: 5.6788052461342886e-05 17.984758377075195 2.0173146724700928\n",
            "21814 2.7477037747303257e-06 5.401847374741919e-05 Traning Loss: 5.6766177294775844e-05 17.984872817993164 2.01727032661438\n",
            "21815 2.801139999064617e-06 5.394516483647749e-05 Traning Loss: 5.674630665453151e-05 17.9849853515625 2.017223834991455\n",
            "21816 2.7055605187342735e-06 5.402579699875787e-05 Traning Loss: 5.673135819961317e-05 17.985097885131836 2.0171802043914795\n",
            "21817 2.7792993932962418e-06 5.394208710640669e-05 Traning Loss: 5.672138649970293e-05 17.985206604003906 2.017133951187134\n",
            "21818 2.662920451257378e-06 5.405792035162449e-05 Traning Loss: 5.672084080288187e-05 17.985315322875977 2.0170910358428955\n",
            "21819 2.767883415799588e-06 5.3968011343386024e-05 Traning Loss: 5.673589475918561e-05 17.98542022705078 2.0170445442199707\n",
            "21820 2.62099752035283e-06 5.4149142670212314e-05 Traning Loss: 5.6770139053696766e-05 17.985525131225586 2.017002820968628\n",
            "21821 2.7744847557187313e-06 5.406545824371278e-05 Traning Loss: 5.683994459104724e-05 17.985626220703125 2.016955852508545\n",
            "21822 2.5858164462988498e-06 5.436945502879098e-05 Traning Loss: 5.695527215721086e-05 17.985729217529297 2.016915798187256\n",
            "21823 2.8134079457231564e-06 5.432870239019394e-05 Traning Loss: 5.714211147278547e-05 17.98582649230957 2.0168678760528564\n",
            "21824 2.5683618787297746e-06 5.486289592226967e-05 Traning Loss: 5.743125802837312e-05 17.985925674438477 2.0168297290802\n",
            "21825 2.911652018156019e-06 5.4961852583801374e-05 Traning Loss: 5.787350528407842e-05 17.98601722717285 2.016780376434326\n",
            "21826 2.596453441583435e-06 5.594880349235609e-05 Traning Loss: 5.8545258070807904e-05 17.986114501953125 2.01674485206604\n",
            "21827 3.1314680200011935e-06 5.6427907111356035e-05 Traning Loss: 5.9559373767115176e-05 17.986202239990234 2.016693115234375\n",
            "21828 2.7410092116042506e-06 5.8363926655147225e-05 Traning Loss: 6.110493268352002e-05 17.986299514770508 2.0166616439819336\n",
            "21829 3.613749868236482e-06 5.9849582612514496e-05 Traning Loss: 6.346333248075098e-05 17.98638153076172 2.016605854034424\n",
            "21830 3.1823165045352653e-06 6.390753696905449e-05 Traning Loss: 6.708985165460035e-05 17.98647689819336 2.01658034324646\n",
            "21831 4.6937707338656764e-06 6.799209222663194e-05 Traning Loss: 7.268586341524497e-05 17.98655128479004 2.0165183544158936\n",
            "21832 4.385551619634498e-06 7.702584844082594e-05 Traning Loss: 8.141140278894454e-05 17.98664665222168 2.0165021419525146\n",
            "21833 7.197252216428751e-06 8.785767568042502e-05 Traning Loss: 9.505492926109582e-05 17.986709594726562 2.016429901123047\n",
            "21834 7.565857231384143e-06 0.00010907874820986763 Traning Loss: 0.00011664460180327296 17.986806869506836 2.016428232192993\n",
            "21835 1.3221758308645803e-05 0.00013757548003923148 Traning Loss: 0.00015079723380040377 17.986852645874023 2.0163395404815674\n",
            "21836 1.5929119399515912e-05 0.00018954054394271225 Traning Loss: 0.00020546966698020697 17.986949920654297 2.016361713409424\n",
            "21837 2.816202504618559e-05 0.0002643009938765317 Traning Loss: 0.0002924630243796855 17.986963272094727 2.016246795654297\n",
            "21838 3.7847425119252875e-05 0.00039458629908040166 Traning Loss: 0.00043243373511359096 17.9870548248291 2.0163075923919678\n",
            "21839 6.56379634165205e-05 0.000587980670388788 Traning Loss: 0.0006536186556331813 17.98700523376465 2.0161521434783936\n",
            "21840 9.399488772032782e-05 0.0009109639213420451 Traning Loss: 0.0010049587581306696 17.987064361572266 2.016275405883789\n",
            "21841 0.00015658221673220396 0.001381632755510509 Traning Loss: 0.001538214972242713 17.98688507080078 2.0160627365112305\n",
            "21842 0.0002256165025755763 0.002108771586790681 Traning Loss: 0.0023343879729509354 17.986845016479492 2.016279458999634\n",
            "21843 0.0003469234798103571 0.003047534730285406 Traning Loss: 0.003394458210095763 17.986413955688477 2.0160069465637207\n",
            "21844 0.00045899429824203253 0.004220004193484783 Traning Loss: 0.0046789986081421375 17.986125946044922 2.016333818435669\n",
            "21845 0.0005921873962506652 0.005173965357244015 Traning Loss: 0.005766152869910002 17.985305786132812 2.016054153442383\n",
            "21846 0.0006051996606402099 0.005551374051719904 Traning Loss: 0.006156573537737131 17.98464012145996 2.0164358615875244\n",
            "21847 0.0005409899749793112 0.004656898323446512 Traning Loss: 0.0051978882402181625 17.983531951904297 2.016287088394165\n",
            "21848 0.0003038898285012692 0.002853921614587307 Traning Loss: 0.0031578114721924067 17.982664108276367 2.0165746212005615\n",
            "21849 0.0001255578245036304 0.0010228856699541211 Traning Loss: 0.0011484434362500906 17.981706619262695 2.016680955886841\n",
            "21850 4.02180667151697e-05 0.00032224509050138295 Traning Loss: 0.00036246314994059503 17.98091697692871 2.0167648792266846\n",
            "21851 9.147891978500411e-05 0.000871791155077517 Traning Loss: 0.0009632700821384788 17.980268478393555 2.017099618911743\n",
            "21852 0.0002206267963629216 0.0017228139331564307 Traning Loss: 0.0019434407586231828 17.979480743408203 2.017083168029785\n",
            "21853 0.0002158103889087215 0.0020023619290441275 Traning Loss: 0.0022181724198162556 17.97884178161621 2.017474889755249\n",
            "21854 0.0001729373325360939 0.0012880266876891255 Traning Loss: 0.0014609639765694737 17.9780216217041 2.017599582672119\n",
            "21855 5.950198828941211e-05 0.0004842777270823717 Traning Loss: 0.000543779693543911 17.977312088012695 2.0178627967834473\n",
            "21856 4.326177804614417e-05 0.0003123327041976154 Traning Loss: 0.00035559447132982314 17.97661018371582 2.018186569213867\n",
            "21857 0.00010712217772379518 0.0007541636005043983 Traning Loss: 0.0008612857782281935 17.975908279418945 2.018296718597412\n",
            "21858 0.00012768871965818107 0.0011710250983014703 Traning Loss: 0.0012987138470634818 17.975303649902344 2.018684148788452\n",
            "21859 0.00012811730266548693 0.0009217056795023382 Traning Loss: 0.0010498230112716556 17.974628448486328 2.018812656402588\n",
            "21860 4.9142836360260844e-05 0.0003833344380836934 Traning Loss: 0.00043247727444395423 17.974063873291016 2.01910138130188\n",
            "21861 2.2842488760943525e-05 9.030284854816273e-05 Traning Loss: 0.00011314533912809566 17.973529815673828 2.019368886947632\n",
            "21862 4.888911280431785e-05 0.00030671473359689116 Traning Loss: 0.0003556038427632302 17.973031997680664 2.0194993019104004\n",
            "21863 7.729863136773929e-05 0.0006787041202187538 Traning Loss: 0.0007560027297586203 17.972623825073242 2.0198099613189697\n",
            "21864 9.232228330802172e-05 0.0006944384076632559 Traning Loss: 0.0007867607055231929 17.972217559814453 2.01989483833313\n",
            "21865 4.657398312701844e-05 0.00040415447438135743 Traning Loss: 0.0004507284611463547 17.971891403198242 2.020113468170166\n",
            "21866 1.9727491235244088e-05 0.00010300859867129475 Traning Loss: 0.00012273609172552824 17.971616744995117 2.0202600955963135\n",
            "21867 1.9479501133901067e-05 0.00010053368896478787 Traning Loss: 0.00012001318827969953 17.971363067626953 2.0203278064727783\n",
            "21868 3.478779399301857e-05 0.0003056396380998194 Traning Loss: 0.0003404274466447532 17.971179962158203 2.020503044128418\n",
            "21869 5.5644730309722945e-05 0.0004118100623600185 Traning Loss: 0.000467454781755805 17.97096061706543 2.020496368408203\n",
            "21870 3.495435157674365e-05 0.00033409465686418116 Traning Loss: 0.0003690489975269884 17.970788955688477 2.0206096172332764\n",
            "21871 2.1529220248339698e-05 0.00015662361693102866 Traning Loss: 0.00017815284081734717 17.970643997192383 2.0206334590911865\n",
            "21872 1.2517453797045164e-05 9.75771399680525e-05 Traning Loss: 0.00011009459558408707 17.970510482788086 2.0206260681152344\n",
            "21873 1.768165202520322e-05 0.0001776001590769738 Traning Loss: 0.00019528181292116642 17.97046661376953 2.020670175552368\n",
            "21874 3.387474862392992e-05 0.00025756118702702224 Traning Loss: 0.00029143592109903693 17.97039794921875 2.020583152770996\n",
            "21875 2.341832259844523e-05 0.00024857104290276766 Traning Loss: 0.00027198935276828706 17.970407485961914 2.020599842071533\n",
            "21876 1.8017652109847404e-05 0.00013936303730588406 Traning Loss: 0.00015738069487269968 17.97040557861328 2.0205297470092773\n",
            "21877 5.519772003026446e-06 6.471542292274535e-05 Traning Loss: 7.023519719950855e-05 17.970439910888672 2.020476818084717\n",
            "21878 6.427005700970767e-06 7.903603545855731e-05 Traning Loss: 8.546304161427543e-05 17.97052574157715 2.0204458236694336\n",
            "21879 1.8075408661388792e-05 0.00014432152966037393 Traning Loss: 0.00016239694377873093 17.97059440612793 2.0203325748443604\n",
            "21880 1.6542851881240495e-05 0.00019094767048954964 Traning Loss: 0.00020749052055180073 17.97072982788086 2.020303964614868\n",
            "21881 1.949665784195531e-05 0.00015770392201375216 Traning Loss: 0.00017720057803671807 17.97083282470703 2.020197868347168\n",
            "21882 8.325043381773867e-06 0.0001024537777993828 Traning Loss: 0.00011077881936216727 17.970979690551758 2.0201356410980225\n",
            "21883 6.51233813186991e-06 6.77743591950275e-05 Traning Loss: 7.428669778164476e-05 17.97112274169922 2.020068407058716\n",
            "21884 8.981880455394275e-06 8.079141116468236e-05 Traning Loss: 8.977328980108723e-05 17.971271514892578 2.01996111869812\n",
            "21885 9.457326086703688e-06 0.0001142036053352058 Traning Loss: 0.00012366092414595187 17.971445083618164 2.0199172496795654\n",
            "21886 1.3988058526592795e-05 0.00011725229705916718 Traning Loss: 0.00013124036195222288 17.971601486206055 2.0198073387145996\n",
            "21887 7.65788627177244e-06 9.673272870713845e-05 Traning Loss: 0.00010439061588840559 17.97178840637207 2.0197558403015137\n",
            "21888 6.654397111560684e-06 6.42873055767268e-05 Traning Loss: 7.094169995980337e-05 17.971975326538086 2.019679069519043\n",
            "21889 4.983299731975421e-06 5.682256232830696e-05 Traning Loss: 6.180586206028238e-05 17.972179412841797 2.0195975303649902\n",
            "21890 5.778884315077448e-06 7.359233859460801e-05 Traning Loss: 7.937122427392751e-05 17.972400665283203 2.019550323486328\n",
            "21891 9.880793186312076e-06 9.063832112587988e-05 Traning Loss: 0.00010051911522168666 17.972620010375977 2.019458293914795\n",
            "21892 7.706007636443246e-06 9.583467908669263e-05 Traning Loss: 0.00010354068945161998 17.972856521606445 2.019422769546509\n",
            "21893 8.191592314688023e-06 7.901355274952948e-05 Traning Loss: 8.72051459737122e-05 17.973093032836914 2.01935076713562\n",
            "21894 5.112337476020912e-06 6.380090053426102e-05 Traning Loss: 6.891323573654518e-05 17.973337173461914 2.0192983150482178\n",
            "21895 4.766361598740332e-06 5.8962606999557465e-05 Traning Loss: 6.37289704172872e-05 17.973600387573242 2.0192508697509766\n",
            "21896 6.525283879454946e-06 6.501429743366316e-05 Traning Loss: 7.153957994887605e-05 17.973852157592773 2.0191805362701416\n",
            "21897 5.6833323469618335e-06 7.459957123501226e-05 Traning Loss: 8.028290176298469e-05 17.974123001098633 2.0191490650177\n",
            "21898 7.616577931912616e-06 7.194415229605511e-05 Traning Loss: 7.956073386594653e-05 17.974374771118164 2.0190839767456055\n",
            "21899 4.905803507426754e-06 6.513770495075732e-05 Traning Loss: 7.004350482020527e-05 17.974637985229492 2.0190484523773193\n",
            "21900 4.984118277207017e-06 5.5598520702915266e-05 Traning Loss: 6.058263898012228e-05 17.974895477294922 2.019000768661499\n",
            "21901 4.521775281318696e-06 5.411843449110165e-05 Traning Loss: 5.8640209317673e-05 17.975149154663086 2.0189507007598877\n",
            "21902 4.339723545854213e-06 5.949827755102888e-05 Traning Loss: 6.383800064213574e-05 17.975414276123047 2.018921136856079\n",
            "21903 6.093870524637168e-06 6.367585592670366e-05 Traning Loss: 6.976972508709878e-05 17.975664138793945 2.0188674926757812\n",
            "21904 4.723015081253834e-06 6.60491714370437e-05 Traning Loss: 7.077218469930813e-05 17.975929260253906 2.0188424587249756\n",
            "21905 5.573378075496294e-06 6.107271474320441e-05 Traning Loss: 6.66460909997113e-05 17.97618293762207 2.018796920776367\n",
            "21906 4.150193490204401e-06 5.748279727413319e-05 Traning Loss: 6.1632992583327e-05 17.976444244384766 2.0187625885009766\n",
            "21907 4.227981662552338e-06 5.5619137128815055e-05 Traning Loss: 5.9847119700862095e-05 17.976699829101562 2.018731117248535\n",
            "21908 4.607807568390854e-06 5.7183489843737334e-05 Traning Loss: 6.179129559313878e-05 17.976951599121094 2.0186893939971924\n",
            "21909 4.2036358536279295e-06 6.047201168257743e-05 Traning Loss: 6.46756452624686e-05 17.977203369140625 2.0186679363250732\n",
            "21910 5.105071068101097e-06 6.0328027757350355e-05 Traning Loss: 6.543310155393556e-05 17.977447509765625 2.018625020980835\n",
            "21911 4.045325113111176e-06 5.933285865467042e-05 Traning Loss: 6.337818194879219e-05 17.977691650390625 2.0185999870300293\n",
            "21912 4.28785369877005e-06 5.592748857452534e-05 Traning Loss: 6.021534136380069e-05 17.977930068969727 2.0185635089874268\n",
            "21913 3.808122755799559e-06 5.44621107110288e-05 Traning Loss: 5.8270234148949385e-05 17.978160858154297 2.0185303688049316\n",
            "21914 3.66659446626727e-06 5.476625301525928e-05 Traning Loss: 5.843284816364758e-05 17.978391647338867 2.018502712249756\n",
            "21915 4.179557436145842e-06 5.56126469746232e-05 Traning Loss: 5.9792204410769045e-05 17.97861099243164 2.018462896347046\n",
            "21916 3.6199521673552226e-06 5.709549077437259e-05 Traning Loss: 6.0715443396475166e-05 17.978836059570312 2.0184361934661865\n",
            "21917 4.1481662265141495e-06 5.612916356767528e-05 Traning Loss: 6.0277328884694725e-05 17.97905158996582 2.0183942317962646\n",
            "21918 3.413769263715949e-06 5.5415144743165e-05 Traning Loss: 5.882891491637565e-05 17.979270935058594 2.018362045288086\n",
            "21919 3.567406338333967e-06 5.3890806157141924e-05 Traning Loss: 5.7458211813354865e-05 17.97948455810547 2.018324136734009\n",
            "21920 3.3944293136300985e-06 5.361017974792048e-05 Traning Loss: 5.7004610425792634e-05 17.97969627380371 2.018284320831299\n",
            "21921 3.2757761800894514e-06 5.4201816965360194e-05 Traning Loss: 5.747759132646024e-05 17.979907989501953 2.018249273300171\n",
            "21922 3.6372605336509878e-06 5.4583553719567135e-05 Traning Loss: 5.822081584483385e-05 17.980113983154297 2.0182032585144043\n",
            "21923 3.22351706927293e-06 5.5300668464042246e-05 Traning Loss: 5.852418689755723e-05 17.980321884155273 2.018167018890381\n",
            "21924 3.572112518668291e-06 5.4584215831710026e-05 Traning Loss: 5.815632903249934e-05 17.98052215576172 2.018120288848877\n",
            "21925 3.1405047593580093e-06 5.428240183391608e-05 Traning Loss: 5.742290522903204e-05 17.980724334716797 2.0180792808532715\n",
            "21926 3.2631983231112827e-06 5.3578678489429876e-05 Traning Loss: 5.684187635779381e-05 17.980920791625977 2.018033981323242\n",
            "21927 3.192148369635106e-06 5.3534149628831074e-05 Traning Loss: 5.6726297771092504e-05 17.981115341186523 2.0179860591888428\n",
            "21928 3.0939006592234364e-06 5.39179309271276e-05 Traning Loss: 5.701183181372471e-05 17.981307983398438 2.017942190170288\n",
            "21929 3.322041038700263e-06 5.405595584306866e-05 Traning Loss: 5.73779980186373e-05 17.981494903564453 2.0178909301757812\n",
            "21930 3.045567609660793e-06 5.447584771900438e-05 Traning Loss: 5.752141441917047e-05 17.98168182373047 2.0178470611572266\n",
            "21931 3.2717339308874216e-06 5.40895271115005e-05 Traning Loss: 5.736125967814587e-05 17.98186492919922 2.0177955627441406\n",
            "21932 3.0136675377434585e-06 5.4038922826293856e-05 Traning Loss: 5.705259172827937e-05 17.982044219970703 2.0177488327026367\n",
            "21933 3.0857170258968836e-06 5.374021930037998e-05 Traning Loss: 5.682593473466113e-05 17.982223510742188 2.0176992416381836\n",
            "21934 3.0638693715445697e-06 5.377350316848606e-05 Traning Loss: 5.683737254003063e-05 17.982397079467773 2.0176496505737305\n",
            "21935 2.9808679755660705e-06 5.410182347986847e-05 Traning Loss: 5.7082692364929244e-05 17.982572555541992 2.0176029205322266\n",
            "21936 3.179158966304385e-06 5.4271415137918666e-05 Traning Loss: 5.745057569583878e-05 17.98274040222168 2.017552137374878\n",
            "21937 2.982898195114103e-06 5.483635686687194e-05 Traning Loss: 5.781925574410707e-05 17.982912063598633 2.0175061225891113\n",
            "21938 3.2332723094441462e-06 5.493547359947115e-05 Traning Loss: 5.816874545416795e-05 17.983074188232422 2.0174560546875\n",
            "21939 3.0455062187684234e-06 5.554315430345014e-05 Traning Loss: 5.858865915797651e-05 17.983240127563477 2.017409563064575\n",
            "21940 3.2579528124188073e-06 5.600635995506309e-05 Traning Loss: 5.9264311857987195e-05 17.983394622802734 2.017362117767334\n",
            "21941 3.238302042518626e-06 5.715168663300574e-05 Traning Loss: 6.038998981239274e-05 17.98355484008789 2.0173144340515137\n",
            "21942 3.4425995636411244e-06 5.8733705373015255e-05 Traning Loss: 6.21763028902933e-05 17.98370361328125 2.0172696113586426\n",
            "21943 3.6649084904638585e-06 6.118003511801362e-05 Traning Loss: 6.484494224423543e-05 17.983858108520508 2.017220973968506\n",
            "21944 3.9947863115230575e-06 6.478336581494659e-05 Traning Loss: 6.877815030748025e-05 17.983997344970703 2.0171782970428467\n",
            "21945 4.524325959209818e-06 7.000477489782497e-05 Traning Loss: 7.45291035855189e-05 17.984146118164062 2.017130136489868\n",
            "21946 5.311754193826346e-06 7.785674824845046e-05 Traning Loss: 8.316850289702415e-05 17.984275817871094 2.0170884132385254\n",
            "21947 6.442257017624797e-06 8.972108480520546e-05 Traning Loss: 9.616334136808291e-05 17.98442268371582 2.0170412063598633\n",
            "21948 8.398917088925373e-06 0.0001078221685020253 Traning Loss: 0.00011622108286246657 17.984539031982422 2.0169997215270996\n",
            "21949 1.1013504263246432e-05 0.00013580769882537425 Traning Loss: 0.00014682119945064187 17.984682083129883 2.016953229904175\n",
            "21950 1.5713505490566604e-05 0.0001790231908671558 Traning Loss: 0.0001947366981767118 17.98477554321289 2.0169126987457275\n",
            "21951 2.2093898223829456e-05 0.0002460604009684175 Traning Loss: 0.0002681543119251728 17.98491096496582 2.0168662071228027\n",
            "21952 3.3306630939478055e-05 0.0003511484246701002 Traning Loss: 0.00038445505197159946 17.984968185424805 2.016827344894409\n",
            "21953 4.905205059912987e-05 0.0005128210177645087 Traning Loss: 0.000561873079277575 17.985082626342773 2.0167787075042725\n",
            "21954 7.584435661556199e-05 0.0007682909490540624 Traning Loss: 0.0008441352983936667 17.98506736755371 2.016742706298828\n",
            "21955 0.00011348267435096204 0.001149425283074379 Traning Loss: 0.0012629079865291715 17.985126495361328 2.0166895389556885\n",
            "21956 0.00017502570699434727 0.001740628620609641 Traning Loss: 0.0019156542839482427 17.984962463378906 2.0166566371917725\n",
            "21957 0.0002551455982029438 0.0025481591001152992 Traning Loss: 0.002803304698318243 17.984878540039062 2.016596555709839\n",
            "21958 0.00037477948353625834 0.003693235572427511 Traning Loss: 0.004068015143275261 17.984426498413086 2.016563653945923\n",
            "21959 0.0004940319340676069 0.004902707878500223 Traning Loss: 0.005396739579737186 17.984045028686523 2.0164973735809326\n",
            "21960 0.0006268189172260463 0.006129347253590822 Traning Loss: 0.006756165996193886 17.98316192626953 2.016453981399536\n",
            "21961 0.0006460697040893137 0.006377741228789091 Traning Loss: 0.007023810874670744 17.982383728027344 2.0163960456848145\n",
            "21962 0.000578835781197995 0.005563368089497089 Traning Loss: 0.006142203696072102 17.981210708618164 2.016331434249878\n",
            "21963 0.0003481196181382984 0.003363810945302248 Traning Loss: 0.003711930476129055 17.980297088623047 2.01631498336792\n",
            "21964 0.0001381968759233132 0.0011473309714347124 Traning Loss: 0.0012855278328061104 17.979368209838867 2.016249656677246\n",
            "21965 3.126585215795785e-05 0.00015025006723590195 Traning Loss: 0.0001815159193938598 17.978649139404297 2.016286611557007\n",
            "21966 9.374472574563697e-05 0.0006958655430935323 Traning Loss: 0.0007896102615632117 17.978086471557617 2.0162789821624756\n",
            "21967 0.00021482612646650523 0.0018636228051036596 Traning Loss: 0.002078448887914419 17.97736930847168 2.0163283348083496\n",
            "21968 0.0002598838182166219 0.0023200742434710264 Traning Loss: 0.00257995817810297 17.97678565979004 2.016418218612671\n",
            "21969 0.00021259099594317377 0.0017130214255303144 Traning Loss: 0.0019256124505773187 17.97597885131836 2.0164573192596436\n",
            "21970 9.708894504001364e-05 0.000693631125614047 Traning Loss: 0.0007907200488261878 17.975313186645508 2.016631603240967\n",
            "21971 6.300424865912646e-05 0.00022113689919933677 Traning Loss: 0.00028414116241037846 17.97464370727539 2.0167086124420166\n",
            "21972 8.145324682118371e-05 0.000543092901352793 Traning Loss: 0.0006245461408980191 17.973966598510742 2.016888380050659\n",
            "21973 0.00013813344412483275 0.0010163990082219243 Traning Loss: 0.0011545324232429266 17.97340965270996 2.0170469284057617\n",
            "21974 0.00014515074144583195 0.0010980382794514298 Traning Loss: 0.0012431889772415161 17.972726821899414 2.017164945602417\n",
            "21975 0.00010245150770060718 0.000733942084480077 Traning Loss: 0.0008363935630768538 17.97220802307129 2.017401695251465\n",
            "21976 7.258173718582839e-05 0.00034913860145024955 Traning Loss: 0.0004217203240841627 17.97163963317871 2.017517566680908\n",
            "21977 4.795714630745351e-05 0.00028255494544282556 Traning Loss: 0.00033051209175027907 17.97113800048828 2.017775297164917\n",
            "21978 6.938930891919881e-05 0.0004192633496131748 Traning Loss: 0.0004886526730842888 17.970712661743164 2.0179731845855713\n",
            "21979 7.616111543029547e-05 0.000555893057025969 Traning Loss: 0.0006320541724562645 17.970230102539062 2.0181686878204346\n",
            "21980 7.029531116131693e-05 0.0005273151327855885 Traning Loss: 0.0005976104293949902 17.969890594482422 2.018433094024658\n",
            "21981 6.40185025986284e-05 0.00039130233926698565 Traning Loss: 0.00045532084186561406 17.96949577331543 2.0185699462890625\n",
            "21982 3.861931691062637e-05 0.0002873210178222507 Traning Loss: 0.0003259403456468135 17.96920394897461 2.0188379287719727\n",
            "21983 4.070317663718015e-05 0.00022944435477256775 Traning Loss: 0.00027014751685783267 17.968929290771484 2.018986463546753\n",
            "21984 3.2278778235195205e-05 0.00024442601716145873 Traning Loss: 0.0002767047844827175 17.968658447265625 2.0191690921783447\n",
            "21985 3.476239726296626e-05 0.000269774958724156 Traning Loss: 0.0003045373596251011 17.968482971191406 2.019347667694092\n",
            "21986 4.1263148887082934e-05 0.0002881075197365135 Traning Loss: 0.00032937066862359643 17.968252182006836 2.0194265842437744\n",
            "21987 3.106956501142122e-05 0.00029311695834621787 Traning Loss: 0.0003241865197196603 17.96811866760254 2.019609212875366\n",
            "21988 3.572849527699873e-05 0.00023995699302759022 Traning Loss: 0.00027568548102863133 17.967958450317383 2.0196568965911865\n",
            "21989 1.7590809875400737e-05 0.0001829275133786723 Traning Loss: 0.00020051831961609423 17.967853546142578 2.0197854042053223\n",
            "21990 1.639322545088362e-05 0.0001262410805793479 Traning Loss: 0.00014263430784922093 17.96779441833496 2.019848108291626\n",
            "21991 1.5212600374070462e-05 0.0001268267078557983 Traning Loss: 0.00014203930913936347 17.967735290527344 2.019881248474121\n",
            "21992 1.6053803847171366e-05 0.0001730599906295538 Traning Loss: 0.00018911379447672516 17.967761993408203 2.019965648651123\n",
            "21993 2.706945815589279e-05 0.00020655509433709085 Traning Loss: 0.00023362455249298364 17.967758178710938 2.0199408531188965\n",
            "21994 1.8610506231198087e-05 0.00020846494589932263 Traning Loss: 0.0002270754484925419 17.967838287353516 2.0200111865997314\n",
            "21995 1.9317159967613406e-05 0.00014822754019405693 Traning Loss: 0.00016754469834268093 17.967918395996094 2.0199790000915527\n",
            "21996 7.76788147049956e-06 9.295575728174299e-05 Traning Loss: 0.00010072364239022136 17.968042373657227 2.0199837684631348\n",
            "21997 6.765116268070415e-06 6.762441626051441e-05 Traning Loss: 7.438953616656363e-05 17.968198776245117 2.0199735164642334\n",
            "21998 1.087366763385944e-05 8.908200106816366e-05 Traning Loss: 9.99556650640443e-05 17.968339920043945 2.019920587539673\n",
            "21999 1.158109080279246e-05 0.0001337076537311077 Traning Loss: 0.00014528873725794256 17.96852684020996 2.0199368000030518\n",
            "22000 1.9135104594170116e-05 0.00014761390048079193 Traning Loss: 0.00016674899961799383 17.968673706054688 2.0198657512664795\n",
            "22001 1.1666733371384908e-05 0.0001346603676211089 Traning Loss: 0.0001463271037209779 17.968870162963867 2.0198681354522705\n",
            "22002 1.1309229194012005e-05 9.006830077851191e-05 Traning Loss: 0.00010137753270100802 17.96905517578125 2.0198121070861816\n",
            "22003 5.3056592150824144e-06 6.120426405686885e-05 Traning Loss: 6.650992145296186e-05 17.969263076782227 2.0197770595550537\n",
            "22004 5.112495273351669e-06 5.7679819292388856e-05 Traning Loss: 6.279231456574053e-05 17.969491958618164 2.0197548866271973\n",
            "22005 8.911963959690183e-06 7.530416769441217e-05 Traning Loss: 8.421613165410236e-05 17.969715118408203 2.0196921825408936\n",
            "22006 8.33015474199783e-06 9.913193207466975e-05 Traning Loss: 0.00010746208863565698 17.96997833251953 2.019685983657837\n",
            "22007 1.2336394320300315e-05 0.0001003590296022594 Traning Loss: 0.00011269542301306501 17.970218658447266 2.019618511199951\n",
            "22008 7.4591303018678445e-06 9.051229426404461e-05 Traning Loss: 9.79714241111651e-05 17.970495223999023 2.0196034908294678\n",
            "22009 7.711341822869144e-06 6.860411667730659e-05 Traning Loss: 7.631546031916514e-05 17.97075653076172 2.0195603370666504\n",
            "22010 5.064786364528118e-06 5.783899177913554e-05 Traning Loss: 6.29037749604322e-05 17.97102928161621 2.0195271968841553\n",
            "22011 5.016868271923158e-06 5.844311090186238e-05 Traning Loss: 6.345997826429084e-05 17.9713077545166 2.019510269165039\n",
            "22012 7.232823463709792e-06 6.535990542033687e-05 Traning Loss: 7.259273115778342e-05 17.97157859802246 2.0194618701934814\n",
            "22013 6.00236944592325e-06 7.46636142139323e-05 Traning Loss: 8.066598093137145e-05 17.97186851501465 2.0194547176361084\n",
            "22014 8.33286321721971e-06 7.33083434170112e-05 Traning Loss: 8.164120663423091e-05 17.972135543823242 2.0194101333618164\n",
            "22015 5.682024038833333e-06 7.054034358588979e-05 Traning Loss: 7.622236444149166e-05 17.972423553466797 2.0193986892700195\n",
            "22016 6.390672751876991e-06 6.270701851462945e-05 Traning Loss: 6.909768853802234e-05 17.97269630432129 2.0193700790405273\n",
            "22017 5.131429588800529e-06 5.937024252489209e-05 Traning Loss: 6.450166983995587e-05 17.972980499267578 2.0193440914154053\n",
            "22018 4.963863375451183e-06 5.857859650859609e-05 Traning Loss: 6.354245851980522e-05 17.973264694213867 2.0193285942077637\n",
            "22019 5.7015604397747666e-06 5.898491508560255e-05 Traning Loss: 6.468647916335613e-05 17.97354507446289 2.0192947387695312\n",
            "22020 4.749555955640972e-06 6.116795702837408e-05 Traning Loss: 6.591751298401505e-05 17.973833084106445 2.019286632537842\n",
            "22021 5.971197879262036e-06 6.0230708186281845e-05 Traning Loss: 6.620190833928064e-05 17.974105834960938 2.019256591796875\n",
            "22022 4.767921382153872e-06 6.094558193581179e-05 Traning Loss: 6.571350240847096e-05 17.974390029907227 2.01924467086792\n",
            "22023 5.4780357459094375e-06 5.9471785789355636e-05 Traning Loss: 6.494982517324388e-05 17.97465705871582 2.019221305847168\n",
            "22024 4.8233423513011076e-06 5.922983109485358e-05 Traning Loss: 6.405317253665999e-05 17.974931716918945 2.019200086593628\n",
            "22025 4.774650278704939e-06 5.81215244892519e-05 Traning Loss: 6.289617158472538e-05 17.975196838378906 2.0191826820373535\n",
            "22026 4.770439318235731e-06 5.6664066505618393e-05 Traning Loss: 6.143450445961207e-05 17.975460052490234 2.0191550254821777\n",
            "22027 4.213366082694847e-06 5.58459032617975e-05 Traning Loss: 6.005926843499765e-05 17.975723266601562 2.019137144088745\n",
            "22028 4.589049240166787e-06 5.4747673857491463e-05 Traning Loss: 5.933672218816355e-05 17.975976943969727 2.0191056728363037\n",
            "22029 4.040932253701612e-06 5.5518990848213434e-05 Traning Loss: 5.9559923101915047e-05 17.976234436035156 2.0190820693969727\n",
            "22030 4.471614829526516e-06 5.598136704065837e-05 Traning Loss: 6.0452981415437534e-05 17.97648048400879 2.0190513134002686\n",
            "22031 4.152331257500919e-06 5.7138506235787645e-05 Traning Loss: 6.129084067652002e-05 17.97673225402832 2.019021987915039\n",
            "22032 4.287815954739926e-06 5.710880577680655e-05 Traning Loss: 6.139661854831502e-05 17.976974487304688 2.018991708755493\n",
            "22033 4.0778609218250494e-06 5.6475142628187314e-05 Traning Loss: 6.0553004004759714e-05 17.97722053527832 2.018956422805786\n",
            "22034 3.886491867888253e-06 5.5235686886589974e-05 Traning Loss: 5.912217966397293e-05 17.977460861206055 2.018922805786133\n",
            "22035 3.79299513042497e-06 5.3990315791452304e-05 Traning Loss: 5.7783312513493e-05 17.97770118713379 2.0188841819763184\n",
            "22036 3.6040489703736966e-06 5.350339779397473e-05 Traning Loss: 5.7107445172732696e-05 17.977941513061523 2.0188469886779785\n",
            "22037 3.6904591524944408e-06 5.354632594389841e-05 Traning Loss: 5.723678623326123e-05 17.97817611694336 2.0188066959381104\n",
            "22038 3.635288749137544e-06 5.425468407338485e-05 Traning Loss: 5.7889974414138123e-05 17.978412628173828 2.0187647342681885\n",
            "22039 3.748088602151256e-06 5.481774132931605e-05 Traning Loss: 5.856583084096201e-05 17.978641510009766 2.018721580505371\n",
            "22040 3.6771496070286958e-06 5.51676275790669e-05 Traning Loss: 5.884477650397457e-05 17.978870391845703 2.018676280975342\n",
            "22041 3.6992223613196984e-06 5.4897027439437807e-05 Traning Loss: 5.85962479817681e-05 17.97909164428711 2.0186305046081543\n",
            "22042 3.5439986731944373e-06 5.442310430225916e-05 Traning Loss: 5.7967103202827275e-05 17.979312896728516 2.0185840129852295\n",
            "22043 3.5472860417939955e-06 5.3713989473180845e-05 Traning Loss: 5.726127710659057e-05 17.97952651977539 2.018535614013672\n",
            "22044 3.3735343549778918e-06 5.336925096344203e-05 Traning Loss: 5.6742785091046244e-05 17.979738235473633 2.0184881687164307\n",
            "22045 3.4300749121030094e-06 5.3099010983714834e-05 Traning Loss: 5.652908657793887e-05 17.97994613647461 2.018437385559082\n",
            "22046 3.3032606552296784e-06 5.326947939465754e-05 Traning Loss: 5.657273868564516e-05 17.98015022277832 2.0183892250061035\n",
            "22047 3.3735871056705946e-06 5.336052709026262e-05 Traning Loss: 5.6734115787548944e-05 17.98035430908203 2.0183379650115967\n",
            "22048 3.3150404306070413e-06 5.3551309974864125e-05 Traning Loss: 5.6866349041229114e-05 17.980552673339844 2.0182881355285645\n",
            "22049 3.3075466490117833e-06 5.358350608730689e-05 Traning Loss: 5.6891054555308074e-05 17.980751037597656 2.0182368755340576\n",
            "22050 3.3230739973078016e-06 5.34844548383262e-05 Traning Loss: 5.6807530199876055e-05 17.98094367980957 2.0181849002838135\n",
            "22051 3.2071880013972986e-06 5.344850796973333e-05 Traning Loss: 5.6655695516383275e-05 17.981136322021484 2.018134832382202\n",
            "22052 3.304847041363246e-06 5.318782859831117e-05 Traning Loss: 5.649267404805869e-05 17.9813232421875 2.0180819034576416\n",
            "22053 3.1209660846798215e-06 5.322650395100936e-05 Traning Loss: 5.634747139993124e-05 17.981510162353516 2.018033027648926\n",
            "22054 3.265854275014135e-06 5.296372182783671e-05 Traning Loss: 5.6229575420729816e-05 17.981691360473633 2.017979621887207\n",
            "22055 3.0724936550541315e-06 5.3053288866067305e-05 Traning Loss: 5.612578388536349e-05 17.981870651245117 2.017930507659912\n",
            "22056 3.1949116419127677e-06 5.282879283186048e-05 Traning Loss: 5.60237058380153e-05 17.982046127319336 2.0178778171539307\n",
            "22057 3.0543978937203065e-06 5.287229214445688e-05 Traning Loss: 5.592669185716659e-05 17.98221778869629 2.0178282260894775\n",
            "22058 3.1031149774207734e-06 5.2734521887032315e-05 Traning Loss: 5.583763777394779e-05 17.98238754272461 2.01777720451355\n",
            "22059 3.0553690066881245e-06 5.271987902233377e-05 Traning Loss: 5.577524643740617e-05 17.982553482055664 2.0177266597747803\n",
            "22060 3.011449052792159e-06 5.27301017427817e-05 Traning Loss: 5.5741551477694884e-05 17.982717514038086 2.0176773071289062\n",
            "22061 3.058499487451627e-06 5.2672141464427114e-05 Traning Loss: 5.5730641179252416e-05 17.982877731323242 2.0176262855529785\n",
            "22062 2.939109208455193e-06 5.279029574012384e-05 Traning Loss: 5.5729404266458005e-05 17.983036041259766 2.0175788402557373\n",
            "22063 3.049907945751329e-06 5.2670315199065953e-05 Traning Loss: 5.5720222007948905e-05 17.983190536499023 2.0175282955169678\n",
            "22064 2.8916851988469716e-06 5.279573088046163e-05 Traning Loss: 5.568741471506655e-05 17.98334503173828 2.017482042312622\n",
            "22065 3.0137753128656186e-06 5.261560727376491e-05 Traning Loss: 5.5629381677135825e-05 17.98349380493164 2.017432451248169\n",
            "22066 2.8563692922034534e-06 5.268794848234393e-05 Traning Loss: 5.554431845666841e-05 17.983642578125 2.0173866748809814\n",
            "22067 2.9478674150595907e-06 5.250183312455192e-05 Traning Loss: 5.5449701903853565e-05 17.98378562927246 2.017338514328003\n",
            "22068 2.828443484759191e-06 5.252649134490639e-05 Traning Loss: 5.5354936193907633e-05 17.983928680419922 2.0172932147979736\n",
            "22069 2.87091074824275e-06 5.240138489170931e-05 Traning Loss: 5.527229586732574e-05 17.984067916870117 2.017246723175049\n",
            "22070 2.8082174594601383e-06 5.2396342653082684e-05 Traning Loss: 5.52045603399165e-05 17.98420524597168 2.0172011852264404\n",
            "22071 2.799203002723516e-06 5.23558774148114e-05 Traning Loss: 5.515508019016124e-05 17.984338760375977 2.0171561241149902\n",
            "22072 2.788219944704906e-06 5.232903276919387e-05 Traning Loss: 5.5117252486525103e-05 17.98447036743164 2.017110824584961\n",
            "22073 2.7377720925869653e-06 5.2346691518323496e-05 Traning Loss: 5.5084463383536786e-05 17.984600067138672 2.0170671939849854\n",
            "22074 2.7611292807705468e-06 5.2293067710706964e-05 Traning Loss: 5.505419539986178e-05 17.984725952148438 2.0170223712921143\n",
            "22075 2.6857348984776763e-06 5.23355120094493e-05 Traning Loss: 5.502124622580595e-05 17.984851837158203 2.016979455947876\n",
            "22076 2.7230241812503664e-06 5.226113717071712e-05 Traning Loss: 5.498416066984646e-05 17.984973907470703 2.016935110092163\n",
            "22077 2.6417465051054023e-06 5.230305760051124e-05 Traning Loss: 5.494480501511134e-05 17.985095977783203 2.016892910003662\n",
            "22078 2.679144927242305e-06 5.222592153586447e-05 Traning Loss: 5.490506737260148e-05 17.985214233398438 2.0168492794036865\n",
            "22079 2.6059490210172953e-06 5.226131543167867e-05 Traning Loss: 5.486726513481699e-05 17.985332489013672 2.0168073177337646\n",
            "22080 2.635078089952003e-06 5.219558806857094e-05 Traning Loss: 5.483066706801765e-05 17.98544692993164 2.0167641639709473\n",
            "22081 2.5744052436493803e-06 5.221961691859178e-05 Traning Loss: 5.479402170749381e-05 17.98556137084961 2.0167222023010254\n",
            "22082 2.592586042737821e-06 5.216877252678387e-05 Traning Loss: 5.476135993376374e-05 17.985671997070312 2.0166797637939453\n",
            "22083 2.5433269001950976e-06 5.218501974013634e-05 Traning Loss: 5.4728345276089385e-05 17.985782623291016 2.0166382789611816\n",
            "22084 2.5540909973642556e-06 5.214211705606431e-05 Traning Loss: 5.469620737130754e-05 17.985889434814453 2.0165963172912598\n",
            "22085 2.5118868052231846e-06 5.214948032516986e-05 Traning Loss: 5.466136644827202e-05 17.98599624633789 2.016555070877075\n",
            "22086 2.520244152037776e-06 5.2108156523900107e-05 Traning Loss: 5.4628399084322155e-05 17.986099243164062 2.0165133476257324\n",
            "22087 2.478607257216936e-06 5.211632014834322e-05 Traning Loss: 5.4594926041318104e-05 17.986202239990234 2.016472339630127\n",
            "22088 2.4902662971726386e-06 5.206950299907476e-05 Traning Loss: 5.455976861412637e-05 17.98630142211914 2.0164310932159424\n",
            "22089 2.4435898922092747e-06 5.20836329087615e-05 Traning Loss: 5.452722325571813e-05 17.986400604248047 2.016390800476074\n",
            "22090 2.464084445819026e-06 5.203205728321336e-05 Traning Loss: 5.4496140364790335e-05 17.98649787902832 2.0163495540618896\n",
            "22091 2.4085602490231395e-06 5.205831985222176e-05 Traning Loss: 5.44668801012449e-05 17.986595153808594 2.0163097381591797\n",
            "22092 2.4418338853138266e-06 5.2000581490574405e-05 Traning Loss: 5.4442414693767205e-05 17.9866886138916 2.016268730163574\n",
            "22093 2.375312533331453e-06 5.2045750635443255e-05 Traning Loss: 5.4421063396148384e-05 17.98678207397461 2.0162293910980225\n",
            "22094 2.4242588096967665e-06 5.19826753588859e-05 Traning Loss: 5.440693348646164e-05 17.98687171936035 2.016188383102417\n",
            "22095 2.3442266865458805e-06 5.2055809646844864e-05 Traning Loss: 5.4400035878643394e-05 17.986963272094727 2.0161497592926025\n",
            "22096 2.411666628177045e-06 5.199084262130782e-05 Traning Loss: 5.4402509704232216e-05 17.987049102783203 2.016108751296997\n",
            "22097 2.3163117930380395e-06 5.210077142692171e-05 Traning Loss: 5.4417083447333425e-05 17.987136840820312 2.01607084274292\n",
            "22098 2.40621352531889e-06 5.204337867326103e-05 Traning Loss: 5.44495924259536e-05 17.987220764160156 2.0160298347473145\n",
            "22099 2.2936658297112444e-06 5.221097308094613e-05 Traning Loss: 5.45046386832837e-05 17.987306594848633 2.0159926414489746\n",
            "22100 2.4141743324435083e-06 5.217664147494361e-05 Traning Loss: 5.459081512526609e-05 17.98738670349121 2.015951633453369\n",
            "22101 2.2811893813923234e-06 5.244326894171536e-05 Traning Loss: 5.472445991472341e-05 17.987470626831055 2.0159153938293457\n",
            "22102 2.4461251086904667e-06 5.2477811550488696e-05 Traning Loss: 5.4923937568673864e-05 17.987546920776367 2.015874147415161\n",
            "22103 2.2894530502526322e-06 5.2925734053133056e-05 Traning Loss: 5.521518687601201e-05 17.987628936767578 2.015838861465454\n",
            "22104 2.5239223759854212e-06 5.3120249503990635e-05 Traning Loss: 5.564417369896546e-05 17.987701416015625 2.0157971382141113\n",
            "22105 2.3430072815244785e-06 5.3925930842524394e-05 Traning Loss: 5.626893835142255e-05 17.987781524658203 2.015763521194458\n",
            "22106 2.694571094252751e-06 5.449374657473527e-05 Traning Loss: 5.7188317441614345e-05 17.987850189208984 2.015720844268799\n",
            "22107 2.499747097317595e-06 5.6035274610621855e-05 Traning Loss: 5.853502079844475e-05 17.98792839050293 2.0156891345977783\n",
            "22108 3.0589958441851195e-06 5.747206159867346e-05 Traning Loss: 6.053105607861653e-05 17.987991333007812 2.0156447887420654\n",
            "22109 2.8944496079930104e-06 6.0599562857532874e-05 Traning Loss: 6.349401519400999e-05 17.988069534301758 2.0156161785125732\n",
            "22110 3.848549113172339e-06 6.409958587028086e-05 Traning Loss: 6.794813816668466e-05 17.988126754760742 2.0155692100524902\n",
            "22111 3.85261728297337e-06 7.080108480295166e-05 Traning Loss: 7.465370435966179e-05 17.98820686340332 2.015544891357422\n",
            "22112 5.608547326119151e-06 7.928855484351516e-05 Traning Loss: 8.489710307912901e-05 17.98825454711914 2.015493631362915\n",
            "22113 6.174819645821117e-06 9.434182720724493e-05 Traning Loss: 0.00010051664867205545 17.98833656311035 2.0154759883880615\n",
            "22114 9.661940566729754e-06 0.00011509284377098083 Traning Loss: 0.0001247547916136682 17.98836898803711 2.0154178142547607\n",
            "22115 1.1860427548526786e-05 0.00015025257016532123 Traning Loss: 0.0001621129922568798 17.988452911376953 2.0154104232788086\n",
            "22116 1.929087557073217e-05 0.0002016406215261668 Traning Loss: 0.00022093149891588837 17.988460540771484 2.01534104347229\n",
            "22117 2.594516809040215e-05 0.0002861771790776402 Traning Loss: 0.0003121223417110741 17.988542556762695 2.015350103378296\n",
            "22118 4.268031261744909e-05 0.00041446552495472133 Traning Loss: 0.000457145826658234 17.988502502441406 2.015263319015503\n",
            "22119 6.090851093176752e-05 0.0006197644979692996 Traning Loss: 0.0006806729943491518 17.98857307434082 2.015298843383789\n",
            "22120 9.952271648216993e-05 0.0009358507813885808 Traning Loss: 0.0010353735415264964 17.98843765258789 2.015185832977295\n",
            "22121 0.0001452161668566987 0.0014189702924340963 Traning Loss: 0.0015641864156350493 17.988454818725586 2.0152628421783447\n",
            "22122 0.0002306238020537421 0.0021437883842736483 Traning Loss: 0.0023744122590869665 17.98813247680664 2.0151147842407227\n",
            "22123 0.00032685400219634175 0.003136169398203492 Traning Loss: 0.003463023342192173 17.987998962402344 2.015251636505127\n",
            "22124 0.0004805591015610844 0.0044490136206150055 Traning Loss: 0.004929572809487581 17.987333297729492 2.0150718688964844\n",
            "22125 0.0006027657072991133 0.005748322699218988 Traning Loss: 0.006351088173687458 17.986879348754883 2.015275001525879\n",
            "22126 0.0007330956868827343 0.00675920071080327 Traning Loss: 0.007492296397686005 17.985774993896484 2.0151100158691406\n",
            "22127 0.0006789208855479956 0.00648526381701231 Traning Loss: 0.007164184935390949 17.98493766784668 2.015338659286499\n",
            "22128 0.0005386629491113126 0.004882482811808586 Traning Loss: 0.00542114581912756 17.983680725097656 2.015298843383789\n",
            "22129 0.0002487424644641578 0.002345382235944271 Traning Loss: 0.00259412475861609 17.982751846313477 2.015465497970581\n",
            "22130 6.831263453932479e-05 0.0004699367273133248 Traning Loss: 0.0005382493836805224 17.98188018798828 2.0156447887420654\n",
            "22131 6.264659896260127e-05 0.00028906395891681314 Traning Loss: 0.000351710565155372 17.98105812072754 2.0157248973846436\n",
            "22132 0.0001654935476835817 0.0014479244127869606 Traning Loss: 0.0016134179895743728 17.9804630279541 2.016087532043457\n",
            "22133 0.000315130571834743 0.00245527271181345 Traning Loss: 0.0027704034000635147 17.97953987121582 2.0162012577056885\n",
            "22134 0.00025732009089551866 0.002296776045113802 Traning Loss: 0.0025540960486978292 17.978796005249023 2.0165963172912598\n",
            "22135 0.00016379254520870745 0.0011365299578756094 Traning Loss: 0.0013003224739804864 17.977874755859375 2.016878128051758\n",
            "22136 5.787866757600568e-05 0.00021979236043989658 Traning Loss: 0.0002776710316538811 17.977035522460938 2.017167091369629\n",
            "22137 6.146117812022567e-05 0.00032577195088379085 Traning Loss: 0.0003872331290040165 17.97629737854004 2.017599582672119\n",
            "22138 0.00016149292059708387 0.0010182406986132264 Traning Loss: 0.001179733662866056 17.97539710998535 2.0178186893463135\n",
            "22139 0.00017270390526391566 0.0013940803473815322 Traning Loss: 0.0015667842235416174 17.97467041015625 2.01829195022583\n",
            "22140 0.00014608557103201747 0.0009653384913690388 Traning Loss: 0.0011114240624010563 17.973800659179688 2.0185933113098145\n",
            "22141 6.015375765855424e-05 0.0003025852784048766 Traning Loss: 0.0003627390251494944 17.973081588745117 2.0189623832702637\n",
            "22142 3.583377838367596e-05 9.674401371739805e-05 Traning Loss: 0.0001325777848251164 17.972475051879883 2.019376277923584\n",
            "22143 8.097657701000571e-05 0.00043002571328543127 Traning Loss: 0.0005110022611916065 17.971847534179688 2.0196218490600586\n",
            "22144 0.00010459591430844739 0.0008074965444393456 Traning Loss: 0.0009120924514718354 17.971403121948242 2.020063877105713\n",
            "22145 0.00010911389108514413 0.0007299085846170783 Traning Loss: 0.0008390224538743496 17.970867156982422 2.0203137397766113\n",
            "22146 5.192916432861239e-05 0.00034656646312214434 Traning Loss: 0.00039849564200267196 17.9704647064209 2.020669460296631\n",
            "22147 2.3678025172557682e-05 6.331659096758813e-05 Traning Loss: 8.699461614014581e-05 17.970102310180664 2.020975351333618\n",
            "22148 3.1900905014481395e-05 0.0001380528847221285 Traning Loss: 0.00016995379701256752 17.969751358032227 2.021182060241699\n",
            "22149 5.007292566006072e-05 0.0004019761399831623 Traning Loss: 0.0004520490765571594 17.96952247619629 2.021496534347534\n",
            "22150 7.231433846754953e-05 0.0005040770629420877 Traning Loss: 0.0005763914086855948 17.969207763671875 2.0216310024261475\n",
            "22151 4.286280818632804e-05 0.00037469432572834194 Traning Loss: 0.0004175571375526488 17.96900177001953 2.021876811981201\n",
            "22152 2.5437122531002387e-05 0.00014104862930253148 Traning Loss: 0.00016648575547151268 17.968788146972656 2.02201771736145\n",
            "22153 1.0773019312182441e-05 5.807923662359826e-05 Traning Loss: 6.88522559357807e-05 17.968608856201172 2.0221376419067383\n",
            "22154 1.740379047987517e-05 0.0001533726608613506 Traning Loss: 0.00017077644588425756 17.968507766723633 2.0222864151000977\n",
            "22155 3.8050773582654074e-05 0.00027864903677254915 Traning Loss: 0.00031669982126913965 17.968360900878906 2.0223264694213867\n",
            "22156 3.0322684324346483e-05 0.00030943623278290033 Traning Loss: 0.00033975893165916204 17.96831703186035 2.0224547386169434\n",
            "22157 2.7371663236408494e-05 0.00020036468049511313 Traning Loss: 0.00022773633827455342 17.968252182006836 2.022481679916382\n",
            "22158 8.422593055001926e-06 9.022867743624374e-05 Traning Loss: 9.865126776276156e-05 17.96826171875 2.0225324630737305\n",
            "22159 6.511751053039916e-06 5.9417336160549894e-05 Traning Loss: 6.592908903257921e-05 17.96832275390625 2.0225677490234375\n",
            "22160 1.4999101040302776e-05 0.00011684887431329116 Traning Loss: 0.00013184797717258334 17.968374252319336 2.022547721862793\n",
            "22161 1.763736509019509e-05 0.0001906815159600228 Traning Loss: 0.00020831887377426028 17.96849822998047 2.022585868835449\n",
            "22162 2.411893365206197e-05 0.00019221582624595612 Traning Loss: 0.0002163347671739757 17.96858024597168 2.0225472450256348\n",
            "22163 1.2918983884446789e-05 0.00014071984332986176 Traning Loss: 0.00015363882994279265 17.968725204467773 2.022547721862793\n",
            "22164 8.847625394992065e-06 7.451808778569102e-05 Traning Loss: 8.336571045219898e-05 17.9688663482666 2.0225157737731934\n",
            "22165 6.3608640630263835e-06 5.603530371445231e-05 Traning Loss: 6.23961677774787e-05 17.96902084350586 2.022463321685791\n",
            "22166 7.531601568189217e-06 8.622190944151953e-05 Traning Loss: 9.375350782647729e-05 17.96921157836914 2.022449493408203\n",
            "22167 1.5116804206627421e-05 0.00011923346028197557 Traning Loss: 0.0001343502663075924 17.969369888305664 2.0223731994628906\n",
            "22168 1.1832311429316178e-05 0.00013019081961829215 Traning Loss: 0.00014202312740962952 17.969581604003906 2.022352933883667\n",
            "22169 1.2326630894676782e-05 9.990463877329603e-05 Traning Loss: 0.00011223126784898341 17.969772338867188 2.0222792625427246\n",
            "22170 6.087231668061577e-06 6.776809459552169e-05 Traning Loss: 7.385532808257267e-05 17.969995498657227 2.0222249031066895\n",
            "22171 5.104855063109426e-06 5.3222524002194405e-05 Traning Loss: 5.832737952005118e-05 17.970233917236328 2.022172212600708\n",
            "22172 7.353891760430997e-06 6.435612885979936e-05 Traning Loss: 7.17100192559883e-05 17.970468521118164 2.022089958190918\n",
            "22173 7.499585535697406e-06 8.717934542801231e-05 Traning Loss: 9.467892959946766e-05 17.970741271972656 2.0220494270324707\n",
            "22174 1.1271559742453974e-05 9.262263483833522e-05 Traning Loss: 0.00010389419185230508 17.970991134643555 2.0219550132751465\n",
            "22175 7.296469448192511e-06 8.490173058817163e-05 Traning Loss: 9.219819912686944e-05 17.97127342224121 2.0219035148620605\n",
            "22176 7.327683306357358e-06 6.410497007891536e-05 Traning Loss: 7.143265247577801e-05 17.971538543701172 2.0218214988708496\n",
            "22177 4.86218732476118e-06 5.386412522057071e-05 Traning Loss: 5.8726313000079244e-05 17.97180938720703 2.0217514038085938\n",
            "22178 5.0530552471173e-06 5.626364145427942e-05 Traning Loss: 6.131669942988083e-05 17.972091674804688 2.021688222885132\n",
            "22179 7.156858373491559e-06 6.55284893582575e-05 Traning Loss: 7.268534682225436e-05 17.97235870361328 2.021601676940918\n",
            "22180 6.2773369791102596e-06 7.428546086885035e-05 Traning Loss: 8.056280057644472e-05 17.9726505279541 2.021542549133301\n",
            "22181 7.783338332956191e-06 7.03253535903059e-05 Traning Loss: 7.810869283275679e-05 17.97292137145996 2.0214552879333496\n",
            "22182 5.23700327903498e-06 6.291626777965575e-05 Traning Loss: 6.815326923970133e-05 17.973209381103516 2.0213916301727295\n",
            "22183 5.24041479366133e-06 5.3817551815882325e-05 Traning Loss: 5.905796570004895e-05 17.97348976135254 2.0213186740875244\n",
            "22184 4.7794801503187045e-06 5.2176033932482824e-05 Traning Loss: 5.6955512263812125e-05 17.97376823425293 2.0212442874908447\n",
            "22185 4.716952389571816e-06 5.6696473620831966e-05 Traning Loss: 6.141342601040378e-05 17.974058151245117 2.0211825370788574\n",
            "22186 6.171129825816024e-06 6.096333163441159e-05 Traning Loss: 6.713446055073291e-05 17.97433090209961 2.0211029052734375\n",
            "22187 5.145869181433227e-06 6.382622814271599e-05 Traning Loss: 6.897209823364392e-05 17.97461700439453 2.021045446395874\n",
            "22188 5.894377409276785e-06 5.990916906739585e-05 Traning Loss: 6.580354965990409e-05 17.974884033203125 2.020972490310669\n",
            "22189 4.556241492537083e-06 5.5933087423909456e-05 Traning Loss: 6.048932846169919e-05 17.975156784057617 2.020909547805786\n",
            "22190 4.509876816882752e-06 5.2438932470977306e-05 Traning Loss: 5.694881110684946e-05 17.975421905517578 2.020845413208008\n",
            "22191 4.58956901638885e-06 5.2439885621424764e-05 Traning Loss: 5.7029454183066264e-05 17.975679397583008 2.02077317237854\n",
            "22192 4.233067556924652e-06 5.539989797398448e-05 Traning Loss: 5.9632966440403834e-05 17.975940704345703 2.020716667175293\n",
            "22193 5.204842182138236e-06 5.68978903174866e-05 Traning Loss: 6.210273568285629e-05 17.976184844970703 2.0206406116485596\n",
            "22194 4.253415681887418e-06 5.8185683883493766e-05 Traning Loss: 6.243909592740238e-05 17.976438522338867 2.0205838680267334\n",
            "22195 4.839268513023853e-06 5.571162546402775e-05 Traning Loss: 6.05508939770516e-05 17.976680755615234 2.020508289337158\n",
            "22196 3.848029336950276e-06 5.4062402341514826e-05 Traning Loss: 5.79104307689704e-05 17.976926803588867 2.020444393157959\n",
            "22197 3.988723619841039e-06 5.223713378654793e-05 Traning Loss: 5.622585740638897e-05 17.977169036865234 2.020373582839966\n",
            "22198 3.837836175080156e-06 5.240154132479802e-05 Traning Loss: 5.623937613563612e-05 17.977407455444336 2.0203027725219727\n",
            "22199 3.7329032238631044e-06 5.369897189666517e-05 Traning Loss: 5.743187648477033e-05 17.977649688720703 2.020235061645508\n",
            "22200 4.1494399738439824e-06 5.4489461035700515e-05 Traning Loss: 5.8638899645302445e-05 17.977882385253906 2.020158290863037\n",
            "22201 3.713311343744863e-06 5.522375431610271e-05 Traning Loss: 5.8937064750352874e-05 17.97812271118164 2.020087718963623\n",
            "22202 4.002952209702926e-06 5.418574801296927e-05 Traning Loss: 5.8188699767924845e-05 17.97835350036621 2.0200092792510986\n",
            "22203 3.5645762181957252e-06 5.339649214874953e-05 Traning Loss: 5.6961067457450554e-05 17.978586196899414 2.019932985305786\n",
            "22204 3.5719813240575604e-06 5.245333886705339e-05 Traning Loss: 5.602532110060565e-05 17.97881317138672 2.0198564529418945\n",
            "22205 3.602020342441392e-06 5.222754771239124e-05 Traning Loss: 5.582956873695366e-05 17.979036331176758 2.019773006439209\n",
            "22206 3.3608857847866602e-06 5.294922448229045e-05 Traning Loss: 5.6310109357582405e-05 17.979259490966797 2.019697666168213\n",
            "22207 3.803826075454708e-06 5.3227115131448954e-05 Traning Loss: 5.7030942116398364e-05 17.979473114013672 2.0196094512939453\n",
            "22208 3.317758000775939e-06 5.4225234634941444e-05 Traning Loss: 5.754299127147533e-05 17.97968864440918 2.0195348262786865\n",
            "22209 3.829633442364866e-06 5.381284063332714e-05 Traning Loss: 5.764247543993406e-05 17.979896545410156 2.0194454193115234\n",
            "22210 3.271326932008378e-06 5.4195425036596134e-05 Traning Loss: 5.746675014961511e-05 17.980106353759766 2.01936936378479\n",
            "22211 3.681920588860521e-06 5.3663818107452244e-05 Traning Loss: 5.734574006055482e-05 17.98031234741211 2.019280433654785\n",
            "22212 3.3079745662689675e-06 5.430948658613488e-05 Traning Loss: 5.76174606976565e-05 17.980514526367188 2.019202947616577\n",
            "22213 3.650625558293541e-06 5.48089956282638e-05 Traning Loss: 5.8459620049688965e-05 17.980716705322266 2.019115924835205\n",
            "22214 3.531501533871051e-06 5.636745481751859e-05 Traning Loss: 5.9898957260884345e-05 17.980913162231445 2.0190391540527344\n",
            "22215 3.909962288162205e-06 5.7980123528977856e-05 Traning Loss: 6.189008854562417e-05 17.981111526489258 2.0189526081085205\n",
            "22216 3.904407549271127e-06 6.061122985556722e-05 Traning Loss: 6.45156396785751e-05 17.981300354003906 2.01887845993042\n",
            "22217 4.526950306171784e-06 6.349226168822497e-05 Traning Loss: 6.801920972066e-05 17.981491088867188 2.018789529800415\n",
            "22218 4.55174085800536e-06 6.848333578091115e-05 Traning Loss: 7.30350730009377e-05 17.981672286987305 2.0187220573425293\n",
            "22219 5.828032954013906e-06 7.45488068787381e-05 Traning Loss: 8.037684165174142e-05 17.981853485107422 2.0186288356781006\n",
            "22220 6.053717697795946e-06 8.545665332349017e-05 Traning Loss: 9.151037374977022e-05 17.982025146484375 2.0185706615448\n",
            "22221 8.647503818792757e-06 9.938632865669206e-05 Traning Loss: 0.00010803383338497952 17.982194900512695 2.018470048904419\n",
            "22222 9.664776371209882e-06 0.00012353912461549044 Traning Loss: 0.0001332038955297321 17.98235511779785 2.0184237957000732\n",
            "22223 1.47765267684008e-05 0.0001555482594994828 Traning Loss: 0.0001703247835393995 17.982511520385742 2.018312692642212\n",
            "22224 1.8118271327693947e-05 0.0002092426730087027 Traning Loss: 0.00022736094251740724 17.982654571533203 2.018282890319824\n",
            "22225 2.8323906008154154e-05 0.0002827838179655373 Traning Loss: 0.00031110772397369146 17.982789993286133 2.0181567668914795\n",
            "22226 3.7640613300027326e-05 0.00040337714017368853 Traning Loss: 0.0004410177643876523 17.982900619506836 2.0181491374969482\n",
            "22227 5.83972578169778e-05 0.000569515919778496 Traning Loss: 0.0006279131630435586 17.98299789428711 2.0179998874664307\n",
            "22228 8.142983278958127e-05 0.0008341842330992222 Traning Loss: 0.0009156140731647611 17.983043670654297 2.0180230140686035\n",
            "22229 0.00012196198804304004 0.0011815737234428525 Traning Loss: 0.0013035356532782316 17.98305892944336 2.017841100692749\n",
            "22230 0.0001701528817648068 0.0017000007210299373 Traning Loss: 0.0018701535882428288 17.982975006103516 2.0179030895233154\n",
            "22231 0.00023468337894883007 0.002275989390909672 Traning Loss: 0.0025106726679950953 17.98282814025879 2.0176820755004883\n",
            "22232 0.0003033650864381343 0.002989258384332061 Traning Loss: 0.0032926234416663647 17.982519149780273 2.0177783966064453\n",
            "22233 0.00035106446011923254 0.0034189035650342703 Traning Loss: 0.0037699679378420115 17.982126235961914 2.0175364017486572\n",
            "22234 0.00036554329562932253 0.0035766162909567356 Traning Loss: 0.003942159470170736 17.981552124023438 2.0176279544830322\n",
            "22235 0.0002988381893374026 0.002922995714470744 Traning Loss: 0.003221833845600486 17.98096466064453 2.0174288749694824\n",
            "22236 0.00019025575602427125 0.0018567980732768774 Traning Loss: 0.0020470537710934877 17.980310440063477 2.017442464828491\n",
            "22237 7.33632841729559e-05 0.0007092048763297498 Traning Loss: 0.0007825681823305786 17.979774475097656 2.017360210418701\n",
            "22238 1.8437365724821575e-05 0.00013789776130579412 Traning Loss: 0.0001563351252116263 17.979293823242188 2.017258644104004\n",
            "22239 3.325580109958537e-05 0.000309027818730101 Traning Loss: 0.0003422836307436228 17.97891616821289 2.0172905921936035\n",
            "22240 9.801418491406366e-05 0.0008496921509504318 Traning Loss: 0.0009477063431404531 17.97857093811035 2.017133951187134\n",
            "22241 0.00012951694952789694 0.0012529812520369887 Traning Loss: 0.0013824981870129704 17.97818946838379 2.0171966552734375\n",
            "22242 0.00012208468979224563 0.0011085415026172996 Traning Loss: 0.0012306261342018843 17.977828979492188 2.0171005725860596\n",
            "22243 7.083646778482944e-05 0.0006431531510315835 Traning Loss: 0.0007139896042644978 17.977413177490234 2.0171077251434326\n",
            "22244 2.9840362913091667e-05 0.00023492902982980013 Traning Loss: 0.0002647693909239024 17.977081298828125 2.017134189605713\n",
            "22245 3.0070421416894533e-05 0.0001855789014371112 Traning Loss: 0.00021564932831097394 17.976757049560547 2.0170655250549316\n",
            "22246 4.595573045662604e-05 0.00042196817230433226 Traning Loss: 0.0004679239063989371 17.976516723632812 2.0171570777893066\n",
            "22247 7.467444811481982e-05 0.0006012966041453183 Traning Loss: 0.0006759710377082229 17.976303100585938 2.017076015472412\n",
            "22248 6.0454920458141714e-05 0.0005774250603280962 Traning Loss: 0.0006378800026141107 17.976093292236328 2.017136573791504\n",
            "22249 4.332793832872994e-05 0.0003547180094756186 Traning Loss: 0.00039804595871828496 17.97592544555664 2.0171351432800293\n",
            "22250 2.4443506845273077e-05 0.00017906897119246423 Traning Loss: 0.0002035124780377373 17.975706100463867 2.017130136489868\n",
            "22251 2.020128522417508e-05 0.0001728998904582113 Traning Loss: 0.0001931011793203652 17.975542068481445 2.0172200202941895\n",
            "22252 3.803008803515695e-05 0.00027176542789675295 Traning Loss: 0.0003097955195698887 17.97534942626953 2.017176389694214\n",
            "22253 3.5809443943435326e-05 0.000359477533493191 Traning Loss: 0.0003952869737986475 17.975204467773438 2.0172736644744873\n",
            "22254 3.871999797411263e-05 0.00030868439353071153 Traning Loss: 0.00034740439150482416 17.975074768066406 2.0172393321990967\n",
            "22255 2.0039607989019714e-05 0.0002009210002142936 Traning Loss: 0.0002209606027463451 17.974956512451172 2.017266273498535\n",
            "22256 1.2735468772007152e-05 0.00011325605737511069 Traning Loss: 0.00012599152978509665 17.974882125854492 2.0172812938690186\n",
            "22257 1.5421439456986263e-05 0.00011688432277878746 Traning Loss: 0.00013230576587375253 17.974773406982422 2.0172319412231445\n",
            "22258 1.7910606402438134e-05 0.00018820815603248775 Traning Loss: 0.0002061187697108835 17.974716186523438 2.0172789096832275\n",
            "22259 2.850387681974098e-05 0.0002318446640856564 Traning Loss: 0.000260348548181355 17.97463607788086 2.0171942710876465\n",
            "22260 1.955896914296318e-05 0.0002223504416178912 Traning Loss: 0.00024190940894186497 17.974609375 2.0172126293182373\n",
            "22261 1.6393953046645038e-05 0.00014632855891250074 Traning Loss: 0.00016272251377813518 17.974597930908203 2.0171380043029785\n",
            "22262 5.8631831052480265e-06 8.242946933023632e-05 Traning Loss: 8.829265425447375e-05 17.9746150970459 2.017091751098633\n",
            "22263 4.376208380563185e-06 6.433293310692534e-05 Traning Loss: 6.870913784950972e-05 17.974668502807617 2.0170528888702393\n",
            "22264 1.0093025593960192e-05 9.525060886517167e-05 Traning Loss: 0.00010534363536862656 17.974708557128906 2.0169525146484375\n",
            "22265 1.1802491826529149e-05 0.00014497348456643522 Traning Loss: 0.00015677597548346967 17.974794387817383 2.0169336795806885\n",
            "22266 1.793700357666239e-05 0.00015934208931867033 Traning Loss: 0.0001772790856193751 17.97486114501953 2.016815423965454\n",
            "22267 1.1256135621806607e-05 0.00014234182890504599 Traning Loss: 0.0001535979681648314 17.9749755859375 2.016777276992798\n",
            "22268 9.813138603931293e-06 9.508898801868781e-05 Traning Loss: 0.0001049021229846403 17.97509002685547 2.016674757003784\n",
            "22269 3.895809186360566e-06 6.233792373677716e-05 Traning Loss: 6.623373337788507e-05 17.97523307800293 2.0165977478027344\n",
            "22270 3.7009608604421373e-06 5.4768661357229576e-05 Traning Loss: 5.846962085342966e-05 17.975383758544922 2.0165293216705322\n",
            "22271 6.819144346081885e-06 7.075710163917392e-05 Traning Loss: 7.757624553050846e-05 17.975526809692383 2.0164265632629395\n",
            "22272 7.089959126460599e-06 9.53156704781577e-05 Traning Loss: 0.00010240563278784975 17.975688934326172 2.0163869857788086\n",
            "22273 1.1026530955859926e-05 0.00010151840251637623 Traning Loss: 0.00011254493438173085 17.975830078125 2.0162875652313232\n",
            "22274 7.021686542429961e-06 9.601298370398581e-05 Traning Loss: 0.00010303466842742637 17.97599983215332 2.0162529945373535\n",
            "22275 7.508265753131127e-06 7.52847918192856e-05 Traning Loss: 8.279305620817468e-05 17.97616195678711 2.0161783695220947\n",
            "22276 4.102221737412037e-06 6.229612336028367e-05 Traning Loss: 6.639834464294836e-05 17.97635269165039 2.016132116317749\n",
            "22277 4.3601157813100144e-06 5.786266046925448e-05 Traning Loss: 6.222277443157509e-05 17.976543426513672 2.016090154647827\n",
            "22278 5.234271156950854e-06 6.324351852526888e-05 Traning Loss: 6.847779150120914e-05 17.97674560546875 2.016036033630371\n",
            "22279 5.097461780678714e-06 7.171795004978776e-05 Traning Loss: 7.681541319470853e-05 17.976951599121094 2.0160210132598877\n",
            "22280 6.8309204834804405e-06 7.256355456775054e-05 Traning Loss: 7.939447823446244e-05 17.977153778076172 2.0159740447998047\n",
            "22281 4.741676093544811e-06 6.983527418924496e-05 Traning Loss: 7.457695028278977e-05 17.977367401123047 2.015969753265381\n",
            "22282 5.401940597948851e-06 6.0800430219387636e-05 Traning Loss: 6.620237400056794e-05 17.97757339477539 2.015941619873047\n",
            "22283 3.65621417586226e-06 5.646603676723316e-05 Traning Loss: 6.012224912410602e-05 17.97779083251953 2.015935182571411\n",
            "22284 4.156796421739273e-06 5.533820149139501e-05 Traning Loss: 5.949499609414488e-05 17.97800064086914 2.015928268432617\n",
            "22285 4.418658136273734e-06 5.886171493330039e-05 Traning Loss: 6.328037125058472e-05 17.97821617126465 2.0159175395965576\n",
            "22286 4.482916665438097e-06 6.320185639197007e-05 Traning Loss: 6.768477032892406e-05 17.978424072265625 2.0159261226654053\n",
            "22287 5.184356268728152e-06 6.388355541275814e-05 Traning Loss: 6.90679153194651e-05 17.9786319732666 2.015916585922241\n",
            "22288 4.194205757812597e-06 6.223184755071998e-05 Traning Loss: 6.642605148954317e-05 17.978836059570312 2.0159292221069336\n",
            "22289 4.2991346163034905e-06 5.707027594326064e-05 Traning Loss: 6.136941374279559e-05 17.979042053222656 2.0159265995025635\n",
            "22290 3.348280415593763e-06 5.347571277525276e-05 Traning Loss: 5.6823992053978145e-05 17.979249954223633 2.0159337520599365\n",
            "22291 3.3423709737689933e-06 5.155814142199233e-05 Traning Loss: 5.4900512623135e-05 17.97945785522461 2.0159389972686768\n",
            "22292 3.446415576036088e-06 5.2455896366154775e-05 Traning Loss: 5.590231012320146e-05 17.979665756225586 2.015939235687256\n",
            "22293 3.4089962355210446e-06 5.5077896831789985e-05 Traning Loss: 5.848689397680573e-05 17.979869842529297 2.0159499645233154\n",
            "22294 3.979222128691617e-06 5.6763285101624206e-05 Traning Loss: 6.074250632082112e-05 17.980072021484375 2.015946865081787\n",
            "22295 3.56495320374961e-06 5.7815192121779546e-05 Traning Loss: 6.138014578027651e-05 17.980270385742188 2.0159566402435303\n",
            "22296 3.799269506998826e-06 5.6432661949656904e-05 Traning Loss: 6.023193054716103e-05 17.98046875 2.015953302383423\n",
            "22297 3.2884624943108065e-06 5.485401561600156e-05 Traning Loss: 5.8142479247180745e-05 17.980663299560547 2.0159552097320557\n",
            "22298 3.181397687512799e-06 5.303079524310306e-05 Traning Loss: 5.621219315798953e-05 17.980859756469727 2.0159528255462646\n",
            "22299 3.1466202017327305e-06 5.2069091907469556e-05 Traning Loss: 5.5215710744960234e-05 17.981050491333008 2.0159449577331543\n",
            "22300 2.8848767215094995e-06 5.236980359768495e-05 Traning Loss: 5.52546807739418e-05 17.981243133544922 2.0159430503845215\n",
            "22301 3.3007788715622155e-06 5.25853865838144e-05 Traning Loss: 5.588616477325559e-05 17.981428146362305 2.015927791595459\n",
            "22302 2.8716406177409226e-06 5.360581781133078e-05 Traning Loss: 5.647746002068743e-05 17.981613159179688 2.0159242153167725\n",
            "22303 3.305315431134659e-06 5.326567043084651e-05 Traning Loss: 5.6570985179860145e-05 17.981792449951172 2.0159058570861816\n",
            "22304 2.8000870315736393e-06 5.329686246113852e-05 Traning Loss: 5.609694926533848e-05 17.981971740722656 2.015897274017334\n",
            "22305 3.017825065398938e-06 5.227314613875933e-05 Traning Loss: 5.529096961254254e-05 17.982149124145508 2.015878677368164\n",
            "22306 2.7268542908132076e-06 5.180568041396327e-05 Traning Loss: 5.453253470477648e-05 17.982324600219727 2.0158629417419434\n",
            "22307 2.7290489015285857e-06 5.138739652466029e-05 Traning Loss: 5.411644451669417e-05 17.982500076293945 2.015845537185669\n",
            "22308 2.806390966725303e-06 5.1325092499610037e-05 Traning Loss: 5.413148392108269e-05 17.982669830322266 2.0158233642578125\n",
            "22309 2.6217260256089503e-06 5.1832448662025854e-05 Traning Loss: 5.445417627925053e-05 17.98284149169922 2.0158071517944336\n",
            "22310 2.9436484965117415e-06 5.191160744288936e-05 Traning Loss: 5.485525616677478e-05 17.983007431030273 2.0157806873321533\n",
            "22311 2.5976000870286953e-06 5.25184441357851e-05 Traning Loss: 5.511604467756115e-05 17.98317527770996 2.015763998031616\n",
            "22312 2.9633083613589406e-06 5.215497003518976e-05 Traning Loss: 5.51182783965487e-05 17.983335494995117 2.015735626220703\n",
            "22313 2.560199391155038e-06 5.231241448200308e-05 Traning Loss: 5.4872612963663414e-05 17.983497619628906 2.015716791152954\n",
            "22314 2.8422666673577623e-06 5.164016329217702e-05 Traning Loss: 5.448242882266641e-05 17.983654022216797 2.015688896179199\n",
            "22315 2.5271060621889774e-06 5.1562914450187236e-05 Traning Loss: 5.4090021876618266e-05 17.983810424804688 2.015666961669922\n",
            "22316 2.681471187315765e-06 5.111851714900695e-05 Traning Loss: 5.3799987654201686e-05 17.983959197998047 2.0156404972076416\n",
            "22317 2.532234702812275e-06 5.1127026381436735e-05 Traning Loss: 5.365925972000696e-05 17.98410987854004 2.015615701675415\n",
            "22318 2.5642727905506035e-06 5.1084891310893e-05 Traning Loss: 5.364916432881728e-05 17.984254837036133 2.0155906677246094\n",
            "22319 2.5626891329011414e-06 5.114611121825874e-05 Traning Loss: 5.370880171540193e-05 17.984399795532227 2.0155632495880127\n",
            "22320 2.501491735529271e-06 5.1262912165839225e-05 Traning Loss: 5.376440458348952e-05 17.984540939331055 2.015538215637207\n",
            "22321 2.572670609879424e-06 5.119253910379484e-05 Traning Loss: 5.376520857680589e-05 17.984682083129883 2.0155091285705566\n",
            "22322 2.4634580313431798e-06 5.122723450767808e-05 Traning Loss: 5.369069185690023e-05 17.984819412231445 2.0154836177825928\n",
            "22323 2.5288127289968543e-06 5.102391878608614e-05 Traning Loss: 5.35527324245777e-05 17.984954833984375 2.0154545307159424\n",
            "22324 2.4286100597237237e-06 5.0954553444171324e-05 Traning Loss: 5.3383162594400346e-05 17.98508644104004 2.015427827835083\n",
            "22325 2.4471848973917076e-06 5.077134846942499e-05 Traning Loss: 5.3218533139443025e-05 17.985218048095703 2.015399217605591\n",
            "22326 2.406789235465112e-06 5.0681930588325486e-05 Traning Loss: 5.308871914166957e-05 17.9853458404541 2.0153708457946777\n",
            "22327 2.3711536414339207e-06 5.063605203758925e-05 Traning Loss: 5.3007206588517874e-05 17.9854736328125 2.0153427124023438\n",
            "22328 2.4090907118079485e-06 5.056240479461849e-05 Traning Loss: 5.297149618854746e-05 17.985597610473633 2.015312671661377\n",
            "22329 2.3218024125526426e-06 5.0646554882405326e-05 Traning Loss: 5.296835661283694e-05 17.985721588134766 2.0152852535247803\n",
            "22330 2.417882569716312e-06 5.0558719522086903e-05 Traning Loss: 5.297660391079262e-05 17.985841751098633 2.015254020690918\n",
            "22331 2.2881897621118696e-06 5.0695678510237485e-05 Traning Loss: 5.298386895447038e-05 17.9859619140625 2.0152268409729004\n",
            "22332 2.4101659619191196e-06 5.0568138249218464e-05 Traning Loss: 5.297830284689553e-05 17.98607635498047 2.015195369720459\n",
            "22333 2.2589902073377743e-06 5.069495819043368e-05 Traning Loss: 5.295394657878205e-05 17.98619270324707 2.0151684284210205\n",
            "22334 2.385209654676146e-06 5.0530998123576865e-05 Traning Loss: 5.291620618663728e-05 17.986303329467773 2.015137195587158\n",
            "22335 2.236814452771796e-06 5.0636648666113615e-05 Traning Loss: 5.287346357363276e-05 17.98641586303711 2.0151102542877197\n",
            "22336 2.3543311726825777e-06 5.0481961807236075e-05 Traning Loss: 5.2836294344160706e-05 17.986522674560547 2.0150794982910156\n",
            "22337 2.222547664132435e-06 5.0593804189702496e-05 Traning Loss: 5.281635094434023e-05 17.986631393432617 2.015052556991577\n",
            "22338 2.3273723854799755e-06 5.049590254202485e-05 Traning Loss: 5.2823274018010125e-05 17.98673439025879 2.0150225162506104\n",
            "22339 2.213205561929499e-06 5.065319419372827e-05 Traning Loss: 5.286639861878939e-05 17.986839294433594 2.014995574951172\n",
            "22340 2.315705160071957e-06 5.064293509349227e-05 Traning Loss: 5.29586395714432e-05 17.9869384765625 2.0149660110473633\n",
            "22341 2.21381674236909e-06 5.089956539450213e-05 Traning Loss: 5.3113381000002846e-05 17.98703956604004 2.014939546585083\n",
            "22342 2.337574414923438e-06 5.101547867525369e-05 Traning Loss: 5.3353054681792855e-05 17.987133026123047 2.0149102210998535\n",
            "22343 2.2383035229722736e-06 5.147451156517491e-05 Traning Loss: 5.3712814406026155e-05 17.98723030090332 2.0148844718933105\n",
            "22344 2.4213397864514263e-06 5.182227323530242e-05 Traning Loss: 5.424361370387487e-05 17.987319946289062 2.014854907989502\n",
            "22345 2.3160712316894205e-06 5.2713170589413494e-05 Traning Loss: 5.502924250322394e-05 17.987415313720703 2.0148303508758545\n",
            "22346 2.6240929855703143e-06 5.3573952754959464e-05 Traning Loss: 5.619804505840875e-05 17.987499237060547 2.0148000717163086\n",
            "22347 2.5265617296099663e-06 5.542310464079492e-05 Traning Loss: 5.7949666370404884e-05 17.987594604492188 2.014777183532715\n",
            "22348 3.0860437618684955e-06 5.751498247263953e-05 Traning Loss: 6.0601025325013325e-05 17.987672805786133 2.014744997024536\n",
            "22349 3.070057800869108e-06 6.155009032227099e-05 Traning Loss: 6.462015153374523e-05 17.98776626586914 2.0147249698638916\n",
            "22350 4.142913894611411e-06 6.665359978796914e-05 Traning Loss: 7.079651550156996e-05 17.987836837768555 2.0146896839141846\n",
            "22351 4.437725692696404e-06 7.584795821458101e-05 Traning Loss: 8.028568117879331e-05 17.987932205200195 2.0146737098693848\n",
            "22352 6.613388904952444e-06 8.84988548932597e-05 Traning Loss: 9.511224197922274e-05 17.987991333007812 2.0146336555480957\n",
            "22353 7.875483788666315e-06 0.00011030353925889358 Traning Loss: 0.00011817902122857049 17.988086700439453 2.0146241188049316\n",
            "22354 1.2569686987262685e-05 0.0001422039495082572 Traning Loss: 0.00015477363194804639 17.988126754760742 2.0145764350891113\n",
            "22355 1.6601501556579024e-05 0.00019559555221349 Traning Loss: 0.00021219704649411142 17.988222122192383 2.0145771503448486\n",
            "22356 2.7332855097483844e-05 0.00027717315242625773 Traning Loss: 0.0003045060147996992 17.98822784423828 2.014517307281494\n",
            "22357 3.893161556334235e-05 0.0004106469568796456 Traning Loss: 0.0004495785688050091 17.988317489624023 2.0145347118377686\n",
            "22358 6.446303450502455e-05 0.0006198192131705582 Traning Loss: 0.0006842822767794132 17.98825454711914 2.014456272125244\n",
            "22359 9.558424062561244e-05 0.0009516806458123028 Traning Loss: 0.0010472649009898305 17.988313674926758 2.014500856399536\n",
            "22360 0.00015616495511494577 0.0014709107344970107 Traning Loss: 0.001627075718715787 17.98811149597168 2.014394521713257\n",
            "22361 0.00023110982147045434 0.002240346511825919 Traning Loss: 0.002471456304192543 17.988067626953125 2.0144810676574707\n",
            "22362 0.0003604436060413718 0.0033716713078320026 Traning Loss: 0.0037321150302886963 17.987585067749023 2.0143401622772217\n",
            "22363 0.000496789813041687 0.004763460252434015 Traning Loss: 0.005260250065475702 17.987281799316406 2.0144824981689453\n",
            "22364 0.0006828768528066576 0.006365983281284571 Traning Loss: 0.007048860192298889 17.986337661743164 2.0143208503723145\n",
            "22365 0.0007640198455192149 0.007306812331080437 Traning Loss: 0.00807083211839199 17.985599517822266 2.014509677886963\n",
            "22366 0.000773298495914787 0.007148131262511015 Traning Loss: 0.007921429350972176 17.9842586517334 2.0143978595733643\n",
            "22367 0.0005306663224473596 0.005066665820777416 Traning Loss: 0.005597332026809454 17.98323631286621 2.014573335647583\n",
            "22368 0.00025830266531556845 0.002225531032308936 Traning Loss: 0.0024838335812091827 17.98207664489746 2.014627695083618\n",
            "22369 5.089233673061244e-05 0.0002984002057928592 Traning Loss: 0.0003492925316095352 17.981164932250977 2.0147314071655273\n",
            "22370 6.429581117117777e-05 0.0004228650068398565 Traning Loss: 0.00048716081073507667 17.98047637939453 2.014991044998169\n",
            "22371 0.0002457592636346817 0.0018571290420368314 Traning Loss: 0.0021028881892561913 17.979562759399414 2.0150749683380127\n",
            "22372 0.00031804657191969454 0.002866333117708564 Traning Loss: 0.0031843797769397497 17.978849411010742 2.015439033508301\n",
            "22373 0.00029917669598944485 0.0023307790979743004 Traning Loss: 0.0026299557648599148 17.97783088684082 2.0156397819519043\n",
            "22374 0.00012681302905548364 0.0009075076668523252 Traning Loss: 0.0010343206813558936 17.976957321166992 2.015958786010742\n",
            "22375 4.8415935452794656e-05 9.906251216307282e-05 Traning Loss: 0.0001474784512538463 17.976119995117188 2.0163216590881348\n",
            "22376 0.00010850973194465041 0.0005259249010123312 Traning Loss: 0.0006344346329569817 17.97519302368164 2.0165584087371826\n",
            "22377 0.00017823510279413313 0.0013838220620527864 Traning Loss: 0.0015620571793988347 17.974430084228516 2.017002820968628\n",
            "22378 0.0002102707076119259 0.0014992018695920706 Traning Loss: 0.0017094726208597422 17.97345542907715 2.0172746181488037\n",
            "22379 0.00011805340909631923 0.0008011932950466871 Traning Loss: 0.0009192466968670487 17.972658157348633 2.017672300338745\n",
            "22380 5.062228592578322e-05 0.00013944300008006394 Traning Loss: 0.00019006528600584716 17.971908569335938 2.018057346343994\n",
            "22381 5.94675620959606e-05 0.00019409885862842202 Traning Loss: 0.00025356642436236143 17.97119140625 2.0183465480804443\n",
            "22382 0.00010097215272253379 0.0007081232033669949 Traning Loss: 0.0008090953342616558 17.97066879272461 2.0187880992889404\n",
            "22383 0.000134661138872616 0.0009274675976485014 Traning Loss: 0.0010621286928653717 17.97003173828125 2.019061326980591\n",
            "22384 8.650730160297826e-05 0.0006191367283463478 Traning Loss: 0.0007056440226733685 17.969562530517578 2.019454002380371\n",
            "22385 4.085358523298055e-05 0.00016751905786804855 Traning Loss: 0.0002083726431010291 17.969106674194336 2.019786834716797\n",
            "22386 2.8752623620675877e-05 7.345412450376898e-05 Traning Loss: 0.00010220674448646605 17.968698501586914 2.020052909851074\n",
            "22387 4.855587030760944e-05 0.00034034065902233124 Traning Loss: 0.0003888965293299407 17.968420028686523 2.020402193069458\n",
            "22388 8.104639709927142e-05 0.0005624389741569757 Traning Loss: 0.0006434853421524167 17.968053817749023 2.020587921142578\n",
            "22389 5.931413761572912e-05 0.0004984621773473918 Traning Loss: 0.0005577763076871634 17.967815399169922 2.020878791809082\n",
            "22390 3.675855987239629e-05 0.0002213120460510254 Traning Loss: 0.00025807059137150645 17.967544555664062 2.021066427230835\n",
            "22391 1.2834359949920326e-05 5.619329022010788e-05 Traning Loss: 6.902765017002821e-05 17.967327117919922 2.021235704421997\n",
            "22392 1.6442239939351566e-05 0.00012535203131847084 Traning Loss: 0.000141794269438833 17.967187881469727 2.0214314460754395\n",
            "22393 4.133961192565039e-05 0.0002894315111916512 Traning Loss: 0.000330771115841344 17.966999053955078 2.0215117931365967\n",
            "22394 3.713613114086911e-05 0.0003657635534182191 Traning Loss: 0.0004028996918350458 17.966917037963867 2.021683931350708\n",
            "22395 3.5379274777369574e-05 0.00025714305229485035 Traning Loss: 0.00029252233798615634 17.966800689697266 2.0217490196228027\n",
            "22396 1.1384491699573118e-05 0.00011412885214667767 Traning Loss: 0.00012551333929877728 17.966772079467773 2.021843910217285\n",
            "22397 7.042815013846848e-06 5.515189695870504e-05 Traning Loss: 6.219471106305718e-05 17.966793060302734 2.021916151046753\n",
            "22398 1.5520872693741694e-05 0.00011507850285852328 Traning Loss: 0.00013059937919024378 17.966815948486328 2.021929979324341\n",
            "22399 1.9669831090141088e-05 0.00020866030536126345 Traning Loss: 0.00022833014372736216 17.96692657470703 2.022003173828125\n",
            "22400 2.8036700314260088e-05 0.00021937608835287392 Traning Loss: 0.000247412797762081 17.966991424560547 2.0219886302948\n",
            "22401 1.4858407666906714e-05 0.00015962382894940674 Traning Loss: 0.00017448223661631346 17.967130661010742 2.0220234394073486\n",
            "22402 1.0095986908709165e-05 7.699149864492938e-05 Traning Loss: 8.708748646313325e-05 17.967267990112305 2.02201247215271\n",
            "22403 6.177173418109305e-06 5.430024611996487e-05 Traning Loss: 6.047741771908477e-05 17.96742057800293 2.0219902992248535\n",
            "22404 8.621002962172497e-06 9.188921831082553e-05 Traning Loss: 0.00010051022400148213 17.967615127563477 2.0219931602478027\n",
            "22405 1.691284887783695e-05 0.0001348939404124394 Traning Loss: 0.00015180678747128695 17.967771530151367 2.021941661834717\n",
            "22406 1.376782438455848e-05 0.00014600790746044368 Traning Loss: 0.00015977573639247566 17.967987060546875 2.021937370300293\n",
            "22407 1.3377824870985933e-05 0.00010729125642683357 Traning Loss: 0.0001206690794788301 17.968177795410156 2.021883249282837\n",
            "22408 6.385482720361324e-06 6.686586857540533e-05 Traning Loss: 7.32513508410193e-05 17.96840476989746 2.0218446254730225\n",
            "22409 5.303923899191432e-06 5.186748239793815e-05 Traning Loss: 5.7171404478140175e-05 17.968650817871094 2.021805763244629\n",
            "22410 8.135960342769977e-06 6.868885247968137e-05 Traning Loss: 7.682481373194605e-05 17.968889236450195 2.0217390060424805\n",
            "22411 8.712500857654959e-06 9.591900743544102e-05 Traning Loss: 0.00010463150829309598 17.969173431396484 2.0217068195343018\n",
            "22412 1.2081703971489333e-05 9.971256804419681e-05 Traning Loss: 0.00011179427383467555 17.969432830810547 2.021629810333252\n",
            "22413 7.733422535238788e-06 8.541486749891192e-05 Traning Loss: 9.314829367212951e-05 17.969728469848633 2.0215823650360107\n",
            "22414 6.882495199533878e-06 6.063446926418692e-05 Traning Loss: 6.751696491846815e-05 17.97001075744629 2.0215163230895996\n",
            "22415 5.1637102842505556e-06 5.0962731620529667e-05 Traning Loss: 5.612644235952757e-05 17.97029685974121 2.021449565887451\n",
            "22416 5.311473614710849e-06 5.87991344218608e-05 Traning Loss: 6.411060894606635e-05 17.970600128173828 2.02139949798584\n",
            "22417 8.329547199537046e-06 7.094562897691503e-05 Traning Loss: 7.927517435746267e-05 17.970882415771484 2.021318197250366\n",
            "22418 6.910590400366345e-06 7.892450230428949e-05 Traning Loss: 8.583509043091908e-05 17.971195220947266 2.021268129348755\n",
            "22419 8.114981937978882e-06 7.046021346468478e-05 Traning Loss: 7.857519813114777e-05 17.971485137939453 2.0211894512176514\n",
            "22420 5.238621270109434e-06 5.964833326288499e-05 Traning Loss: 6.488695362349972e-05 17.971790313720703 2.021129846572876\n",
            "22421 5.061107913206797e-06 5.114328087074682e-05 Traning Loss: 5.620438969344832e-05 17.97209358215332 2.021066188812256\n",
            "22422 5.302857061906252e-06 5.236369906924665e-05 Traning Loss: 5.76665552216582e-05 17.97239112854004 2.0209944248199463\n",
            "22423 5.167308245290769e-06 6.005310206091963e-05 Traning Loss: 6.522041076095775e-05 17.97270393371582 2.0209410190582275\n",
            "22424 6.943491825950332e-06 6.385461165336892e-05 Traning Loss: 7.079810166032985e-05 17.97299575805664 2.0208656787872314\n",
            "22425 5.3886974455963355e-06 6.417967961169779e-05 Traning Loss: 6.956837751204148e-05 17.973304748535156 2.020814895629883\n",
            "22426 5.900371888856171e-06 5.724340371671133e-05 Traning Loss: 6.314377242233604e-05 17.973594665527344 2.020747661590576\n",
            "22427 4.504296157392673e-06 5.240597602096386e-05 Traning Loss: 5.691027035936713e-05 17.973888397216797 2.0206894874572754\n",
            "22428 4.498560429055942e-06 5.0597071094671264e-05 Traning Loss: 5.509563197847456e-05 17.974178314208984 2.0206310749053955\n",
            "22429 4.954532414558344e-06 5.276583760860376e-05 Traning Loss: 5.77203682041727e-05 17.974454879760742 2.020564317703247\n",
            "22430 4.603952220350038e-06 5.688154124072753e-05 Traning Loss: 6.148549437057227e-05 17.97473907470703 2.0205113887786865\n",
            "22431 5.4781999097031076e-06 5.746159877162427e-05 Traning Loss: 6.293979822658002e-05 17.975006103515625 2.020442485809326\n",
            "22432 4.4045623326383065e-06 5.6656495871720836e-05 Traning Loss: 6.106105865910649e-05 17.975282669067383 2.0203864574432373\n",
            "22433 4.601274667948019e-06 5.292446439852938e-05 Traning Loss: 5.75257399759721e-05 17.975547790527344 2.0203192234039307\n",
            "22434 3.912671672878787e-06 5.097381290397607e-05 Traning Loss: 5.4886484576854855e-05 17.975814819335938 2.020254611968994\n",
            "22435 3.832273250736762e-06 5.0738548452500254e-05 Traning Loss: 5.4570820793742314e-05 17.97608184814453 2.0201916694641113\n",
            "22436 4.1757225517358165e-06 5.192420940147713e-05 Traning Loss: 5.6099932407960296e-05 17.97633934020996 2.020120143890381\n",
            "22437 3.821849531959742e-06 5.399938527261838e-05 Traning Loss: 5.7821234804578125e-05 17.976606369018555 2.0200576782226562\n",
            "22438 4.378222456580261e-06 5.392999082687311e-05 Traning Loss: 5.830821464769542e-05 17.97686195373535 2.019981861114502\n",
            "22439 3.6929463931301143e-06 5.358024645829573e-05 Traning Loss: 5.7273191487183794e-05 17.97712516784668 2.019913673400879\n",
            "22440 3.896509497280931e-06 5.164129470358603e-05 Traning Loss: 5.5537802836624905e-05 17.977378845214844 2.0198373794555664\n",
            "22441 3.511298473313218e-06 5.0717644626274705e-05 Traning Loss: 5.422894173534587e-05 17.977632522583008 2.019761800765991\n",
            "22442 3.4748425150610274e-06 5.050555773777887e-05 Traning Loss: 5.398039866122417e-05 17.97788429260254 2.019686222076416\n",
            "22443 3.6890237424813677e-06 5.092940045869909e-05 Traning Loss: 5.461842374643311e-05 17.978126525878906 2.0196032524108887\n",
            "22444 3.407446683922899e-06 5.203334512771107e-05 Traning Loss: 5.5440792493755e-05 17.97837257385254 2.019526958465576\n",
            "22445 3.804288098763209e-06 5.197694918024354e-05 Traning Loss: 5.578123818850145e-05 17.978607177734375 2.0194406509399414\n",
            "22446 3.323242481201305e-06 5.2091098041273654e-05 Traning Loss: 5.5414340749848634e-05 17.978843688964844 2.019361972808838\n",
            "22447 3.5524124086805386e-06 5.105320815346204e-05 Traning Loss: 5.460562169901095e-05 17.97907257080078 2.0192761421203613\n",
            "22448 3.2127543363458244e-06 5.063214121037163e-05 Traning Loss: 5.3844894864596426e-05 17.97930145263672 2.019193649291992\n",
            "22449 3.250165036661201e-06 5.024735946790315e-05 Traning Loss: 5.349752609618008e-05 17.97952651977539 2.0191097259521484\n",
            "22450 3.2629664019623306e-06 5.035155845689587e-05 Traning Loss: 5.361452349461615e-05 17.979747772216797 2.019024133682251\n",
            "22451 3.142127752653323e-06 5.0821625336539e-05 Traning Loss: 5.396375490818173e-05 17.979970932006836 2.0189425945281982\n",
            "22452 3.352765133968205e-06 5.087912722956389e-05 Traning Loss: 5.423189213615842e-05 17.980186462402344 2.0188565254211426\n",
            "22453 3.1100023534236243e-06 5.110713027534075e-05 Traning Loss: 5.421713285613805e-05 17.980403900146484 2.018775701522827\n",
            "22454 3.280199052824173e-06 5.064481229055673e-05 Traning Loss: 5.39250104338862e-05 17.980613708496094 2.018691062927246\n",
            "22455 3.0671299100504257e-06 5.045233410783112e-05 Traning Loss: 5.3519463108386844e-05 17.98082160949707 2.0186100006103516\n",
            "22456 3.1121949177759234e-06 5.0084487156709656e-05 Traning Loss: 5.319668343872763e-05 17.98102378845215 2.018528938293457\n",
            "22457 3.0812971090199426e-06 4.9987254897132516e-05 Traning Loss: 5.3068550187163055e-05 17.98122215270996 2.0184476375579834\n",
            "22458 3.0001390314282617e-06 5.0122027460020036e-05 Traning Loss: 5.312216671882197e-05 17.98141860961914 2.0183699131011963\n",
            "22459 3.1283300359064015e-06 5.0126669520977885e-05 Traning Loss: 5.325500023900531e-05 17.98160743713379 2.0182888507843018\n",
            "22460 2.940274725915515e-06 5.040318865212612e-05 Traning Loss: 5.334346496965736e-05 17.981796264648438 2.0182135105133057\n",
            "22461 3.1071199373400304e-06 5.02078328281641e-05 Traning Loss: 5.3314954129746184e-05 17.981979370117188 2.018134355545044\n",
            "22462 2.8869696961919544e-06 5.0287497288081795e-05 Traning Loss: 5.317446630215272e-05 17.982162475585938 2.0180606842041016\n",
            "22463 3.0075480026425794e-06 4.997749056201428e-05 Traning Loss: 5.298503674566746e-05 17.98233985900879 2.01798415184021\n",
            "22464 2.8482224934123224e-06 4.997277574148029e-05 Traning Loss: 5.2820996643276885e-05 17.982515335083008 2.017911434173584\n",
            "22465 2.8985357403144008e-06 4.983583130524494e-05 Traning Loss: 5.273436545394361e-05 17.98268699645996 2.0178377628326416\n",
            "22466 2.8386875783326104e-06 4.989155422663316e-05 Traning Loss: 5.273023998597637e-05 17.98285675048828 2.017766237258911\n",
            "22467 2.832043719536159e-06 4.9946567742154e-05 Traning Loss: 5.277861055219546e-05 17.98302459716797 2.017695188522339\n",
            "22468 2.8312749691394856e-06 5.0007871323032305e-05 Traning Loss: 5.2839146519545466e-05 17.98318862915039 2.017625331878662\n",
            "22469 2.801830987664289e-06 5.007469007978216e-05 Traning Loss: 5.287651947583072e-05 17.98335075378418 2.0175557136535645\n",
            "22470 2.7885889721801504e-06 5.010046879760921e-05 Traning Loss: 5.2889059588778764e-05 17.983509063720703 2.0174882411956787\n",
            "22471 2.7939947813138133e-06 5.0106540584238246e-05 Traning Loss: 5.290053377393633e-05 17.983665466308594 2.0174193382263184\n",
            "22472 2.717221605053055e-06 5.023013727623038e-05 Traning Loss: 5.294735819916241e-05 17.98381805419922 2.017354726791382\n",
            "22473 2.8223494155099615e-06 5.026061262469739e-05 Traning Loss: 5.308296385919675e-05 17.983966827392578 2.0172855854034424\n",
            "22474 2.6586778858472826e-06 5.0695154641289264e-05 Traning Loss: 5.335383320925757e-05 17.984113693237305 2.0172243118286133\n",
            "22475 2.9140078368072864e-06 5.0905680836876854e-05 Traning Loss: 5.381968730944209e-05 17.984256744384766 2.0171544551849365\n",
            "22476 2.66534038928512e-06 5.188859722693451e-05 Traning Loss: 5.45539369340986e-05 17.984397888183594 2.017097234725952\n",
            "22477 3.124378281427198e-06 5.255848736851476e-05 Traning Loss: 5.568286724155769e-05 17.984535217285156 2.01702618598938\n",
            "22478 2.8210756681801286e-06 5.459950989461504e-05 Traning Loss: 5.7420584198553115e-05 17.98467254638672 2.0169734954833984\n",
            "22479 3.596907845349051e-06 5.6502121879020706e-05 Traning Loss: 6.009903154335916e-05 17.984806060791016 2.016899824142456\n",
            "22480 3.334359462314751e-06 6.098558151279576e-05 Traning Loss: 6.431993824662641e-05 17.98493766784668 2.016853094100952\n",
            "22481 4.719890057458542e-06 6.623932131333277e-05 Traning Loss: 7.095921318978071e-05 17.985065460205078 2.016775131225586\n",
            "22482 4.779527444043197e-06 7.685519085498527e-05 Traning Loss: 8.163471648003906e-05 17.985191345214844 2.0167369842529297\n",
            "22483 7.513740001741098e-06 9.109544043894857e-05 Traning Loss: 9.860918362392113e-05 17.98531150817871 2.016650676727295\n",
            "22484 8.718820936337579e-06 0.00011760595953091979 Traning Loss: 0.00012632478319574147 17.985427856445312 2.0166256427764893\n",
            "22485 1.4643101167166606e-05 0.00015594938304275274 Traning Loss: 0.00017059248057194054 17.98553466796875 2.0165247917175293\n",
            "22486 1.9402750695007853e-05 0.00022446116781793535 Traning Loss: 0.0002438639203319326 17.98563575744629 2.0165212154388428\n",
            "22487 3.3141222957056016e-05 0.00032716491841711104 Traning Loss: 0.00036030614865012467 17.9857177734375 2.016395330429077\n",
            "22488 4.8173664254136384e-05 0.0005061891279183328 Traning Loss: 0.000554362777620554 17.985782623291016 2.0164263248443604\n",
            "22489 8.067681483225897e-05 0.0007729685748927295 Traning Loss: 0.0008536453824490309 17.98580551147461 2.0162596702575684\n",
            "22490 0.00012228709238115698 0.0012217144249007106 Traning Loss: 0.0013440015027299523 17.985780715942383 2.016343593597412\n",
            "22491 0.00019306551257614046 0.0018380142282694578 Traning Loss: 0.002031079726293683 17.98566436767578 2.016117572784424\n",
            "22492 0.00028483173809945583 0.0027744139079004526 Traning Loss: 0.0030592456459999084 17.98543357849121 2.0162699222564697\n",
            "22493 0.00039130725781433284 0.003735370235517621 Traning Loss: 0.004126677289605141 17.985031127929688 2.015983819961548\n",
            "22494 0.0004932135925628245 0.004742116201668978 Traning Loss: 0.005235329736024141 17.98443031311035 2.0161826610565186\n",
            "22495 0.0004952306626364589 0.004755210131406784 Traning Loss: 0.005250440910458565 17.983654022216797 2.0159032344818115\n",
            "22496 0.0004047706024721265 0.0038763831835240126 Traning Loss: 0.004281153902411461 17.982742309570312 2.016042947769165\n",
            "22497 0.00020208510977681726 0.0019508299883455038 Traning Loss: 0.0021529151126742363 17.981863021850586 2.0159082412719727\n",
            "22498 4.6202541852835566e-05 0.0004362183972261846 Traning Loss: 0.00048242093180306256 17.981048583984375 2.0158631801605225\n",
            "22499 2.391362068010494e-05 0.0002208125515608117 Traning Loss: 0.000244726164964959 17.980398178100586 2.01592755317688\n",
            "22500 0.00012628250988200307 0.0010704176966100931 Traning Loss: 0.0011967001482844353 17.979785919189453 2.015730857849121\n",
            "22501 0.00019754514505621046 0.0019018222810700536 Traning Loss: 0.0020993673242628574 17.97920036315918 2.0158681869506836\n",
            "22502 0.00018604111392050982 0.0016539529897272587 Traning Loss: 0.0018399941036477685 17.978593826293945 2.0157246589660645\n",
            "22503 8.310740668093786e-05 0.0007549030124209821 Traning Loss: 0.0008380103972740471 17.97796630859375 2.0157663822174072\n",
            "22504 2.2049982362659648e-05 0.00014587710029445589 Traning Loss: 0.00016792707901913673 17.977407455444336 2.015827178955078\n",
            "22505 5.593616879195906e-05 0.00039634210406802595 Traning Loss: 0.0004522782692220062 17.97685432434082 2.015735149383545\n",
            "22506 0.00010613800259307027 0.00098947633523494 Traning Loss: 0.0010956143960356712 17.976398468017578 2.0159003734588623\n",
            "22507 0.00012709065049421042 0.0010315291583538055 Traning Loss: 0.0011586197651922703 17.97595977783203 2.015803575515747\n",
            "22508 5.985550887999125e-05 0.0005536929238587618 Traning Loss: 0.0006135484436526895 17.975601196289062 2.015880823135376\n",
            "22509 1.9064311345573515e-05 0.00010513428424019367 Traning Loss: 0.00012419858830980957 17.975299835205078 2.0159265995025635\n",
            "22510 3.112525519100018e-05 0.000189295518794097 Traning Loss: 0.00022042077034711838 17.974977493286133 2.0158817768096924\n",
            "22511 6.188103725435212e-05 0.000563590379897505 Traning Loss: 0.0006254714098758996 17.974685668945312 2.0160398483276367\n",
            "22512 8.367661212105304e-05 0.0006516521098092198 Traning Loss: 0.0007353287073783576 17.974349975585938 2.0159928798675537\n",
            "22513 4.179717871011235e-05 0.00039910952909849584 Traning Loss: 0.0004409067041706294 17.974063873291016 2.016099691390991\n",
            "22514 1.6209243767661974e-05 9.68761887634173e-05 Traning Loss: 0.00011308543616905808 17.973812103271484 2.01613712310791\n",
            "22515 1.4931246369087603e-05 9.638204937800765e-05 Traning Loss: 0.00011131329665658996 17.973604202270508 2.0161170959472656\n",
            "22516 3.151322380290367e-05 0.00031041871989145875 Traning Loss: 0.0003419319400563836 17.973430633544922 2.0162131786346436\n",
            "22517 5.129195415065624e-05 0.00041040644282475114 Traning Loss: 0.00046169839333742857 17.973230361938477 2.016141653060913\n",
            "22518 3.1091058190213516e-05 0.00030556111596524715 Traning Loss: 0.0003366521850693971 17.973064422607422 2.016197681427002\n",
            "22519 1.5352508853538893e-05 0.00011427245772210881 Traning Loss: 0.0001296249683946371 17.972896575927734 2.0161702632904053\n",
            "22520 7.074609584378777e-06 6.922571628820151e-05 Traning Loss: 7.630032632732764e-05 17.972774505615234 2.0161240100860596\n",
            "22521 1.4866199308016803e-05 0.00017340747581329197 Traning Loss: 0.00018827366875484586 17.97269058227539 2.0161335468292236\n",
            "22522 2.9196078685345128e-05 0.0002610690426081419 Traning Loss: 0.00029026513220742345 17.97264289855957 2.0160248279571533\n",
            "22523 2.054698961728718e-05 0.00023796770256012678 Traning Loss: 0.00025851468672044575 17.97262954711914 2.0160069465637207\n",
            "22524 1.3068663065496366e-05 0.00012240845535416156 Traning Loss: 0.00013547712296713144 17.972633361816406 2.0159130096435547\n",
            "22525 3.985313014709391e-06 5.770900679635815e-05 Traning Loss: 6.169432163005695e-05 17.972665786743164 2.0158309936523438\n",
            "22526 6.8801978159171995e-06 8.947467722464353e-05 Traning Loss: 9.635487367631868e-05 17.97271156311035 2.015777111053467\n",
            "22527 1.6084512026282027e-05 0.000157293543452397 Traning Loss: 0.0001733780518407002 17.972782135009766 2.0156493186950684\n",
            "22528 1.4930561519577168e-05 0.00018187450768891722 Traning Loss: 0.00019680506375152618 17.972877502441406 2.0155911445617676\n",
            "22529 1.3049595509073697e-05 0.00012992897245567292 Traning Loss: 0.0001429785625077784 17.973003387451172 2.0154688358306885\n",
            "22530 4.949896720063407e-06 7.117004133760929e-05 Traning Loss: 7.61199407861568e-05 17.97314453125 2.0153684616088867\n",
            "22531 3.8321463762258645e-06 5.6209813919849694e-05 Traning Loss: 6.0041958931833506e-05 17.973302841186523 2.015286684036255\n",
            "22532 9.028869499161374e-06 8.596133557148278e-05 Traning Loss: 9.499020234216005e-05 17.973453521728516 2.0151636600494385\n",
            "22533 9.728725672175642e-06 0.00011980360432062298 Traning Loss: 0.00012953233090229332 17.973608016967773 2.015110969543457\n",
            "22534 1.1826399713754654e-05 0.00011114427616121247 Traning Loss: 0.00012297066859900951 17.973764419555664 2.0150067806243896\n",
            "22535 5.92996593695716e-06 8.074520155787468e-05 Traning Loss: 8.667516522109509e-05 17.973936080932617 2.014946460723877\n",
            "22536 4.2209785533486865e-06 5.52387282368727e-05 Traning Loss: 5.945970769971609e-05 17.974130630493164 2.0148868560791016\n",
            "22537 5.375556611397769e-06 5.888060695724562e-05 Traning Loss: 6.425616447813809e-05 17.974332809448242 2.0148096084594727\n",
            "22538 5.8887171690003015e-06 8.151362999342382e-05 Traning Loss: 8.740234625292942e-05 17.974559783935547 2.0147886276245117\n",
            "22539 9.617100658942945e-06 8.97358258953318e-05 Traning Loss: 9.935292473528534e-05 17.97477912902832 2.0147225856781006\n",
            "22540 6.130351721367333e-06 8.162590529536828e-05 Traning Loss: 8.775625610724092e-05 17.975011825561523 2.014713764190674\n",
            "22541 5.691365913662594e-06 5.999784843879752e-05 Traning Loss: 6.568921526195481e-05 17.97524070739746 2.0146865844726562\n",
            "22542 3.7975250961608253e-06 5.0981496315216646e-05 Traning Loss: 5.477902232087217e-05 17.97547721862793 2.014671564102173\n",
            "22543 4.0193031054513995e-06 5.7538658438716084e-05 Traning Loss: 6.155796290840954e-05 17.97572135925293 2.0146825313568115\n",
            "22544 6.582989044545684e-06 6.767913873773068e-05 Traning Loss: 7.426212687278166e-05 17.975963592529297 2.014665365219116\n",
            "22545 5.201840394875035e-06 7.291966176126152e-05 Traning Loss: 7.812150579411536e-05 17.976213455200195 2.014693021774292\n",
            "22546 6.133783244877122e-06 6.37491830275394e-05 Traning Loss: 6.988296809140593e-05 17.976451873779297 2.014693260192871\n",
            "22547 3.840164481516695e-06 5.5003929446684197e-05 Traning Loss: 5.884409256395884e-05 17.9766902923584 2.0147175788879395\n",
            "22548 3.910687610186869e-06 5.1160452130716294e-05 Traning Loss: 5.5071141105145216e-05 17.9769229888916 2.0147438049316406\n",
            "22549 4.415149305714294e-06 5.54561847820878e-05 Traning Loss: 5.98713340878021e-05 17.977157592773438 2.0147595405578613\n",
            "22550 4.226885266689351e-06 6.203328666742891e-05 Traning Loss: 6.626017420785502e-05 17.977394104003906 2.0147979259490967\n",
            "22551 5.176776994630927e-06 6.188947736518458e-05 Traning Loss: 6.706625572405756e-05 17.97763442993164 2.01481294631958\n",
            "22552 3.7713314213760896e-06 5.804195097880438e-05 Traning Loss: 6.181328353704885e-05 17.977876663208008 2.0148472785949707\n",
            "22553 3.714691956702154e-06 5.1944229198852554e-05 Traning Loss: 5.565892206504941e-05 17.978116989135742 2.0148727893829346\n",
            "22554 3.40263795806095e-06 5.027863153372891e-05 Traning Loss: 5.368126949178986e-05 17.978351593017578 2.0148961544036865\n",
            "22555 3.2978082344925497e-06 5.294142829370685e-05 Traning Loss: 5.623923789244145e-05 17.978586196899414 2.0149312019348145\n",
            "22556 4.157658167969203e-06 5.5524455092381686e-05 Traning Loss: 5.9682111896108836e-05 17.97881507873535 2.014946460723877\n",
            "22557 3.439707143115811e-06 5.697190499631688e-05 Traning Loss: 6.0411613958422095e-05 17.979045867919922 2.0149779319763184\n",
            "22558 3.7860982047277503e-06 5.414702900452539e-05 Traning Loss: 5.7933128118747845e-05 17.979276657104492 2.0149919986724854\n",
            "22559 3.0630505989392987e-06 5.1637267461046576e-05 Traning Loss: 5.470031828735955e-05 17.97950553894043 2.015009641647339\n",
            "22560 2.9840341539966175e-06 5.035816866438836e-05 Traning Loss: 5.33422025910113e-05 17.979734420776367 2.015026569366455\n",
            "22561 3.341166802783846e-06 5.10480094817467e-05 Traning Loss: 5.4389176511904225e-05 17.97995376586914 2.015031099319458\n",
            "22562 2.9811819786118576e-06 5.325528763933107e-05 Traning Loss: 5.6236469390569255e-05 17.980173110961914 2.0150487422943115\n",
            "22563 3.6252865811547963e-06 5.3303472668631e-05 Traning Loss: 5.692875856766477e-05 17.980384826660156 2.0150468349456787\n",
            "22564 2.850800001397147e-06 5.303218131302856e-05 Traning Loss: 5.588298154179938e-05 17.98059844970703 2.0150575637817383\n",
            "22565 3.1117599519348005e-06 5.093578511150554e-05 Traning Loss: 5.404754483606666e-05 17.980806350708008 2.0150556564331055\n",
            "22566 2.6911593522527255e-06 5.0147897127317265e-05 Traning Loss: 5.283905557007529e-05 17.981016159057617 2.015054941177368\n",
            "22567 2.724708110690699e-06 5.0174188800156116e-05 Traning Loss: 5.289889668347314e-05 17.98122215270996 2.015054702758789\n",
            "22568 2.9546060886787018e-06 5.079936818219721e-05 Traning Loss: 5.375397449824959e-05 17.981426239013672 2.015045642852783\n",
            "22569 2.7245707769907312e-06 5.168749703443609e-05 Traning Loss: 5.4412066674558446e-05 17.981626510620117 2.015044927597046\n",
            "22570 3.0136998248053715e-06 5.1225248171249405e-05 Traning Loss: 5.423894617706537e-05 17.981822967529297 2.015033006668091\n",
            "22571 2.6162490485148737e-06 5.078400863567367e-05 Traning Loss: 5.340025745681487e-05 17.982019424438477 2.0150272846221924\n",
            "22572 2.7103646971227136e-06 4.983058897778392e-05 Traning Loss: 5.254095231066458e-05 17.98221206665039 2.0150160789489746\n",
            "22573 2.5995896066888236e-06 4.961110607837327e-05 Traning Loss: 5.2210696594556794e-05 17.982402801513672 2.0150036811828613\n",
            "22574 2.559258064138703e-06 4.989500666852109e-05 Traning Loss: 5.245426291367039e-05 17.98259162902832 2.0149941444396973\n",
            "22575 2.7614037207968067e-06 5.013701957068406e-05 Traning Loss: 5.289842374622822e-05 17.98277473449707 2.0149776935577393\n",
            "22576 2.563372163422173e-06 5.053702989243902e-05 Traning Loss: 5.310040069161914e-05 17.982954025268555 2.014967918395996\n",
            "22577 2.73599698630278e-06 5.014816269977018e-05 Traning Loss: 5.288415923132561e-05 17.983129501342773 2.014951229095459\n",
            "22578 2.511749016775866e-06 4.990881643607281e-05 Traning Loss: 5.24205643159803e-05 17.98330307006836 2.0149381160736084\n",
            "22579 2.5302367703261552e-06 4.949384674546309e-05 Traning Loss: 5.202408283366822e-05 17.983474731445312 2.014922618865967\n",
            "22580 2.525213403714588e-06 4.937514313496649e-05 Traning Loss: 5.190035517443903e-05 17.983642578125 2.014904737472534\n",
            "22581 2.4155565370165277e-06 4.961460581398569e-05 Traning Loss: 5.203016189625487e-05 17.983810424804688 2.014890432357788\n",
            "22582 2.611134732433129e-06 4.9615500756772235e-05 Traning Loss: 5.222663457971066e-05 17.983972549438477 2.014869451522827\n",
            "22583 2.3875934402894927e-06 4.9909736844711006e-05 Traning Loss: 5.2297331421868876e-05 17.984134674072266 2.014854907989502\n",
            "22584 2.588991492302739e-06 4.9587953981244937e-05 Traning Loss: 5.2176947065163404e-05 17.984291076660156 2.014833450317383\n",
            "22585 2.3482562028220855e-06 4.9595550080994144e-05 Traning Loss: 5.194380719331093e-05 17.984447479248047 2.0148167610168457\n",
            "22586 2.46603076448082e-06 4.927546615363099e-05 Traning Loss: 5.1741495553869754e-05 17.98459815979004 2.014796018600464\n",
            "22587 2.345406755921431e-06 4.931965668220073e-05 Traning Loss: 5.166506161913276e-05 17.98474884033203 2.0147762298583984\n",
            "22588 2.3857480755395954e-06 4.933214222546667e-05 Traning Loss: 5.1717888709390536e-05 17.984895706176758 2.014755964279175\n",
            "22589 2.382815637247404e-06 4.944456304656342e-05 Traning Loss: 5.182737731956877e-05 17.985042572021484 2.0147342681884766\n",
            "22590 2.3691391106694937e-06 4.95401764055714e-05 Traning Loss: 5.1909315516240895e-05 17.985183715820312 2.0147135257720947\n",
            "22591 2.371490836594603e-06 4.955047552357428e-05 Traning Loss: 5.192196476855315e-05 17.98532485961914 2.0146915912628174\n",
            "22592 2.3694765332038514e-06 4.952227754984051e-05 Traning Loss: 5.189175499253906e-05 17.98546028137207 2.014669418334961\n",
            "22593 2.3031391265249113e-06 4.9579673941479996e-05 Traning Loss: 5.188281284063123e-05 17.985597610473633 2.0146484375\n",
            "22594 2.4068108359642792e-06 4.9552807467989624e-05 Traning Loss: 5.195961784920655e-05 17.985727310180664 2.0146241188049316\n",
            "22595 2.2561359855899354e-06 4.990071101929061e-05 Traning Loss: 5.215684723225422e-05 17.98586082458496 2.0146048069000244\n",
            "22596 2.5008830562001094e-06 4.998408621759154e-05 Traning Loss: 5.2484967454802245e-05 17.985984802246094 2.0145788192749023\n",
            "22597 2.270841605422902e-06 5.066965240985155e-05 Traning Loss: 5.294049333315343e-05 17.986114501953125 2.0145609378814697\n",
            "22598 2.6332872948842123e-06 5.091625280329026e-05 Traning Loss: 5.354954191716388e-05 17.98623275756836 2.0145339965820312\n",
            "22599 2.355100832573953e-06 5.203162436373532e-05 Traning Loss: 5.4386724514188245e-05 17.986358642578125 2.014517307281494\n",
            "22600 2.8421764000086114e-06 5.27495976712089e-05 Traning Loss: 5.559177225222811e-05 17.98647117614746 2.0144896507263184\n",
            "22601 2.5816514153120806e-06 5.4785476095275953e-05 Traning Loss: 5.736712773796171e-05 17.98659324645996 2.0144739151000977\n",
            "22602 3.2897007713472703e-06 5.673264604411088e-05 Traning Loss: 6.0022346588084474e-05 17.986698150634766 2.0144450664520264\n",
            "22603 3.135800398013089e-06 6.0834441683255136e-05 Traning Loss: 6.397024117177352e-05 17.986818313598633 2.0144314765930176\n",
            "22604 4.30279715146753e-06 6.558631139341742e-05 Traning Loss: 6.98891089996323e-05 17.986913681030273 2.0144004821777344\n",
            "22605 4.427724434208358e-06 7.429839752148837e-05 Traning Loss: 7.872612331993878e-05 17.98703384399414 2.014390707015991\n",
            "22606 6.581462912436109e-06 8.555743988836184e-05 Traning Loss: 9.213890007231385e-05 17.987117767333984 2.014355421066284\n",
            "22607 7.475447546312353e-06 0.00010495389142306522 Traning Loss: 0.00011242934124311432 17.98723602294922 2.0143520832061768\n",
            "22608 1.1815335710707586e-05 0.0001319603470619768 Traning Loss: 0.00014377568732015789 17.987300872802734 2.0143089294433594\n",
            "22609 1.4838454262644518e-05 0.00017689605010673404 Traning Loss: 0.00019173450709786266 17.98741912841797 2.0143158435821533\n",
            "22610 2.415192466287408e-05 0.00024275090254377574 Traning Loss: 0.0002669028181117028 17.98745346069336 2.014260768890381\n",
            "22611 3.282944089733064e-05 0.00034944701474159956 Traning Loss: 0.0003822764556389302 17.987565994262695 2.014284133911133\n",
            "22612 5.3660351113649085e-05 0.0005107598844915628 Traning Loss: 0.0005644202465191483 17.98754119873047 2.0142109394073486\n",
            "22613 7.65141739975661e-05 0.0007641429547220469 Traning Loss: 0.0008406571578234434 17.98763084411621 2.0142600536346436\n",
            "22614 0.00012343416165094823 0.001149398391135037 Traning Loss: 0.0012728325091302395 17.98749351501465 2.0141608715057373\n",
            "22615 0.00017773342551663518 0.0017198340501636267 Traning Loss: 0.001897567417472601 17.987506866455078 2.014249086380005\n",
            "22616 0.0002757245965767652 0.002549037802964449 Traning Loss: 0.0028247623704373837 17.98714828491211 2.014117479324341\n",
            "22617 0.00037817039992660284 0.003608940402045846 Traning Loss: 0.003987110685557127 17.986970901489258 2.014259099960327\n",
            "22618 0.0005310038686729968 0.004894837737083435 Traning Loss: 0.005425841547548771 17.98624038696289 2.0141031742095947\n",
            "22619 0.0006219958304427564 0.0059119537472724915 Traning Loss: 0.006533949635922909 17.985715866088867 2.014296770095825\n",
            "22620 0.0006908783107064664 0.0063316840678453445 Traning Loss: 0.007022562436759472 17.984594345092773 2.0141689777374268\n",
            "22621 0.0005595929105766118 0.005325867794454098 Traning Loss: 0.005885460879653692 17.983755111694336 2.0143704414367676\n",
            "22622 0.00036895961966365576 0.0032699161674827337 Traning Loss: 0.0036388756707310677 17.982635498046875 2.014371871948242\n",
            "22623 0.0001211075359606184 0.001078158849850297 Traning Loss: 0.0011992664076387882 17.9818115234375 2.0145158767700195\n",
            "22624 2.651863542268984e-05 8.086457819445059e-05 Traning Loss: 0.00010738320997916162 17.98111343383789 2.0147078037261963\n",
            "22625 9.861478611128405e-05 0.0006214987370185554 Traning Loss: 0.0007201135158538818 17.980358123779297 2.0148017406463623\n",
            "22626 0.00020583321747835726 0.0018322947435081005 Traning Loss: 0.002038127975538373 17.979801177978516 2.0151267051696777\n",
            "22627 0.00029754702700302005 0.002389623085036874 Traning Loss: 0.0026871701702475548 17.978912353515625 2.015272617340088\n",
            "22628 0.00020749888790305704 0.0017984415171667933 Traning Loss: 0.002005940303206444 17.97818374633789 2.0156025886535645\n",
            "22629 0.00010578853107290342 0.0006711403839290142 Traning Loss: 0.00077692890772596 17.977344512939453 2.0158767700195312\n",
            "22630 4.291735604056157e-05 8.3818486018572e-05 Traning Loss: 0.00012673584569711238 17.976552963256836 2.016130208969116\n",
            "22631 6.944123015273362e-05 0.0004093259631190449 Traning Loss: 0.00047876720782369375 17.975879669189453 2.0165040493011475\n",
            "22632 0.0001516160264145583 0.0010448152897879481 Traning Loss: 0.0011964313453063369 17.975038528442383 2.0167336463928223\n",
            "22633 0.00015499029541388154 0.0012404770823195577 Traning Loss: 0.0013954674359411001 17.974369049072266 2.017116069793701\n",
            "22634 0.00011594227544264868 0.000802286434918642 Traning Loss: 0.0009182287030853331 17.973600387573242 2.01741099357605\n",
            "22635 5.1815397455357015e-05 0.0002437784569337964 Traning Loss: 0.00029559386894106865 17.972972869873047 2.017711639404297\n",
            "22636 3.2285806810250506e-05 0.00010368095536250621 Traning Loss: 0.00013596676581073552 17.97246551513672 2.0180702209472656\n",
            "22637 6.774174835300073e-05 0.0003784493310377002 Traning Loss: 0.0004461910866666585 17.97193145751953 2.018308401107788\n",
            "22638 8.770930435275659e-05 0.0006756311049684882 Traning Loss: 0.000763340387493372 17.9715633392334 2.0186638832092285\n",
            "22639 8.842728129820898e-05 0.0006431364454329014 Traning Loss: 0.0007315637194551528 17.971113204956055 2.0189151763916016\n",
            "22640 5.140125722391531e-05 0.00035941501846536994 Traning Loss: 0.00041081628296524286 17.97079086303711 2.0191845893859863\n",
            "22641 2.4129119992721826e-05 0.00012562103802338243 Traning Loss: 0.00014975015074014664 17.970500946044922 2.0194568634033203\n",
            "22642 2.7968193535343744e-05 0.00012483744649216533 Traning Loss: 0.00015280564548447728 17.97022247314453 2.0196266174316406\n",
            "22643 3.628675767686218e-05 0.0002926528686657548 Traning Loss: 0.0003289396408945322 17.97005271911621 2.0198802947998047\n",
            "22644 5.42456800758373e-05 0.0003944167692679912 Traning Loss: 0.00044866243842989206 17.969804763793945 2.020015001296997\n",
            "22645 3.99465934606269e-05 0.000351015041815117 Traning Loss: 0.00039096164982765913 17.96965980529785 2.0201902389526367\n",
            "22646 2.69248521362897e-05 0.00020902277901768684 Traning Loss: 0.00023594763479195535 17.969491958618164 2.020329475402832\n",
            "22647 1.6723941371310502e-05 0.00011162690498167649 Traning Loss: 0.000128350846352987 17.969369888305664 2.0204105377197266\n",
            "22648 1.249847082362976e-05 0.00012507732026278973 Traning Loss: 0.00013757578562945127 17.96930503845215 2.0205512046813965\n",
            "22649 2.654980016814079e-05 0.0001829871180234477 Traning Loss: 0.00020953691273462027 17.969221115112305 2.02058744430542\n",
            "22650 2.196535024268087e-05 0.00022872955014463514 Traning Loss: 0.0002506948949303478 17.969234466552734 2.020691394805908\n",
            "22651 2.459257302689366e-05 0.00019763635646086186 Traning Loss: 0.00022222893312573433 17.969223022460938 2.020726203918457\n",
            "22652 1.4429520888370462e-05 0.00014302197087090462 Traning Loss: 0.00015745148994028568 17.969287872314453 2.0207571983337402\n",
            "22653 1.0075063983094878e-05 0.00010566428682068363 Traning Loss: 0.0001157393489847891 17.969371795654297 2.020808219909668\n",
            "22654 1.3835109712090343e-05 0.00010511612344998866 Traning Loss: 0.000118951233162079 17.969465255737305 2.0207855701446533\n",
            "22655 1.1459827874205075e-05 0.00013248993491288275 Traning Loss: 0.0001439497573301196 17.96961212158203 2.0208332538604736\n",
            "22656 1.7915412172442302e-05 0.0001352968974970281 Traning Loss: 0.00015321231330744922 17.969730377197266 2.02079701423645\n",
            "22657 1.1053220077883452e-05 0.00012306636199355125 Traning Loss: 0.00013411958934739232 17.96990394592285 2.0208046436309814\n",
            "22658 1.0656750419002492e-05 9.359526302432641e-05 Traning Loss: 0.00010425201617181301 17.970056533813477 2.020785093307495\n",
            "22659 8.394327778660227e-06 8.00138441263698e-05 Traning Loss: 8.840817463351414e-05 17.97023582458496 2.020745277404785\n",
            "22660 7.612348781549372e-06 8.708488167030737e-05 Traning Loss: 9.469722863286734e-05 17.970430374145508 2.020745277404785\n",
            "22661 1.2144575521233492e-05 9.684477117843926e-05 Traning Loss: 0.00010898934851866215 17.970617294311523 2.020676374435425\n",
            "22662 8.981906830740627e-06 0.00010351177479606122 Traning Loss: 0.00011249368253629655 17.970844268798828 2.0206682682037354\n",
            "22663 1.0805241799971554e-05 8.754511509323493e-05 Traning Loss: 9.835035598371178e-05 17.971054077148438 2.0206024646759033\n",
            "22664 6.091984232625691e-06 7.124045805539936e-05 Traning Loss: 7.73324427427724e-05 17.971298217773438 2.020564556121826\n",
            "22665 5.891815362701891e-06 5.9408903325675055e-05 Traning Loss: 6.530071550514549e-05 17.971540451049805 2.0205202102661133\n",
            "22666 6.5059330154326744e-06 6.251284503377974e-05 Traning Loss: 6.901878077769652e-05 17.9717960357666 2.020448684692383\n",
            "22667 6.27316649115528e-06 7.533838652307168e-05 Traning Loss: 8.16115498309955e-05 17.972068786621094 2.020416498184204\n",
            "22668 9.448952368984465e-06 8.025327406357974e-05 Traning Loss: 8.97022255230695e-05 17.97233009338379 2.0203301906585693\n",
            "22669 6.496702553704381e-06 7.915464084362611e-05 Traning Loss: 8.565134339733049e-05 17.972612380981445 2.0202932357788086\n",
            "22670 7.357464255619561e-06 6.478201976278797e-05 Traning Loss: 7.213948265416548e-05 17.972877502441406 2.0202178955078125\n",
            "22671 4.52530639449833e-06 5.4745094530517235e-05 Traning Loss: 5.927040183451027e-05 17.973155975341797 2.0201597213745117\n",
            "22672 4.5533643060480244e-06 5.074430737295188e-05 Traning Loss: 5.52976707695052e-05 17.973434448242188 2.0201022624969482\n",
            "22673 5.5388368309650104e-06 5.525146480067633e-05 Traning Loss: 6.079030208638869e-05 17.973711013793945 2.0200228691101074\n",
            "22674 5.091865205031354e-06 6.435067189158872e-05 Traning Loss: 6.944253982510418e-05 17.9739990234375 2.0199778079986572\n",
            "22675 7.224073215184035e-06 6.645641406066716e-05 Traning Loss: 7.368048682110384e-05 17.974275588989258 2.0198938846588135\n",
            "22676 5.1324368541827425e-06 6.538483285112306e-05 Traning Loss: 7.051727152429521e-05 17.974563598632812 2.0198473930358887\n",
            "22677 5.77429909753846e-06 5.67705137655139e-05 Traning Loss: 6.254481559153646e-05 17.974842071533203 2.0197739601135254\n",
            "22678 4.082789018866606e-06 5.129036071593873e-05 Traning Loss: 5.537315155379474e-05 17.97512435913086 2.0197155475616455\n",
            "22679 4.068619091412984e-06 4.89547019242309e-05 Traning Loss: 5.302332283463329e-05 17.975404739379883 2.0196595191955566\n",
            "22680 4.6313034545164555e-06 5.096939639770426e-05 Traning Loss: 5.5600699852220714e-05 17.975677490234375 2.019590139389038\n",
            "22681 4.2088026930287015e-06 5.5806820455472916e-05 Traning Loss: 6.001562360324897e-05 17.975954055786133 2.0195446014404297\n",
            "22682 5.532846444111783e-06 5.712614438380115e-05 Traning Loss: 6.265898991841823e-05 17.97621726989746 2.0194714069366455\n",
            "22683 4.2891292650892865e-06 5.760574640589766e-05 Traning Loss: 6.18948761257343e-05 17.976484298706055 2.0194251537323\n",
            "22684 4.831536898564082e-06 5.3730927902506664e-05 Traning Loss: 5.856246571056545e-05 17.97673988342285 2.019357919692993\n",
            "22685 3.769384420593269e-06 5.120050263940357e-05 Traning Loss: 5.496988887898624e-05 17.976993560791016 2.019303798675537\n",
            "22686 3.748090193766984e-06 4.9285001296084374e-05 Traning Loss: 5.303309080773033e-05 17.977245330810547 2.0192458629608154\n",
            "22687 3.808094106716453e-06 4.943620297126472e-05 Traning Loss: 5.324429730535485e-05 17.97749137878418 2.0191810131073\n",
            "22688 3.4686570415942697e-06 5.123325900058262e-05 Traning Loss: 5.4701915360055864e-05 17.97774314880371 2.0191280841827393\n",
            "22689 4.150249878875911e-06 5.187153146835044e-05 Traning Loss: 5.602178134722635e-05 17.97798728942871 2.019057512283325\n",
            "22690 3.4519498512963764e-06 5.2838910050923005e-05 Traning Loss: 5.629086081171408e-05 17.978235244750977 2.0190041065216064\n",
            "22691 3.945276148442645e-06 5.152155790710822e-05 Traning Loss: 5.546683314605616e-05 17.978477478027344 2.0189337730407715\n",
            "22692 3.2914824714680435e-06 5.089242768008262e-05 Traning Loss: 5.418391083367169e-05 17.97871971130371 2.018873929977417\n",
            "22693 3.422808276809519e-06 4.9749236495699733e-05 Traning Loss: 5.31720434082672e-05 17.978960037231445 2.018805980682373\n",
            "22694 3.2866084893612424e-06 4.9521895562065765e-05 Traning Loss: 5.280850382405333e-05 17.97919464111328 2.018737316131592\n",
            "22695 3.1349600249086507e-06 4.9849481001729146e-05 Traning Loss: 5.2984440117143095e-05 17.979433059692383 2.0186715126037598\n",
            "22696 3.442048864599201e-06 4.985757186659612e-05 Traning Loss: 5.329962004907429e-05 17.97966194152832 2.0185961723327637\n",
            "22697 3.044883214897709e-06 5.0346327043371275e-05 Traning Loss: 5.339120980352163e-05 17.97989273071289 2.0185294151306152\n",
            "22698 3.4034562759188702e-06 4.9731985200196505e-05 Traning Loss: 5.313544170348905e-05 17.980113983154297 2.01845121383667\n",
            "22699 2.9570749120466644e-06 4.9711255996953696e-05 Traning Loss: 5.266833250061609e-05 17.980335235595703 2.0183815956115723\n",
            "22700 3.1535482776234858e-06 4.9084756028605625e-05 Traning Loss: 5.223830521572381e-05 17.98055076599121 2.0183045864105225\n",
            "22701 2.9400036964943865e-06 4.909331619273871e-05 Traning Loss: 5.203331966185942e-05 17.980764389038086 2.01823091506958\n",
            "22702 2.9486932362488005e-06 4.912924487143755e-05 Traning Loss: 5.20779394719284e-05 17.98097801208496 2.018156051635742\n",
            "22703 3.0193268685252406e-06 4.922298467135988e-05 Traning Loss: 5.224231063039042e-05 17.981185913085938 2.0180788040161133\n",
            "22704 2.8524655135697685e-06 4.9495014536660165e-05 Traning Loss: 5.2347480959724635e-05 17.981395721435547 2.018005609512329\n",
            "22705 3.0434455311478814e-06 4.923014057567343e-05 Traning Loss: 5.227358633419499e-05 17.981597900390625 2.017927885055542\n",
            "22706 2.784509888442699e-06 4.922608059132472e-05 Traning Loss: 5.2010589570272714e-05 17.981801986694336 2.0178558826446533\n",
            "22707 2.9397954222076805e-06 4.8718167818151414e-05 Traning Loss: 5.165796392248012e-05 17.981998443603516 2.017779588699341\n",
            "22708 2.7402136311138747e-06 4.860194167122245e-05 Traning Loss: 5.13421546202153e-05 17.982194900512695 2.017707347869873\n",
            "22709 2.8051272238371894e-06 4.835777508560568e-05 Traning Loss: 5.1162904128432274e-05 17.98238754272461 2.0176336765289307\n",
            "22710 2.7646508442558115e-06 4.837953019887209e-05 Traning Loss: 5.114417945151217e-05 17.982574462890625 2.017561197280884\n",
            "22711 2.7313840291753877e-06 4.850698314839974e-05 Traning Loss: 5.123836672282778e-05 17.982759475708008 2.0174906253814697\n",
            "22712 2.8135671072959667e-06 4.854445796809159e-05 Traning Loss: 5.135802348377183e-05 17.98293685913086 2.017418622970581\n",
            "22713 2.6913157853414305e-06 4.873142097494565e-05 Traning Loss: 5.142273585079238e-05 17.98311424255371 2.017350196838379\n",
            "22714 2.7954554298048606e-06 4.859601176576689e-05 Traning Loss: 5.1391467422945425e-05 17.983285903930664 2.017279863357544\n",
            "22715 2.6433169750816887e-06 4.862291825702414e-05 Traning Loss: 5.12662336404901e-05 17.983457565307617 2.0172128677368164\n",
            "22716 2.7050814424001146e-06 4.838417225982994e-05 Traning Loss: 5.10892532474827e-05 17.983623504638672 2.0171449184417725\n",
            "22717 2.6037421321234433e-06 4.831004480365664e-05 Traning Loss: 5.0913786253659055e-05 17.983787536621094 2.0170786380767822\n",
            "22718 2.6029865693999454e-06 4.817620720132254e-05 Traning Loss: 5.077919195173308e-05 17.983949661254883 2.0170133113861084\n",
            "22719 2.588653842394706e-06 4.811171311303042e-05 Traning Loss: 5.0700367864919826e-05 17.984107971191406 2.0169479846954346\n",
            "22720 2.52796667155053e-06 4.814369822270237e-05 Traning Loss: 5.067166421213187e-05 17.984264373779297 2.016885280609131\n",
            "22721 2.58112004303257e-06 4.80883609270677e-05 Traning Loss: 5.066948142484762e-05 17.984416961669922 2.0168211460113525\n",
            "22722 2.479689783285721e-06 4.8191039240919054e-05 Traning Loss: 5.067072925157845e-05 17.984567642211914 2.0167603492736816\n",
            "22723 2.553793592596776e-06 4.8100348067237064e-05 Traning Loss: 5.065414006821811e-05 17.98471450805664 2.016697645187378\n",
            "22724 2.446448434056947e-06 4.816650471184403e-05 Traning Loss: 5.061295451014303e-05 17.984861373901367 2.0166378021240234\n",
            "22725 2.500286200302071e-06 4.805248318007216e-05 Traning Loss: 5.05527677887585e-05 17.985002517700195 2.0165770053863525\n",
            "22726 2.422044417471625e-06 4.80621492897626e-05 Traning Loss: 5.048419552622363e-05 17.985143661499023 2.0165178775787354\n",
            "22727 2.4318330815731315e-06 4.7988152800826356e-05 Traning Loss: 5.041998520027846e-05 17.985279083251953 2.0164589881896973\n",
            "22728 2.400102857791353e-06 4.796810753759928e-05 Traning Loss: 5.036820948589593e-05 17.98541259765625 2.0164003372192383\n",
            "22729 2.3621350919711404e-06 4.7964680561563e-05 Traning Loss: 5.032681656302884e-05 17.985544204711914 2.016343355178833\n",
            "22730 2.3770016923663206e-06 4.7919340431690216e-05 Traning Loss: 5.0296341214561835e-05 17.985673904418945 2.0162854194641113\n",
            "22731 2.3038219296722673e-06 4.796804932993837e-05 Traning Loss: 5.0271872169105336e-05 17.985801696777344 2.0162301063537598\n",
            "22732 2.3496356789109996e-06 4.789427839568816e-05 Traning Loss: 5.0243914301972836e-05 17.985925674438477 2.0161728858947754\n",
            "22733 2.2589345007872907e-06 4.7953155444702134e-05 Traning Loss: 5.02120892633684e-05 17.98604965209961 2.0161190032958984\n",
            "22734 2.312484639332979e-06 4.785925557371229e-05 Traning Loss: 5.0171740440418944e-05 17.986169815063477 2.0160629749298096\n",
            "22735 2.2220287974050734e-06 4.7902140067890286e-05 Traning Loss: 5.012417022953741e-05 17.986289978027344 2.016010046005249\n",
            "22736 2.265968305437127e-06 4.7807123337406665e-05 Traning Loss: 5.0073093007085845e-05 17.986406326293945 2.0159554481506348\n",
            "22737 2.191633484471822e-06 4.7829966206336394e-05 Traning Loss: 5.002160105505027e-05 17.986520767211914 2.0159029960632324\n",
            "22738 2.2164788333611796e-06 4.775408160639927e-05 Traning Loss: 4.997056021238677e-05 17.98663330078125 2.0158495903015137\n",
            "22739 2.1667490273102885e-06 4.775815978064202e-05 Traning Loss: 4.992490721633658e-05 17.986743927001953 2.0157976150512695\n",
            "22740 2.169413164665457e-06 4.771429667016491e-05 Traning Loss: 4.988371074432507e-05 17.986852645874023 2.0157456398010254\n",
            "22741 2.1441921944642672e-06 4.770220766658895e-05 Traning Loss: 4.984639963367954e-05 17.98695945739746 2.0156943798065186\n",
            "22742 2.1264258975861594e-06 4.768631697515957e-05 Traning Loss: 4.9812741053756326e-05 17.987064361572266 2.01564359664917\n",
            "22743 2.119971668435028e-06 4.766160054714419e-05 Traning Loss: 4.9781570851337165e-05 17.987167358398438 2.0155928134918213\n",
            "22744 2.0876880171272205e-06 4.76618624816183e-05 Traning Loss: 4.974954936187714e-05 17.98727035522461 2.015542984008789\n",
            "22745 2.093275270453887e-06 4.7625268052797765e-05 Traning Loss: 4.97185428685043e-05 17.987369537353516 2.0154929161071777\n",
            "22746 2.0547013264149427e-06 4.763322067447007e-05 Traning Loss: 4.968792200088501e-05 17.987468719482422 2.015443801879883\n",
            "22747 2.0651364138757344e-06 4.758976137964055e-05 Traning Loss: 4.965489642927423e-05 17.987565994262695 2.015394449234009\n",
            "22748 2.026558831857983e-06 4.759386138175614e-05 Traning Loss: 4.962041930411942e-05 17.987661361694336 2.015346050262451\n",
            "22749 2.0349352780613117e-06 4.755029658554122e-05 Traning Loss: 4.958523277309723e-05 17.987754821777344 2.0152974128723145\n",
            "22750 2.001317625399679e-06 4.7549274313496426e-05 Traning Loss: 4.9550591938896105e-05 17.98784637451172 2.015249729156494\n",
            "22751 2.003779400183703e-06 4.751171582029201e-05 Traning Loss: 4.951549635734409e-05 17.98793601989746 2.015202045440674\n",
            "22752 1.9791345948760863e-06 4.75021697639022e-05 Traning Loss: 4.9481302994536236e-05 17.988025665283203 2.0151548385620117\n",
            "22753 1.9732240161829395e-06 4.747522689285688e-05 Traning Loss: 4.94484520459082e-05 17.988113403320312 2.0151078701019287\n",
            "22754 1.9593508113757707e-06 4.7459161578444764e-05 Traning Loss: 4.941851148032583e-05 17.98819923400879 2.015061140060425\n",
            "22755 1.9439723928371677e-06 4.744546095025726e-05 Traning Loss: 4.938943311572075e-05 17.988283157348633 2.015015125274658\n",
            "22756 1.9409615106269484e-06 4.742198143503629e-05 Traning Loss: 4.936294135404751e-05 17.988367080688477 2.0149688720703125\n",
            "22757 1.916173005156452e-06 4.7422297939192504e-05 Traning Loss: 4.933847230859101e-05 17.988449096679688 2.0149238109588623\n",
            "22758 1.924345497172908e-06 4.739374344353564e-05 Traning Loss: 4.931808871333487e-05 17.988529205322266 2.014878034591675\n",
            "22759 1.8904939906860818e-06 4.741397060570307e-05 Traning Loss: 4.9304464482702315e-05 17.98860740661621 2.014833927154541\n",
            "22760 1.910904757096432e-06 4.738878124044277e-05 Traning Loss: 4.9299684178549796e-05 17.988685607910156 2.0147886276245117\n",
            "22761 1.8682496829569573e-06 4.743448880617507e-05 Traning Loss: 4.930274008074775e-05 17.98876190185547 2.0147454738616943\n",
            "22762 1.9035340983464266e-06 4.74211155960802e-05 Traning Loss: 4.932464798912406e-05 17.98883819580078 2.014700412750244\n",
            "22763 1.8512969290895853e-06 4.751672895508818e-05 Traning Loss: 4.936802724841982e-05 17.98891258239746 2.0146584510803223\n",
            "22764 1.907699015646358e-06 4.753923349198885e-05 Traning Loss: 4.9446931370766833e-05 17.988985061645508 2.014613628387451\n",
            "22765 1.845858605520334e-06 4.773600448970683e-05 Traning Loss: 4.9581864004721865e-05 17.989055633544922 2.014572858810425\n",
            "22766 1.936423359438777e-06 4.786154386238195e-05 Traning Loss: 4.979796722182073e-05 17.989126205444336 2.014528274536133\n",
            "22767 1.8685865370571264e-06 4.827296288567595e-05 Traning Loss: 5.014154885429889e-05 17.989194869995117 2.01448917388916\n",
            "22768 2.0208412934152875e-06 4.866208109888248e-05 Traning Loss: 5.068292375653982e-05 17.9892635345459 2.01444411277771\n",
            "22769 1.961066800504341e-06 4.957223427481949e-05 Traning Loss: 5.153330130269751e-05 17.989328384399414 2.01440691947937\n",
            "22770 2.2358385649567936e-06 5.063698336016387e-05 Traning Loss: 5.2872823289362714e-05 17.989395141601562 2.0143609046936035\n",
            "22771 2.232194447060465e-06 5.276391311781481e-05 Traning Loss: 5.499610779224895e-05 17.989456176757812 2.014326333999634\n",
            "22772 2.7714017960533965e-06 5.5586788221262395e-05 Traning Loss: 5.835818956256844e-05 17.989521026611328 2.0142781734466553\n",
            "22773 2.9722425551881315e-06 6.078640217310749e-05 Traning Loss: 6.375864177243784e-05 17.989578247070312 2.0142476558685303\n",
            "22774 4.121511210541939e-06 6.82715472066775e-05 Traning Loss: 7.239305705297738e-05 17.989641189575195 2.0141959190368652\n",
            "22775 4.964882009517169e-06 8.149900531861931e-05 Traning Loss: 8.646388596389443e-05 17.98969078063965 2.014171600341797\n",
            "22776 7.6107717177364975e-06 0.00010156021744478494 Traning Loss: 0.00010917098552454263 17.989749908447266 2.014113187789917\n",
            "22777 1.0366677088313736e-05 0.00013636439689435065 Traning Loss: 0.0001467310794396326 17.98978614807129 2.0140984058380127\n",
            "22778 1.6851996406330727e-05 0.00019079910998698324 Traning Loss: 0.00020765110093634576 17.989837646484375 2.014028549194336\n",
            "22779 2.515657979529351e-05 0.00028454806306399405 Traning Loss: 0.00030970462830737233 17.989849090576172 2.014029026031494\n",
            "22780 4.166102007729933e-05 0.0004325227637309581 Traning Loss: 0.00047418379108421504 17.989877700805664 2.01393985748291\n",
            "22781 6.549245154019445e-05 0.0006852222140878439 Traning Loss: 0.0007507146801799536 17.98982810974121 2.0139641761779785\n",
            "22782 0.00010708759509725496 0.0010743469465523958 Traning Loss: 0.0011814344907179475 17.989791870117188 2.013842821121216\n",
            "22783 0.00017004451365210116 0.0017174187814816833 Traning Loss: 0.001887463266029954 17.98959732055664 2.013901710510254\n",
            "22784 0.0002627331705298275 0.00261043687351048 Traning Loss: 0.002873169956728816 17.989383697509766 2.0137338638305664\n",
            "22785 0.0003941667964681983 0.003917515743523836 Traning Loss: 0.004311682656407356 17.988866806030273 2.013828992843628\n",
            "22786 0.0005232463008724153 0.005200189538300037 Traning Loss: 0.005723435897380114 17.988279342651367 2.013618230819702\n",
            "22787 0.0006437820266000926 0.006342283450067043 Traning Loss: 0.006986065302044153 17.987260818481445 2.0137085914611816\n",
            "22788 0.0006035126280039549 0.006017384119331837 Traning Loss: 0.006620896980166435 17.98623275756836 2.0135278701782227\n",
            "22789 0.00045955064706504345 0.0044676256366074085 Traning Loss: 0.004927176050841808 17.98493003845215 2.013507604598999\n",
            "22790 0.0002203665062552318 0.0021482338197529316 Traning Loss: 0.0023686003405600786 17.98388671875 2.0134928226470947\n",
            "22791 0.00011227549111936241 0.0008813836611807346 Traning Loss: 0.0009936591377481818 17.98284339904785 2.0133025646209717\n",
            "22792 0.00014653577818535268 0.0013061119243502617 Traning Loss: 0.0014526477316394448 17.98203468322754 2.013467788696289\n",
            "22793 0.0002711334964260459 0.002255460247397423 Traning Loss: 0.0025265938602387905 17.981189727783203 2.0132455825805664\n",
            "22794 0.0002535842068027705 0.00236513395793736 Traning Loss: 0.0026187181938439608 17.980314254760742 2.0134122371673584\n",
            "22795 0.0001442756038159132 0.0011552664218470454 Traning Loss: 0.0012995420256629586 17.979461669921875 2.013396978378296\n",
            "22796 4.578591324388981e-05 0.0002250111720059067 Traning Loss: 0.0002707970852497965 17.97857666015625 2.0134057998657227\n",
            "22797 7.542764797108248e-05 0.0005682322080247104 Traning Loss: 0.0006436598487198353 17.977783203125 2.013641119003296\n",
            "22798 0.00019243248971179128 0.001420287531800568 Traning Loss: 0.0016127200797200203 17.976947784423828 2.0135536193847656\n",
            "22799 0.00017608757480047643 0.0015820530243217945 Traning Loss: 0.0017581406282261014 17.97621726989746 2.0138180255889893\n",
            "22800 0.00011224961053812876 0.0007304451428353786 Traning Loss: 0.0008426947752013803 17.975479125976562 2.013840913772583\n",
            "22801 3.256405398133211e-05 0.00013727604527957737 Traning Loss: 0.0001698401028988883 17.97484588623047 2.013951063156128\n",
            "22802 5.377220441005193e-05 0.00037294052890501916 Traning Loss: 0.0004267127369530499 17.97421646118164 2.0141761302948\n",
            "22803 0.00011243987682973966 0.000820726272650063 Traning Loss: 0.0009331661276519299 17.973575592041016 2.0142130851745605\n",
            "22804 8.862426329869777e-05 0.000777481880504638 Traning Loss: 0.0008661061292514205 17.97296905517578 2.014478921890259\n",
            "22805 5.4015108617022634e-05 0.00031069203396327794 Traning Loss: 0.00036470714258030057 17.97239112854004 2.0146000385284424\n",
            "22806 2.7007677999790758e-05 0.00016557348135393113 Traning Loss: 0.00019258115207776427 17.971935272216797 2.0147595405578613\n",
            "22807 5.1185383199481294e-05 0.0004356092831585556 Traning Loss: 0.0004867946554441005 17.97146987915039 2.0149593353271484\n",
            "22808 7.563250983366743e-05 0.000622564519289881 Traning Loss: 0.0006981970509514213 17.971099853515625 2.015047073364258\n",
            "22809 4.9682523240335286e-05 0.00044372028787620366 Traning Loss: 0.0004934028256684542 17.970718383789062 2.0152130126953125\n",
            "22810 1.992796387639828e-05 0.00012448425695765764 Traning Loss: 0.0001444122171960771 17.97039031982422 2.0153234004974365\n",
            "22811 1.3399017007031944e-05 7.587745494674891e-05 Traning Loss: 8.927647286327556e-05 17.97011375427246 2.015383720397949\n",
            "22812 2.8167825803393498e-05 0.0002758001210168004 Traning Loss: 0.0003039679431822151 17.969865798950195 2.0155091285705566\n",
            "22813 4.443353827809915e-05 0.00038686871994286776 Traning Loss: 0.0004313022654969245 17.969684600830078 2.015517234802246\n",
            "22814 2.8224410925759003e-05 0.00028908930835314095 Traning Loss: 0.00031731370836496353 17.969505310058594 2.015561819076538\n",
            "22815 1.273425186809618e-05 0.00013103682431392372 Traning Loss: 0.0001437710743630305 17.96941375732422 2.0155959129333496\n",
            "22816 1.5691959561081603e-05 0.0001192831332446076 Traning Loss: 0.000134975096443668 17.969314575195312 2.015543222427368\n",
            "22817 2.093086004606448e-05 0.00023249386867973953 Traning Loss: 0.0002534247178118676 17.96925926208496 2.015596389770508\n",
            "22818 3.2257976272376254e-05 0.00026797453756444156 Traning Loss: 0.00030023252475075424 17.969215393066406 2.0155107975006104\n",
            "22819 1.6142097592819482e-05 0.00019013890414498746 Traning Loss: 0.00020628099446184933 17.969228744506836 2.0155112743377686\n",
            "22820 8.246939614764415e-06 7.684298179810867e-05 Traning Loss: 8.508992323186249e-05 17.969274520874023 2.0154480934143066\n",
            "22821 5.532326213142369e-06 6.325277354335412e-05 Traning Loss: 6.878509884700179e-05 17.96937370300293 2.0153610706329346\n",
            "22822 1.042423627950484e-05 0.0001303961907979101 Traning Loss: 0.00014082042616792023 17.96950340270996 2.015331506729126\n",
            "22823 1.9269347831141204e-05 0.0001672208891250193 Traning Loss: 0.00018649024423211813 17.969636917114258 2.01521372795105\n",
            "22824 1.1773576261475682e-05 0.0001393128914060071 Traning Loss: 0.0001510864676674828 17.969785690307617 2.0151755809783936\n",
            "22825 9.334959031548351e-06 7.826436922186986e-05 Traning Loss: 8.759932825341821e-05 17.969928741455078 2.0150914192199707\n",
            "22826 5.869905635336181e-06 6.908394425408915e-05 Traning Loss: 7.495385216316208e-05 17.970102310180664 2.0150249004364014\n",
            "22827 9.596992640581448e-06 0.00010615352221066132 Traning Loss: 0.00011575051757972687 17.970264434814453 2.014979124069214\n",
            "22828 1.3494373888534028e-05 0.00013384863268584013 Traning Loss: 0.00014734300202690065 17.970474243164062 2.014906883239746\n",
            "22829 1.0650479453033768e-05 0.00011852628813358024 Traning Loss: 0.0001291767694056034 17.970680236816406 2.014866352081299\n",
            "22830 7.166548130044248e-06 7.734657992841676e-05 Traning Loss: 8.451312896795571e-05 17.970918655395508 2.0148227214813232\n",
            "22831 6.004456736263819e-06 5.717996828025207e-05 Traning Loss: 6.318442319752648e-05 17.971155166625977 2.014768362045288\n",
            "22832 5.734349088015733e-06 7.213142816908658e-05 Traning Loss: 7.786577771184966e-05 17.97141456604004 2.014766216278076\n",
            "22833 9.929058251145761e-06 8.752099529374391e-05 Traning Loss: 9.745005081640556e-05 17.971670150756836 2.014718532562256\n",
            "22834 6.866773674119031e-06 8.567480108467862e-05 Traning Loss: 9.25415733945556e-05 17.97193717956543 2.014725685119629\n",
            "22835 6.4164983086811844e-06 6.219198257895187e-05 Traning Loss: 6.86084822518751e-05 17.972213745117188 2.014711618423462\n",
            "22836 4.254780833434779e-06 4.933330274070613e-05 Traning Loss: 5.358808266464621e-05 17.972492218017578 2.0147061347961426\n",
            "22837 4.420764980750391e-06 5.634194894810207e-05 Traning Loss: 6.0762715293094516e-05 17.972774505615234 2.0147297382354736\n",
            "22838 7.513582204410341e-06 6.884531467221677e-05 Traning Loss: 7.635889778612182e-05 17.973051071166992 2.014721632003784\n",
            "22839 5.895532467548037e-06 7.439606270054355e-05 Traning Loss: 8.029159653233364e-05 17.97333335876465 2.0147616863250732\n",
            "22840 6.718368240399286e-06 6.298900552792475e-05 Traning Loss: 6.970737013034523e-05 17.973604202270508 2.01476788520813\n",
            "22841 4.091956270713126e-06 5.482444612425752e-05 Traning Loss: 5.8916401030728593e-05 17.973894119262695 2.014799118041992\n",
            "22842 4.788029855262721e-06 5.461530963657424e-05 Traning Loss: 5.940334085607901e-05 17.97417640686035 2.0148274898529053\n",
            "22843 5.3058315643284e-06 6.2382168835029e-05 Traning Loss: 6.768800085410476e-05 17.97447395324707 2.0148496627807617\n",
            "22844 5.497985966940178e-06 6.664736429229379e-05 Traning Loss: 7.214534707600251e-05 17.974760055541992 2.0148861408233643\n",
            "22845 5.283980499370955e-06 6.212822336237878e-05 Traning Loss: 6.741220568073913e-05 17.97505760192871 2.0149152278900146\n",
            "22846 4.523724328464596e-06 5.460960164782591e-05 Traning Loss: 5.913332643103786e-05 17.975339889526367 2.0149450302124023\n",
            "22847 3.7169509141676826e-06 5.1808256102958694e-05 Traning Loss: 5.5525208153994754e-05 17.975627899169922 2.0149879455566406\n",
            "22848 4.7791436372790486e-06 5.3468636906472966e-05 Traning Loss: 5.8247780543752015e-05 17.975906372070312 2.0150082111358643\n",
            "22849 3.93284926758497e-06 5.7977460528491065e-05 Traning Loss: 6.191030843183398e-05 17.9761905670166 2.0150537490844727\n",
            "22850 4.990002707927488e-06 5.625328776659444e-05 Traning Loss: 6.124329229351133e-05 17.97646713256836 2.015069007873535\n",
            "22851 3.5072507671429776e-06 5.3210125770419836e-05 Traning Loss: 5.6717377447057515e-05 17.976749420166016 2.0151021480560303\n",
            "22852 3.7383451854111627e-06 4.9057212891057134e-05 Traning Loss: 5.279555625747889e-05 17.977020263671875 2.015122175216675\n",
            "22853 3.3988421819231007e-06 4.919456841889769e-05 Traning Loss: 5.259341196506284e-05 17.977291107177734 2.015139579772949\n",
            "22854 3.463356506472337e-06 5.1425766287138686e-05 Traning Loss: 5.4889122111489996e-05 17.977554321289062 2.015162229537964\n",
            "22855 3.7985839753673645e-06 5.246275395620614e-05 Traning Loss: 5.6261338613694534e-05 17.97781753540039 2.0151703357696533\n",
            "22856 3.257064690842526e-06 5.174743273528293e-05 Traning Loss: 5.50044969713781e-05 17.97807502746582 2.0151853561401367\n",
            "22857 3.199804723408306e-06 4.922008156427182e-05 Traning Loss: 5.241988765192218e-05 17.978334426879883 2.0151920318603516\n",
            "22858 2.97483484246186e-06 4.797556175617501e-05 Traning Loss: 5.095039523439482e-05 17.978591918945312 2.01519513130188\n",
            "22859 2.8519018542283447e-06 4.8650654207449406e-05 Traning Loss: 5.150255674379878e-05 17.97884750366211 2.015202045440674\n",
            "22860 3.2789516808406916e-06 4.958616409567185e-05 Traning Loss: 5.2865114412270486e-05 17.97909927368164 2.01519513130188\n",
            "22861 2.912219770223601e-06 5.0402966735418886e-05 Traning Loss: 5.331518696038984e-05 17.979351043701172 2.015197515487671\n",
            "22862 3.146059725622763e-06 4.921866275253706e-05 Traning Loss: 5.236472134129144e-05 17.979598999023438 2.0151875019073486\n",
            "22863 2.7450305424281396e-06 4.824477946385741e-05 Traning Loss: 5.098980909679085e-05 17.979843139648438 2.0151803493499756\n",
            "22864 2.7598973701969953e-06 4.7625431761844084e-05 Traning Loss: 5.0385329814162105e-05 17.980085372924805 2.0151710510253906\n",
            "22865 2.871768629120197e-06 4.792721301782876e-05 Traning Loss: 5.079898255644366e-05 17.980323791503906 2.0151548385620117\n",
            "22866 2.72912552645721e-06 4.8774007154861465e-05 Traning Loss: 5.15031315444503e-05 17.98055648803711 2.0151445865631104\n",
            "22867 3.0029736990400124e-06 4.864759466727264e-05 Traning Loss: 5.1650567911565304e-05 17.980785369873047 2.015124797821045\n",
            "22868 2.6764212179841707e-06 4.8405167035525665e-05 Traning Loss: 5.108158802613616e-05 17.981008529663086 2.015111207962036\n",
            "22869 2.767224259514478e-06 4.7562432882841676e-05 Traning Loss: 5.032965782447718e-05 17.981225967407227 2.015091896057129\n",
            "22870 2.6221448479191167e-06 4.737327981274575e-05 Traning Loss: 4.9995425797533244e-05 17.981441497802734 2.0150721073150635\n",
            "22871 2.601002051960677e-06 4.7592933697160333e-05 Traning Loss: 5.019393574912101e-05 17.98165512084961 2.0150535106658936\n",
            "22872 2.708413603613735e-06 4.7847690439084545e-05 Traning Loss: 5.0556103815324605e-05 17.98186683654785 2.0150294303894043\n",
            "22873 2.586990831332514e-06 4.8065157898236066e-05 Traning Loss: 5.065215009381063e-05 17.982074737548828 2.0150094032287598\n",
            "22874 2.655485786817735e-06 4.772134707309306e-05 Traning Loss: 5.037683149566874e-05 17.982280731201172 2.0149853229522705\n",
            "22875 2.5643103072070517e-06 4.741853990708478e-05 Traning Loss: 4.998284930479713e-05 17.982479095458984 2.0149614810943604\n",
            "22876 2.485245431671501e-06 4.730036380351521e-05 Traning Loss: 4.9785609007813036e-05 17.982677459716797 2.014939308166504\n",
            "22877 2.622170086397091e-06 4.727771010948345e-05 Traning Loss: 4.9899881560122594e-05 17.98287010192871 2.014911651611328\n",
            "22878 2.4086248231469654e-06 4.777220237883739e-05 Traning Loss: 5.018082811147906e-05 17.983060836791992 2.0148909091949463\n",
            "22879 2.713915364438435e-06 4.770866144099273e-05 Traning Loss: 5.042257544118911e-05 17.983245849609375 2.0148606300354004\n",
            "22880 2.3929662802402163e-06 4.8162328312173486e-05 Traning Loss: 5.055529618402943e-05 17.983430862426758 2.0148394107818604\n",
            "22881 2.7414298529038206e-06 4.79765294585377e-05 Traning Loss: 5.0717957492452115e-05 17.983606338500977 2.014808177947998\n",
            "22882 2.422198804197251e-06 4.87263678223826e-05 Traning Loss: 5.1148566853953525e-05 17.98378562927246 2.0147860050201416\n",
            "22883 2.85547230305383e-06 4.920861465507187e-05 Traning Loss: 5.2064086048631e-05 17.98395347595215 2.0147545337677\n",
            "22884 2.5926519811036997e-06 5.1035134674748406e-05 Traning Loss: 5.362778756534681e-05 17.984128952026367 2.014732837677002\n",
            "22885 3.281497811258305e-06 5.2783856517635286e-05 Traning Loss: 5.606535341939889e-05 17.98428726196289 2.0146985054016113\n",
            "22886 3.0512912871927256e-06 5.676712316926569e-05 Traning Loss: 5.981841604807414e-05 17.984460830688477 2.0146806240081787\n",
            "22887 4.395848918647971e-06 6.139957258710638e-05 Traning Loss: 6.579542241524905e-05 17.984607696533203 2.014641046524048\n",
            "22888 4.357181751402095e-06 7.113684841897339e-05 Traning Loss: 7.549402653239667e-05 17.984779357910156 2.0146312713623047\n",
            "22889 7.235014436446363e-06 8.424035331699997e-05 Traning Loss: 9.147536911768839e-05 17.984909057617188 2.014582633972168\n",
            "22890 8.163175152731128e-06 0.0001095226762117818 Traning Loss: 0.00011768584954552352 17.985078811645508 2.014585256576538\n",
            "22891 1.4607166122004855e-05 0.00014654452388640493 Traning Loss: 0.0001611516927368939 17.985183715820312 2.0145223140716553\n",
            "22892 1.8987142539117485e-05 0.0002137698174919933 Traning Loss: 0.00023275695275515318 17.985353469848633 2.0145442485809326\n",
            "22893 3.4335782402195036e-05 0.00031832250533625484 Traning Loss: 0.00035265827318653464 17.98541259765625 2.0144598484039307\n",
            "22894 4.9481830501463264e-05 0.0005007415893487632 Traning Loss: 0.0005502233980223536 17.985572814941406 2.0145134925842285\n",
            "22895 8.796024485491216e-05 0.0007934304303489625 Traning Loss: 0.0008813906461000443 17.985538482666016 2.014394760131836\n",
            "22896 0.0001333323452854529 0.0012806709855794907 Traning Loss: 0.0014140033163130283 17.98565101623535 2.0145015716552734\n",
            "22897 0.00022865255596116185 0.0020508517045527697 Traning Loss: 0.0022795042023062706 17.985416412353516 2.0143325328826904\n",
            "22898 0.00034177827183157206 0.0032080740202218294 Traning Loss: 0.0035498524084687233 17.985368728637695 2.0145232677459717\n",
            "22899 0.0005383294774219394 0.004825807176530361 Traning Loss: 0.005364136770367622 17.984731674194336 2.014300584793091\n",
            "22900 0.0007089738501235843 0.006604756228625774 Traning Loss: 0.0073137301951646805 17.984294891357422 2.014592170715332\n",
            "22901 0.0008986435132101178 0.008029797114431858 Traning Loss: 0.008928440511226654 17.983095169067383 2.014378309249878\n",
            "22902 0.0008273121202364564 0.007738909684121609 Traning Loss: 0.00856622215360403 17.982154846191406 2.0147109031677246\n",
            "22903 0.0006222220254130661 0.005438397638499737 Traning Loss: 0.006060619838535786 17.980751037597656 2.0146713256835938\n",
            "22904 0.0002258581225760281 0.0021053734235465527 Traning Loss: 0.00233123148791492 17.97970962524414 2.014902353286743\n",
            "22905 3.811320129898377e-05 0.00016567921556998044 Traning Loss: 0.0002037924132309854 17.97879409790039 2.0151526927948\n",
            "22906 0.0001232236681971699 0.0007595869246870279 Traning Loss: 0.0008828105637803674 17.97785186767578 2.015252113342285\n",
            "22907 0.000284785870462656 0.002579871565103531 Traning Loss: 0.002864657435566187 17.97716522216797 2.015695571899414\n",
            "22908 0.00041130921454168856 0.0032377501484006643 Traning Loss: 0.0036490594502538443 17.9760799407959 2.0158658027648926\n",
            "22909 0.0002370053407503292 0.002060264116153121 Traning Loss: 0.0022972694132477045 17.97517967224121 2.0162863731384277\n",
            "22910 9.120369213633239e-05 0.0004684268787968904 Traning Loss: 0.0005596305709332228 17.974224090576172 2.01667857170105\n",
            "22911 7.616771472385153e-05 0.000225052164751105 Traning Loss: 0.00030121987219899893 17.97325325012207 2.0169601440429688\n",
            "22912 0.00015537215222138911 0.0011924674035981297 Traning Loss: 0.0013478395994752645 17.97243881225586 2.0174617767333984\n",
            "22913 0.000249334960244596 0.0017687209183350205 Traning Loss: 0.0020180558785796165 17.97137451171875 2.017756700515747\n",
            "22914 0.00016283831791952252 0.00124472682364285 Traning Loss: 0.0014075650833547115 17.97047996520996 2.0182108879089355\n",
            "22915 7.57275556679815e-05 0.00036326830741018057 Traning Loss: 0.0004389958630781621 17.969575881958008 2.018653392791748\n",
            "22916 6.606768147321418e-05 0.000200703289010562 Traning Loss: 0.0002667709777597338 17.968734741210938 2.0189707279205322\n",
            "22917 0.00010479710181243718 0.0007149562006816268 Traning Loss: 0.0008197532733902335 17.968111038208008 2.019446849822998\n",
            "22918 0.000147389859193936 0.0010011940030381083 Traning Loss: 0.0011485838331282139 17.967390060424805 2.0197532176971436\n",
            "22919 0.00010053339065052569 0.0007087645935826004 Traning Loss: 0.0008092980133369565 17.96685028076172 2.0201363563537598\n",
            "22920 5.146023249835707e-05 0.00027159214369021356 Traning Loss: 0.0003230523725505918 17.966354370117188 2.020529270172119\n",
            "22921 5.0660753913689405e-05 0.0001986898569157347 Traning Loss: 0.00024935061810538173 17.965906143188477 2.0207810401916504\n",
            "22922 6.375517841661349e-05 0.0004438863543327898 Traning Loss: 0.0005076415254734457 17.965606689453125 2.021141529083252\n",
            "22923 8.354565215995535e-05 0.0005626626661978662 Traning Loss: 0.0006462082965299487 17.965232849121094 2.021361827850342\n",
            "22924 5.799802966066636e-05 0.00043428014032542706 Traning Loss: 0.0004922781954519451 17.96497344970703 2.021608591079712\n",
            "22925 3.4703523851931095e-05 0.00024401429982390255 Traning Loss: 0.00027871783822774887 17.964712142944336 2.0218663215637207\n",
            "22926 3.667727287393063e-05 0.00018796016229316592 Traning Loss: 0.00022463743516709656 17.964475631713867 2.021988868713379\n",
            "22927 3.22053674608469e-05 0.00026754807913675904 Traning Loss: 0.00029975344659760594 17.96434211730957 2.022196054458618\n",
            "22928 4.49091057816986e-05 0.0002998251002281904 Traning Loss: 0.0003447342023719102 17.96415901184082 2.0222904682159424\n",
            "22929 3.185428795404732e-05 0.00027329009026288986 Traning Loss: 0.0003051443782169372 17.964061737060547 2.022399663925171\n",
            "22930 2.4927943741204217e-05 0.00022045569494366646 Traning Loss: 0.0002453836495988071 17.963970184326172 2.0225307941436768\n",
            "22931 2.780592694762163e-05 0.00018584774807095528 Traning Loss: 0.0002136536786565557 17.963926315307617 2.0225532054901123\n",
            "22932 1.7710124666336924e-05 0.0001825034705689177 Traning Loss: 0.00020021360251121223 17.963960647583008 2.0226492881774902\n",
            "22933 2.3829112251405604e-05 0.00015713459288235754 Traning Loss: 0.00018096370331477374 17.963991165161133 2.022655963897705\n",
            "22934 1.6019070244510658e-05 0.0001548596628708765 Traning Loss: 0.00017087873129639775 17.964099884033203 2.0226826667785645\n",
            "22935 1.7618865967961028e-05 0.00016580156807322055 Traning Loss: 0.00018342043040320277 17.964202880859375 2.022728443145752\n",
            "22936 2.2296600945992395e-05 0.00017116333765443414 Traning Loss: 0.00019345994223840535 17.964340209960938 2.022690534591675\n",
            "22937 1.4576151443179697e-05 0.00015548319788649678 Traning Loss: 0.0001700593566056341 17.96451759338379 2.022719144821167\n",
            "22938 1.568234984006267e-05 0.00010390365787316114 Traning Loss: 0.00011958600953221321 17.964689254760742 2.0226714611053467\n",
            "22939 8.399445505347103e-06 8.286824595415965e-05 Traning Loss: 9.126769145950675e-05 17.96489906311035 2.022648572921753\n",
            "22940 1.099383007385768e-05 9.934832633007318e-05 Traning Loss: 0.00011034215276595205 17.96510124206543 2.0226376056671143\n",
            "22941 1.620923649170436e-05 0.00013005359505768865 Traning Loss: 0.0001462628279114142 17.965328216552734 2.0225672721862793\n",
            "22942 1.3307796507433522e-05 0.0001366254291497171 Traning Loss: 0.00014993322838563472 17.9655704498291 2.0225517749786377\n",
            "22943 1.31751257868018e-05 9.664751996751875e-05 Traning Loss: 0.00010982264939229935 17.965818405151367 2.0224719047546387\n",
            "22944 6.2085978242976125e-06 6.173948349896818e-05 Traning Loss: 6.794807995902374e-05 17.966093063354492 2.022423267364502\n",
            "22945 6.661405222985195e-06 5.766655158367939e-05 Traning Loss: 6.432795635191724e-05 17.96636962890625 2.022376775741577\n",
            "22946 1.0159071280213539e-05 8.370565774384886e-05 Traning Loss: 9.38647281145677e-05 17.966665267944336 2.0222909450531006\n",
            "22947 1.0336172636016272e-05 0.00010631049372022972 Traning Loss: 0.00011664666817523539 17.966978073120117 2.0222482681274414\n",
            "22948 1.1650255146378186e-05 9.316114301327616e-05 Traning Loss: 0.00010481139906914905 17.96729850769043 2.022151470184326\n",
            "22949 6.535773991345195e-06 6.633572047576308e-05 Traning Loss: 7.287149492185563e-05 17.967626571655273 2.022085666656494\n",
            "22950 5.581962341238977e-06 4.817852095584385e-05 Traning Loss: 5.376048284233548e-05 17.967958450317383 2.022014617919922\n",
            "22951 6.740186563547468e-06 5.53903155378066e-05 Traning Loss: 6.213050073711202e-05 17.968294143676758 2.0219221115112305\n",
            "22952 7.24990513845114e-06 7.472594734281301e-05 Traning Loss: 8.197584975278005e-05 17.9686336517334 2.0218605995178223\n",
            "22953 9.649327694205567e-06 7.901791832409799e-05 Traning Loss: 8.866724965628237e-05 17.968975067138672 2.021756410598755\n",
            "22954 6.729829692631029e-06 6.985015352256596e-05 Traning Loss: 7.657998503418639e-05 17.969316482543945 2.02168345451355\n",
            "22955 5.970767688268097e-06 5.3858708270126954e-05 Traning Loss: 5.98294755036477e-05 17.969661712646484 2.0216000080108643\n",
            "22956 5.5043828979250975e-06 4.8745012463768944e-05 Traning Loss: 5.424939445219934e-05 17.970001220703125 2.0215091705322266\n",
            "22957 5.280162895360263e-06 5.576904732151888e-05 Traning Loss: 6.10492134001106e-05 17.97035026550293 2.021440029144287\n",
            "22958 7.308974090847187e-06 6.190565909491852e-05 Traning Loss: 6.921463500475511e-05 17.9706974029541 2.021340847015381\n",
            "22959 5.955486358288908e-06 6.370107439579442e-05 Traning Loss: 6.965656211832538e-05 17.971040725708008 2.021270275115967\n",
            "22960 6.081212177377893e-06 5.706925367121585e-05 Traning Loss: 6.315046630334109e-05 17.971385955810547 2.0211873054504395\n",
            "22961 5.303845227899728e-06 5.159977081348188e-05 Traning Loss: 5.690361649612896e-05 17.971721649169922 2.0211071968078613\n",
            "22962 4.725699000118766e-06 5.089424303150736e-05 Traning Loss: 5.561994112213142e-05 17.972063064575195 2.02103853225708\n",
            "22963 5.7915403885999694e-06 5.2097224397584796e-05 Traning Loss: 5.788876296719536e-05 17.972393035888672 2.0209498405456543\n",
            "22964 4.889910542260623e-06 5.4781357903266326e-05 Traning Loss: 5.9671267081284896e-05 17.972721099853516 2.0208845138549805\n",
            "22965 5.478274943016004e-06 5.353546657715924e-05 Traning Loss: 5.9013742429669946e-05 17.97304344177246 2.0208048820495605\n",
            "22966 4.880610049440293e-06 5.223036714596674e-05 Traning Loss: 5.711097765015438e-05 17.97335433959961 2.020732879638672\n",
            "22967 4.560819434118457e-06 5.125850293552503e-05 Traning Loss: 5.5819320550654083e-05 17.973670959472656 2.0206634998321533\n",
            "22968 4.963988885720028e-06 5.0498900236561894e-05 Traning Loss: 5.546288957702927e-05 17.973974227905273 2.0205814838409424\n",
            "22969 4.135383733228082e-06 5.10267200297676e-05 Traning Loss: 5.516210512723774e-05 17.97428321838379 2.020514965057373\n",
            "22970 4.625117981049698e-06 4.978436481906101e-05 Traning Loss: 5.440948370960541e-05 17.974584579467773 2.020435094833374\n",
            "22971 4.027502654935233e-06 4.967797940480523e-05 Traning Loss: 5.370548024075106e-05 17.974884033203125 2.020364284515381\n",
            "22972 4.082145551365102e-06 4.9641432269709185e-05 Traning Loss: 5.372357918531634e-05 17.97518539428711 2.020289421081543\n",
            "22973 4.285623163013952e-06 4.998968870495446e-05 Traning Loss: 5.427531141322106e-05 17.975479125976562 2.0202083587646484\n",
            "22974 3.8019620660634246e-06 5.0648395699681714e-05 Traning Loss: 5.4450356401503086e-05 17.975778579711914 2.0201358795166016\n",
            "22975 4.217252808302874e-06 4.944724787492305e-05 Traning Loss: 5.366450204746798e-05 17.9760684967041 2.020051956176758\n",
            "22976 3.557420313882176e-06 4.8758720367914066e-05 Traning Loss: 5.231613977230154e-05 17.97635841369629 2.0199761390686035\n",
            "22977 3.741227374121081e-06 4.7684683522675186e-05 Traning Loss: 5.142591180629097e-05 17.97664451599121 2.019892930984497\n",
            "22978 3.6523765629681293e-06 4.7926521801855415e-05 Traning Loss: 5.15788997290656e-05 17.976924896240234 2.019808053970337\n",
            "22979 3.512516741466243e-06 4.888631519861519e-05 Traning Loss: 5.2398831030586734e-05 17.977205276489258 2.019726276397705\n",
            "22980 3.869399279210484e-06 4.908694245386869e-05 Traning Loss: 5.295634036883712e-05 17.977474212646484 2.019636869430542\n",
            "22981 3.3880332921398804e-06 4.9253038014285266e-05 Traning Loss: 5.264107312541455e-05 17.977745056152344 2.019554615020752\n",
            "22982 3.6159769933874486e-06 4.8051449994090945e-05 Traning Loss: 5.166742630535737e-05 17.978008270263672 2.0194647312164307\n",
            "22983 3.2552197808399796e-06 4.751533197122626e-05 Traning Loss: 5.077055175206624e-05 17.978271484375 2.0193769931793213\n",
            "22984 3.246612550356076e-06 4.727672421722673e-05 Traning Loss: 5.052333654020913e-05 17.978532791137695 2.019289493560791\n",
            "22985 3.360756409165333e-06 4.753345638164319e-05 Traning Loss: 5.08942139276769e-05 17.97878646850586 2.0191988945007324\n",
            "22986 3.1513613976130728e-06 4.82245086459443e-05 Traning Loss: 5.137587140779942e-05 17.979042053222656 2.0191142559051514\n",
            "22987 3.4289257655473193e-06 4.8058591346489266e-05 Traning Loss: 5.148751733941026e-05 17.979290008544922 2.019023895263672\n",
            "22988 3.114060973530286e-06 4.802178227691911e-05 Traning Loss: 5.1135844842065126e-05 17.97953987121582 2.0189380645751953\n",
            "22989 3.2210459721682128e-06 4.7382382035721093e-05 Traning Loss: 5.0603426643647254e-05 17.979782104492188 2.018850564956665\n",
            "22990 3.105959422100568e-06 4.7133675252553076e-05 Traning Loss: 5.0239636038895696e-05 17.98002052307129 2.018763780593872\n",
            "22991 3.03737101603474e-06 4.7143970732577145e-05 Traning Loss: 5.018134106649086e-05 17.980255126953125 2.0186808109283447\n",
            "22992 3.181210786351585e-06 4.7129673475865275e-05 Traning Loss: 5.031088585383259e-05 17.980484008789062 2.018594264984131\n",
            "22993 2.9892194106651004e-06 4.742849705507979e-05 Traning Loss: 5.0417715101502836e-05 17.980710983276367 2.018512725830078\n",
            "22994 3.1470178782910807e-06 4.723071833723225e-05 Traning Loss: 5.0377737352391705e-05 17.98093032836914 2.0184285640716553\n",
            "22995 2.966723968711449e-06 4.72511128464248e-05 Traning Loss: 5.02178372698836e-05 17.98114776611328 2.01834774017334\n",
            "22996 2.9936040846223477e-06 4.704859384219162e-05 Traning Loss: 5.004219929105602e-05 17.98135757446289 2.01826810836792\n",
            "22997 2.968980197692872e-06 4.6955898142186925e-05 Traning Loss: 4.992487811250612e-05 17.981565475463867 2.0181875228881836\n",
            "22998 2.8642100460274378e-06 4.700207136920653e-05 Traning Loss: 4.9866281187860295e-05 17.981769561767578 2.018110752105713\n",
            "22999 2.9515408641600516e-06 4.6869772631907836e-05 Traning Loss: 4.9821312131825835e-05 17.981969833374023 2.018031358718872\n",
            "23000 2.796343551381142e-06 4.696179530583322e-05 Traning Loss: 4.975813862984069e-05 17.982168197631836 2.017956495285034\n",
            "23001 2.8733848012052476e-06 4.680856363847852e-05 Traning Loss: 4.9681948439683765e-05 17.98236083984375 2.01788067817688\n",
            "23002 2.781478087854339e-06 4.683872975874692e-05 Traning Loss: 4.962020830134861e-05 17.982553482055664 2.0178062915802\n",
            "23003 2.774251242954051e-06 4.6813132939860225e-05 Traning Loss: 4.958738281857222e-05 17.982742309570312 2.017733335494995\n",
            "23004 2.7828054953715764e-06 4.678219556808472e-05 Traning Loss: 4.9565001972950995e-05 17.982927322387695 2.0176591873168945\n",
            "23005 2.692975613172166e-06 4.68308680865448e-05 Traning Loss: 4.952384188072756e-05 17.983108520507812 2.0175886154174805\n",
            "23006 2.7497135306475684e-06 4.669752888730727e-05 Traning Loss: 4.9447240598965436e-05 17.983285903930664 2.0175163745880127\n",
            "23007 2.641206265252549e-06 4.670668567996472e-05 Traning Loss: 4.934789103572257e-05 17.983461380004883 2.017446994781494\n",
            "23008 2.6715483727457467e-06 4.6585118980146945e-05 Traning Loss: 4.9256668717134744e-05 17.983633041381836 2.017376661300659\n",
            "23009 2.612246817079722e-06 4.6585832023993134e-05 Traning Loss: 4.919807906844653e-05 17.983800888061523 2.0173075199127197\n",
            "23010 2.5848339646472596e-06 4.659214027924463e-05 Traning Loss: 4.917697515338659e-05 17.983964920043945 2.017239809036255\n",
            "23011 2.5910544536600355e-06 4.6578163164667785e-05 Traning Loss: 4.916921898256987e-05 17.984127044677734 2.0171713829040527\n",
            "23012 2.5185688627971103e-06 4.663250729208812e-05 Traning Loss: 4.915107638225891e-05 17.984285354614258 2.0171055793762207\n",
            "23013 2.5485394417046336e-06 4.6559373004129156e-05 Traning Loss: 4.9107911763712764e-05 17.98444175720215 2.017038345336914\n",
            "23014 2.4690889404155314e-06 4.6569995902245864e-05 Traning Loss: 4.9039084842661396e-05 17.984596252441406 2.0169734954833984\n",
            "23015 2.4781411411822774e-06 4.6487311919918284e-05 Traning Loss: 4.896545215160586e-05 17.9847469329834 2.0169084072113037\n",
            "23016 2.4351875254069455e-06 4.646500019589439e-05 Traning Loss: 4.8900186811806634e-05 17.984895706176758 2.0168442726135254\n",
            "23017 2.408241925877519e-06 4.6447195927612484e-05 Traning Loss: 4.88554360345006e-05 17.985042572021484 2.0167810916900635\n",
            "23018 2.4085181848931825e-06 4.641581836040132e-05 Traning Loss: 4.882433495367877e-05 17.985185623168945 2.0167174339294434\n",
            "23019 2.3542361304862425e-06 4.644404543796554e-05 Traning Loss: 4.879828338744119e-05 17.985326766967773 2.016655683517456\n",
            "23020 2.3716763735137647e-06 4.6395518438657746e-05 Traning Loss: 4.876719322055578e-05 17.98546600341797 2.0165932178497314\n",
            "23021 2.3152524590841495e-06 4.641336272470653e-05 Traning Loss: 4.872861609328538e-05 17.9856014251709 2.0165326595306396\n",
            "23022 2.3203660930448677e-06 4.636156518245116e-05 Traning Loss: 4.8681929911253974e-05 17.985734939575195 2.0164716243743896\n",
            "23023 2.285010396008147e-06 4.635264485841617e-05 Traning Loss: 4.863765570917167e-05 17.98586654663086 2.016411542892456\n",
            "23024 2.263573151140008e-06 4.63325523014646e-05 Traning Loss: 4.859612454310991e-05 17.98599624633789 2.0163519382476807\n",
            "23025 2.255185791000258e-06 4.630039256880991e-05 Traning Loss: 4.855557926930487e-05 17.986122131347656 2.0162925720214844\n",
            "23026 2.2140995952213416e-06 4.630369585356675e-05 Traning Loss: 4.851779522141442e-05 17.986248016357422 2.0162343978881836\n",
            "23027 2.220569058408728e-06 4.6259032387752086e-05 Traning Loss: 4.847960008191876e-05 17.986370086669922 2.0161757469177246\n",
            "23028 2.1760324671049602e-06 4.626308145816438e-05 Traning Loss: 4.843911301577464e-05 17.98649024963379 2.0161185264587402\n",
            "23029 2.17894603338209e-06 4.622204141924158e-05 Traning Loss: 4.8400986997876316e-05 17.986608505249023 2.0160610675811768\n",
            "23030 2.1466285033966415e-06 4.62167190562468e-05 Traning Loss: 4.8363348469138145e-05 17.986724853515625 2.0160043239593506\n",
            "23031 2.1353012016334105e-06 4.619244282366708e-05 Traning Loss: 4.832774357055314e-05 17.986839294433594 2.0159480571746826\n",
            "23032 2.1220589587755967e-06 4.617189188138582e-05 Traning Loss: 4.8293950385414064e-05 17.98695182800293 2.0158920288085938\n",
            "23033 2.0960071651643375e-06 4.61655217804946e-05 Traning Loss: 4.826152871828526e-05 17.987062454223633 2.015836715698242\n",
            "23034 2.095396439472097e-06 4.613215787685476e-05 Traning Loss: 4.822755363420583e-05 17.987171173095703 2.0157814025878906\n",
            "23035 2.062554813164752e-06 4.6129553084028885e-05 Traning Loss: 4.819210880668834e-05 17.98727798461914 2.0157270431518555\n",
            "23036 2.0642212348320754e-06 4.6092136471997947e-05 Traning Loss: 4.815635838895105e-05 17.987382888793945 2.015672445297241\n",
            "23037 2.0347254121588776e-06 4.608400195138529e-05 Traning Loss: 4.811872713617049e-05 17.98748779296875 2.0156188011169434\n",
            "23038 2.029683400905924e-06 4.605156209436245e-05 Traning Loss: 4.8081245040521026e-05 17.987590789794922 2.0155651569366455\n",
            "23039 2.0104446321056457e-06 4.603468187269755e-05 Traning Loss: 4.804512718692422e-05 17.98769187927246 2.015511989593506\n",
            "23040 1.995345428440487e-06 4.601625187206082e-05 Traning Loss: 4.8011595936259255e-05 17.987791061401367 2.0154595375061035\n",
            "23041 1.9870408323185984e-06 4.599046224029735e-05 Traning Loss: 4.7977504436858e-05 17.98788833618164 2.015407085418701\n",
            "23042 1.964616785699036e-06 4.59813563793432e-05 Traning Loss: 4.7945974074536934e-05 17.98798370361328 2.0153555870056152\n",
            "23043 1.9622100353444694e-06 4.595235805027187e-05 Traning Loss: 4.791456740349531e-05 17.98807716369629 2.0153040885925293\n",
            "23044 1.937831939358148e-06 4.5943066652398556e-05 Traning Loss: 4.788089790963568e-05 17.988168716430664 2.0152533054351807\n",
            "23045 1.935041836986784e-06 4.591285323840566e-05 Traning Loss: 4.784789416589774e-05 17.98826026916504 2.015202760696411\n",
            "23046 1.914237145683728e-06 4.590096068568528e-05 Traning Loss: 4.781519965035841e-05 17.98834991455078 2.0151526927948\n",
            "23047 1.906585794131388e-06 4.5874920033384115e-05 Traning Loss: 4.7781504690647125e-05 17.98843765258789 2.0151031017303467\n",
            "23048 1.8924574760603718e-06 4.585642091115005e-05 Traning Loss: 4.774887929670513e-05 17.988523483276367 2.0150537490844727\n",
            "23049 1.878613716144173e-06 4.583894042298198e-05 Traning Loss: 4.771755266119726e-05 17.98860740661621 2.015005111694336\n",
            "23050 1.87052239652985e-06 4.5814515033271164e-05 Traning Loss: 4.7685036406619474e-05 17.988691329956055 2.014956474304199\n",
            "23051 1.8526143321651034e-06 4.580101085593924e-05 Traning Loss: 4.7653626097599044e-05 17.988773345947266 2.0149085521698\n",
            "23052 1.8475557226338424e-06 4.5774158934364095e-05 Traning Loss: 4.7621713747503236e-05 17.988853454589844 2.0148608684539795\n",
            "23053 1.82896883416106e-06 4.576059291139245e-05 Traning Loss: 4.758956129080616e-05 17.98893165588379 2.0148136615753174\n",
            "23054 1.82347605459654e-06 4.573348996927962e-05 Traning Loss: 4.755696500069462e-05 17.989009857177734 2.0147666931152344\n",
            "23055 1.8072215652864543e-06 4.571816680254415e-05 Traning Loss: 4.752538734464906e-05 17.989086151123047 2.0147204399108887\n",
            "23056 1.7992621224038885e-06 4.569540760712698e-05 Traning Loss: 4.749466825160198e-05 17.989160537719727 2.014674425125122\n",
            "23057 1.7869481325760717e-06 4.5675275032408535e-05 Traning Loss: 4.7462224756600335e-05 17.989234924316406 2.0146286487579346\n",
            "23058 1.7756491388354334e-06 4.5656612201128155e-05 Traning Loss: 4.743226236314513e-05 17.989307403564453 2.0145833492279053\n",
            "23059 1.7669664202912827e-06 4.5633820263901725e-05 Traning Loss: 4.740078657050617e-05 17.989377975463867 2.014538288116455\n",
            "23060 1.7532850051793503e-06 4.56176494481042e-05 Traning Loss: 4.7370933316415176e-05 17.98944854736328 2.014493703842163\n",
            "23061 1.746840439409425e-06 4.5593078539241105e-05 Traning Loss: 4.733991954708472e-05 17.989517211914062 2.01444935798645\n",
            "23062 1.7322596477242769e-06 4.557736247079447e-05 Traning Loss: 4.730962245957926e-05 17.98958396911621 2.0144054889678955\n",
            "23063 1.7265874703298323e-06 4.555282066576183e-05 Traning Loss: 4.727940904558636e-05 17.98965072631836 2.01436185836792\n",
            "23064 1.71282272276585e-06 4.553730832412839e-05 Traning Loss: 4.725013059214689e-05 17.989715576171875 2.0143187046051025\n",
            "23065 1.7065081010514405e-06 4.551215533865616e-05 Traning Loss: 4.7218662075465545e-05 17.98978042602539 2.0142757892608643\n",
            "23066 1.6944854905887041e-06 4.5493994548451155e-05 Traning Loss: 4.718848140328191e-05 17.989843368530273 2.014233112335205\n",
            "23067 1.686794121269486e-06 4.547225762507878e-05 Traning Loss: 4.715905015473254e-05 17.989906311035156 2.014190912246704\n",
            "23068 1.676890860835556e-06 4.5453176426235586e-05 Traning Loss: 4.713006637757644e-05 17.989967346191406 2.0141489505767822\n",
            "23069 1.6677062149028643e-06 4.5432239858200774e-05 Traning Loss: 4.709994755103253e-05 17.990026473999023 2.0141072273254395\n",
            "23070 1.6596178511463222e-06 4.541064481600188e-05 Traning Loss: 4.707026164396666e-05 17.99008560180664 2.014065980911255\n",
            "23071 1.6494893770868657e-06 4.5391159801511094e-05 Traning Loss: 4.704064849647693e-05 17.990142822265625 2.0140249729156494\n",
            "23072 1.6428442677351995e-06 4.5368491555564106e-05 Traning Loss: 4.7011337301228195e-05 17.99020004272461 2.013984203338623\n",
            "23073 1.6320828990501468e-06 4.5348791900323704e-05 Traning Loss: 4.6980876504676417e-05 17.99025535583496 2.013943910598755\n",
            "23074 1.626324660719547e-06 4.532750608632341e-05 Traning Loss: 4.69538317702245e-05 17.990310668945312 2.013903856277466\n",
            "23075 1.6154658624145668e-06 4.530805381364189e-05 Traning Loss: 4.692352013080381e-05 17.99036407470703 2.013864278793335\n",
            "23076 1.6098948663056944e-06 4.528565114014782e-05 Traning Loss: 4.689554771175608e-05 17.99041748046875 2.013824939727783\n",
            "23077 1.5995323110473691e-06 4.526694101514295e-05 Traning Loss: 4.6866472985129803e-05 17.99047088623047 2.0137860774993896\n",
            "23078 1.5937918078634539e-06 4.524399992078543e-05 Traning Loss: 4.683779116021469e-05 17.990522384643555 2.013747453689575\n",
            "23079 1.5841668528082664e-06 4.522500967141241e-05 Traning Loss: 4.680917481891811e-05 17.99057388305664 2.01370906829834\n",
            "23080 1.5779177147123846e-06 4.520405491348356e-05 Traning Loss: 4.6781973651377484e-05 17.990623474121094 2.0136709213256836\n",
            "23081 1.5694176909164526e-06 4.518475543591194e-05 Traning Loss: 4.675417221733369e-05 17.990673065185547 2.0136332511901855\n",
            "23082 1.5624756315446575e-06 4.5163680624682456e-05 Traning Loss: 4.6726156142540276e-05 17.990720748901367 2.0135958194732666\n",
            "23083 1.5550519947282737e-06 4.5145094190957025e-05 Traning Loss: 4.670014459406957e-05 17.990768432617188 2.0135586261749268\n",
            "23084 1.5477714896405814e-06 4.512592204264365e-05 Traning Loss: 4.6673692850163206e-05 17.990814208984375 2.013521909713745\n",
            "23085 1.541344545330503e-06 4.510940925683826e-05 Traning Loss: 4.665075539378449e-05 17.990859985351562 2.0134854316711426\n",
            "23086 1.5340505115091219e-06 4.509387508733198e-05 Traning Loss: 4.662792707676999e-05 17.990903854370117 2.0134494304656982\n",
            "23087 1.5288771919585997e-06 4.507828634814359e-05 Traning Loss: 4.660716513171792e-05 17.990947723388672 2.013413429260254\n",
            "23088 1.5218284943330218e-06 4.5072090870235115e-05 Traning Loss: 4.65939192508813e-05 17.990991592407227 2.0133779048919678\n",
            "23089 1.5182827155513223e-06 4.5066848542774096e-05 Traning Loss: 4.6585129894083366e-05 17.99103546142578 2.0133423805236816\n",
            "23090 1.512445464868506e-06 4.507390258368105e-05 Traning Loss: 4.658634861698374e-05 17.991077423095703 2.013307571411133\n",
            "23091 1.5118915825951262e-06 4.509065911406651e-05 Traning Loss: 4.660255217459053e-05 17.991119384765625 2.013272523880005\n",
            "23092 1.509064077254152e-06 4.513374005910009e-05 Traning Loss: 4.664280277211219e-05 17.991159439086914 2.0132384300231934\n",
            "23093 1.5152155583564308e-06 4.520577203948051e-05 Traning Loss: 4.67209865746554e-05 17.991201400756836 2.0132038593292236\n",
            "23094 1.5206492207653355e-06 4.533910396276042e-05 Traning Loss: 4.6859753638273105e-05 17.991239547729492 2.0131704807281494\n",
            "23095 1.5421910575241782e-06 4.555274063022807e-05 Traning Loss: 4.709493077825755e-05 17.99127960205078 2.013136386871338\n",
            "23096 1.5695177353336476e-06 4.591326796798967e-05 Traning Loss: 4.748278661281802e-05 17.991315841674805 2.013103723526001\n",
            "23097 1.63016466103727e-06 4.649099719244987e-05 Traning Loss: 4.8121160943992436e-05 17.991355895996094 2.0130698680877686\n",
            "23098 1.7154911802208517e-06 4.745099431602284e-05 Traning Loss: 4.916648686048575e-05 17.99138832092285 2.013038158416748\n",
            "23099 1.878206944638805e-06 4.900024941889569e-05 Traning Loss: 5.087845784146339e-05 17.99142837524414 2.0130043029785156\n",
            "23100 2.123197873515892e-06 5.157956184120849e-05 Traning Loss: 5.370275903260335e-05 17.991456985473633 2.0129737854003906\n",
            "23101 2.5619319785619155e-06 5.5798453104216605e-05 Traning Loss: 5.836038326378912e-05 17.991497039794922 2.012939453125\n",
            "23102 3.2550340165471425e-06 6.287988071562722e-05 Traning Loss: 6.613491132156923e-05 17.991519927978516 2.0129103660583496\n",
            "23103 4.465446181711741e-06 7.462310895789415e-05 Traning Loss: 7.908855332061648e-05 17.991561889648438 2.0128750801086426\n",
            "23104 6.447749456128804e-06 9.456482075620443e-05 Traning Loss: 0.00010101257066708058 17.991575241088867 2.012848377227783\n",
            "23105 9.867346307146363e-06 0.00012798637908417732 Traning Loss: 0.00013785372721031308 17.99161720275879 2.0128109455108643\n",
            "23106 1.5627238099114038e-05 0.00018542379257269204 Traning Loss: 0.00020105103612877429 17.991609573364258 2.0127875804901123\n",
            "23107 2.5467637897236273e-05 0.0002821285743266344 Traning Loss: 0.00030759620130993426 17.99164581298828 2.0127463340759277\n",
            "23108 4.237665780237876e-05 0.00044976649223826826 Traning Loss: 0.0004921431536786258 17.991588592529297 2.012727975845337\n",
            "23109 7.069129787851125e-05 0.0007292317459359765 Traning Loss: 0.0007999230292625725 17.991596221923828 2.012679100036621\n",
            "23110 0.00011953884677495807 0.0012111415853723884 Traning Loss: 0.0013306804466992617 17.991424560546875 2.0126678943634033\n",
            "23111 0.00019679058459587395 0.0019766578916460276 Traning Loss: 0.002173448447138071 17.99132537841797 2.0126054286956787\n",
            "23112 0.0003246735141146928 0.003230063244700432 Traning Loss: 0.003554736729711294 17.990873336791992 2.0126004219055176\n",
            "23113 0.00049450749065727 0.004921791609376669 Traning Loss: 0.005416299216449261 17.990463256835938 2.012519359588623\n",
            "23114 0.0007264656596817076 0.007157960906624794 Traning Loss: 0.007884426973760128 17.98944854736328 2.0125064849853516\n",
            "23115 0.0008817862253636122 0.008734514005482197 Traning Loss: 0.009616300463676453 17.98844337463379 2.012420892715454\n",
            "23116 0.0009273166069760919 0.00900680385529995 Traning Loss: 0.009934120811522007 17.986818313598633 2.012359380722046\n",
            "23117 0.0006519907037727535 0.006351321469992399 Traning Loss: 0.00700331199914217 17.985427856445312 2.012338638305664\n",
            "23118 0.00030409145983867347 0.00266845989972353 Traning Loss: 0.0029725513886660337 17.983936309814453 2.012209415435791\n",
            "23119 8.437767974101007e-05 0.0005906397709622979 Traning Loss: 0.0006750174798071384 17.982805252075195 2.012312650680542\n",
            "23120 0.00018450047355145216 0.00138843955937773 Traning Loss: 0.001572940032929182 17.981826782226562 2.012205123901367\n",
            "23121 0.00035225081956014037 0.0031807750929147005 Traning Loss: 0.003533025970682502 17.980709075927734 2.012335777282715\n",
            "23122 0.0003568356914911419 0.0030986627098172903 Traning Loss: 0.0034554984886199236 17.97970962524414 2.012388229370117\n",
            "23123 0.00019940949277952313 0.0014384464593604207 Traning Loss: 0.0016378560103476048 17.978525161743164 2.0124258995056152\n",
            "23124 9.242801024811342e-05 0.0005163957248441875 Traning Loss: 0.0006088237278163433 17.977474212646484 2.0126938819885254\n",
            "23125 0.00020267882791813463 0.0012642752844840288 Traning Loss: 0.0014669541269540787 17.976404190063477 2.012714385986328\n",
            "23126 0.0002465695724822581 0.002090736757963896 Traning Loss: 0.002337306272238493 17.975255966186523 2.013021230697632\n",
            "23127 0.0001992463512578979 0.00132958241738379 Traning Loss: 0.0015288287540897727 17.97420883178711 2.0131735801696777\n",
            "23128 8.022012480068952e-05 0.00028856360586360097 Traning Loss: 0.00036878371611237526 17.973182678222656 2.0133206844329834\n",
            "23129 8.638212602818385e-05 0.0004564112168736756 Traning Loss: 0.0005427933647297323 17.972248077392578 2.0136678218841553\n",
            "23130 0.0001914701279019937 0.0012486891355365515 Traning Loss: 0.0014401592779904604 17.971309661865234 2.0137782096862793\n",
            "23131 0.0001576168288011104 0.0012742092367261648 Traning Loss: 0.0014318260364234447 17.970382690429688 2.0141637325286865\n",
            "23132 8.58735220390372e-05 0.0004023787332698703 Traning Loss: 0.0004882522625848651 17.969545364379883 2.0144271850585938\n",
            "23133 4.133186303079128e-05 7.383419870166108e-05 Traning Loss: 0.00011516606173245236 17.96880340576172 2.0146665573120117\n",
            "23134 7.836471922928467e-05 0.0005803189123980701 Traning Loss: 0.0006586836534552276 17.968120574951172 2.01505970954895\n",
            "23135 0.00012804975267499685 0.0008911735494621098 Traning Loss: 0.0010192233603447676 17.967500686645508 2.0152480602264404\n",
            "23136 7.109201396815479e-05 0.0005641530733555555 Traning Loss: 0.0006352451164275408 17.966930389404297 2.0156047344207764\n",
            "23137 3.463819302851334e-05 0.00012315659841988236 Traning Loss: 0.0001577947987243533 17.966405868530273 2.015852928161621\n",
            "23138 3.8492813473567367e-05 0.00022874838032294065 Traning Loss: 0.00026724120834842324 17.965991973876953 2.016054630279541\n",
            "23139 6.172832945594564e-05 0.0005409273435361683 Traning Loss: 0.0006026556948199868 17.965560913085938 2.0163283348083496\n",
            "23140 6.255917105590925e-05 0.00047530554002150893 Traning Loss: 0.0005378646892495453 17.965200424194336 2.016462802886963\n",
            "23141 2.0691419194918126e-05 0.00018160247418563813 Traning Loss: 0.00020229388610459864 17.96489715576172 2.016662359237671\n",
            "23142 1.5779798559378833e-05 9.287264401791617e-05 Traning Loss: 0.000108652442577295 17.964624404907227 2.0168073177337646\n",
            "23143 3.37862475134898e-05 0.00029512864421121776 Traning Loss: 0.000328914902638644 17.964447021484375 2.016899585723877\n",
            "23144 4.336714846431278e-05 0.0004188297607470304 Traning Loss: 0.00046219691284932196 17.9642391204834 2.0170230865478516\n",
            "23145 2.919229882536456e-05 0.00026727610384114087 Traning Loss: 0.00029646840994246304 17.964126586914062 2.0170915126800537\n",
            "23146 8.88980594027089e-06 7.921823271317407e-05 Traning Loss: 8.810804138192907e-05 17.964048385620117 2.0171449184417725\n",
            "23147 8.510754923918284e-06 9.251141455024481e-05 Traning Loss: 0.00010102216765517369 17.964021682739258 2.0172085762023926\n",
            "23148 2.4585229766671546e-05 0.00022405099298339337 Traning Loss: 0.00024863623548299074 17.96408462524414 2.0171942710876465\n",
            "23149 2.5070514311664738e-05 0.00026148505276069045 Traning Loss: 0.00028655555797740817 17.96414566040039 2.017211437225342\n",
            "23150 1.597109621798154e-05 0.00015540876484010369 Traning Loss: 0.00017137985560111701 17.96427345275879 2.017197608947754\n",
            "23151 8.894399798009545e-06 7.200383697636425e-05 Traning Loss: 8.08982367743738e-05 17.964405059814453 2.017148733139038\n",
            "23152 9.556797522236593e-06 0.00011006283602910116 Traning Loss: 0.00011961963173234835 17.964563369750977 2.0171573162078857\n",
            "23153 2.125259743479546e-05 0.00017301246407441795 Traning Loss: 0.0001942650560522452 17.964733123779297 2.017076015472412\n",
            "23154 1.5156805602600798e-05 0.00016622035764157772 Traning Loss: 0.00018137716688215733 17.964906692504883 2.0170624256134033\n",
            "23155 1.023977074510185e-05 8.69864015839994e-05 Traning Loss: 9.722616960061714e-05 17.96510887145996 2.01701021194458\n",
            "23156 5.6233966461149976e-06 5.008249718230218e-05 Traning Loss: 5.570589564740658e-05 17.9653263092041 2.016948699951172\n",
            "23157 7.933509550639428e-06 8.74897014000453e-05 Traning Loss: 9.542320913169533e-05 17.965564727783203 2.0169310569763184\n",
            "23158 1.5514429833274335e-05 0.0001273268135264516 Traning Loss: 0.00014284125063568354 17.965822219848633 2.0168449878692627\n",
            "23159 1.0929281415883452e-05 0.00011910800822079182 Traning Loss: 0.00013003728236071765 17.966096878051758 2.0168259143829346\n",
            "23160 8.911173608794343e-06 7.082364027155563e-05 Traning Loss: 7.973481115186587e-05 17.96637535095215 2.016763925552368\n",
            "23161 5.168412371858722e-06 5.4394655307987705e-05 Traning Loss: 5.9563066315604374e-05 17.96668243408203 2.0167200565338135\n",
            "23162 7.602020559716038e-06 7.531690062023699e-05 Traning Loss: 8.291892299894243e-05 17.966983795166016 2.016692876815796\n",
            "23163 1.026620975608239e-05 9.319092350779101e-05 Traning Loss: 0.0001034571323543787 17.967304229736328 2.016637086868286\n",
            "23164 7.654819455638062e-06 8.134789823088795e-05 Traning Loss: 8.90027149580419e-05 17.967622756958008 2.016618490219116\n",
            "23165 6.296008905337658e-06 5.459539170260541e-05 Traning Loss: 6.089140151743777e-05 17.96794319152832 2.0165834426879883\n",
            "23166 5.041153144702548e-06 5.112060171086341e-05 Traning Loss: 5.616175621980801e-05 17.96827507019043 2.016558885574341\n",
            "23167 7.038204785203561e-06 6.787262100260705e-05 Traning Loss: 7.491082942578942e-05 17.96859359741211 2.0165460109710693\n",
            "23168 8.076287485891953e-06 7.862527127144858e-05 Traning Loss: 8.670156239531934e-05 17.968936920166016 2.0165231227874756\n",
            "23169 6.936383215361275e-06 6.871511141071096e-05 Traning Loss: 7.565149280708283e-05 17.969268798828125 2.016512632369995\n",
            "23170 5.0465350796002895e-06 5.2615359891206026e-05 Traning Loss: 5.7661894970806316e-05 17.96961784362793 2.016507387161255\n",
            "23171 5.278151547827292e-06 4.901126885670237e-05 Traning Loss: 5.428941949503496e-05 17.969968795776367 2.0164926052093506\n",
            "23172 5.3894623306405265e-06 5.8852801885223016e-05 Traning Loss: 6.4242267399095e-05 17.970317840576172 2.0165040493011475\n",
            "23173 6.776072041247971e-06 6.318688974715769e-05 Traning Loss: 6.996295996941626e-05 17.970670700073242 2.016496419906616\n",
            "23174 5.5086684369598515e-06 5.7856559578794986e-05 Traning Loss: 6.336523074423894e-05 17.971012115478516 2.0165069103240967\n",
            "23175 4.725549388240324e-06 4.918486229144037e-05 Traning Loss: 5.391041122493334e-05 17.971357345581055 2.016519546508789\n",
            "23176 5.011057965020882e-06 4.820226240553893e-05 Traning Loss: 5.321331991581246e-05 17.97169303894043 2.0165202617645264\n",
            "23177 4.712708232545992e-06 5.5095308198360726e-05 Traning Loss: 5.9808015066664666e-05 17.972028732299805 2.016545295715332\n",
            "23178 6.073546046536649e-06 5.732524368795566e-05 Traning Loss: 6.339878746075556e-05 17.97235870361328 2.0165421962738037\n",
            "23179 4.562871254165657e-06 5.468238305184059e-05 Traning Loss: 5.924525612499565e-05 17.972686767578125 2.0165610313415527\n",
            "23180 4.604551122611156e-06 4.8111269279615954e-05 Traning Loss: 5.271581903798506e-05 17.97300910949707 2.0165648460388184\n",
            "23181 3.949641268263804e-06 4.704425737145357e-05 Traning Loss: 5.099389818497002e-05 17.973331451416016 2.0165700912475586\n",
            "23182 4.116826403333107e-06 5.001319368602708e-05 Traning Loss: 5.413002145360224e-05 17.97364616394043 2.016580104827881\n",
            "23183 4.506647201196756e-06 5.203209366300143e-05 Traning Loss: 5.653874177369289e-05 17.97396469116211 2.0165746212005615\n",
            "23184 3.843390004476532e-06 5.096929453429766e-05 Traning Loss: 5.481268453877419e-05 17.974281311035156 2.0165796279907227\n",
            "23185 3.8201865208975505e-06 4.7418154281331226e-05 Traning Loss: 5.123834125697613e-05 17.97459602355957 2.0165722370147705\n",
            "23186 3.376423592271749e-06 4.660244667320512e-05 Traning Loss: 4.997886935598217e-05 17.974912643432617 2.0165655612945557\n",
            "23187 3.637610006990144e-06 4.8129993956536055e-05 Traning Loss: 5.176760532776825e-05 17.975221633911133 2.016556978225708\n",
            "23188 3.7404515751404688e-06 4.996347342967056e-05 Traning Loss: 5.370392318582162e-05 17.975534439086914 2.01654052734375\n",
            "23189 3.6700782857224112e-06 4.960144724464044e-05 Traning Loss: 5.327152393874712e-05 17.97583770751953 2.016523599624634\n",
            "23190 3.373016397745232e-06 4.770440500578843e-05 Traning Loss: 5.107742254040204e-05 17.97614288330078 2.016502618789673\n",
            "23191 3.3599235393921845e-06 4.620091203832999e-05 Traning Loss: 4.9560836487216875e-05 17.976442337036133 2.0164740085601807\n",
            "23192 3.104953293586732e-06 4.679224730352871e-05 Traning Loss: 4.989720036974177e-05 17.97673797607422 2.0164501667022705\n",
            "23193 3.5205534913984593e-06 4.7455709136556834e-05 Traning Loss: 5.097626126371324e-05 17.977027893066406 2.0164129734039307\n",
            "23194 3.1649487937102094e-06 4.794692722498439e-05 Traning Loss: 5.1111877837684005e-05 17.977312088012695 2.016383409500122\n",
            "23195 3.2967641345749144e-06 4.6723143896088004e-05 Traning Loss: 5.001990939490497e-05 17.977590560913086 2.0163447856903076\n",
            "23196 3.008747626154218e-06 4.585428541759029e-05 Traning Loss: 4.8863032134249806e-05 17.97786521911621 2.0163068771362305\n",
            "23197 2.9501163680833997e-06 4.5738070184597746e-05 Traning Loss: 4.868818723480217e-05 17.97813606262207 2.0162696838378906\n",
            "23198 3.122041562164668e-06 4.620318577508442e-05 Traning Loss: 4.932522642775439e-05 17.978403091430664 2.0162246227264404\n",
            "23199 2.9119091777829453e-06 4.688887929660268e-05 Traning Loss: 4.980078665539622e-05 17.978668212890625 2.016186475753784\n",
            "23200 3.1405031677422812e-06 4.6377655962714925e-05 Traning Loss: 4.951815935783088e-05 17.97892951965332 2.016139268875122\n",
            "23201 2.8207596187712625e-06 4.599005478667095e-05 Traning Loss: 4.881081258645281e-05 17.979188919067383 2.0160977840423584\n",
            "23202 2.9602222184621496e-06 4.542915121419355e-05 Traning Loss: 4.838937456952408e-05 17.979440689086914 2.016051769256592\n",
            "23203 2.844044956873404e-06 4.5683867938350886e-05 Traning Loss: 4.852791244047694e-05 17.979690551757812 2.0160064697265625\n",
            "23204 2.915897539423895e-06 4.594477650243789e-05 Traning Loss: 4.886067472398281e-05 17.97993278503418 2.0159616470336914\n",
            "23205 2.8784752430510707e-06 4.6012853999855e-05 Traning Loss: 4.8891328333411366e-05 17.980173110961914 2.0159153938293457\n",
            "23206 2.856743549273233e-06 4.566866846289486e-05 Traning Loss: 4.852541314903647e-05 17.980405807495117 2.015869379043579\n",
            "23207 2.7469598080642754e-06 4.53394022770226e-05 Traning Loss: 4.8086363676702604e-05 17.980634689331055 2.015824556350708\n",
            "23208 2.813134869938949e-06 4.510001963353716e-05 Traning Loss: 4.791315586771816e-05 17.980857849121094 2.0157766342163086\n",
            "23209 2.6635630092641804e-06 4.534695472102612e-05 Traning Loss: 4.8010519094532356e-05 17.981077194213867 2.015733242034912\n",
            "23210 2.811176727846032e-06 4.531108061200939e-05 Traning Loss: 4.812225597561337e-05 17.981292724609375 2.0156848430633545\n",
            "23211 2.6180978238699026e-06 4.541524685919285e-05 Traning Loss: 4.803334377356805e-05 17.981504440307617 2.015641689300537\n",
            "23212 2.7053592930315062e-06 4.506568438955583e-05 Traning Loss: 4.777104186359793e-05 17.981712341308594 2.015594959259033\n",
            "23213 2.5721446945681237e-06 4.4964937842451036e-05 Traning Loss: 4.753708344651386e-05 17.981918334960938 2.0155506134033203\n",
            "23214 2.5887843548844103e-06 4.48760692961514e-05 Traning Loss: 4.746485501527786e-05 17.982120513916016 2.0155062675476074\n",
            "23215 2.582427669040044e-06 4.49364488304127e-05 Traning Loss: 4.751887536258437e-05 17.982318878173828 2.0154612064361572\n",
            "23216 2.5337376428069547e-06 4.501648072618991e-05 Traning Loss: 4.755021655000746e-05 17.982513427734375 2.015418767929077\n",
            "23217 2.5576387088221963e-06 4.4909200369147584e-05 Traning Loss: 4.746683771372773e-05 17.982704162597656 2.0153746604919434\n",
            "23218 2.478795067872852e-06 4.482694566831924e-05 Traning Loss: 4.730574073619209e-05 17.982891082763672 2.0153322219848633\n",
            "23219 2.4746011604293017e-06 4.469796476769261e-05 Traning Loss: 4.7172565246000886e-05 17.983076095581055 2.015289545059204\n",
            "23220 2.443386847517104e-06 4.468296901904978e-05 Traning Loss: 4.712635563919321e-05 17.983257293701172 2.015246868133545\n",
            "23221 2.4120477064570878e-06 4.472833461477421e-05 Traning Loss: 4.714038368547335e-05 17.983434677124023 2.0152053833007812\n",
            "23222 2.4137204945873236e-06 4.472517321119085e-05 Traning Loss: 4.7138892114162445e-05 17.98360824584961 2.015162944793701\n",
            "23223 2.364256715736701e-06 4.47140664618928e-05 Traning Loss: 4.7078323405003175e-05 17.98377799987793 2.0151216983795166\n",
            "23224 2.3394306936097564e-06 4.463728691916913e-05 Traning Loss: 4.697671829489991e-05 17.983945846557617 2.015080451965332\n",
            "23225 2.31894887292583e-06 4.4567877921508625e-05 Traning Loss: 4.688682747655548e-05 17.98410987854004 2.0150389671325684\n",
            "23226 2.25608437176561e-06 4.45867954113055e-05 Traning Loss: 4.6842880692565814e-05 17.984272003173828 2.0149991512298584\n",
            "23227 2.2907879611011595e-06 4.454375812201761e-05 Traning Loss: 4.6834546083118767e-05 17.98443031311035 2.0149571895599365\n",
            "23228 2.1961161564831855e-06 4.462796641746536e-05 Traning Loss: 4.682408325606957e-05 17.984586715698242 2.014918088912964\n",
            "23229 2.256434299852117e-06 4.453108704183251e-05 Traning Loss: 4.67875215690583e-05 17.984739303588867 2.014876127243042\n",
            "23230 2.1471880700119073e-06 4.458288458408788e-05 Traning Loss: 4.6730074245715514e-05 17.98488998413086 2.0148372650146484\n",
            "23231 2.2076901586842723e-06 4.446882303454913e-05 Traning Loss: 4.66765122837387e-05 17.985036849975586 2.0147957801818848\n",
            "23232 2.1070359252917115e-06 4.454551526578143e-05 Traning Loss: 4.6652552555315197e-05 17.985183715820312 2.014756679534912\n",
            "23233 2.1695709619962145e-06 4.4486369006335735e-05 Traning Loss: 4.66559395135846e-05 17.985326766967773 2.0147151947021484\n",
            "23234 2.0712852801807458e-06 4.4606644223676994e-05 Traning Loss: 4.667793109547347e-05 17.985469818115234 2.014676332473755\n",
            "23235 2.1547818960243603e-06 4.455326416064054e-05 Traning Loss: 4.6708046284038574e-05 17.985607147216797 2.014634370803833\n",
            "23236 2.0283289359213086e-06 4.472303407965228e-05 Traning Loss: 4.6751363697694615e-05 17.98574447631836 2.014596462249756\n",
            "23237 2.1664088762918254e-06 4.466433892957866e-05 Traning Loss: 4.683074803324416e-05 17.985876083374023 2.0145535469055176\n",
            "23238 1.9869942207151325e-06 4.4990043534198776e-05 Traning Loss: 4.6977038437034935e-05 17.98600959777832 2.014517068862915\n",
            "23239 2.219949010395794e-06 4.500606519286521e-05 Traning Loss: 4.722601443063468e-05 17.986135482788086 2.014472723007202\n",
            "23240 1.975737404791289e-06 4.563905531540513e-05 Traning Loss: 4.761479431181215e-05 17.986265182495117 2.0144381523132324\n",
            "23241 2.348889438508195e-06 4.585933857015334e-05 Traning Loss: 4.820822869078256e-05 17.986385345458984 2.0143916606903076\n",
            "23242 2.038583716057474e-06 4.707173866336234e-05 Traning Loss: 4.9110323743661866e-05 17.986513137817383 2.014359712600708\n",
            "23243 2.6321604309487157e-06 4.786164208780974e-05 Traning Loss: 5.049380342825316e-05 17.98662757873535 2.014310598373413\n",
            "23244 2.278966803714866e-06 5.0357633881503716e-05 Traning Loss: 5.2636602049460635e-05 17.986753463745117 2.014282464981079\n",
            "23245 3.2775603813206544e-06 5.269607208902016e-05 Traning Loss: 5.597363269771449e-05 17.986860275268555 2.0142292976379395\n",
            "23246 2.9730647383985342e-06 5.8221787185175344e-05 Traning Loss: 6.11948489677161e-05 17.986984252929688 2.014207124710083\n",
            "23247 4.798087957169628e-06 6.461772136390209e-05 Traning Loss: 6.941580795682967e-05 17.987079620361328 2.0141470432281494\n",
            "23248 4.855093266087351e-06 7.756319246254861e-05 Traning Loss: 8.241828618338332e-05 17.987205505371094 2.014134645462036\n",
            "23249 8.48929903440876e-06 9.467922063777223e-05 Traning Loss: 0.00010316851694369689 17.987285614013672 2.0140628814697266\n",
            "23250 9.896263691189233e-06 0.00012648128904402256 Traning Loss: 0.0001363775518257171 17.987411499023438 2.0140671730041504\n",
            "23251 1.7743055650498718e-05 0.0001722668093862012 Traning Loss: 0.0001900098577607423 17.987464904785156 2.013976573944092\n",
            "23252 2.3395985408569686e-05 0.00025311627541668713 Traning Loss: 0.0002765122626442462 17.987590789794922 2.014008045196533\n",
            "23253 4.1478826460661367e-05 0.00037545058876276016 Traning Loss: 0.00041692942613735795 17.98759651184082 2.013887643814087\n",
            "23254 5.9276957472320646e-05 0.0005831675953231752 Traning Loss: 0.000642444530967623 17.987709045410156 2.0139639377593994\n",
            "23255 0.00010217359522357583 0.0009019037242978811 Traning Loss: 0.001004077261313796 17.98761749267578 2.013798475265503\n",
            "23256 0.0001506182597950101 0.0014142695581540465 Traning Loss: 0.0015648878179490566 17.987674713134766 2.013947010040283\n",
            "23257 0.0002468575839884579 0.0021653727162629366 Traning Loss: 0.0024122302420437336 17.987382888793945 2.013723134994507\n",
            "23258 0.00035063139512203634 0.003225775435566902 Traning Loss: 0.003576406743377447 17.987274169921875 2.013974189758301\n",
            "23259 0.000514073355589062 0.004498849157243967 Traning Loss: 0.005012922454625368 17.98661994934082 2.013707399368286\n",
            "23260 0.000620228995103389 0.005676172208040953 Traning Loss: 0.0062964013777673244 17.986162185668945 2.014058828353882\n",
            "23261 0.000702989986166358 0.006109665613621473 Traning Loss: 0.006812655366957188 17.985090255737305 2.013845205307007\n",
            "23262 0.0005602567689493299 0.005179127212613821 Traning Loss: 0.005739383865147829 17.98427391052246 2.01419997215271\n",
            "23263 0.0003574125003069639 0.0030070755165070295 Traning Loss: 0.0033644880168139935 17.9831600189209 2.014200210571289\n",
            "23264 9.3534414190799e-05 0.000873542099725455 Traning Loss: 0.000967076513916254 17.982316970825195 2.0144095420837402\n",
            "23265 1.8912640371127054e-05 9.745563147589564e-05 Traning Loss: 0.0001163682754850015 17.981595993041992 2.0146844387054443\n",
            "23266 0.0001252410584129393 0.0008614728576503694 Traning Loss: 0.0009867139160633087 17.98079490661621 2.0147483348846436\n",
            "23267 0.0002255188301205635 0.0020728514064103365 Traning Loss: 0.0022983702365309 17.980209350585938 2.015178918838501\n",
            "23268 0.0002954760566353798 0.002301811473444104 Traning Loss: 0.002597287530079484 17.979297637939453 2.0153021812438965\n",
            "23269 0.00016165518900379539 0.0014395638136193156 Traning Loss: 0.00160121894441545 17.97854995727539 2.0156822204589844\n",
            "23270 6.481947639258578e-05 0.00037203196552582085 Traning Loss: 0.000436851434642449 17.97774314880371 2.0160021781921387\n",
            "23271 4.5237960875965655e-05 0.00015927896311040968 Traning Loss: 0.00020451692398637533 17.976947784423828 2.016227960586548\n",
            "23272 9.291571768699214e-05 0.0007493095472455025 Traning Loss: 0.0008422252722084522 17.976287841796875 2.016664505004883\n",
            "23273 0.00016949503333307803 0.0012115705758333206 Traning Loss: 0.0013810655800625682 17.975454330444336 2.0168697834014893\n",
            "23274 0.00012393301585689187 0.0010375662241131067 Traning Loss: 0.0011614991817623377 17.974796295166016 2.017273426055908\n",
            "23275 6.995302101131529e-05 0.0004465462698135525 Traning Loss: 0.0005164992762729526 17.974092483520508 2.017591953277588\n",
            "23276 3.458453284110874e-05 0.00013751663209404796 Traning Loss: 0.0001721011649351567 17.973493576049805 2.0178451538085938\n",
            "23277 4.7821646148804575e-05 0.0003263867401983589 Traning Loss: 0.00037420837907120585 17.973045349121094 2.0182254314422607\n",
            "23278 9.093379776459187e-05 0.0006178651819936931 Traning Loss: 0.0007087989943102002 17.9725341796875 2.018413782119751\n",
            "23279 7.939392526168376e-05 0.0006459566648118198 Traning Loss: 0.0007253505755215883 17.972187042236328 2.0187385082244873\n",
            "23280 5.5359698308166116e-05 0.0003885844489559531 Traning Loss: 0.00044394415454007685 17.9718074798584 2.0189762115478516\n",
            "23281 2.942886931123212e-05 0.0001719552237773314 Traning Loss: 0.00020138410036452115 17.97151756286621 2.019157648086548\n",
            "23282 2.5404377083759755e-05 0.00018471053044777364 Traning Loss: 0.00021011490025557578 17.971294403076172 2.0194098949432373\n",
            "23283 4.7520567022729665e-05 0.0003055485140066594 Traning Loss: 0.00035306907375343144 17.971031188964844 2.019507884979248\n",
            "23284 4.2382678657304496e-05 0.00037152564618736506 Traning Loss: 0.00041390833212062716 17.970869064331055 2.0197086334228516\n",
            "23285 3.912039028364234e-05 0.0002938265388365835 Traning Loss: 0.0003329469182062894 17.97064781188965 2.019822597503662\n",
            "23286 2.4155502615030855e-05 0.00019302184227854013 Traning Loss: 0.0002171773521695286 17.970508575439453 2.019902467727661\n",
            "23287 1.67553353094263e-05 0.00015842872380744666 Traning Loss: 0.00017518406093586236 17.970407485961914 2.0200235843658447\n",
            "23288 2.5679048121673986e-05 0.00017171331273857504 Traning Loss: 0.00019739236449822783 17.97031021118164 2.0200252532958984\n",
            "23289 1.9310529751237482e-05 0.00019847335352096707 Traning Loss: 0.00021778387599624693 17.970293045043945 2.020113706588745\n",
            "23290 2.3504830096499063e-05 0.00017892954929266125 Traning Loss: 0.00020243438484612852 17.970252990722656 2.020127058029175\n",
            "23291 1.6310739738401026e-05 0.00015910509682726115 Traning Loss: 0.00017541582928970456 17.970293045043945 2.0201363563537598\n",
            "23292 1.4558301700162701e-05 0.00015018756675999612 Traning Loss: 0.00016474587027914822 17.970338821411133 2.0201668739318848\n",
            "23293 1.8061631635646336e-05 0.00014260182797443122 Traning Loss: 0.00016066346142906696 17.970413208007812 2.020116090774536\n",
            "23294 1.131786575570004e-05 0.00013297097757458687 Traning Loss: 0.00014428884605877101 17.970531463623047 2.020141363143921\n",
            "23295 1.3863074855180457e-05 0.00010256066161673516 Traning Loss: 0.00011642373283393681 17.970632553100586 2.020097255706787\n",
            "23296 8.662419531901833e-06 9.41391263040714e-05 Traning Loss: 0.00010280154674546793 17.970783233642578 2.020071506500244\n",
            "23297 1.06590114228311e-05 0.00010490803106222302 Traning Loss: 0.00011556703975657001 17.97092628479004 2.020049571990967\n",
            "23298 1.3647982996189967e-05 0.00012081090244464576 Traning Loss: 0.00013445888180285692 17.971092224121094 2.0199761390686035\n",
            "23299 1.0716067663452122e-05 0.00012051608064211905 Traning Loss: 0.00013123215467203408 17.97126579284668 2.0199620723724365\n",
            "23300 1.1059921234846115e-05 8.920405525714159e-05 Traning Loss: 0.0001002639764919877 17.97144317626953 2.0198888778686523\n",
            "23301 5.358399448596174e-06 6.347388989524916e-05 Traning Loss: 6.883229070808738e-05 17.971651077270508 2.0198473930358887\n",
            "23302 6.082193522161106e-06 5.7989793276647106e-05 Traning Loss: 6.407198816305026e-05 17.97185707092285 2.01979398727417\n",
            "23303 7.950407052703667e-06 7.706541509833187e-05 Traning Loss: 8.501582487951964e-05 17.972087860107422 2.019716739654541\n",
            "23304 8.670854185766075e-06 9.659580973675475e-05 Traning Loss: 0.00010526666301302612 17.972322463989258 2.0196824073791504\n",
            "23305 1.0116975317941979e-05 9.094175038626418e-05 Traning Loss: 0.00010105872934218496 17.972564697265625 2.0196001529693604\n",
            "23306 6.010026936564827e-06 7.019641634542495e-05 Traning Loss: 7.620644464623183e-05 17.972820281982422 2.0195531845092773\n",
            "23307 5.07307231600862e-06 4.860992339672521e-05 Traning Loss: 5.368299753172323e-05 17.973081588745117 2.019484519958496\n",
            "23308 4.454474037629552e-06 4.7437231842195615e-05 Traning Loss: 5.189170769881457e-05 17.973352432250977 2.019409656524658\n",
            "23309 5.355590928957099e-06 6.186745304148644e-05 Traning Loss: 6.72230453346856e-05 17.973621368408203 2.0193588733673096\n",
            "23310 7.775163794576656e-06 7.306471525225788e-05 Traning Loss: 8.083987631835043e-05 17.973896026611328 2.019273281097412\n",
            "23311 6.255344487726688e-06 7.29143648641184e-05 Traning Loss: 7.916970935184509e-05 17.974170684814453 2.0192201137542725\n",
            "23312 5.991825673845597e-06 5.881096512894146e-05 Traning Loss: 6.480279262177646e-05 17.97444725036621 2.0191404819488525\n",
            "23313 4.095535132364603e-06 4.7521090891677886e-05 Traning Loss: 5.161662556929514e-05 17.974721908569336 2.0190694332122803\n",
            "23314 3.7630891256412724e-06 4.581527173286304e-05 Traning Loss: 4.9578360631130636e-05 17.975000381469727 2.0190086364746094\n",
            "23315 5.06869673699839e-06 5.178598075872287e-05 Traning Loss: 5.6854678405215964e-05 17.975276947021484 2.018927574157715\n",
            "23316 4.750173957290826e-06 5.93769655097276e-05 Traning Loss: 6.412713992176577e-05 17.975555419921875 2.0188729763031006\n",
            "23317 5.680494268744951e-06 5.867403160664253e-05 Traning Loss: 6.435452814912423e-05 17.97583770751953 2.0187935829162598\n",
            "23318 4.3893505790038034e-06 5.404727198765613e-05 Traning Loss: 5.843662074767053e-05 17.97611427307129 2.018730640411377\n",
            "23319 3.976236712333048e-06 4.818542583961971e-05 Traning Loss: 5.216166391619481e-05 17.976394653320312 2.0186657905578613\n",
            "23320 4.017948413093109e-06 4.606018410413526e-05 Traning Loss: 5.007813160773367e-05 17.976665496826172 2.0185940265655518\n",
            "23321 3.5842367651639506e-06 4.8381287342635915e-05 Traning Loss: 5.196552228881046e-05 17.976938247680664 2.0185389518737793\n",
            "23322 4.515892214840278e-06 4.99630332342349e-05 Traning Loss: 5.447892544907518e-05 17.977205276489258 2.018463134765625\n",
            "23323 3.806920403803815e-06 5.113912629894912e-05 Traning Loss: 5.494604556588456e-05 17.977466583251953 2.0184056758880615\n",
            "23324 4.011237706436077e-06 4.9331170885125175e-05 Traning Loss: 5.3342409955803305e-05 17.97772789001465 2.018338441848755\n",
            "23325 3.682574060803745e-06 4.773522960022092e-05 Traning Loss: 5.141780275152996e-05 17.977977752685547 2.0182735919952393\n",
            "23326 3.325711531942943e-06 4.7172117774607614e-05 Traning Loss: 5.049783067079261e-05 17.97823143005371 2.018214702606201\n",
            "23327 3.7443346627696883e-06 4.6754314098507166e-05 Traning Loss: 5.0498649216024205e-05 17.97847557067871 2.0181427001953125\n",
            "23328 3.1142192256083945e-06 4.740905569633469e-05 Traning Loss: 5.052327469456941e-05 17.978723526000977 2.018085241317749\n",
            "23329 3.506593429847271e-06 4.655644443118945e-05 Traning Loss: 5.006303763366304e-05 17.978967666625977 2.018014430999756\n",
            "23330 3.0453907129412983e-06 4.6374178054975346e-05 Traning Loss: 4.94195701321587e-05 17.979209899902344 2.0179519653320312\n",
            "23331 3.081307568209013e-06 4.608495146385394e-05 Traning Loss: 4.91662576678209e-05 17.979455947875977 2.01788592338562\n",
            "23332 3.2186408134293742e-06 4.6211913286242634e-05 Traning Loss: 4.9430553190177307e-05 17.979692459106445 2.0178143978118896\n",
            "23333 2.888262315536849e-06 4.6845056203892455e-05 Traning Loss: 4.97333167004399e-05 17.979934692382812 2.0177509784698486\n",
            "23334 3.2755719985289034e-06 4.6244535042205825e-05 Traning Loss: 4.9520105676492676e-05 17.980167388916016 2.0176761150360107\n",
            "23335 2.7308560675010085e-06 4.5976390538271517e-05 Traning Loss: 4.8707246605772525e-05 17.98040008544922 2.017610549926758\n",
            "23336 2.9426676064758794e-06 4.4831991544924676e-05 Traning Loss: 4.777465801453218e-05 17.980628967285156 2.0175364017486572\n",
            "23337 2.694183194762445e-06 4.463025470613502e-05 Traning Loss: 4.732443630928174e-05 17.980854034423828 2.0174639225006104\n",
            "23338 2.6655425244825892e-06 4.4888507545692846e-05 Traning Loss: 4.755405097967014e-05 17.9810791015625 2.017392158508301\n",
            "23339 2.878332225009217e-06 4.525566328084096e-05 Traning Loss: 4.8133995733223855e-05 17.981294631958008 2.0173146724700928\n",
            "23340 2.5936276415450266e-06 4.590628304868005e-05 Traning Loss: 4.8499910917598754e-05 17.98151397705078 2.017244338989258\n",
            "23341 2.884052719309693e-06 4.543392060440965e-05 Traning Loss: 4.831797195947729e-05 17.98172378540039 2.017165422439575\n",
            "23342 2.494720774848247e-06 4.519507638178766e-05 Traning Loss: 4.768979852087796e-05 17.981937408447266 2.01709246635437\n",
            "23343 2.612697244330775e-06 4.440491829882376e-05 Traning Loss: 4.7017616452649236e-05 17.982145309448242 2.0170159339904785\n",
            "23344 2.472298547218088e-06 4.420243567437865e-05 Traning Loss: 4.6674733312102035e-05 17.982351303100586 2.0169403553009033\n",
            "23345 2.4274681891256478e-06 4.4324613554636016e-05 Traning Loss: 4.675208037951961e-05 17.982555389404297 2.0168676376342773\n",
            "23346 2.593400040495908e-06 4.445980812306516e-05 Traning Loss: 4.7053206799319014e-05 17.98275375366211 2.0167903900146484\n",
            "23347 2.398229980826727e-06 4.4882002839585766e-05 Traning Loss: 4.7280231228796765e-05 17.982954025268555 2.016719341278076\n",
            "23348 2.6080410862050485e-06 4.464876838028431e-05 Traning Loss: 4.725680992123671e-05 17.98314666748047 2.016643524169922\n",
            "23349 2.3760032945574494e-06 4.4630509364651516e-05 Traning Loss: 4.700651334132999e-05 17.983339309692383 2.0165727138519287\n",
            "23350 2.4665059754624963e-06 4.4225445890333503e-05 Traning Loss: 4.6691951865796e-05 17.983524322509766 2.016500473022461\n",
            "23351 2.3779609819030156e-06 4.409163011587225e-05 Traning Loss: 4.646959132514894e-05 17.983707427978516 2.0164287090301514\n",
            "23352 2.3324269022850785e-06 4.406675725476816e-05 Traning Loss: 4.6399185521295294e-05 17.98388671875 2.016359806060791\n",
            "23353 2.409352646282059e-06 4.401932892506011e-05 Traning Loss: 4.6428682253463194e-05 17.98406219482422 2.0162882804870605\n",
            "23354 2.267853005832876e-06 4.419694596435875e-05 Traning Loss: 4.646480010706e-05 17.984235763549805 2.0162220001220703\n",
            "23355 2.3889231215434847e-06 4.4059001083951443e-05 Traning Loss: 4.64479235233739e-05 17.984403610229492 2.016152858734131\n",
            "23356 2.2416763840737985e-06 4.4133565097581595e-05 Traning Loss: 4.6375240344787017e-05 17.98457145690918 2.016087293624878\n",
            "23357 2.301840140717104e-06 4.398152304929681e-05 Traning Loss: 4.6283363190013915e-05 17.98473358154297 2.0160210132598877\n",
            "23358 2.2376339074980933e-06 4.3972337152808905e-05 Traning Loss: 4.620997060555965e-05 17.984895706176758 2.0159554481506348\n",
            "23359 2.211982746302965e-06 4.395110590849072e-05 Traning Loss: 4.6163087972672656e-05 17.98505401611328 2.015892505645752\n",
            "23360 2.24138739213231e-06 4.388253000797704e-05 Traning Loss: 4.61239178548567e-05 17.98520851135254 2.0158278942108154\n",
            "23361 2.1521411781577626e-06 4.3919411837123334e-05 Traning Loss: 4.607155278790742e-05 17.985361099243164 2.0157666206359863\n",
            "23362 2.2119902496342547e-06 4.37875060015358e-05 Traning Loss: 4.5999495341675356e-05 17.985509872436523 2.0157034397125244\n",
            "23363 2.1157002265681513e-06 4.3805364839499816e-05 Traning Loss: 4.5921064156573266e-05 17.98565673828125 2.0156431198120117\n",
            "23364 2.1490645849553403e-06 4.370734313852154e-05 Traning Loss: 4.5856406359234825e-05 17.98579978942871 2.015582323074341\n",
            "23365 2.1016737719037337e-06 4.371831164462492e-05 Traning Loss: 4.581998655339703e-05 17.98594093322754 2.0155222415924072\n",
            "23366 2.08625647246663e-06 4.372229886939749e-05 Traning Loss: 4.580855602398515e-05 17.986080169677734 2.015463352203369\n",
            "23367 2.092216391247348e-06 4.371006798464805e-05 Traning Loss: 4.580228414852172e-05 17.986215591430664 2.0154037475585938\n",
            "23368 2.0349561964394525e-06 4.374851050670259e-05 Traning Loss: 4.578346852213144e-05 17.98634910583496 2.0153467655181885\n",
            "23369 2.0617644622689113e-06 4.3678443034878e-05 Traning Loss: 4.574020567815751e-05 17.986480712890625 2.0152883529663086\n",
            "23370 1.9947501641581766e-06 4.3682426621671766e-05 Traning Loss: 4.5677177695324644e-05 17.986610412597656 2.0152323246002197\n",
            "23371 2.0086743006686447e-06 4.3598716729320586e-05 Traning Loss: 4.5607390347868204e-05 17.986738204956055 2.0151755809783936\n",
            "23372 1.966633362826542e-06 4.35778965766076e-05 Traning Loss: 4.5544529712060466e-05 17.98686408996582 2.015120029449463\n",
            "23373 1.9529034034349024e-06 4.354533302830532e-05 Traning Loss: 4.5498236431740224e-05 17.98698616027832 2.0150651931762695\n",
            "23374 1.947325017681578e-06 4.3520711187738925e-05 Traning Loss: 4.5468037569662556e-05 17.987106323242188 2.015010356903076\n",
            "23375 1.9091685317107476e-06 4.35380388807971e-05 Traning Loss: 4.544720650301315e-05 17.987224578857422 2.0149571895599365\n",
            "23376 1.9248786884418223e-06 4.350044036982581e-05 Traning Loss: 4.5425320422509685e-05 17.987340927124023 2.0149030685424805\n",
            "23377 1.876137275758083e-06 4.3522773921722546e-05 Traning Loss: 4.5398912334349006e-05 17.987455368041992 2.0148508548736572\n",
            "23378 1.8897676454798784e-06 4.3473413825267926e-05 Traning Loss: 4.536318010650575e-05 17.987567901611328 2.014798164367676\n",
            "23379 1.8504367744753836e-06 4.347284266259521e-05 Traning Loss: 4.532327875494957e-05 17.98767852783203 2.01474666595459\n",
            "23380 1.8468192592990818e-06 4.343670661910437e-05 Traning Loss: 4.528352656052448e-05 17.9877872467041 2.014695167541504\n",
            "23381 1.829256689234171e-06 4.3415198888396844e-05 Traning Loss: 4.5244454668136314e-05 17.98789405822754 2.014644145965576\n",
            "23382 1.8052068071483518e-06 4.340437226346694e-05 Traning Loss: 4.5209577365312725e-05 17.987998962402344 2.0145938396453857\n",
            "23383 1.807330704650667e-06 4.3369152990635484e-05 Traning Loss: 4.517648267210461e-05 17.988101959228516 2.0145435333251953\n",
            "23384 1.7700493799566175e-06 4.3374329834477976e-05 Traning Loss: 4.514437750913203e-05 17.988203048706055 2.0144944190979004\n",
            "23385 1.780994239197753e-06 4.332890239311382e-05 Traning Loss: 4.510989674599841e-05 17.98830223083496 2.0144448280334473\n",
            "23386 1.7426477825210895e-06 4.3332413042662665e-05 Traning Loss: 4.50750594609417e-05 17.988399505615234 2.0143964290618896\n",
            "23387 1.7498305169283412e-06 4.329039802541956e-05 Traning Loss: 4.5040229451842606e-05 17.988494873046875 2.014347791671753\n",
            "23388 1.7217859067386598e-06 4.328430804889649e-05 Traning Loss: 4.5006094296695665e-05 17.988590240478516 2.0142998695373535\n",
            "23389 1.717476607154822e-06 4.325600457377732e-05 Traning Loss: 4.497347981669009e-05 17.988683700561523 2.014252185821533\n",
            "23390 1.7043785192072392e-06 4.3238746002316475e-05 Traning Loss: 4.4943124521523714e-05 17.9887752532959 2.014204740524292\n",
            "23391 1.6873688082341687e-06 4.322620952734724e-05 Traning Loss: 4.4913576857652515e-05 17.98886489868164 2.014158010482788\n",
            "23392 1.6865778889041394e-06 4.319795698393136e-05 Traning Loss: 4.48845348728355e-05 17.98895263671875 2.014111042022705\n",
            "23393 1.660885573073756e-06 4.319578147260472e-05 Traning Loss: 4.485666795517318e-05 17.98904037475586 2.0140650272369385\n",
            "23394 1.666255798227212e-06 4.316040940466337e-05 Traning Loss: 4.4826665543951094e-05 17.989124298095703 2.0140187740325928\n",
            "23395 1.6387634786951821e-06 4.315823025535792e-05 Traning Loss: 4.4796994188800454e-05 17.989208221435547 2.0139734745025635\n",
            "23396 1.642998881834501e-06 4.3122861825395375e-05 Traning Loss: 4.476586036616936e-05 17.989290237426758 2.013928174972534\n",
            "23397 1.6198307548620505e-06 4.3114156142110005e-05 Traning Loss: 4.473398803384043e-05 17.98937225341797 2.013883590698242\n",
            "23398 1.6185010736080585e-06 4.308492862037383e-05 Traning Loss: 4.470342901186086e-05 17.989452362060547 2.01383900642395\n",
            "23399 1.6027613582991762e-06 4.306731352698989e-05 Traning Loss: 4.4670076022157446e-05 17.989530563354492 2.0137948989868164\n",
            "23400 1.5947127849358367e-06 4.30456166213844e-05 Traning Loss: 4.464032826945186e-05 17.989606857299805 2.013751268386841\n",
            "23401 1.5862538020883221e-06 4.3024840124417096e-05 Traning Loss: 4.461109347175807e-05 17.989683151245117 2.0137078762054443\n",
            "23402 1.5728368225609302e-06 4.300836371839978e-05 Traning Loss: 4.4581200199900195e-05 17.989757537841797 2.013665199279785\n",
            "23403 1.56896874159429e-06 4.298290514270775e-05 Traning Loss: 4.455187445273623e-05 17.989830017089844 2.013622522354126\n",
            "23404 1.5530167729593813e-06 4.2969550122506917e-05 Traning Loss: 4.45225668954663e-05 17.98990249633789 2.013580560684204\n",
            "23405 1.5502955648116767e-06 4.2941894207615405e-05 Traning Loss: 4.449218977242708e-05 17.989973068237305 2.0135385990142822\n",
            "23406 1.5348548458860023e-06 4.292873200029135e-05 Traning Loss: 4.446358798304573e-05 17.990041732788086 2.0134973526000977\n",
            "23407 1.5306675322790397e-06 4.290339711587876e-05 Traning Loss: 4.443406578502618e-05 17.990110397338867 2.013456106185913\n",
            "23408 1.5179506362983375e-06 4.288736454327591e-05 Traning Loss: 4.440531483851373e-05 17.990177154541016 2.0134153366088867\n",
            "23409 1.5109044397831894e-06 4.286568218958564e-05 Traning Loss: 4.437658571987413e-05 17.99024200439453 2.0133750438690186\n",
            "23410 1.5018357544249739e-06 4.284563328837976e-05 Traning Loss: 4.434746733750217e-05 17.990306854248047 2.0133349895477295\n",
            "23411 1.491824605182046e-06 4.2826010030694306e-05 Traning Loss: 4.4317836000118405e-05 17.99036979675293 2.0132954120635986\n",
            "23412 1.486014184592932e-06 4.280467692296952e-05 Traning Loss: 4.4290689402259886e-05 17.990432739257812 2.013256072998047\n",
            "23413 1.4738077425135998e-06 4.27888153353706e-05 Traning Loss: 4.4262622395763174e-05 17.990493774414062 2.0132172107696533\n",
            "23414 1.470037091166887e-06 4.276420077076182e-05 Traning Loss: 4.4234238885110244e-05 17.99055290222168 2.0131783485412598\n",
            "23415 1.4570132407243364e-06 4.274958700989373e-05 Traning Loss: 4.420660116011277e-05 17.990612030029297 2.0131399631500244\n",
            "23416 1.453884806323913e-06 4.272540536476299e-05 Traning Loss: 4.417929085320793e-05 17.99066925048828 2.013101816177368\n",
            "23417 1.441293534298893e-06 4.270928184268996e-05 Traning Loss: 4.4150576286483556e-05 17.990726470947266 2.01306414604187\n",
            "23418 1.437717742192035e-06 4.268470365786925e-05 Traning Loss: 4.412242196849547e-05 17.990781784057617 2.013026714324951\n",
            "23419 1.4264177252698573e-06 4.2669147660490125e-05 Traning Loss: 4.409556640894152e-05 17.99083709716797 2.0129895210266113\n",
            "23420 1.421458478034765e-06 4.264603921910748e-05 Traning Loss: 4.406749940244481e-05 17.990890502929688 2.0129525661468506\n",
            "23421 1.4121483218332287e-06 4.262771471985616e-05 Traning Loss: 4.4039861677447334e-05 17.990943908691406 2.012916088104248\n",
            "23422 1.4058099395697354e-06 4.260614878148772e-05 Traning Loss: 4.401195837999694e-05 17.990995407104492 2.0128798484802246\n",
            "23423 1.3983484450363903e-06 4.25870057370048e-05 Traning Loss: 4.398535384098068e-05 17.991046905517578 2.0128438472747803\n",
            "23424 1.3910768075220403e-06 4.256775355315767e-05 Traning Loss: 4.395882933749817e-05 17.99109649658203 2.012808084487915\n",
            "23425 1.3848516573489178e-06 4.254676605341956e-05 Traning Loss: 4.393161725602113e-05 17.991146087646484 2.012772560119629\n",
            "23426 1.3772404372502933e-06 4.252840153640136e-05 Traning Loss: 4.390564208733849e-05 17.991193771362305 2.012737512588501\n",
            "23427 1.3716428384213941e-06 4.2508469050517306e-05 Traning Loss: 4.388011075207032e-05 17.991241455078125 2.012702465057373\n",
            "23428 1.3642410294778529e-06 4.2491246858844534e-05 Traning Loss: 4.3855488911503926e-05 17.991289138793945 2.0126678943634033\n",
            "23429 1.3586721934188972e-06 4.247250035405159e-05 Traning Loss: 4.3831172661157325e-05 17.991334915161133 2.0126335620880127\n",
            "23430 1.3522389963327441e-06 4.245605305186473e-05 Traning Loss: 4.3808293412439525e-05 17.99138069152832 2.012599468231201\n",
            "23431 1.3463011327985441e-06 4.244078081683256e-05 Traning Loss: 4.378708035801537e-05 17.991426467895508 2.0125656127929688\n",
            "23432 1.3417662785286666e-06 4.242637805873528e-05 Traning Loss: 4.376814467832446e-05 17.991470336914062 2.0125322341918945\n",
            "23433 1.3349699656828307e-06 4.242014983901754e-05 Traning Loss: 4.375512071419507e-05 17.991514205932617 2.0124990940093994\n",
            "23434 1.3335007906789542e-06 4.2414729250594974e-05 Traning Loss: 4.374823038233444e-05 17.99155616760254 2.0124661922454834\n",
            "23435 1.325974039900757e-06 4.242670547682792e-05 Traning Loss: 4.3752679630415514e-05 17.99159812927246 2.0124335289001465\n",
            "23436 1.33018727410672e-06 4.244286901666783e-05 Traning Loss: 4.3773055949714035e-05 17.99163818359375 2.0124011039733887\n",
            "23437 1.3232752280600835e-06 4.2495972593314946e-05 Traning Loss: 4.381924736662768e-05 17.991680145263672 2.01236891746521\n",
            "23438 1.338210722678923e-06 4.2568863136693835e-05 Traning Loss: 4.3907075450988486e-05 17.991718292236328 2.0123369693756104\n",
            "23439 1.3359608601604123e-06 4.272275327821262e-05 Traning Loss: 4.405871368362568e-05 17.991758346557617 2.012305498123169\n",
            "23440 1.3735595985053806e-06 4.2944495362462476e-05 Traning Loss: 4.431805427884683e-05 17.99179458618164 2.0122742652893066\n",
            "23441 1.3889865613236907e-06 4.3359206756576896e-05 Traning Loss: 4.4748194341082126e-05 17.99183464050293 2.0122432708740234\n",
            "23442 1.4787439113206347e-06 4.3981130147585645e-05 Traning Loss: 4.545987394521944e-05 17.99186897277832 2.0122125148773193\n",
            "23443 1.5501021835007123e-06 4.5081909775035456e-05 Traning Loss: 4.663201252697036e-05 17.99190902709961 2.0121819972991943\n",
            "23444 1.7697258272164618e-06 4.680129131884314e-05 Traning Loss: 4.857101885136217e-05 17.991939544677734 2.0121514797210693\n",
            "23445 2.0096299522265326e-06 4.9773716455092654e-05 Traning Loss: 5.178334686206654e-05 17.991979598999023 2.0121216773986816\n",
            "23446 2.5732003905432066e-06 5.458413215819746e-05 Traning Loss: 5.7157332776114345e-05 17.99200439453125 2.012091636657715\n",
            "23447 3.313052957309992e-06 6.282116373768076e-05 Traning Loss: 6.613422010559589e-05 17.992046356201172 2.0120623111724854\n",
            "23448 4.830125817534281e-06 7.650753104826435e-05 Traning Loss: 8.133765368256718e-05 17.992061614990234 2.0120325088500977\n",
            "23449 7.054844900267199e-06 9.992402192438021e-05 Traning Loss: 0.00010697887046262622 17.99210548400879 2.0120036602020264\n",
            "23450 1.1313324648654088e-05 0.00013967447739560157 Traning Loss: 0.00015098780568223447 17.992103576660156 2.0119740962982178\n",
            "23451 1.7974565707845613e-05 0.0002076986711472273 Traning Loss: 0.0002256732404930517 17.99214744567871 2.011945962905884\n",
            "23452 3.0299472200567834e-05 0.00032503888360224664 Traning Loss: 0.0003553383576218039 17.99210548400879 2.011916160583496\n",
            "23453 5.015052738599479e-05 0.0005245030042715371 Traning Loss: 0.0005746535025537014 17.992136001586914 2.0118885040283203\n",
            "23454 8.609868382336572e-05 0.000870303250849247 Traning Loss: 0.0009564019273966551 17.99200439453125 2.011857271194458\n",
            "23455 0.0001432581921108067 0.0014401179505512118 Traning Loss: 0.0015833762008696795 17.991968154907227 2.0118298530578613\n",
            "23456 0.0002428941079415381 0.0024016110692173243 Traning Loss: 0.0026445051189512014 17.991615295410156 2.011793851852417\n",
            "23457 0.0003862185694742948 0.0038269751239567995 Traning Loss: 0.0042131938971579075 17.991355895996094 2.011766195297241\n",
            "23458 0.0006060505984351039 0.005935391411185265 Traning Loss: 0.00654144212603569 17.990516662597656 2.011716365814209\n",
            "23459 0.0008218365255743265 0.008092883974313736 Traning Loss: 0.008914720267057419 17.989730834960938 2.011693239212036\n",
            "23460 0.0010115521727129817 0.009804422967135906 Traning Loss: 0.01081597525626421 17.98819923400879 2.0116124153137207\n",
            "23461 0.0009029944194480777 0.008806309662759304 Traning Loss: 0.009709304198622704 17.98683738708496 2.011620044708252\n",
            "23462 0.0005906252772547305 0.0054693883284926414 Traning Loss: 0.006060013547539711 17.985143661499023 2.01151442527771\n",
            "23463 0.00018207747780252248 0.0015543308109045029 Traning Loss: 0.001736408332362771 17.983869552612305 2.011592388153076\n",
            "23464 8.21101784822531e-05 0.0003816372191067785 Traning Loss: 0.000463747390313074 17.982789993286133 2.011537790298462\n",
            "23465 0.00025571841979399323 0.002157987793907523 Traning Loss: 0.0024137061554938555 17.981670379638672 2.0116536617279053\n",
            "23466 0.00042631535325199366 0.0037499486934393644 Traning Loss: 0.00417626416310668 17.98069953918457 2.011725425720215\n",
            "23467 0.00035913067404180765 0.0029708649963140488 Traning Loss: 0.0033299955539405346 17.979429244995117 2.0118050575256348\n",
            "23468 0.0001425010123057291 0.0009234515018761158 Traning Loss: 0.0010659524705260992 17.978336334228516 2.0120370388031006\n",
            "23469 0.00011973008804488927 0.0004455869202502072 Traning Loss: 0.0005653169937431812 17.977237701416016 2.0121238231658936\n",
            "23470 0.00021603744244202971 0.0016887651290744543 Traning Loss: 0.001904802629724145 17.97603416442871 2.012432813644409\n",
            "23471 0.00029044575057923794 0.0021385811269283295 Traning Loss: 0.0024290268775075674 17.974929809570312 2.0126113891601562\n",
            "23472 0.0001680163259152323 0.0011069148313254118 Traning Loss: 0.0012749311281368136 17.97374725341797 2.012831449508667\n",
            "23473 7.464567897841334e-05 0.00019439826428424567 Traning Loss: 0.00026904395781457424 17.972702026367188 2.0131547451019287\n",
            "23474 0.0001367897493764758 0.000678137643262744 Traning Loss: 0.0008149273926392198 17.971717834472656 2.0133347511291504\n",
            "23475 0.00018604144861456007 0.0014698042068630457 Traning Loss: 0.001655845670029521 17.970693588256836 2.0137455463409424\n",
            "23476 0.00016489101108163595 0.0010649105533957481 Traning Loss: 0.001229801564477384 17.969785690307617 2.0140223503112793\n",
            "23477 6.078010846977122e-05 0.00022166651615407318 Traning Loss: 0.0002824466209858656 17.968944549560547 2.0143418312072754\n",
            "23478 4.97599394293502e-05 0.00018117365834768862 Traning Loss: 0.0002309335977770388 17.96820068359375 2.0147416591644287\n",
            "23479 0.00011980780982412398 0.0007765733753331006 Traning Loss: 0.000896381214261055 17.967552185058594 2.0149991512298584\n",
            "23480 0.00011363090015947819 0.0009429900092072785 Traning Loss: 0.0010566208511590958 17.966890335083008 2.015418767929077\n",
            "23481 6.945333734620363e-05 0.0003920206509064883 Traning Loss: 0.00046147400280460715 17.96632957458496 2.0157012939453125\n",
            "23482 2.4168495656340383e-05 6.124462379375473e-05 Traning Loss: 8.541312126908451e-05 17.965858459472656 2.0159871578216553\n",
            "23483 4.517808702075854e-05 0.00032140917028300464 Traning Loss: 0.00036658725002780557 17.965383529663086 2.0163004398345947\n",
            "23484 7.857547461753711e-05 0.000608620117418468 Traning Loss: 0.0006871955702081323 17.964998245239258 2.0164968967437744\n",
            "23485 5.140829671290703e-05 0.0004666258755605668 Traning Loss: 0.0005180341540835798 17.964611053466797 2.016777515411377\n",
            "23486 2.4112257960950956e-05 0.00012783007696270943 Traning Loss: 0.0001519423385616392 17.96428871154785 2.016955852508545\n",
            "23487 1.5790468751220033e-05 0.0001126388378906995 Traning Loss: 0.00012842931027989835 17.964059829711914 2.0171210765838623\n",
            "23488 3.7788959161844105e-05 0.00034217259963043034 Traning Loss: 0.00037996156606823206 17.96381187438965 2.0172975063323975\n",
            "23489 4.5452488848241046e-05 0.00041041491203941405 Traning Loss: 0.0004558674118015915 17.963661193847656 2.017408847808838\n",
            "23490 2.333038537472021e-05 0.00022531177091877908 Traning Loss: 0.00024864214356057346 17.963520050048828 2.017542600631714\n",
            "23491 7.90223384683486e-06 5.8255973272025585e-05 Traning Loss: 6.615820893784985e-05 17.963457107543945 2.0176305770874023\n",
            "23492 1.312878248427296e-05 0.00011969330080319196 Traning Loss: 0.00013282208237797022 17.963489532470703 2.0176801681518555\n",
            "23493 2.6760606488096528e-05 0.00026511435862630606 Traning Loss: 0.00029187495238147676 17.96350860595703 2.017739772796631\n",
            "23494 2.7611469704424962e-05 0.00026438667555339634 Traning Loss: 0.0002919981488958001 17.96361541748047 2.017759084701538\n",
            "23495 1.4219080185284838e-05 0.00013387302169576287 Traning Loss: 0.00014809210551902652 17.96371841430664 2.0177717208862305\n",
            "23496 5.5716641327308025e-06 5.490173498401418e-05 Traning Loss: 6.047339775250293e-05 17.96385955810547 2.017784357070923\n",
            "23497 1.2658403647947125e-05 0.00010459373879712075 Traning Loss: 0.00011725214426405728 17.964035034179688 2.01774525642395\n",
            "23498 1.7811435100156814e-05 0.0001824345818022266 Traning Loss: 0.00020024602417834103 17.96419334411621 2.017747402191162\n",
            "23499 1.7876800484373234e-05 0.00016294604574795812 Traning Loss: 0.00018082284077536315 17.964397430419922 2.0177114009857178\n",
            "23500 9.190798664349131e-06 8.503025310346857e-05 Traning Loss: 9.42210535868071e-05 17.964597702026367 2.01767635345459\n",
            "23501 5.238655830908101e-06 5.201252133701928e-05 Traning Loss: 5.725117807742208e-05 17.964832305908203 2.017653703689575\n",
            "23502 1.106243780668592e-05 8.995374810183421e-05 Traning Loss: 0.00010101618681801483 17.965091705322266 2.0175812244415283\n",
            "23503 1.2834742847189773e-05 0.00013266706082504243 Traning Loss: 0.0001455018064007163 17.965349197387695 2.017561435699463\n",
            "23504 1.2795486327377148e-05 0.00011085481673944741 Traning Loss: 0.00012365030124783516 17.965633392333984 2.0174970626831055\n",
            "23505 6.3248762671719305e-06 6.263229442993179e-05 Traning Loss: 6.895716796861961e-05 17.965927124023438 2.017453670501709\n",
            "23506 5.055906513007358e-06 4.6221557568060234e-05 Traning Loss: 5.127746408106759e-05 17.966230392456055 2.0174100399017334\n",
            "23507 8.296896339743398e-06 7.252937939483672e-05 Traning Loss: 8.082627755356953e-05 17.966554641723633 2.017340898513794\n",
            "23508 9.344398677058052e-06 9.684741235105321e-05 Traning Loss: 0.00010619180829962716 17.966869354248047 2.0173144340515137\n",
            "23509 9.529358976578806e-06 8.211312524508685e-05 Traning Loss: 9.164248331217095e-05 17.967193603515625 2.017254114151001\n",
            "23510 5.375278306019027e-06 5.5324548156931996e-05 Traning Loss: 6.069982555345632e-05 17.9675235748291 2.0172202587127686\n",
            "23511 5.5043951761035714e-06 4.747293496620841e-05 Traning Loss: 5.297733150655404e-05 17.967851638793945 2.017179250717163\n",
            "23512 6.667712113994639e-06 6.397036486305296e-05 Traning Loss: 7.06380742485635e-05 17.96820068359375 2.017133951187134\n",
            "23513 7.665721568628214e-06 7.533264579251409e-05 Traning Loss: 8.29983691801317e-05 17.968538284301758 2.0171093940734863\n",
            "23514 6.972794835746754e-06 6.515315180877224e-05 Traning Loss: 7.21259493730031e-05 17.968894958496094 2.0170722007751465\n",
            "23515 4.941858151141787e-06 4.884285226580687e-05 Traning Loss: 5.378471178119071e-05 17.969253540039062 2.017047882080078\n",
            "23516 4.936789991916157e-06 4.555621853796765e-05 Traning Loss: 5.0493006710894406e-05 17.969608306884766 2.017024040222168\n",
            "23517 5.822677849209867e-06 5.614305700873956e-05 Traning Loss: 6.196573667693883e-05 17.969974517822266 2.0169997215270996\n",
            "23518 6.580530225619441e-06 6.31736038485542e-05 Traning Loss: 6.975413270993158e-05 17.970319747924805 2.016988754272461\n",
            "23519 5.903827059228206e-06 5.759361738455482e-05 Traning Loss: 6.349744217004627e-05 17.970672607421875 2.0169756412506104\n",
            "23520 4.99428733746754e-06 4.7230420022970065e-05 Traning Loss: 5.2224706450942904e-05 17.97101402282715 2.0169625282287598\n",
            "23521 4.327740953158354e-06 4.486605757847428e-05 Traning Loss: 4.9193797167390585e-05 17.971355438232422 2.016958236694336\n",
            "23522 5.169190899323439e-06 4.99230191053357e-05 Traning Loss: 5.5092208640417084e-05 17.971696853637695 2.0169413089752197\n",
            "23523 5.099814188724849e-06 5.470335963764228e-05 Traning Loss: 5.980317291687243e-05 17.972026824951172 2.016939878463745\n",
            "23524 5.004690137866419e-06 5.175723345018923e-05 Traning Loss: 5.6761924497550353e-05 17.972362518310547 2.016927480697632\n",
            "23525 4.207950041745789e-06 4.600477041094564e-05 Traning Loss: 5.021272227168083e-05 17.97269058227539 2.016915798187256\n",
            "23526 3.7302797863958403e-06 4.411365807754919e-05 Traning Loss: 4.784393968293443e-05 17.973020553588867 2.0169076919555664\n",
            "23527 4.233032996125985e-06 4.6811783249722794e-05 Traning Loss: 5.104481533635408e-05 17.973352432250977 2.016885995864868\n",
            "23528 4.008985797554487e-06 5.044644422014244e-05 Traning Loss: 5.445543138193898e-05 17.973678588867188 2.016878843307495\n",
            "23529 4.321658707340248e-06 4.917084515909664e-05 Traning Loss: 5.3492505685426295e-05 17.974008560180664 2.0168538093566895\n",
            "23530 3.5184632452001097e-06 4.6010940423002467e-05 Traning Loss: 4.95294043503236e-05 17.974336624145508 2.0168356895446777\n",
            "23531 3.494837756079505e-06 4.357405850896612e-05 Traning Loss: 4.7068897401914e-05 17.974660873413086 2.0168097019195557\n",
            "23532 3.435384087424609e-06 4.4635926315095276e-05 Traning Loss: 4.807130972039886e-05 17.974985122680664 2.0167789459228516\n",
            "23533 3.55521910933021e-06 4.6695433411514387e-05 Traning Loss: 5.025065183872357e-05 17.975299835205078 2.016751289367676\n",
            "23534 3.6811554764426546e-06 4.6837802074151114e-05 Traning Loss: 5.051895641372539e-05 17.975614547729492 2.016711473464966\n",
            "23535 3.2894895412027836e-06 4.530641308519989e-05 Traning Loss: 4.8595902626402676e-05 17.975923538208008 2.016674280166626\n",
            "23536 3.2397658742411295e-06 4.3479307350935414e-05 Traning Loss: 4.671907299780287e-05 17.976228713989258 2.016629219055176\n",
            "23537 3.0690023322677007e-06 4.362023537396453e-05 Traning Loss: 4.668923793360591e-05 17.976530075073242 2.016583204269409\n",
            "23538 3.220961389160948e-06 4.473646549740806e-05 Traning Loss: 4.795742643182166e-05 17.976821899414062 2.016536235809326\n",
            "23539 3.2093271329358686e-06 4.547350908978842e-05 Traning Loss: 4.868283576797694e-05 17.977115631103516 2.0164847373962402\n",
            "23540 3.1390684398502344e-06 4.483055818127468e-05 Traning Loss: 4.7969628212740645e-05 17.977399826049805 2.016432523727417\n",
            "23541 2.918228574344539e-06 4.370705210021697e-05 Traning Loss: 4.662528226617724e-05 17.977684020996094 2.0163791179656982\n",
            "23542 2.919548478530487e-06 4.305625770939514e-05 Traning Loss: 4.5975804823683575e-05 17.977964401245117 2.016322135925293\n",
            "23543 2.81625943898689e-06 4.353900294518098e-05 Traning Loss: 4.6355264203157276e-05 17.978240966796875 2.016268730163574\n",
            "23544 3.0073483685555402e-06 4.396097574499436e-05 Traning Loss: 4.696832547779195e-05 17.978515625 2.0162084102630615\n",
            "23545 2.870758407880203e-06 4.407862070365809e-05 Traning Loss: 4.6949378884164616e-05 17.978784561157227 2.0161521434783936\n",
            "23546 2.889055394916795e-06 4.3383111915318295e-05 Traning Loss: 4.627216549124569e-05 17.979049682617188 2.016092300415039\n",
            "23547 2.7793530534836464e-06 4.283034650143236e-05 Traning Loss: 4.560970046441071e-05 17.979307174682617 2.016033172607422\n",
            "23548 2.7292649065202568e-06 4.277930565876886e-05 Traning Loss: 4.550857192953117e-05 17.97955894470215 2.0159759521484375\n",
            "23549 2.8332108286122093e-06 4.301357330405153e-05 Traning Loss: 4.584678390529007e-05 17.979806900024414 2.0159144401550293\n",
            "23550 2.713317599045695e-06 4.3383348383940756e-05 Traning Loss: 4.6096665755612776e-05 17.98004913330078 2.015857696533203\n",
            "23551 2.8122799449192826e-06 4.3112166167702526e-05 Traning Loss: 4.592444747686386e-05 17.98028564453125 2.015796422958374\n",
            "23552 2.620338818815071e-06 4.2869542085099965e-05 Traning Loss: 4.5489879994420335e-05 17.980518341064453 2.015739679336548\n",
            "23553 2.6700206490204437e-06 4.250552228768356e-05 Traning Loss: 4.517554407357238e-05 17.980745315551758 2.0156807899475098\n",
            "23554 2.574885684225592e-06 4.2604075133567676e-05 Traning Loss: 4.517896013567224e-05 17.98097038269043 2.015623092651367\n",
            "23555 2.6061959488288267e-06 4.2739549826364964e-05 Traning Loss: 4.534574691206217e-05 17.981189727783203 2.015565872192383\n",
            "23556 2.565484692240716e-06 4.2833653424168006e-05 Traning Loss: 4.5399137889035046e-05 17.981409072875977 2.0155086517333984\n",
            "23557 2.5457886749791214e-06 4.2678955651354045e-05 Traning Loss: 4.522474409895949e-05 17.98162269592285 2.0154528617858887\n",
            "23558 2.487094661773881e-06 4.246290336595848e-05 Traning Loss: 4.494999666349031e-05 17.981834411621094 2.015397787094116\n",
            "23559 2.48689661930257e-06 4.2290561395930126e-05 Traning Loss: 4.477745824260637e-05 17.98204231262207 2.0153422355651855\n",
            "23560 2.4244711767096305e-06 4.234947482473217e-05 Traning Loss: 4.477394759305753e-05 17.98224449157715 2.015289068222046\n",
            "23561 2.4635583031340502e-06 4.237666507833637e-05 Traning Loss: 4.4840224290965125e-05 17.982444763183594 2.0152342319488525\n",
            "23562 2.3873190002632327e-06 4.2449089960427955e-05 Traning Loss: 4.4836408051196486e-05 17.98263931274414 2.0151824951171875\n",
            "23563 2.4021649096539477e-06 4.231307320878841e-05 Traning Loss: 4.471523789106868e-05 17.982830047607422 2.0151290893554688\n",
            "23564 2.333493739570258e-06 4.222009010845795e-05 Traning Loss: 4.455358430277556e-05 17.983016967773438 2.0150771141052246\n",
            "23565 2.3098552901501535e-06 4.213616921333596e-05 Traning Loss: 4.444602382136509e-05 17.983200073242188 2.0150253772735596\n",
            "23566 2.2922267817193642e-06 4.213409556541592e-05 Traning Loss: 4.442632052814588e-05 17.983381271362305 2.0149734020233154\n",
            "23567 2.24215682465001e-06 4.220251503284089e-05 Traning Loss: 4.444467049324885e-05 17.983558654785156 2.014923572540283\n",
            "23568 2.25234998652013e-06 4.2175579437753186e-05 Traning Loss: 4.4427928514778614e-05 17.983734130859375 2.0148723125457764\n",
            "23569 2.179691591663868e-06 4.217170499032363e-05 Traning Loss: 4.4351396354613826e-05 17.983905792236328 2.0148229598999023\n",
            "23570 2.183500555474893e-06 4.206430821795948e-05 Traning Loss: 4.4247808546060696e-05 17.98407554626465 2.014772653579712\n",
            "23571 2.122961404893431e-06 4.204430297249928e-05 Traning Loss: 4.4167263695271686e-05 17.984241485595703 2.014723539352417\n",
            "23572 2.122566911566537e-06 4.200928015052341e-05 Traning Loss: 4.413184797158465e-05 17.984403610229492 2.014674186706543\n",
            "23573 2.0855832190136425e-06 4.203644857625477e-05 Traning Loss: 4.4122032704763114e-05 17.98456573486328 2.014625310897827\n",
            "23574 2.07873085855681e-06 4.20234864577651e-05 Traning Loss: 4.4102216634200886e-05 17.984724044799805 2.0145764350891113\n",
            "23575 2.0388279153849e-06 4.2015715735033154e-05 Traning Loss: 4.4054544559912756e-05 17.984880447387695 2.0145277976989746\n",
            "23576 2.0357451830932405e-06 4.1950443119276315e-05 Traning Loss: 4.39861869381275e-05 17.98503303527832 2.014478921890259\n",
            "23577 1.981617742785602e-06 4.194161738269031e-05 Traning Loss: 4.392323535284959e-05 17.985183715820312 2.0144312381744385\n",
            "23578 2.0024294826725964e-06 4.188063758192584e-05 Traning Loss: 4.388306842884049e-05 17.98533058166504 2.0143821239471436\n",
            "23579 1.9313533812237438e-06 4.1930779843823984e-05 Traning Loss: 4.3862131860805675e-05 17.985475540161133 2.0143353939056396\n",
            "23580 1.9754297682084143e-06 4.187105514574796e-05 Traning Loss: 4.3846484913956374e-05 17.985618591308594 2.0142862796783447\n",
            "23581 1.8868402094085468e-06 4.193405038677156e-05 Traning Loss: 4.3820891733048484e-05 17.985759735107422 2.014240264892578\n",
            "23582 1.9445992620603647e-06 4.1842176869977266e-05 Traning Loss: 4.378677476779558e-05 17.985897064208984 2.0141913890838623\n",
            "23583 1.845122369559249e-06 4.1907238482963294e-05 Traning Loss: 4.375235948828049e-05 17.986034393310547 2.014145851135254\n",
            "23584 1.9156034340994665e-06 4.1818497265921906e-05 Traning Loss: 4.37341004726477e-05 17.986167907714844 2.014097213745117\n",
            "23585 1.8086612953993608e-06 4.192624328425154e-05 Traning Loss: 4.3734904465964064e-05 17.98630142211914 2.014052152633667\n",
            "23586 1.9020517356693745e-06 4.185810394119471e-05 Traning Loss: 4.376015567686409e-05 17.98642921447754 2.0140037536621094\n",
            "23587 1.7761304889063467e-06 4.202751733828336e-05 Traning Loss: 4.380364771350287e-05 17.986557006835938 2.0139596462249756\n",
            "23588 1.9088120097876526e-06 4.19638745370321e-05 Traning Loss: 4.387268563732505e-05 17.98668098449707 2.0139105319976807\n",
            "23589 1.7444621107642888e-06 4.2240240873070434e-05 Traning Loss: 4.398470264277421e-05 17.986804962158203 2.0138680934906006\n",
            "23590 1.947318878592341e-06 4.221713243168779e-05 Traning Loss: 4.4164451537653804e-05 17.986923217773438 2.0138182640075684\n",
            "23591 1.7297584236075636e-06 4.2724524973891675e-05 Traning Loss: 4.4454282033257186e-05 17.987045288085938 2.0137779712677\n",
            "23592 2.049584281849093e-06 4.285760223865509e-05 Traning Loss: 4.4907184928888455e-05 17.987157821655273 2.0137264728546143\n",
            "23593 1.7677796222415054e-06 4.383523264550604e-05 Traning Loss: 4.560301385936327e-05 17.987276077270508 2.0136890411376953\n",
            "23594 2.277751036672271e-06 4.4383934437064454e-05 Traning Loss: 4.6661683882121e-05 17.987382888793945 2.0136349201202393\n",
            "23595 1.9361302747711306e-06 4.635105869965628e-05 Traning Loss: 4.828718738281168e-05 17.987499237060547 2.013601779937744\n",
            "23596 2.784586285997648e-06 4.800639726454392e-05 Traning Loss: 5.079098264104687e-05 17.987600326538086 2.0135436058044434\n",
            "23597 2.4388878046011087e-06 5.224203414400108e-05 Traning Loss: 5.468092058436014e-05 17.987714767456055 2.013516664505005\n",
            "23598 3.94617154597654e-06 5.680482354364358e-05 Traning Loss: 6.0750993725378066e-05 17.98780632019043 2.0134520530700684\n",
            "23599 3.809175950664212e-06 6.64943945594132e-05 Traning Loss: 7.030356937320903e-05 17.9879207611084 2.013434886932373\n",
            "23600 6.705630312353605e-06 7.869752880651504e-05 Traning Loss: 8.54031604831107e-05 17.988000869750977 2.013359308242798\n",
            "23601 7.459931111952756e-06 0.00010201006807619706 Traning Loss: 0.00010946999827865511 17.988117218017578 2.0133581161499023\n",
            "23602 1.349608282907866e-05 0.00013443634088616818 Traning Loss: 0.00014793242735322565 17.988176345825195 2.0132648944854736\n",
            "23603 1.7159636627184227e-05 0.0001927166449604556 Traning Loss: 0.00020987627794966102 17.988290786743164 2.013289451599121\n",
            "23604 3.066430508624762e-05 0.00027874953229911625 Traning Loss: 0.00030941382283344865 17.988313674926758 2.0131678581237793\n",
            "23605 4.28362654929515e-05 0.0004272193182259798 Traning Loss: 0.0004700555873569101 17.988420486450195 2.013235092163086\n",
            "23606 7.432350685121492e-05 0.0006514144479297101 Traning Loss: 0.0007257379475049675 17.98836898803711 2.0130696296691895\n",
            "23607 0.00010879521869355813 0.001021338626742363 Traning Loss: 0.001130133867263794 17.98843765258789 2.0132052898406982\n",
            "23608 0.00018021537107415497 0.0015631370479241014 Traning Loss: 0.0017433523898944259 17.988237380981445 2.012979745864868\n",
            "23609 0.00026033990434370935 0.0023763137869536877 Traning Loss: 0.0026366536039859056 17.98818588256836 2.013216018676758\n",
            "23610 0.00039294574526138604 0.003398679196834564 Traning Loss: 0.00379162491299212 17.98769760131836 2.0129337310791016\n",
            "23611 0.0005047659506089985 0.0045591434463858604 Traning Loss: 0.005063909571617842 17.98736572265625 2.0132806301116943\n",
            "23612 0.0006183788063935935 0.0053216563537716866 Traning Loss: 0.005940034985542297 17.986480712890625 2.0130136013031006\n",
            "23613 0.0005755801685154438 0.005221140570938587 Traning Loss: 0.005796720739454031 17.985780715942383 2.013394832611084\n",
            "23614 0.00045036751544103026 0.003803990548476577 Traning Loss: 0.004254357889294624 17.984703063964844 2.0132973194122314\n",
            "23615 0.00019029155373573303 0.001785428379662335 Traning Loss: 0.001975719816982746 17.983884811401367 2.013552665710449\n",
            "23616 4.438346149981953e-05 0.0003129681572318077 Traning Loss: 0.00035735161509364843 17.98307991027832 2.0137271881103516\n",
            "23617 4.184376302873716e-05 0.0002463500131852925 Traning Loss: 0.00028819378348998725 17.982341766357422 2.013787031173706\n",
            "23618 0.00013179810775909573 0.0012134588323533535 Traning Loss: 0.001345256925560534 17.981788635253906 2.0141618251800537\n",
            "23619 0.00025597107014618814 0.0019880777690559626 Traning Loss: 0.0022440487518906593 17.980974197387695 2.014186143875122\n",
            "23620 0.0002049437171081081 0.0018713014433160424 Traning Loss: 0.0020762451458722353 17.980327606201172 2.0145668983459473\n",
            "23621 0.00012955362035427243 0.0009446940966881812 Traning Loss: 0.0010742477606981993 17.979516983032227 2.014760732650757\n",
            "23622 3.644769822130911e-05 0.00022854923736304045 Traning Loss: 0.00026499692467041314 17.978797912597656 2.014986276626587\n",
            "23623 3.7688125303247944e-05 0.00027477569528855383 Traning Loss: 0.00031246381695382297 17.97815704345703 2.0153427124023438\n",
            "23624 0.00011287445522611961 0.000762441020924598 Traning Loss: 0.0008753154543228447 17.977432250976562 2.0154685974121094\n",
            "23625 0.00012147076631663367 0.0010589779121801257 Traning Loss: 0.001180448685772717 17.976869583129883 2.0158517360687256\n",
            "23626 0.00010830083192558959 0.0007883123471401632 Traning Loss: 0.0008966131717897952 17.976194381713867 2.016051769256592\n",
            "23627 4.683881707023829e-05 0.0003475789853837341 Traning Loss: 0.00039441778790205717 17.97566795349121 2.016308546066284\n",
            "23628 2.7436397431301884e-05 0.00017147528706118464 Traning Loss: 0.00019891168631147593 17.975200653076172 2.016610622406006\n",
            "23629 5.229482121649198e-05 0.0003240897785872221 Traning Loss: 0.00037638458888977766 17.97474479675293 2.0167436599731445\n",
            "23630 6.38202836853452e-05 0.0005327276885509491 Traning Loss: 0.0005965479649603367 17.97443389892578 2.0170493125915527\n",
            "23631 6.973648123675957e-05 0.0005056551308371127 Traning Loss: 0.0005753916339017451 17.974061965942383 2.0171899795532227\n",
            "23632 4.009783151559532e-05 0.00032793328864499927 Traning Loss: 0.0003680311201605946 17.973812103271484 2.017380714416504\n",
            "23633 2.4713128368603066e-05 0.00018845278827939183 Traning Loss: 0.00021316591301001608 17.973560333251953 2.017566442489624\n",
            "23634 2.9298054869286716e-05 0.000189513637451455 Traning Loss: 0.0002188116923207417 17.97333335876465 2.017634391784668\n",
            "23635 2.975756797241047e-05 0.0002689664834178984 Traning Loss: 0.00029872404411435127 17.973169326782227 2.017820119857788\n",
            "23636 3.9523769373772666e-05 0.00027910343487747014 Traning Loss: 0.0003186271933373064 17.97295570373535 2.017866373062134\n",
            "23637 2.5320041459053755e-05 0.00023610876814927906 Traning Loss: 0.00026142882416024804 17.97283935546875 2.017967939376831\n",
            "23638 2.0785579181392677e-05 0.00018005252059083432 Traning Loss: 0.0002008380979532376 17.972713470458984 2.0180375576019287\n",
            "23639 2.013326775340829e-05 0.00016523886006325483 Traning Loss: 0.00018537213327363133 17.972644805908203 2.018033504486084\n",
            "23640 1.6446521840407513e-05 0.000178634247276932 Traning Loss: 0.0001950807636603713 17.972620010375977 2.0181095600128174\n",
            "23641 2.2042140699340962e-05 0.00016137800412252545 Traning Loss: 0.0001834201393648982 17.97259521484375 2.0180768966674805\n",
            "23642 1.2472592970880214e-05 0.00013822033361066133 Traning Loss: 0.00015069292567204684 17.972639083862305 2.0181076526641846\n",
            "23643 1.3659391697729006e-05 0.00011725888180080801 Traning Loss: 0.0001309182698605582 17.972667694091797 2.018096685409546\n",
            "23644 1.3231142474978697e-05 0.000128899744595401 Traning Loss: 0.00014213088434189558 17.972753524780273 2.018057346343994\n",
            "23645 1.3018108802498318e-05 0.0001481331419199705 Traning Loss: 0.00016115125617943704 17.972843170166016 2.018069267272949\n",
            "23646 1.6347670680261217e-05 0.00013481304631568491 Traning Loss: 0.00015116071153897792 17.972946166992188 2.018001079559326\n",
            "23647 8.498098395648412e-06 0.00010339340951759368 Traning Loss: 0.0001118915097322315 17.973081588745117 2.0179951190948486\n",
            "23648 8.391911251237616e-06 6.79424629197456e-05 Traning Loss: 7.63343705330044e-05 17.973209381103516 2.0179409980773926\n",
            "23649 6.361227860907093e-06 6.967601802898571e-05 Traning Loss: 7.603724952787161e-05 17.973369598388672 2.0178892612457275\n",
            "23650 8.735178198548965e-06 9.576805314281955e-05 Traning Loss: 0.00010450323316035792 17.973522186279297 2.0178639888763428\n",
            "23651 1.2298583897063509e-05 0.00011334559530951083 Traning Loss: 0.00012564417556859553 17.973697662353516 2.017787456512451\n",
            "23652 8.987168257590383e-06 0.00010531498264754191 Traning Loss: 0.0001143021509051323 17.97388458251953 2.017763376235962\n",
            "23653 7.876177733123768e-06 7.088459096848965e-05 Traning Loss: 7.876077143009752e-05 17.974082946777344 2.017692804336548\n",
            "23654 3.7252202673698775e-06 4.796554276254028e-05 Traning Loss: 5.169076393940486e-05 17.974302291870117 2.017643451690674\n",
            "23655 4.345020442997338e-06 4.952249219059013e-05 Traning Loss: 5.3867512178840116e-05 17.97452163696289 2.017601251602173\n",
            "23656 6.9511438596236985e-06 6.938569276826456e-05 Traning Loss: 7.633683708263561e-05 17.974754333496094 2.0175321102142334\n",
            "23657 7.333890607696958e-06 8.566747419536114e-05 Traning Loss: 9.300136298406869e-05 17.974987030029297 2.0175023078918457\n",
            "23658 8.211531167034991e-06 7.873986760387197e-05 Traning Loss: 8.695139695191756e-05 17.975231170654297 2.0174310207366943\n",
            "23659 4.895609436061932e-06 6.0524795117089525e-05 Traning Loss: 6.542040500789881e-05 17.97547721862793 2.0173866748809814\n",
            "23660 3.872874458465958e-06 4.410641486174427e-05 Traning Loss: 4.797929068445228e-05 17.97572898864746 2.017333507537842\n",
            "23661 3.8863827285240404e-06 4.343325781519525e-05 Traning Loss: 4.731964145321399e-05 17.975982666015625 2.017270088195801\n",
            "23662 4.2909828152915e-06 5.4733172873966396e-05 Traning Loss: 5.902415432501584e-05 17.97623634338379 2.017230749130249\n",
            "23663 6.247235432965681e-06 6.276962085394189e-05 Traning Loss: 6.901685264892876e-05 17.976491928100586 2.0171597003936768\n",
            "23664 5.010346285416745e-06 6.311994366114959e-05 Traning Loss: 6.813029176555574e-05 17.976743698120117 2.0171172618865967\n",
            "23665 4.775021807290614e-06 5.374480315367691e-05 Traning Loss: 5.8519824960967526e-05 17.977001190185547 2.0170583724975586\n",
            "23666 3.627776550274575e-06 4.5780565415043384e-05 Traning Loss: 4.940834332956001e-05 17.97725486755371 2.017003297805786\n",
            "23667 3.1221402423398104e-06 4.4050881115254015e-05 Traning Loss: 4.717301999335177e-05 17.977516174316406 2.016958713531494\n",
            "23668 4.1463454181212e-06 4.669950430979952e-05 Traning Loss: 5.0845850637415424e-05 17.977773666381836 2.0168933868408203\n",
            "23669 3.6388885291671613e-06 5.133301965543069e-05 Traning Loss: 5.4971907957224175e-05 17.978031158447266 2.016852378845215\n",
            "23670 4.396993062982801e-06 5.103212970425375e-05 Traning Loss: 5.542912185774185e-05 17.978288650512695 2.016791582107544\n",
            "23671 3.5860255138686625e-06 4.896109749097377e-05 Traning Loss: 5.254712232272141e-05 17.978538513183594 2.0167434215545654\n",
            "23672 3.3474000247224467e-06 4.595569771481678e-05 Traning Loss: 4.930309660267085e-05 17.978792190551758 2.016693115234375\n",
            "23673 3.481170551822288e-06 4.450981941772625e-05 Traning Loss: 4.799099042429589e-05 17.979034423828125 2.0166337490081787\n",
            "23674 2.9250863917695824e-06 4.549124059849419e-05 Traning Loss: 4.841632835450582e-05 17.97928237915039 2.0165903568267822\n",
            "23675 3.609492068790132e-06 4.5345361286308616e-05 Traning Loss: 4.89548547193408e-05 17.97951889038086 2.016528844833374\n",
            "23676 2.896615796998958e-06 4.5629894884768873e-05 Traning Loss: 4.852651181863621e-05 17.979755401611328 2.0164833068847656\n",
            "23677 3.1367287647299236e-06 4.432645800989121e-05 Traning Loss: 4.746318518300541e-05 17.979990005493164 2.0164268016815186\n",
            "23678 2.8946742531843483e-06 4.389651076053269e-05 Traning Loss: 4.679118501371704e-05 17.98021697998047 2.0163722038269043\n",
            "23679 2.696421915970859e-06 4.426691884873435e-05 Traning Loss: 4.6963341446826234e-05 17.980449676513672 2.016322612762451\n",
            "23680 3.104529696429381e-06 4.43467652075924e-05 Traning Loss: 4.745129626826383e-05 17.980674743652344 2.016261339187622\n",
            "23681 2.5547506083967164e-06 4.482559597818181e-05 Traning Loss: 4.738034476758912e-05 17.980905532836914 2.0162129402160645\n",
            "23682 2.943426579804509e-06 4.350597009761259e-05 Traning Loss: 4.6449396904790774e-05 17.981130599975586 2.0161499977111816\n",
            "23683 2.3922075342852622e-06 4.2805222619790584e-05 Traning Loss: 4.5197430154075846e-05 17.981355667114258 2.0160956382751465\n",
            "23684 2.4960882001323625e-06 4.197500311420299e-05 Traning Loss: 4.447108949534595e-05 17.981578826904297 2.0160367488861084\n",
            "23685 2.5035362796188565e-06 4.217012246954255e-05 Traning Loss: 4.467365943128243e-05 17.981794357299805 2.0159752368927\n",
            "23686 2.3462043827748857e-06 4.310475196689367e-05 Traning Loss: 4.545095725916326e-05 17.982011795043945 2.0159196853637695\n",
            "23687 2.7325156679580687e-06 4.331219315645285e-05 Traning Loss: 4.604470814228989e-05 17.982219696044922 2.015852928161621\n",
            "23688 2.287344614160247e-06 4.3629028368741274e-05 Traning Loss: 4.591637116391212e-05 17.982431411743164 2.0157957077026367\n",
            "23689 2.5557862954883603e-06 4.256812098901719e-05 Traning Loss: 4.512390660238452e-05 17.982635498046875 2.0157291889190674\n",
            "23690 2.1677135464415187e-06 4.202091076876968e-05 Traning Loss: 4.418862590682693e-05 17.982839584350586 2.015667676925659\n",
            "23691 2.218239615103812e-06 4.1442141082370654e-05 Traning Loss: 4.366038047010079e-05 17.98303985595703 2.015604257583618\n",
            "23692 2.2303638615994714e-06 4.149514643358998e-05 Traning Loss: 4.372551120468415e-05 17.98323631286621 2.015537738800049\n",
            "23693 2.0945451524312375e-06 4.205678851576522e-05 Traning Loss: 4.415133298607543e-05 17.983434677124023 2.015476703643799\n",
            "23694 2.3581744699185947e-06 4.216752131469548e-05 Traning Loss: 4.45256955572404e-05 17.983625411987305 2.015408515930176\n",
            "23695 2.0751454030687455e-06 4.248689583619125e-05 Traning Loss: 4.456204260350205e-05 17.98381996154785 2.015347957611084\n",
            "23696 2.2858760075905593e-06 4.1974435589509085e-05 Traning Loss: 4.426031227922067e-05 17.984006881713867 2.0152816772460938\n",
            "23697 2.0512607079581358e-06 4.177974187768996e-05 Traning Loss: 4.383100167615339e-05 17.984193801879883 2.01521897315979\n",
            "23698 2.101855443470413e-06 4.1409552068216726e-05 Traning Loss: 4.351140887592919e-05 17.984375 2.0151560306549072\n",
            "23699 2.085478399749263e-06 4.131946843699552e-05 Traning Loss: 4.340494706411846e-05 17.984554290771484 2.015091896057129\n",
            "23700 1.9949702618760057e-06 4.146400169702247e-05 Traning Loss: 4.345897104940377e-05 17.984729766845703 2.0150322914123535\n",
            "23701 2.132647296093637e-06 4.140759483561851e-05 Traning Loss: 4.35402434959542e-05 17.984899520874023 2.014967918395996\n",
            "23702 1.9574858924897853e-06 4.158920273766853e-05 Traning Loss: 4.354668999440037e-05 17.985069274902344 2.014909267425537\n",
            "23703 2.089824874929036e-06 4.137603173148818e-05 Traning Loss: 4.3465857743285596e-05 17.985233306884766 2.0148470401763916\n",
            "23704 1.9452779724815628e-06 4.140359669690952e-05 Traning Loss: 4.3348874896764755e-05 17.985397338867188 2.0147883892059326\n",
            "23705 1.9908763988496503e-06 4.126227941014804e-05 Traning Loss: 4.3253156036371365e-05 17.98555564880371 2.0147297382354736\n",
            "23706 1.958356733666733e-06 4.124757106183097e-05 Traning Loss: 4.32059277954977e-05 17.9857120513916 2.0146706104278564\n",
            "23707 1.9069650534220273e-06 4.127270949538797e-05 Traning Loss: 4.317967614042573e-05 17.98586654663086 2.0146145820617676\n",
            "23708 1.96051496459404e-06 4.118123251828365e-05 Traning Loss: 4.314174657338299e-05 17.98601722717285 2.0145561695098877\n",
            "23709 1.85418230103096e-06 4.121437086723745e-05 Traning Loss: 4.306855407776311e-05 17.98616600036621 2.0145018100738525\n",
            "23710 1.9214719486626564e-06 4.1050156141864136e-05 Traning Loss: 4.2971627408405766e-05 17.986310958862305 2.0144450664520264\n",
            "23711 1.830543965297693e-06 4.1052233427762985e-05 Traning Loss: 4.2882777052000165e-05 17.986454010009766 2.0143909454345703\n",
            "23712 1.8589520323075703e-06 4.097039345651865e-05 Traning Loss: 4.282934605726041e-05 17.986595153808594 2.014336585998535\n",
            "23713 1.8292247432327713e-06 4.098777935723774e-05 Traning Loss: 4.281700239516795e-05 17.986732482910156 2.014282464981079\n",
            "23714 1.8041354223896633e-06 4.1022336517926306e-05 Traning Loss: 4.2826472054002807e-05 17.986867904663086 2.014230251312256\n",
            "23715 1.8257138663102523e-06 4.10025822930038e-05 Traning Loss: 4.282829468138516e-05 17.987001419067383 2.014176607131958\n",
            "23716 1.7626365433898172e-06 4.103969695279375e-05 Traning Loss: 4.280233406461775e-05 17.987131118774414 2.0141258239746094\n",
            "23717 1.7929088471646537e-06 4.094940231880173e-05 Traning Loss: 4.274231105227955e-05 17.987258911132812 2.014073371887207\n",
            "23718 1.7299233832090977e-06 4.0934086428023875e-05 Traning Loss: 4.266401083441451e-05 17.987384796142578 2.0140233039855957\n",
            "23719 1.7378588381689042e-06 4.084856846020557e-05 Traning Loss: 4.2586427298374474e-05 17.98750877380371 2.013972759246826\n",
            "23720 1.7081946452890406e-06 4.0818038542056456e-05 Traning Loss: 4.2526233301032335e-05 17.987628936767578 2.013922929763794\n",
            "23721 1.6852774251674418e-06 4.0804730815580115e-05 Traning Loss: 4.249000994605012e-05 17.987749099731445 2.013874053955078\n",
            "23722 1.6926085208979202e-06 4.077763151144609e-05 Traning Loss: 4.2470241169212386e-05 17.987865447998047 2.013824462890625\n",
            "23723 1.6477633835165761e-06 4.080861253896728e-05 Traning Loss: 4.245637683197856e-05 17.98798179626465 2.013777017593384\n",
            "23724 1.6707926988601685e-06 4.076462937518954e-05 Traning Loss: 4.243542207404971e-05 17.988094329833984 2.013728380203247\n",
            "23725 1.6230391111093923e-06 4.078315760125406e-05 Traning Loss: 4.2406198190292343e-05 17.988204956054688 2.013681650161743\n",
            "23726 1.6357215599782648e-06 4.073333184351213e-05 Traning Loss: 4.236905442667194e-05 17.988313674926758 2.013634204864502\n",
            "23727 1.6038642343119136e-06 4.072531737620011e-05 Traning Loss: 4.232918217894621e-05 17.988420486450195 2.0135879516601562\n",
            "23728 1.5936032013996737e-06 4.069604619871825e-05 Traning Loss: 4.2289648263249546e-05 17.988525390625 2.0135419368743896\n",
            "23729 1.5873680467848317e-06 4.066910332767293e-05 Traning Loss: 4.225646989652887e-05 17.988628387451172 2.013495922088623\n",
            "23730 1.555482754156401e-06 4.067092959303409e-05 Traning Loss: 4.222641291562468e-05 17.98872947692871 2.013451099395752\n",
            "23731 1.5687046470702626e-06 4.062709922436625e-05 Traning Loss: 4.219580296194181e-05 17.988828659057617 2.0134055614471436\n",
            "23732 1.5254283880494768e-06 4.0638122300151736e-05 Traning Loss: 4.216355227981694e-05 17.988927841186523 2.013361692428589\n",
            "23733 1.5434352462762035e-06 4.058607737533748e-05 Traning Loss: 4.212951171211898e-05 17.989023208618164 2.013317108154297\n",
            "23734 1.5025913171484717e-06 4.0591781726107e-05 Traning Loss: 4.2094372474821284e-05 17.989118576049805 2.0132737159729004\n",
            "23735 1.5125935988180572e-06 4.0544618968851864e-05 Traning Loss: 4.205721415928565e-05 17.989212036132812 2.0132298469543457\n",
            "23736 1.4855010022074566e-06 4.0537313907407224e-05 Traning Loss: 4.202281343168579e-05 17.989303588867188 2.0131869316101074\n",
            "23737 1.4811434994044248e-06 4.051031282870099e-05 Traning Loss: 4.199145769234747e-05 17.98939323425293 2.013144016265869\n",
            "23738 1.471835162192292e-06 4.0491591789759696e-05 Traning Loss: 4.1963427065638825e-05 17.98948097229004 2.01310133934021\n",
            "23739 1.4533227385982173e-06 4.048253322253004e-05 Traning Loss: 4.193585482425988e-05 17.98956871032715 2.013059377670288\n",
            "23740 1.4573950011254055e-06 4.045288005727343e-05 Traning Loss: 4.1910276195267215e-05 17.989654541015625 2.013017177581787\n",
            "23741 1.429748181180912e-06 4.0454127884004265e-05 Traning Loss: 4.188387538306415e-05 17.98973846435547 2.0129759311676025\n",
            "23742 1.4393160654435633e-06 4.0416536648990586e-05 Traning Loss: 4.185585203231312e-05 17.98982048034668 2.0129342079162598\n",
            "23743 1.410444042448944e-06 4.04175189032685e-05 Traning Loss: 4.182796328677796e-05 17.98990249633789 2.0128934383392334\n",
            "23744 1.4183547136781272e-06 4.038008410134353e-05 Traning Loss: 4.17984374507796e-05 17.98998260498047 2.012852430343628\n",
            "23745 1.3945467571829795e-06 4.037423786940053e-05 Traning Loss: 4.1768784285523e-05 17.990060806274414 2.0128121376037598\n",
            "23746 1.3964159961687983e-06 4.0342874854104593e-05 Traning Loss: 4.1739291191333905e-05 17.990137100219727 2.0127720832824707\n",
            "23747 1.380641606374411e-06 4.03303092753049e-05 Traning Loss: 4.171095133642666e-05 17.99021339416504 2.0127322673797607\n",
            "23748 1.3751388223681715e-06 4.030744457850233e-05 Traning Loss: 4.168258237768896e-05 17.99028778076172 2.012692928314209\n",
            "23749 1.3667938674188918e-06 4.0287519368575886e-05 Traning Loss: 4.165431164437905e-05 17.990360260009766 2.0126535892486572\n",
            "23750 1.3557749980463996e-06 4.027181421406567e-05 Traning Loss: 4.162759069004096e-05 17.99043083190918 2.0126149654388428\n",
            "23751 1.3521662367566023e-06 4.024858571938239e-05 Traning Loss: 4.1600753320381045e-05 17.990501403808594 2.0125763416290283\n",
            "23752 1.3386555792749277e-06 4.023413930553943e-05 Traning Loss: 4.1572795453248546e-05 17.990570068359375 2.012538433074951\n",
            "23753 1.3360995581024326e-06 4.0208949940279126e-05 Traning Loss: 4.154504858888686e-05 17.990638732910156 2.012500524520874\n",
            "23754 1.3232908031568513e-06 4.019312473246828e-05 Traning Loss: 4.151641405769624e-05 17.990705490112305 2.012463092803955\n",
            "23755 1.3188368939154316e-06 4.0170649299398065e-05 Traning Loss: 4.1489485738566145e-05 17.99077033996582 2.0124258995056152\n",
            "23756 1.3090466381981969e-06 4.015367812826298e-05 Traning Loss: 4.146272476646118e-05 17.990835189819336 2.0123891830444336\n",
            "23757 1.3012733006689814e-06 4.0134022128768265e-05 Traning Loss: 4.143529440625571e-05 17.99089813232422 2.012352705001831\n",
            "23758 1.2956231785210548e-06 4.0112605347530916e-05 Traning Loss: 4.1408227843930945e-05 17.99095916748047 2.0123164653778076\n",
            "23759 1.2844226375818835e-06 4.009723124909215e-05 Traning Loss: 4.138165240874514e-05 17.99102020263672 2.0122807025909424\n",
            "23760 1.282655375689501e-06 4.007295137853362e-05 Traning Loss: 4.135560811846517e-05 17.991079330444336 2.012244939804077\n",
            "23761 1.268529445042077e-06 4.006236849818379e-05 Traning Loss: 4.13308989664074e-05 17.991138458251953 2.012209892272949\n",
            "23762 1.2697576039499836e-06 4.003561844001524e-05 Traning Loss: 4.130537490709685e-05 17.991195678710938 2.0121748447418213\n",
            "23763 1.253664322575787e-06 4.0026396163739264e-05 Traning Loss: 4.1280061850557104e-05 17.991252899169922 2.0121402740478516\n",
            "23764 1.2565776614792412e-06 4.0000828448683023e-05 Traning Loss: 4.1257404518546537e-05 17.991308212280273 2.012105703353882\n",
            "23765 1.2398618309816811e-06 3.9994032704271376e-05 Traning Loss: 4.123389589949511e-05 17.991363525390625 2.0120716094970703\n",
            "23766 1.2436694305506535e-06 3.996836312580854e-05 Traning Loss: 4.121203164686449e-05 17.991416931152344 2.012037515640259\n",
            "23767 1.2270253364476957e-06 3.9965427276911214e-05 Traning Loss: 4.119245204492472e-05 17.991470336914062 2.0120041370391846\n",
            "23768 1.231952751368226e-06 3.994138023699634e-05 Traning Loss: 4.1173334466293454e-05 17.99152183532715 2.0119705200195312\n",
            "23769 1.2153780062362785e-06 3.994452708866447e-05 Traning Loss: 4.115990668651648e-05 17.991573333740234 2.0119376182556152\n",
            "23770 1.2219815062053385e-06 3.992831625510007e-05 Traning Loss: 4.115029878448695e-05 17.991622924804688 2.011904716491699\n",
            "23771 1.2056847253916203e-06 3.9942846342455596e-05 Traning Loss: 4.11485307267867e-05 17.99167251586914 2.0118722915649414\n",
            "23772 1.2156674529251177e-06 3.994101280113682e-05 Traning Loss: 4.115667979931459e-05 17.99172019958496 2.0118398666381836\n",
            "23773 1.1997101410088362e-06 3.99793207179755e-05 Traning Loss: 4.117903154110536e-05 17.99176788330078 2.011807918548584\n",
            "23774 1.2165531870778068e-06 4.0006554627325386e-05 Traning Loss: 4.122310929233208e-05 17.99181365966797 2.0117759704589844\n",
            "23775 1.2012956176477019e-06 4.0098108001984656e-05 Traning Loss: 4.129940498387441e-05 17.99186134338379 2.011744499206543\n",
            "23776 1.2317634627834195e-06 4.0192790038418025e-05 Traning Loss: 4.142455509281717e-05 17.991905212402344 2.0117127895355225\n",
            "23777 1.2203431651869323e-06 4.0402854210697114e-05 Traning Loss: 4.1623196011641994e-05 17.99195098876953 2.0116820335388184\n",
            "23778 1.278254217140784e-06 4.0656097553437576e-05 Traning Loss: 4.193435233901255e-05 17.99199104309082 2.011650800704956\n",
            "23779 1.2805830920115113e-06 4.1135430365102366e-05 Traning Loss: 4.241601345711388e-05 17.992036819458008 2.0116207599639893\n",
            "23780 1.3956060911368695e-06 4.176852235104889e-05 Traning Loss: 4.316412741900422e-05 17.992074966430664 2.011589765548706\n",
            "23781 1.4408083188754972e-06 4.2882315028691664e-05 Traning Loss: 4.432312198332511e-05 17.99212074279785 2.0115604400634766\n",
            "23782 1.6806486655696062e-06 4.445391823537648e-05 Traning Loss: 4.613456621882506e-05 17.992155075073242 2.0115294456481934\n",
            "23783 1.8498695908419904e-06 4.711996371042915e-05 Traning Loss: 4.89698322780896e-05 17.99220085144043 2.0115010738372803\n",
            "23784 2.3791208150214516e-06 5.107518154545687e-05 Traning Loss: 5.3454303269973025e-05 17.992229461669922 2.011470079421997\n",
            "23785 2.894647877837997e-06 5.765483729192056e-05 Traning Loss: 6.0549486079253256e-05 17.992277145385742 2.0114428997039795\n",
            "23786 4.131474270252511e-06 6.780555850127712e-05 Traning Loss: 7.193702913355082e-05 17.992298126220703 2.011411428451538\n",
            "23787 5.60021589990356e-06 8.454647468170151e-05 Traning Loss: 9.014669194584712e-05 17.992347717285156 2.011385917663574\n",
            "23788 8.644219633424655e-06 0.00011115756933577359 Traning Loss: 0.00011980178533121943 17.992353439331055 2.0113532543182373\n",
            "23789 1.2738292753056157e-05 0.00015491407248191535 Traning Loss: 0.00016765236796345562 17.992403030395508 2.0113303661346436\n",
            "23790 2.0550638510030694e-05 0.00022603762045037001 Traning Loss: 0.0002465882571414113 17.992380142211914 2.0112953186035156\n",
            "23791 3.183847002219409e-05 0.0003423212911002338 Traning Loss: 0.0003741597756743431 17.99242401123047 2.0112762451171875\n",
            "23792 5.236109427642077e-05 0.0005337761249393225 Traning Loss: 0.0005861372337676585 17.992341995239258 2.0112369060516357\n",
            "23793 8.278807945316657e-05 0.0008408878929913044 Traning Loss: 0.0009236759506165981 17.992355346679688 2.011223554611206\n",
            "23794 0.00013589428272098303 0.0013426638906821609 Traning Loss: 0.0014785581734031439 17.992143630981445 2.011176347732544\n",
            "23795 0.00021174456924200058 0.0021006313618272543 Traning Loss: 0.002312375931069255 17.992050170898438 2.011172294616699\n",
            "23796 0.00033447035821154714 0.003263157559558749 Traning Loss: 0.0035976278595626354 17.991554260253906 2.0111114978790283\n",
            "23797 0.00048097237595357 0.004725567065179348 Traning Loss: 0.005206539295613766 17.99117660522461 2.0111210346221924\n",
            "23798 0.0006695803022012115 0.006481255870312452 Traning Loss: 0.007150836288928986 17.990182876586914 2.011040687561035\n",
            "23799 0.0007697202381677926 0.007517793215811253 Traning Loss: 0.008287513628602028 17.989320755004883 2.011070966720581\n",
            "23800 0.0007714062812738121 0.007352035492658615 Traning Loss: 0.008123441599309444 17.987878799438477 2.010979413986206\n",
            "23801 0.0005221681203693151 0.005010615102946758 Traning Loss: 0.005532783456146717 17.986736297607422 2.011040687561035\n",
            "23802 0.0002337886398890987 0.0019785878248512745 Traning Loss: 0.00221237656660378 17.985502243041992 2.0109899044036865\n",
            "23803 4.572677062242292e-05 0.00022740475833415985 Traning Loss: 0.0002731315325945616 17.98455047607422 2.011080741882324\n",
            "23804 0.00010988328722305596 0.0007648467435501516 Traning Loss: 0.000874730059877038 17.98379135131836 2.011131763458252\n",
            "23805 0.0002781937946565449 0.002382484497502446 Traning Loss: 0.00266067823395133 17.982816696166992 2.011227607727051\n",
            "23806 0.0003321286931168288 0.0029279496520757675 Traning Loss: 0.0032600783742964268 17.981992721557617 2.011387586593628\n",
            "23807 0.00023841705115046352 0.001854089554399252 Traning Loss: 0.002092506503686309 17.98090934753418 2.0114905834198\n",
            "23808 8.556104876333848e-05 0.000470058002974838 Traning Loss: 0.0005556190735660493 17.97995948791504 2.011737823486328\n",
            "23809 8.379020437132567e-05 0.0003054706903640181 Traning Loss: 0.0003892608801834285 17.979047775268555 2.0118930339813232\n",
            "23810 0.0001565933634992689 0.001189208123832941 Traning Loss: 0.0013458015164360404 17.978029251098633 2.012157678604126\n",
            "23811 0.00021296720660757273 0.001617236528545618 Traning Loss: 0.001830203691497445 17.977140426635742 2.012383460998535\n",
            "23812 0.00014741950144525617 0.0010525642428547144 Traning Loss: 0.0011999837588518858 17.976144790649414 2.0126140117645264\n",
            "23813 6.392397335730493e-05 0.00025918259052559733 Traning Loss: 0.00032310656388290226 17.975296020507812 2.0129334926605225\n",
            "23814 6.58944045426324e-05 0.0002306541136931628 Traning Loss: 0.00029654853278771043 17.974519729614258 2.0131704807281494\n",
            "23815 0.00010618098167469725 0.0007997979409992695 Traning Loss: 0.0009059789008460939 17.973737716674805 2.0135371685028076\n",
            "23816 0.00013666202721651644 0.0010016330052167177 Traning Loss: 0.0011382950469851494 17.973093032836914 2.0138132572174072\n",
            "23817 8.245815843110904e-05 0.0005986678297631443 Traning Loss: 0.0006811259663663805 17.972429275512695 2.0141329765319824\n",
            "23818 3.322332486277446e-05 0.00011634733527898788 Traning Loss: 0.00014957066741771996 17.97188949584961 2.014465093612671\n",
            "23819 3.613534863688983e-05 0.00014344701776281 Traning Loss: 0.00017958236276172101 17.971424102783203 2.0147297382354736\n",
            "23820 6.312203913694248e-05 0.0005181428859941661 Traning Loss: 0.0005812649033032358 17.970943450927734 2.015075445175171\n",
            "23821 8.347691618837416e-05 0.0006503210170194507 Traning Loss: 0.0007337979041039944 17.97056007385254 2.015298366546631\n",
            "23822 4.810325845028274e-05 0.00040901536704041064 Traning Loss: 0.00045711861457675695 17.97016143798828 2.0155699253082275\n",
            "23823 1.8680430002859794e-05 9.480058361077681e-05 Traning Loss: 0.00011348101543262601 17.96982765197754 2.015789031982422\n",
            "23824 1.419068212271668e-05 7.881659985287115e-05 Traning Loss: 9.300728561356664e-05 17.96955680847168 2.0159780979156494\n",
            "23825 3.1884930649539456e-05 0.0002937470853794366 Traning Loss: 0.0003256320196669549 17.969276428222656 2.0161995887756348\n",
            "23826 4.790516686625779e-05 0.00041462836088612676 Traning Loss: 0.00046253352775238454 17.969083786010742 2.016324520111084\n",
            "23827 3.1549428967991844e-05 0.0003137469757348299 Traning Loss: 0.00034529640106484294 17.968889236450195 2.0164942741394043\n",
            "23828 1.4890040802129079e-05 0.00011595387331908569 Traning Loss: 0.00013084390957374126 17.96876335144043 2.0165979862213135\n",
            "23829 5.586810402746778e-06 5.707675518351607e-05 Traning Loss: 6.266356649575755e-05 17.968704223632812 2.016706943511963\n",
            "23830 1.5933357644826174e-05 0.00015267662820406258 Traning Loss: 0.00016860998584888875 17.968650817871094 2.0168046951293945\n",
            "23831 2.6322917619836517e-05 0.00025295684463344514 Traning Loss: 0.0002792797749862075 17.968692779541016 2.016850233078003\n",
            "23832 2.2856092982692644e-05 0.00023518127272836864 Traning Loss: 0.00025803735479712486 17.968727111816406 2.016911506652832\n",
            "23833 1.3330747606232762e-05 0.00012407972826622427 Traning Loss: 0.00013741047587245703 17.96881675720215 2.0169243812561035\n",
            "23834 4.396502390591195e-06 5.179627987672575e-05 Traning Loss: 5.619278090307489e-05 17.968935012817383 2.0169501304626465\n",
            "23835 8.067158887570258e-06 7.505654502892867e-05 Traning Loss: 8.312370482599363e-05 17.96904945373535 2.0169565677642822\n",
            "23836 1.4575176464859396e-05 0.00014519276737701148 Traning Loss: 0.0001597679511178285 17.969219207763672 2.01694655418396\n",
            "23837 1.7290221876464784e-05 0.00017000004299916327 Traning Loss: 0.00018729026487562805 17.969356536865234 2.0169365406036377\n",
            "23838 1.2409845112415496e-05 0.00012393039651215076 Traning Loss: 0.00013634024071507156 17.969545364379883 2.016911268234253\n",
            "23839 6.075566489016637e-06 6.126094376668334e-05 Traning Loss: 6.733651389367878e-05 17.9697322845459 2.016885757446289\n",
            "23840 4.218828507873695e-06 4.3477793951751664e-05 Traning Loss: 4.769662336912006e-05 17.969934463500977 2.0168569087982178\n",
            "23841 7.441711659339489e-06 7.478341285604984e-05 Traning Loss: 8.222512406064197e-05 17.970176696777344 2.016812562942505\n",
            "23842 1.0962595297314692e-05 0.00010874857980525121 Traning Loss: 0.00011971117783105001 17.970396041870117 2.0167734622955322\n",
            "23843 1.0273589396092575e-05 0.00010588054283289239 Traning Loss: 0.00011615412950050086 17.970657348632812 2.016726016998291\n",
            "23844 7.333644134632777e-06 7.213355274870992e-05 Traning Loss: 7.946719415485859e-05 17.970903396606445 2.0166773796081543\n",
            "23845 3.986578576586908e-06 4.5770721044391394e-05 Traning Loss: 4.975729825673625e-05 17.97117042541504 2.0166354179382324\n",
            "23846 4.881530458078487e-06 4.7823046770645306e-05 Traning Loss: 5.270457768347114e-05 17.97144889831543 2.0165746212005615\n",
            "23847 6.373469204845605e-06 6.925463821971789e-05 Traning Loss: 7.562810787931085e-05 17.971717834472656 2.0165348052978516\n",
            "23848 7.907744475232903e-06 8.019411325221881e-05 Traning Loss: 8.810185681795701e-05 17.972007751464844 2.016477584838867\n",
            "23849 6.6765273913915735e-06 7.016715971985832e-05 Traning Loss: 7.684368756599724e-05 17.97228240966797 2.016435146331787\n",
            "23850 4.659994374378584e-06 5.0650858611334115e-05 Traning Loss: 5.53108548047021e-05 17.972576141357422 2.0163893699645996\n",
            "23851 3.951934104406973e-06 4.137409632676281e-05 Traning Loss: 4.532602906692773e-05 17.97287368774414 2.0163376331329346\n",
            "23852 4.236083441355731e-06 4.882628854829818e-05 Traning Loss: 5.306237289914861e-05 17.97317123413086 2.016303062438965\n",
            "23853 5.9130134104634635e-06 6.006774856359698e-05 Traning Loss: 6.598076288355514e-05 17.973482131958008 2.016251802444458\n",
            "23854 5.692493687092792e-06 6.330610631266609e-05 Traning Loss: 6.899859727127478e-05 17.97378158569336 2.016223430633545\n",
            "23855 5.316956503520487e-06 5.427572614280507e-05 Traning Loss: 5.959268310107291e-05 17.974088668823242 2.0161800384521484\n",
            "23856 3.8741623029636685e-06 4.412611451698467e-05 Traning Loss: 4.800027818419039e-05 17.974388122558594 2.0161490440368652\n",
            "23857 3.714346576089156e-06 4.082981831743382e-05 Traning Loss: 4.4544165575644e-05 17.974681854248047 2.0161190032958984\n",
            "23858 4.08845426136395e-06 4.577198706101626e-05 Traning Loss: 4.986044223187491e-05 17.974977493286133 2.0160863399505615\n",
            "23859 4.527055352809839e-06 5.174006582819857e-05 Traning Loss: 5.6267119362019e-05 17.97525978088379 2.016064167022705\n",
            "23860 4.6823352022329345e-06 5.2012652304256335e-05 Traning Loss: 5.669498932547867e-05 17.97555160522461 2.0160298347473145\n",
            "23861 3.862993253278546e-06 4.735859329230152e-05 Traning Loss: 5.122158472659066e-05 17.975833892822266 2.0160064697265625\n",
            "23862 3.479560291452799e-06 4.1920033254427835e-05 Traning Loss: 4.539959263638593e-05 17.976118087768555 2.015974998474121\n",
            "23863 3.047102381970035e-06 4.1027014958672225e-05 Traning Loss: 4.407411688589491e-05 17.97640609741211 2.0159482955932617\n",
            "23864 3.441910848778207e-06 4.36113950854633e-05 Traning Loss: 4.7053305024746805e-05 17.9766845703125 2.0159192085266113\n",
            "23865 3.4942381716973614e-06 4.682177313952707e-05 Traning Loss: 5.031601176597178e-05 17.976974487304688 2.0158867835998535\n",
            "23866 3.573745743779e-06 4.684284795075655e-05 Traning Loss: 5.041659460403025e-05 17.97725486755371 2.015855550765991\n",
            "23867 3.1421752737514907e-06 4.4253691157791764e-05 Traning Loss: 4.739586802315898e-05 17.977542877197266 2.0158209800720215\n",
            "23868 2.877709448512178e-06 4.119304503547028e-05 Traning Loss: 4.407075539347716e-05 17.97782325744629 2.0157852172851562\n",
            "23869 2.715300979616586e-06 4.0281989640789106e-05 Traning Loss: 4.299728971091099e-05 17.97810173034668 2.0157477855682373\n",
            "23870 2.816112782966229e-06 4.1523424442857504e-05 Traning Loss: 4.433953654370271e-05 17.97838020324707 2.015705108642578\n",
            "23871 2.96299003821332e-06 4.3258285586489365e-05 Traning Loss: 4.622127744369209e-05 17.978649139404297 2.015662908554077\n",
            "23872 2.9270572667883243e-06 4.3790820200229064e-05 Traning Loss: 4.6717876102775335e-05 17.97892189025879 2.015615701675415\n",
            "23873 2.838695991158602e-06 4.2636325815692544e-05 Traning Loss: 4.547502248897217e-05 17.979183197021484 2.0155670642852783\n",
            "23874 2.5496306079730857e-06 4.1103045077761635e-05 Traning Loss: 4.365267523098737e-05 17.979446411132812 2.0155162811279297\n",
            "23875 2.533070301069529e-06 4.014364458271302e-05 Traning Loss: 4.2676714656408876e-05 17.979703903198242 2.01546049118042\n",
            "23876 2.3983834580576513e-06 4.060281571582891e-05 Traning Loss: 4.3001200538128614e-05 17.979955673217773 2.0154082775115967\n",
            "23877 2.587208882687264e-06 4.13656271120999e-05 Traning Loss: 4.395283758640289e-05 17.980207443237305 2.0153489112854004\n",
            "23878 2.4749374460952822e-06 4.202506897854619e-05 Traning Loss: 4.450000778888352e-05 17.980453491210938 2.0152945518493652\n",
            "23879 2.541602952987887e-06 4.1593859350541607e-05 Traning Loss: 4.41354641225189e-05 17.98069953918457 2.015233039855957\n",
            "23880 2.332463509446825e-06 4.0846574847819284e-05 Traning Loss: 4.3179039494134486e-05 17.980941772460938 2.0151748657226562\n",
            "23881 2.3230354599945713e-06 4.00258504669182e-05 Traning Loss: 4.234888547216542e-05 17.98118019104004 2.0151138305664062\n",
            "23882 2.253597585877287e-06 3.9880604163045064e-05 Traning Loss: 4.2134201066801324e-05 17.981416702270508 2.0150527954101562\n",
            "23883 2.298930439792457e-06 4.016865568701178e-05 Traning Loss: 4.246758544468321e-05 17.981645584106445 2.0149922370910645\n",
            "23884 2.3397651602863334e-06 4.055375757161528e-05 Traning Loss: 4.2893523641396314e-05 17.98187255859375 2.0149290561676025\n",
            "23885 2.3071395389706595e-06 4.068214911967516e-05 Traning Loss: 4.2989289795514196e-05 17.982091903686523 2.0148684978485107\n",
            "23886 2.3027134830044815e-06 4.0360471757594496e-05 Traning Loss: 4.26631850132253e-05 17.98230743408203 2.014805316925049\n",
            "23887 2.195230990764685e-06 3.996816303697415e-05 Traning Loss: 4.2163395846728235e-05 17.98251724243164 2.014744758605957\n",
            "23888 2.1945829757896718e-06 3.961963375331834e-05 Traning Loss: 4.181421536486596e-05 17.982723236083984 2.0146825313568115\n",
            "23889 2.127859033862478e-06 3.9648872188990936e-05 Traning Loss: 4.1776729631237686e-05 17.982927322387695 2.0146217346191406\n",
            "23890 2.1666212433046894e-06 3.97902840632014e-05 Traning Loss: 4.195690416963771e-05 17.983123779296875 2.014561176300049\n",
            "23891 2.1231355731288204e-06 4.000878834631294e-05 Traning Loss: 4.2131923692068085e-05 17.983320236206055 2.0145013332366943\n",
            "23892 2.14528881770093e-06 3.998193278675899e-05 Traning Loss: 4.2127219785470515e-05 17.983510971069336 2.014441967010498\n",
            "23893 2.0696240881079575e-06 3.9860475226305425e-05 Traning Loss: 4.193009954178706e-05 17.983701705932617 2.014383316040039\n",
            "23894 2.0769793991348706e-06 3.958705929107964e-05 Traning Loss: 4.166403959970921e-05 17.983888626098633 2.014324426651001\n",
            "23895 1.9955955394834746e-06 3.948178346036002e-05 Traning Loss: 4.1477378545096144e-05 17.984073638916016 2.014267921447754\n",
            "23896 2.03783793040202e-06 3.939609450753778e-05 Traning Loss: 4.1433933802181855e-05 17.9842529296875 2.0142099857330322\n",
            "23897 1.9686594896484166e-06 3.952623228542507e-05 Traning Loss: 4.1494891775073484e-05 17.98442840576172 2.014155387878418\n",
            "23898 2.025557932938682e-06 3.9533788367407396e-05 Traning Loss: 4.155934584559873e-05 17.984601974487305 2.0140984058380127\n",
            "23899 1.939572030096315e-06 3.960728281526826e-05 Traning Loss: 4.1546853026375175e-05 17.984769821166992 2.014045238494873\n",
            "23900 1.9778742625931045e-06 3.946675860788673e-05 Traning Loss: 4.144463309785351e-05 17.984935760498047 2.0139899253845215\n",
            "23901 1.882481910797651e-06 3.9415517676388845e-05 Traning Loss: 4.129800072405487e-05 17.985097885131836 2.0139377117156982\n",
            "23902 1.91072922461899e-06 3.926496719941497e-05 Traning Loss: 4.117569551453926e-05 17.98525619506836 2.0138838291168213\n",
            "23903 1.8307850950805005e-06 3.928404112230055e-05 Traning Loss: 4.1114824853139e-05 17.98541259765625 2.0138320922851562\n",
            "23904 1.8598663018565276e-06 3.925106648239307e-05 Traning Loss: 4.111093221581541e-05 17.985565185546875 2.013779640197754\n",
            "23905 1.7931098454937455e-06 3.9336584450211376e-05 Traning Loss: 4.112969327252358e-05 17.9857177734375 2.0137288570404053\n",
            "23906 1.8218677269032924e-06 3.930622187908739e-05 Traning Loss: 4.1128088923869655e-05 17.985864639282227 2.0136773586273193\n",
            "23907 1.7495956399216084e-06 3.933904008590616e-05 Traning Loss: 4.1088635043706745e-05 17.986011505126953 2.013627529144287\n",
            "23908 1.7809099972510012e-06 3.923719850718044e-05 Traning Loss: 4.1018109186552465e-05 17.986154556274414 2.0135765075683594\n",
            "23909 1.69694317264657e-06 3.9247170207090676e-05 Traning Loss: 4.094411269761622e-05 17.986297607421875 2.0135278701782227\n",
            "23910 1.7465787323089899e-06 3.9145248592831194e-05 Traning Loss: 4.08918276662007e-05 17.98643684387207 2.013477087020874\n",
            "23911 1.650995386626164e-06 3.922108589904383e-05 Traning Loss: 4.0872080717235804e-05 17.986574172973633 2.013429641723633\n",
            "23912 1.7273621324420674e-06 3.915327397407964e-05 Traning Loss: 4.088063724339008e-05 17.98670768737793 2.013378620147705\n",
            "23913 1.6124525927807554e-06 3.929289596271701e-05 Traning Loss: 4.090535003342666e-05 17.986841201782227 2.0133323669433594\n",
            "23914 1.7173508695123019e-06 3.9213697164086625e-05 Traning Loss: 4.093104871571995e-05 17.986970901489258 2.0132813453674316\n",
            "23915 1.5756705806779792e-06 3.938551162718795e-05 Traning Loss: 4.0961182094179094e-05 17.987098693847656 2.0132362842559814\n",
            "23916 1.7167736814371892e-06 3.929000740754418e-05 Traning Loss: 4.100678052054718e-05 17.98722267150879 2.0131850242614746\n",
            "23917 1.5427867765538394e-06 3.9545524487039074e-05 Traning Loss: 4.1088311263592914e-05 17.987346649169922 2.013141393661499\n",
            "23918 1.7406100596417673e-06 3.949241363443434e-05 Traning Loss: 4.123302278458141e-05 17.98746681213379 2.013089895248413\n",
            "23919 1.5278131968443631e-06 3.9946688048075885e-05 Traning Loss: 4.1474500903859735e-05 17.98758888244629 2.0130481719970703\n",
            "23920 1.8189934962720145e-06 4.003538560937159e-05 Traning Loss: 4.185437865089625e-05 17.987703323364258 2.012995719909668\n",
            "23921 1.556520032863773e-06 4.087004708708264e-05 Traning Loss: 4.242656723363325e-05 17.987821578979492 2.0129566192626953\n",
            "23922 2.003043391596293e-06 4.127886859350838e-05 Traning Loss: 4.328191062086262e-05 17.987930297851562 2.012902021408081\n",
            "23923 1.6837445855344413e-06 4.288005584385246e-05 Traning Loss: 4.4563799747265875e-05 17.988046646118164 2.012866973876953\n",
            "23924 2.405070745226112e-06 4.409446773934178e-05 Traning Loss: 4.649953916668892e-05 17.988149642944336 2.0128087997436523\n",
            "23925 2.055668346656603e-06 4.7397879825439304e-05 Traning Loss: 4.945354885421693e-05 17.988264083862305 2.012779474258423\n",
            "23926 3.295956958027091e-06 5.0686972826952115e-05 Traning Loss: 5.398293069447391e-05 17.988359451293945 2.0127153396606445\n",
            "23927 3.0502328627335373e-06 5.794819298898801e-05 Traning Loss: 6.09984272159636e-05 17.98847198486328 2.012695074081421\n",
            "23928 5.335185051080771e-06 6.65415936964564e-05 Traning Loss: 7.187677692854777e-05 17.988557815551758 2.0126214027404785\n",
            "23929 5.636310561385471e-06 8.332553989021108e-05 Traning Loss: 8.896185318008065e-05 17.988670349121094 2.012615203857422\n",
            "23930 1.0169399502046872e-05 0.00010556397319305688 Traning Loss: 0.00011573337542358786 17.988739013671875 2.012526035308838\n",
            "23931 1.2318767403485253e-05 0.0001459841150790453 Traning Loss: 0.00015830287884455174 17.988849639892578 2.0125420093536377\n",
            "23932 2.1973020920995623e-05 0.0002034306962741539 Traning Loss: 0.00022540372447110713 17.9888916015625 2.0124285221099854\n",
            "23933 2.9558190362877212e-05 0.00030328528373502195 Traning Loss: 0.0003328434831928462 17.988994598388672 2.0124804973602295\n",
            "23934 5.119166962685995e-05 0.00045005930587649345 Traning Loss: 0.000501250964589417 17.988985061645508 2.0123291015625\n",
            "23935 7.3256938776467e-05 0.0006954282289370894 Traning Loss: 0.0007686851895414293 17.989063262939453 2.0124382972717285\n",
            "23936 0.00012168679677415639 0.0010516595793887973 Traning Loss: 0.0011733464198186994 17.98894691467285 2.0122323036193848\n",
            "23937 0.00017605051107238978 0.00160873937420547 Traning Loss: 0.0017847898416221142 17.988941192626953 2.01242733001709\n",
            "23938 0.0002717395545914769 0.002338538644835353 Traning Loss: 0.0026102783158421516 17.988615036010742 2.0121593475341797\n",
            "23939 0.0003669356519822031 0.0032962714321911335 Traning Loss: 0.003663207171484828 17.988407135009766 2.012460231781006\n",
            "23940 0.0004846345982514322 0.004157682880759239 Traning Loss: 0.004642317537218332 17.987741470336914 2.0121676921844482\n",
            "23941 0.0005247510271146894 0.004698426462709904 Traning Loss: 0.005223177373409271 17.98720359802246 2.0125350952148438\n",
            "23942 0.0004997986252419651 0.004242958500981331 Traning Loss: 0.004742756951600313 17.986242294311523 2.012338876724243\n",
            "23943 0.0003210997674614191 0.0029273831751197577 Traning Loss: 0.0032484829425811768 17.985485076904297 2.0126380920410156\n",
            "23944 0.00015175825683400035 0.0012307115830481052 Traning Loss: 0.0013824698980897665 17.984600067138672 2.012672185897827\n",
            "23945 2.626916830195114e-05 0.00023868180869612843 Traning Loss: 0.00026495096972212195 17.983898162841797 2.012777805328369\n",
            "23946 3.789349284488708e-05 0.00035408904659561813 Traning Loss: 0.00039198255399242043 17.983306884765625 2.013045072555542\n",
            "23947 0.00014546349120792001 0.001103552640415728 Traning Loss: 0.0012490161461755633 17.982620239257812 2.0130159854888916\n",
            "23948 0.00018567849474493414 0.0016952239675447345 Traning Loss: 0.0018809024477377534 17.982078552246094 2.013371706008911\n",
            "23949 0.00018999222083948553 0.0014692562399432063 Traning Loss: 0.0016592484898865223 17.981313705444336 2.013417959213257\n",
            "23950 9.009773202706128e-05 0.0008033543126657605 Traning Loss: 0.0008934520301409066 17.98067855834961 2.0136771202087402\n",
            "23951 4.140611781622283e-05 0.000291009055217728 Traning Loss: 0.00033241516212001443 17.98000717163086 2.0139050483703613\n",
            "23952 4.900298154097982e-05 0.00032352335983887315 Traning Loss: 0.0003725263522937894 17.979381561279297 2.0140063762664795\n",
            "23953 7.436198211507872e-05 0.0006661101360805333 Traning Loss: 0.0007404721109196544 17.978851318359375 2.0143327713012695\n",
            "23954 0.00010805935016833246 0.0007869266555644572 Traning Loss: 0.0008949859766289592 17.978254318237305 2.014410972595215\n",
            "23955 7.13137851562351e-05 0.0006117795710451901 Traning Loss: 0.0006830933270975947 17.977794647216797 2.0146870613098145\n",
            "23956 4.6257508074631914e-05 0.0003254236653447151 Traning Loss: 0.00037168117705732584 17.9772891998291 2.014864683151245\n",
            "23957 3.329423634568229e-05 0.00024881778517737985 Traning Loss: 0.0002821120142471045 17.97688865661621 2.0149970054626465\n",
            "23958 4.189009268884547e-05 0.0003707507567014545 Traning Loss: 0.0004126408603042364 17.97654151916504 2.015246629714966\n",
            "23959 6.021286026225425e-05 0.00044493310269899666 Traning Loss: 0.0005051459884271026 17.976205825805664 2.015300750732422\n",
            "23960 4.195295332465321e-05 0.00037428230280056596 Traning Loss: 0.00041623524157330394 17.975969314575195 2.015505790710449\n",
            "23961 3.0397104637813754e-05 0.00020547119493130594 Traning Loss: 0.0002358682977501303 17.9757137298584 2.0155887603759766\n",
            "23962 1.8269729480380192e-05 0.00014730625844094902 Traning Loss: 0.0001655759842833504 17.975526809692383 2.015672445297241\n",
            "23963 2.4285305698867887e-05 0.0002277795283589512 Traning Loss: 0.0002520648413337767 17.975326538085938 2.0158040523529053\n",
            "23964 3.908126018359326e-05 0.0003156788880005479 Traning Loss: 0.00035476015182211995 17.97515296936035 2.015803098678589\n",
            "23965 3.010520049429033e-05 0.00030588736990466714 Traning Loss: 0.0003359925758559257 17.975011825561523 2.015916109085083\n",
            "23966 2.3218510250444524e-05 0.00017350151028949767 Traning Loss: 0.0001967200223589316 17.974889755249023 2.015907049179077\n",
            "23967 6.378121724992525e-06 7.142964750528336e-05 Traning Loss: 7.780776650179178e-05 17.974834442138672 2.0159316062927246\n",
            "23968 7.114709660527296e-06 7.418356835842133e-05 Traning Loss: 8.129827619995922e-05 17.97479820251465 2.015957832336426\n",
            "23969 1.8175365767092444e-05 0.0001591518521308899 Traning Loss: 0.00017732722335495055 17.97480010986328 2.015906810760498\n",
            "23970 2.0976342057110742e-05 0.0002308530965819955 Traning Loss: 0.00025182944955304265 17.974815368652344 2.015941619873047\n",
            "23971 2.337522164452821e-05 0.00020182036678306758 Traning Loss: 0.0002251955884275958 17.974857330322266 2.015875816345215\n",
            "23972 1.0025563824456185e-05 0.00012088447692804039 Traning Loss: 0.00013091004802845418 17.974916458129883 2.0158658027648926\n",
            "23973 4.786224508279702e-06 5.042484190198593e-05 Traning Loss: 5.521106504602358e-05 17.975004196166992 2.015831708908081\n",
            "23974 5.069387952971738e-06 4.900293060927652e-05 Traning Loss: 5.407231947174296e-05 17.975109100341797 2.0157675743103027\n",
            "23975 7.720099347352516e-06 9.753205813467503e-05 Traning Loss: 0.00010525215475354344 17.975231170654297 2.015758991241455\n",
            "23976 1.499730296927737e-05 0.00013072013098280877 Traning Loss: 0.00014571743668057024 17.97536277770996 2.0156731605529785\n",
            "23977 1.0967265552608296e-05 0.00012800066906493157 Traning Loss: 0.00013896793825551867 17.975500106811523 2.015650987625122\n",
            "23978 9.076082278625108e-06 8.900334069039673e-05 Traning Loss: 9.807942115003243e-05 17.975656509399414 2.0155885219573975\n",
            "23979 5.22847767570056e-06 5.9585920098470524e-05 Traning Loss: 6.481439777417108e-05 17.97580909729004 2.015531063079834\n",
            "23980 4.179638835921651e-06 5.7619541621534154e-05 Traning Loss: 6.179918273119256e-05 17.97599983215332 2.0155017375946045\n",
            "23981 7.900967830209993e-06 7.014274160610512e-05 Traning Loss: 7.804371125530452e-05 17.976184844970703 2.0154216289520264\n",
            "23982 6.312101504590828e-06 8.174797403626144e-05 Traning Loss: 8.806007826933637e-05 17.976394653320312 2.0154004096984863\n",
            "23983 7.66818720876472e-06 7.19883173587732e-05 Traning Loss: 7.965650729602203e-05 17.976608276367188 2.0153329372406006\n",
            "23984 4.535635525826365e-06 5.868157677468844e-05 Traning Loss: 6.321721593849361e-05 17.976821899414062 2.0152952671051025\n",
            "23985 4.096135398867773e-06 5.1244336646050215e-05 Traning Loss: 5.534047159017064e-05 17.977052688598633 2.0152578353881836\n",
            "23986 5.556401902140351e-06 5.574234091909602e-05 Traning Loss: 6.129874236648902e-05 17.97726821899414 2.0151970386505127\n",
            "23987 4.930313025397481e-06 6.621777720283717e-05 Traning Loss: 7.11480897734873e-05 17.97751235961914 2.0151774883270264\n",
            "23988 7.047316103125922e-06 6.503425538539886e-05 Traning Loss: 7.208157330751419e-05 17.977737426757812 2.01511287689209\n",
            "23989 4.042612090415787e-06 5.743865767726675e-05 Traning Loss: 6.148126703919843e-05 17.97797966003418 2.015087127685547\n",
            "23990 3.9612618820683565e-06 4.419033575686626e-05 Traning Loss: 4.8151596274692565e-05 17.97821807861328 2.0150394439697266\n",
            "23991 2.915021923399763e-06 3.9866743463790044e-05 Traning Loss: 4.2781764932442456e-05 17.978452682495117 2.0149946212768555\n",
            "23992 2.9865564101783093e-06 4.49136714451015e-05 Traning Loss: 4.7900226491037756e-05 17.97869873046875 2.014967679977417\n",
            "23993 4.950762559019495e-06 5.193246033741161e-05 Traning Loss: 5.6883221986936405e-05 17.978927612304688 2.0149126052856445\n",
            "23994 3.8611892705375794e-06 5.717889507650398e-05 Traning Loss: 6.10400820733048e-05 17.979177474975586 2.014892101287842\n",
            "23995 4.8827537284523714e-06 5.2320698159746826e-05 Traning Loss: 5.7203451433451846e-05 17.979412078857422 2.014841318130493\n",
            "23996 2.914919832619489e-06 4.6323868446052074e-05 Traning Loss: 4.923878805129789e-05 17.979660034179688 2.014810562133789\n",
            "23997 2.8873294013465056e-06 4.055333920405246e-05 Traning Loss: 4.344066837802529e-05 17.979900360107422 2.0147745609283447\n",
            "23998 2.8172682959848316e-06 4.0147369873011485e-05 Traning Loss: 4.296463885111734e-05 17.980138778686523 2.01473069190979\n",
            "23999 2.6272289233020274e-06 4.3470288801472634e-05 Traning Loss: 4.6097517042653635e-05 17.980379104614258 2.0147056579589844\n",
            "24000 3.6811322843277594e-06 4.517294655670412e-05 Traning Loss: 4.8854079068405554e-05 17.980607986450195 2.0146565437316895\n",
            "24001 2.7240341751166852e-06 4.604203422786668e-05 Traning Loss: 4.876606908510439e-05 17.980844497680664 2.01462984085083\n",
            "24002 3.224680540370173e-06 4.318073479225859e-05 Traning Loss: 4.6405413741013035e-05 17.981067657470703 2.0145862102508545\n",
            "24003 2.4188707357097883e-06 4.170070315012708e-05 Traning Loss: 4.411957343108952e-05 17.98129653930664 2.01455020904541\n",
            "24004 2.515313781259465e-06 4.1075061744777486e-05 Traning Loss: 4.3590374843915924e-05 17.981515884399414 2.0145156383514404\n",
            "24005 2.68847861661925e-06 4.186502701486461e-05 Traning Loss: 4.4553504267241806e-05 17.981735229492188 2.0144708156585693\n",
            "24006 2.3971276732481783e-06 4.306513073970564e-05 Traning Loss: 4.546225682133809e-05 17.981952667236328 2.0144405364990234\n",
            "24007 2.779679562081583e-06 4.2277941247448325e-05 Traning Loss: 4.5057618990540504e-05 17.982168197631836 2.0143935680389404\n",
            "24008 2.135774366252008e-06 4.12874978792388e-05 Traning Loss: 4.342327156336978e-05 17.982385635375977 2.014359474182129\n",
            "24009 2.282685045429389e-06 3.940276656066999e-05 Traning Loss: 4.1685452742967755e-05 17.98259925842285 2.0143158435821533\n",
            "24010 2.016691951212124e-06 3.89601118513383e-05 Traning Loss: 4.0976803575176746e-05 17.982812881469727 2.014273166656494\n",
            "24011 2.041365178229171e-06 3.9507482142653316e-05 Traning Loss: 4.154884663876146e-05 17.983022689819336 2.0142340660095215\n",
            "24012 2.3050213258102303e-06 4.04035636165645e-05 Traning Loss: 4.2708583350759e-05 17.983230590820312 2.0141854286193848\n",
            "24013 2.087102529912954e-06 4.138570511713624e-05 Traning Loss: 4.347280628280714e-05 17.983434677124023 2.0141470432281494\n",
            "24014 2.3398035864374833e-06 4.0967835957417265e-05 Traning Loss: 4.330763840698637e-05 17.98363494873047 2.014096975326538\n",
            "24015 1.980445631488692e-06 4.043011358589865e-05 Traning Loss: 4.2410560126882046e-05 17.98383140563965 2.0140540599823\n",
            "24016 2.0047364159836434e-06 3.9401871617883444e-05 Traning Loss: 4.140660894336179e-05 17.984027862548828 2.0140061378479004\n",
            "24017 1.919680471473839e-06 3.892399399774149e-05 Traning Loss: 4.084367537871003e-05 17.98421859741211 2.0139570236206055\n",
            "24018 1.7957930822376511e-06 3.904761979356408e-05 Traning Loss: 4.084341344423592e-05 17.984411239624023 2.0139119625091553\n",
            "24019 2.0122984096815344e-06 3.911338353645988e-05 Traning Loss: 4.112568058189936e-05 17.98459815979004 2.0138587951660156\n",
            "24020 1.7591977439224138e-06 3.95526185457129e-05 Traning Loss: 4.131181776756421e-05 17.984785079956055 2.0138139724731445\n",
            "24021 1.9798683297267416e-06 3.922020914615132e-05 Traning Loss: 4.1200077248504385e-05 17.984968185424805 2.013760805130005\n",
            "24022 1.7317335050393012e-06 3.913645196007565e-05 Traning Loss: 4.086818444193341e-05 17.985149383544922 2.0137135982513428\n",
            "24023 1.8081113921653014e-06 3.8743110053474084e-05 Traning Loss: 4.0551221900386736e-05 17.985328674316406 2.0136630535125732\n",
            "24024 1.7725072893881588e-06 3.8664886233164e-05 Traning Loss: 4.043739318149164e-05 17.985504150390625 2.0136125087738037\n",
            "24025 1.7011301451930194e-06 3.8841430068714544e-05 Traning Loss: 4.054255987284705e-05 17.98567771911621 2.0135650634765625\n",
            "24026 1.865328613348538e-06 3.8866197428433225e-05 Traning Loss: 4.0731527406023815e-05 17.985843658447266 2.01351261138916\n",
            "24027 1.6685125956428237e-06 3.9161895983852446e-05 Traning Loss: 4.083040767000057e-05 17.986011505126953 2.0134668350219727\n",
            "24028 1.8579096376925008e-06 3.888881110469811e-05 Traning Loss: 4.074671960552223e-05 17.98617172241211 2.0134146213531494\n",
            "24029 1.6332583072653506e-06 3.8874288293300197e-05 Traning Loss: 4.05075479648076e-05 17.986331939697266 2.0133681297302246\n",
            "24030 1.737113166200288e-06 3.849320637527853e-05 Traning Loss: 4.023031942779198e-05 17.986486434936523 2.0133183002471924\n",
            "24031 1.6240206832662807e-06 3.840149656753056e-05 Traning Loss: 4.002551577286795e-05 17.98664093017578 2.013270378112793\n",
            "24032 1.624080141482409e-06 3.832377842627466e-05 Traning Loss: 3.994785947725177e-05 17.986791610717773 2.013223648071289\n",
            "24033 1.6555031834286638e-06 3.830962668871507e-05 Traning Loss: 3.996512896264903e-05 17.9869384765625 2.0131747722625732\n",
            "24034 1.565492993904627e-06 3.844115053652786e-05 Traning Loss: 4.0006641938816756e-05 17.987083435058594 2.0131301879882812\n",
            "24035 1.663617581471044e-06 3.834592644125223e-05 Traning Loss: 4.0009545045904815e-05 17.987224578857422 2.0130817890167236\n",
            "24036 1.534787543278071e-06 3.841755460598506e-05 Traning Loss: 3.9952341467142105e-05 17.98736572265625 2.013037919998169\n",
            "24037 1.6172452887985855e-06 3.8236794352997094e-05 Traning Loss: 3.985403964179568e-05 17.987503051757812 2.012991189956665\n",
            "24038 1.5239120330079459e-06 3.823578663286753e-05 Traning Loss: 3.975969957537018e-05 17.987638473510742 2.012946844100952\n",
            "24039 1.551242917230411e-06 3.815288073383272e-05 Traning Loss: 3.970412217313424e-05 17.987770080566406 2.01290225982666\n",
            "24040 1.5309474292735104e-06 3.81648242182564e-05 Traning Loss: 3.969577301177196e-05 17.987899780273438 2.012857437133789\n",
            "24041 1.5005234672571532e-06 3.821171776507981e-05 Traning Loss: 3.9712242141831666e-05 17.988025665283203 2.01281476020813\n",
            "24042 1.5322677882068092e-06 3.819485937128775e-05 Traning Loss: 3.9727128751110286e-05 17.988149642944336 2.012770175933838\n",
            "24043 1.467573724767135e-06 3.824909799732268e-05 Traning Loss: 3.9716673200018704e-05 17.988271713256836 2.012728452682495\n",
            "24044 1.5048799468786456e-06 3.816967364400625e-05 Traning Loss: 3.9674552681390196e-05 17.988391876220703 2.0126848220825195\n",
            "24045 1.4458144050877308e-06 3.8165617297636345e-05 Traning Loss: 3.961143011110835e-05 17.988508224487305 2.012643337249756\n",
            "24046 1.4551048934663413e-06 3.809036934399046e-05 Traning Loss: 3.9545473555335775e-05 17.988624572753906 2.012601375579834\n",
            "24047 1.433830789210333e-06 3.805791129707359e-05 Traning Loss: 3.949174060835503e-05 17.988737106323242 2.0125598907470703\n",
            "24048 1.4049168157725944e-06 3.8052385207265615e-05 Traning Loss: 3.94573035009671e-05 17.988849639892578 2.012519359588623\n",
            "24049 1.42752583087713e-06 3.8009380659786984e-05 Traning Loss: 3.943690535379574e-05 17.98895835876465 2.0124778747558594\n",
            "24050 1.3681125210496248e-06 3.8053800381021574e-05 Traning Loss: 3.942191324313171e-05 17.98906707763672 2.0124385356903076\n",
            "24051 1.4151368077364168e-06 3.798748002736829e-05 Traning Loss: 3.94026174035389e-05 17.989171981811523 2.012397527694702\n",
            "24052 1.3427775229502004e-06 3.803211802733131e-05 Traning Loss: 3.937489600502886e-05 17.989276885986328 2.0123589038848877\n",
            "24053 1.3882572602597065e-06 3.794878284679726e-05 Traning Loss: 3.933703919756226e-05 17.989377975463867 2.0123188495635986\n",
            "24054 1.323778974438028e-06 3.797216777456924e-05 Traning Loss: 3.929594822693616e-05 17.989479064941406 2.0122807025909424\n",
            "24055 1.3516654462364386e-06 3.7908630474703386e-05 Traning Loss: 3.926029603462666e-05 17.98957633972168 2.0122416019439697\n",
            "24056 1.3105736798024736e-06 3.792060670093633e-05 Traning Loss: 3.923118129023351e-05 17.989673614501953 2.0122034549713135\n",
            "24057 1.317491069130483e-06 3.789357651839964e-05 Traning Loss: 3.92110669054091e-05 17.98976707458496 2.0121655464172363\n",
            "24058 1.300839358009398e-06 3.790011760429479e-05 Traning Loss: 3.920095696230419e-05 17.98986053466797 2.012127637863159\n",
            "24059 1.2926008139402256e-06 3.7904315831838176e-05 Traning Loss: 3.919691516784951e-05 17.989952087402344 2.0120904445648193\n",
            "24060 1.2893826806248398e-06 3.790891787502915e-05 Traning Loss: 3.919830123777501e-05 17.990041732788086 2.0120527744293213\n",
            "24061 1.2770573221132508e-06 3.7926158256595954e-05 Traning Loss: 3.9203216147143394e-05 17.990129470825195 2.0120162963867188\n",
            "24062 1.2739770909320214e-06 3.794274016399868e-05 Traning Loss: 3.921671668649651e-05 17.990217208862305 2.011979341506958\n",
            "24063 1.2718115840471e-06 3.796979217440821e-05 Traning Loss: 3.9241604099515826e-05 17.99030113220215 2.0119431018829346\n",
            "24064 1.259325358660135e-06 3.80256497010123e-05 Traning Loss: 3.928497608285397e-05 17.990385055541992 2.011906862258911\n",
            "24065 1.2799431488019763e-06 3.807849498116411e-05 Traning Loss: 3.9358437788905576e-05 17.99046516418457 2.0118706226348877\n",
            "24066 1.2540122042992152e-06 3.821806240011938e-05 Traning Loss: 3.947207369492389e-05 17.99054718017578 2.0118353366851807\n",
            "24067 1.3065401844869484e-06 3.8337417208822444e-05 Traning Loss: 3.9643957279622555e-05 17.990623474121094 2.0117990970611572\n",
            "24068 1.2705399967671838e-06 3.862516678054817e-05 Traning Loss: 3.98957054130733e-05 17.990703582763672 2.0117645263671875\n",
            "24069 1.3652646657646983e-06 3.889883009833284e-05 Traning Loss: 4.026409442303702e-05 17.99077606201172 2.011728525161743\n",
            "24070 1.332030706180376e-06 3.946374999941327e-05 Traning Loss: 4.079578138771467e-05 17.990854263305664 2.0116946697235107\n",
            "24071 1.4897764231136534e-06 4.0080660255625844e-05 Traning Loss: 4.157043804298155e-05 17.990922927856445 2.0116586685180664\n",
            "24072 1.487088184148888e-06 4.1211686038877815e-05 Traning Loss: 4.269877535989508e-05 17.990999221801758 2.0116257667541504\n",
            "24073 1.7561382037456497e-06 4.260517016518861e-05 Traning Loss: 4.436130984686315e-05 17.991064071655273 2.011590003967285\n",
            "24074 1.8447281036060303e-06 4.49676881544292e-05 Traning Loss: 4.681241625803523e-05 17.991140365600586 2.0115578174591064\n",
            "24075 2.338861122552771e-06 4.8138688725885004e-05 Traning Loss: 5.047754893894307e-05 17.991199493408203 2.011522054672241\n",
            "24076 2.662782435436384e-06 5.328951374394819e-05 Traning Loss: 5.59522959520109e-05 17.991273880004883 2.011491060256958\n",
            "24077 3.649349764600629e-06 6.061220483388752e-05 Traning Loss: 6.42615559627302e-05 17.991323471069336 2.0114550590515137\n",
            "24078 4.5615511226060335e-06 7.227371679618955e-05 Traning Loss: 7.683526928303763e-05 17.991397857666016 2.011425495147705\n",
            "24079 6.67758240524563e-06 8.95421690074727e-05 Traning Loss: 9.621975186746567e-05 17.991436004638672 2.0113887786865234\n",
            "24080 9.062973731488455e-06 0.00011681440810207278 Traning Loss: 0.00012587738456204534 17.99151039123535 2.0113613605499268\n",
            "24081 1.3858772945241071e-05 0.00015842817083466798 Traning Loss: 0.00017228694923687726 17.991527557373047 2.0113229751586914\n",
            "24082 1.9943510778830387e-05 0.0002237605076516047 Traning Loss: 0.0002437040238874033 17.991600036621094 2.011298894882202\n",
            "24083 3.121385088888928e-05 0.0003254406328778714 Traning Loss: 0.0003566544910427183 17.991579055786133 2.0112574100494385\n",
            "24084 4.645105582312681e-05 0.00048321776557713747 Traning Loss: 0.0005296688177622855 17.991636276245117 2.0112383365631104\n",
            "24085 7.319911674130708e-05 0.0007302407757379115 Traning Loss: 0.0008034398779273033 17.99153709411621 2.0111916065216064\n",
            "24086 0.0001097368513001129 0.0011010431917384267 Traning Loss: 0.001210779999382794 17.991544723510742 2.0111799240112305\n",
            "24087 0.00017039790691342205 0.0016676218947395682 Traning Loss: 0.0018380198162049055 17.991291046142578 2.0111243724823\n",
            "24088 0.00024748544092290103 0.0024432609789073467 Traning Loss: 0.002690746448934078 17.99116325378418 2.0111238956451416\n",
            "24089 0.0003620498173404485 0.003511089598760009 Traning Loss: 0.003873139386996627 17.99061393737793 2.0110552310943604\n",
            "24090 0.00047211849596351385 0.004625758156180382 Traning Loss: 0.0050978767685592175 17.99020004272461 2.011070728302002\n",
            "24091 0.0005884739221073687 0.00566017534583807 Traning Loss: 0.006248649209737778 17.989234924316406 2.010989189147949\n",
            "24092 0.0005912345950491726 0.0057566422037780285 Traning Loss: 0.006347876973450184 17.988454818725586 2.0110251903533936\n",
            "24093 0.0005092753563076258 0.004793324042111635 Traning Loss: 0.005302599631249905 17.98726463317871 2.010951042175293\n",
            "24094 0.00028608544380404055 0.0027080774307250977 Traning Loss: 0.0029941629618406296 17.986379623413086 2.0110087394714355\n",
            "24095 0.00010255947563564405 0.0007710187928751111 Traning Loss: 0.0008735782466828823 17.985485076904297 2.0109915733337402\n",
            "24096 2.7461512217996642e-05 9.110203245654702e-05 Traning Loss: 0.00011856354831252247 17.984752655029297 2.0110607147216797\n",
            "24097 0.00010135514457942918 0.0007765229674987495 Traning Loss: 0.0008778781048022211 17.984180450439453 2.011136531829834\n",
            "24098 0.00021799308888148516 0.0018561622127890587 Traning Loss: 0.002074155258014798 17.983373641967773 2.011204719543457\n",
            "24099 0.00024337861395906657 0.0021509400103241205 Traning Loss: 0.0023943185806274414 17.982707977294922 2.0113658905029297\n",
            "24100 0.00018421892309561372 0.0014443584950640798 Traning Loss: 0.0016285774763673544 17.981801986694336 2.0114517211914062\n",
            "24101 7.435481529682875e-05 0.00047248744522221386 Traning Loss: 0.0005468422314152122 17.98102378845215 2.0116641521453857\n",
            "24102 4.902671207673848e-05 0.00013489864068105817 Traning Loss: 0.00018392535275779665 17.98027229309082 2.0118048191070557\n",
            "24103 8.179606084013358e-05 0.0005623200559057295 Traning Loss: 0.000644116138573736 17.979473114013672 2.0120151042938232\n",
            "24104 0.00013650993059854954 0.001048266887664795 Traning Loss: 0.0011847767746075988 17.978809356689453 2.0122265815734863\n",
            "24105 0.00013231833872850984 0.0010355295380577445 Traning Loss: 0.0011678478913381696 17.978015899658203 2.012416124343872\n",
            "24106 8.027937292354181e-05 0.000559084233827889 Traning Loss: 0.0006393635994754732 17.977388381958008 2.0126969814300537\n",
            "24107 4.413201168063097e-05 0.00016980698273982853 Traning Loss: 0.0002139389980584383 17.976770401000977 2.0128958225250244\n",
            "24108 3.7329453334677964e-05 0.00021829745674040169 Traning Loss: 0.00025562691735103726 17.976221084594727 2.013197898864746\n",
            "24109 7.034595910226926e-05 0.0004940864746458828 Traning Loss: 0.0005644324119202793 17.975786209106445 2.0134294033050537\n",
            "24110 7.645961159141734e-05 0.0006375260418280959 Traning Loss: 0.0007139856461435556 17.975292205810547 2.0136890411376953\n",
            "24111 5.958099427516572e-05 0.0004670643829740584 Traning Loss: 0.0005266453954391181 17.974929809570312 2.0139548778533936\n",
            "24112 3.33120406139642e-05 0.00020330115512479097 Traning Loss: 0.00023661319573875517 17.97454261779785 2.014157295227051\n",
            "24113 1.6957224943325855e-05 0.00011325589002808556 Traning Loss: 0.000130213113152422 17.974225997924805 2.0144290924072266\n",
            "24114 3.288485459052026e-05 0.00021994604321662337 Traning Loss: 0.00025283091235905886 17.97395896911621 2.0145914554595947\n",
            "24115 3.9220078178914264e-05 0.00037129592965357006 Traning Loss: 0.00041051601874642074 17.97367286682129 2.0148158073425293\n",
            "24116 4.233536310493946e-05 0.0003653898893389851 Traning Loss: 0.00040772525244392455 17.973468780517578 2.0149688720703125\n",
            "24117 2.57734973274637e-05 0.0002312209107913077 Traning Loss: 0.0002569944190327078 17.97323989868164 2.0151193141937256\n",
            "24118 1.0424068022985011e-05 9.842411236604676e-05 Traning Loss: 0.00010884818038903177 17.97308921813965 2.015282154083252\n",
            "24119 1.1849316251755226e-05 7.934433233458549e-05 Traning Loss: 9.119365131482482e-05 17.97296714782715 2.0153679847717285\n",
            "24120 1.5270401490852237e-05 0.00016970762226264924 Traning Loss: 0.00018497802375350147 17.972875595092773 2.015516757965088\n",
            "24121 2.7555348424357362e-05 0.00024261997896246612 Traning Loss: 0.0002701753401197493 17.972848892211914 2.015566110610962\n",
            "24122 2.2068838006816804e-05 0.00023773570137564093 Traning Loss: 0.00025980453938245773 17.972824096679688 2.0156657695770264\n",
            "24123 1.5599540347466245e-05 0.00014798650227021426 Traning Loss: 0.0001635860389797017 17.972862243652344 2.015709400177002\n",
            "24124 6.118907549534924e-06 6.53417082503438e-05 Traning Loss: 7.146061398088932e-05 17.972915649414062 2.015745162963867\n",
            "24125 3.689707682497101e-06 4.913294833386317e-05 Traning Loss: 5.282265556161292e-05 17.973003387451172 2.015793561935425\n",
            "24126 1.0474231203261297e-05 9.319008677266538e-05 Traning Loss: 0.00010366431524744257 17.973119735717773 2.0157787799835205\n",
            "24127 1.311273626924958e-05 0.00015000392158981413 Traning Loss: 0.0001631166524020955 17.973237991333008 2.0158181190490723\n",
            "24128 1.7114456568378955e-05 0.00015614218136761338 Traning Loss: 0.00017325664521194994 17.973386764526367 2.0157835483551025\n",
            "24129 1.0463266335136723e-05 0.00011940085823880509 Traning Loss: 0.00012986412912141532 17.973529815673828 2.015794515609741\n",
            "24130 6.773794211767381e-06 6.448934436775744e-05 Traning Loss: 7.126313721528277e-05 17.97369384765625 2.0157673358917236\n",
            "24131 3.267946794949239e-06 3.9701299101579934e-05 Traning Loss: 4.296924453228712e-05 17.973873138427734 2.015742540359497\n",
            "24132 4.397376414999599e-06 5.275517105474137e-05 Traning Loss: 5.715254883398302e-05 17.974056243896484 2.0157277584075928\n",
            "24133 8.246227480412927e-06 8.188767242245376e-05 Traning Loss: 9.013390081236139e-05 17.974267959594727 2.015674114227295\n",
            "24134 8.64831599756144e-06 0.00010016394662670791 Traning Loss: 0.00010881226626224816 17.974475860595703 2.0156590938568115\n",
            "24135 9.174962542601861e-06 8.887807780411094e-05 Traning Loss: 9.80530385277234e-05 17.974700927734375 2.0155961513519287\n",
            "24136 5.2031268751306925e-06 6.515720451716334e-05 Traning Loss: 7.036032911855727e-05 17.974929809570312 2.015568256378174\n",
            "24137 4.28573503086227e-06 4.423586869961582e-05 Traning Loss: 4.852160418522544e-05 17.975160598754883 2.015514612197876\n",
            "24138 3.2436880701425252e-06 4.312110831961036e-05 Traning Loss: 4.6364795707631856e-05 17.975406646728516 2.0154690742492676\n",
            "24139 4.6808854676783085e-06 5.448969386634417e-05 Traning Loss: 5.917057933402248e-05 17.975643157958984 2.015427827835083\n",
            "24140 5.87064369028667e-06 6.567306263605133e-05 Traning Loss: 7.15437054168433e-05 17.975900650024414 2.0153696537017822\n",
            "24141 5.49963397133979e-06 6.662387022515759e-05 Traning Loss: 7.212350465124473e-05 17.976150512695312 2.015334129333496\n",
            "24142 5.166911705600796e-06 5.6182216212619096e-05 Traning Loss: 6.134912837296724e-05 17.976411819458008 2.0152747631073\n",
            "24143 3.342882791912416e-06 4.564511255011894e-05 Traning Loss: 4.898799670627341e-05 17.97667694091797 2.015237808227539\n",
            "24144 3.5683021906152135e-06 4.044724482810125e-05 Traning Loss: 4.401554542710073e-05 17.97693634033203 2.0151865482330322\n",
            "24145 3.3129927032859996e-06 4.462846845854074e-05 Traning Loss: 4.7941459342837334e-05 17.977209091186523 2.0151450634002686\n",
            "24146 4.353986696514767e-06 5.0706574256764725e-05 Traning Loss: 5.506056186277419e-05 17.977466583251953 2.015103340148926\n",
            "24147 4.38789720647037e-06 5.3895193559583277e-05 Traning Loss: 5.828309076605365e-05 17.977737426757812 2.015058994293213\n",
            "24148 4.126726707909256e-06 5.082061034045182e-05 Traning Loss: 5.494733704836108e-05 17.97799301147461 2.0150229930877686\n",
            "24149 3.531544734869385e-06 4.430101762409322e-05 Traning Loss: 4.783256372320466e-05 17.978252410888672 2.0149803161621094\n",
            "24150 2.813809487633989e-06 3.936008579330519e-05 Traning Loss: 4.21738950535655e-05 17.978506088256836 2.014946222305298\n",
            "24151 2.8922843284817645e-06 3.834786912193522e-05 Traning Loss: 4.124015322304331e-05 17.97875213623047 2.014907121658325\n",
            "24152 2.8586487132997718e-06 4.155471833655611e-05 Traning Loss: 4.4413365685613826e-05 17.979005813598633 2.014871835708618\n",
            "24153 3.3921676276804646e-06 4.496628753258847e-05 Traning Loss: 4.835845538764261e-05 17.979248046875 2.014833927154541\n",
            "24154 3.2202578950091265e-06 4.662303035729565e-05 Traning Loss: 4.9843289161799476e-05 17.97950553894043 2.014796495437622\n",
            "24155 3.2223156267718878e-06 4.4742704631062225e-05 Traning Loss: 4.796501889359206e-05 17.979751586914062 2.0147576332092285\n",
            "24156 2.6394318410893902e-06 4.1581264667911455e-05 Traning Loss: 4.422069469001144e-05 17.980005264282227 2.0147197246551514\n",
            "24157 2.481506271578837e-06 3.858721174765378e-05 Traning Loss: 4.106871710973792e-05 17.980253219604492 2.014678716659546\n",
            "24158 2.2293579604593106e-06 3.7856782000744715e-05 Traning Loss: 4.0086139051709324e-05 17.980501174926758 2.0146400928497314\n",
            "24159 2.367720298934728e-06 3.8808379031252116e-05 Traning Loss: 4.1176099330186844e-05 17.980751037597656 2.014596462249756\n",
            "24160 2.4744472284510266e-06 4.051182622788474e-05 Traning Loss: 4.298627391108312e-05 17.98099136352539 2.014554977416992\n",
            "24161 2.526328671592637e-06 4.151448229094967e-05 Traning Loss: 4.4040811189915985e-05 17.981237411499023 2.0145092010498047\n",
            "24162 2.548686779846321e-06 4.113970135222189e-05 Traning Loss: 4.368838926893659e-05 17.98147201538086 2.014462471008301\n",
            "24163 2.2827775865152944e-06 4.001459456048906e-05 Traning Loss: 4.2297371692257e-05 17.98171043395996 2.0144152641296387\n",
            "24164 2.278450892845285e-06 3.8529542507603765e-05 Traning Loss: 4.0807994082570076e-05 17.981937408447266 2.0143632888793945\n",
            "24165 1.9829080883937422e-06 3.802928767981939e-05 Traning Loss: 4.0012197132455185e-05 17.98216438293457 2.014315366744995\n",
            "24166 2.1586693037534133e-06 3.796125383814797e-05 Traning Loss: 4.011992132291198e-05 17.98238754272461 2.014259099960327\n",
            "24167 1.9790438727795845e-06 3.877968993037939e-05 Traning Loss: 4.075873221154325e-05 17.982608795166016 2.014209747314453\n",
            "24168 2.195699380536098e-06 3.9122118323575705e-05 Traning Loss: 4.13178167946171e-05 17.982830047607422 2.0141518115997314\n",
            "24169 2.0223096726113e-06 3.934149208362214e-05 Traning Loss: 4.136380084673874e-05 17.98304557800293 2.0141000747680664\n",
            "24170 2.083094386762241e-06 3.876976188621484e-05 Traning Loss: 4.085285763721913e-05 17.983261108398438 2.014042615890503\n",
            "24171 1.928999836309231e-06 3.8149519241414964e-05 Traning Loss: 4.007851748610847e-05 17.983470916748047 2.0139880180358887\n",
            "24172 1.8794322613757686e-06 3.7551886634901166e-05 Traning Loss: 3.9431317418348044e-05 17.983678817749023 2.013932228088379\n",
            "24173 1.8737165419224766e-06 3.7298919778550044e-05 Traning Loss: 3.917263529729098e-05 17.983882904052734 2.0138745307922363\n",
            "24174 1.8067856899506296e-06 3.7504913052544e-05 Traning Loss: 3.931169703719206e-05 17.98408317565918 2.013820171356201\n",
            "24175 1.941494247148512e-06 3.77092910639476e-05 Traning Loss: 3.9650785765843466e-05 17.98427963256836 2.013760566711426\n",
            "24176 1.8292478216608288e-06 3.81041390937753e-05 Traning Loss: 3.993338759755716e-05 17.98447036743164 2.013706922531128\n",
            "24177 1.9694882666954072e-06 3.801714046858251e-05 Traning Loss: 3.9986629417398944e-05 17.984657287597656 2.0136470794677734\n",
            "24178 1.790241185517516e-06 3.7996669561835006e-05 Traning Loss: 3.9786911656847224e-05 17.984838485717773 2.0135936737060547\n",
            "24179 1.8779983292915858e-06 3.756232763407752e-05 Traning Loss: 3.94403250538744e-05 17.985017776489258 2.013535261154175\n",
            "24180 1.7060731352103176e-06 3.7397112464532256e-05 Traning Loss: 3.91031862818636e-05 17.985193252563477 2.013481378555298\n",
            "24181 1.7711280406729202e-06 3.712258694577031e-05 Traning Loss: 3.8893715100130066e-05 17.98536491394043 2.0134246349334717\n",
            "24182 1.6754736407165183e-06 3.716794526553713e-05 Traning Loss: 3.884342004312202e-05 17.985536575317383 2.0133702754974365\n",
            "24183 1.724714365991531e-06 3.718037260114215e-05 Traning Loss: 3.8905087421881035e-05 17.985702514648438 2.013315439224243\n",
            "24184 1.6884864635358099e-06 3.730347452801652e-05 Traning Loss: 3.8991962355794385e-05 17.985868453979492 2.013261318206787\n",
            "24185 1.696562549113878e-06 3.732812183443457e-05 Traning Loss: 3.9024685975164175e-05 17.98602867126465 2.0132083892822266\n",
            "24186 1.67748544299684e-06 3.728928277269006e-05 Traning Loss: 3.8966769352555275e-05 17.986188888549805 2.0131547451019287\n",
            "24187 1.6459655398648465e-06 3.718848529388197e-05 Traning Loss: 3.883445242536254e-05 17.986343383789062 2.0131030082702637\n",
            "24188 1.6351347085219459e-06 3.7038142181700096e-05 Traning Loss: 3.867327541229315e-05 17.986495971679688 2.0130505561828613\n",
            "24189 1.5939456261548912e-06 3.6938810808351263e-05 Traning Loss: 3.8532754842890427e-05 17.986644744873047 2.0129997730255127\n",
            "24190 1.5933802615109016e-06 3.6853722122032195e-05 Traning Loss: 3.844710226985626e-05 17.98678970336914 2.012948751449585\n",
            "24191 1.5584081438646535e-06 3.6863002605969086e-05 Traning Loss: 3.842141086352058e-05 17.9869327545166 2.0128989219665527\n",
            "24192 1.5624796105839778e-06 3.6874251236440614e-05 Traning Loss: 3.843673039227724e-05 17.987071990966797 2.0128490924835205\n",
            "24193 1.5295835282813641e-06 3.693373582791537e-05 Traning Loss: 3.8463320379378274e-05 17.98720932006836 2.0127999782562256\n",
            "24194 1.5342194501499762e-06 3.693763937917538e-05 Traning Loss: 3.847185871563852e-05 17.987342834472656 2.0127511024475098\n",
            "24195 1.4948482203180902e-06 3.6955858377041295e-05 Traning Loss: 3.845070750685409e-05 17.987476348876953 2.0127031803131104\n",
            "24196 1.5025856328065856e-06 3.689499862957746e-05 Traning Loss: 3.8397585740312934e-05 17.987606048583984 2.012655258178711\n",
            "24197 1.4518715261147008e-06 3.6874211218673736e-05 Traning Loss: 3.8326081266859546e-05 17.987735748291016 2.012608528137207\n",
            "24198 1.46898059938394e-06 3.678449138533324e-05 Traning Loss: 3.82534708478488e-05 17.98786163330078 2.012561082839966\n",
            "24199 1.4071480336497189e-06 3.6784684198210016e-05 Traning Loss: 3.819183257292025e-05 17.987985610961914 2.0125155448913574\n",
            "24200 1.4403075283553335e-06 3.670889782370068e-05 Traning Loss: 3.8149206375237554e-05 17.988107681274414 2.0124683380126953\n",
            "24201 1.3699782357434742e-06 3.675587504403666e-05 Traning Loss: 3.812585418927483e-05 17.98822784423828 2.0124239921569824\n",
            "24202 1.4211324241841794e-06 3.6697249015560374e-05 Traning Loss: 3.8118381780805066e-05 17.988344192504883 2.0123770236968994\n",
            "24203 1.3413426813713158e-06 3.677393033285625e-05 Traning Loss: 3.8115271308925e-05 17.988460540771484 2.012333631515503\n",
            "24204 1.4055790416023228e-06 3.670728983706795e-05 Traning Loss: 3.811287024291232e-05 17.98857307434082 2.012286901473999\n",
            "24205 1.3131481182426796e-06 3.679258225020021e-05 Traning Loss: 3.8105728890514e-05 17.988685607910156 2.012244462966919\n",
            "24206 1.3884415466236533e-06 3.67039829143323e-05 Traning Loss: 3.809242480201647e-05 17.988794326782227 2.0121982097625732\n",
            "24207 1.2834073004341917e-06 3.67954489775002e-05 Traning Loss: 3.8078855141066015e-05 17.988903045654297 2.0121564865112305\n",
            "24208 1.3738120969719603e-06 3.6696310417028144e-05 Traning Loss: 3.8070123991928995e-05 17.9890079498291 2.012110471725464\n",
            "24209 1.2567600151669467e-06 3.681713133119047e-05 Traning Loss: 3.807389293797314e-05 17.989112854003906 2.0120697021484375\n",
            "24210 1.3691969797946513e-06 3.672633829410188e-05 Traning Loss: 3.809553527389653e-05 17.989213943481445 2.01202392578125\n",
            "24211 1.2358898402453633e-06 3.6910361814079806e-05 Traning Loss: 3.8146252336446196e-05 17.989315032958984 2.011984348297119\n",
            "24212 1.3817808621752192e-06 3.685199044412002e-05 Traning Loss: 3.82337711926084e-05 17.989412307739258 2.0119385719299316\n",
            "24213 1.224429183821485e-06 3.71457172150258e-05 Traning Loss: 3.837014810414985e-05 17.989511489868164 2.0119004249572754\n",
            "24214 1.4239492429624079e-06 3.715102138812654e-05 Traning Loss: 3.857496994896792e-05 17.989604949951172 2.0118541717529297\n",
            "24215 1.2333669019426452e-06 3.764099164982326e-05 Traning Loss: 3.887435741489753e-05 17.989702224731445 2.0118179321289062\n",
            "24216 1.5196026197372703e-06 3.77901887986809e-05 Traning Loss: 3.9309790736297145e-05 17.989789962768555 2.011770486831665\n",
            "24217 1.2874506865045987e-06 3.8654619856970385e-05 Traning Loss: 3.9942071452969685e-05 17.989883422851562 2.011737108230591\n",
            "24218 1.7184672742587281e-06 3.9150152588263154e-05 Traning Loss: 4.086862099939026e-05 17.989967346191406 2.011687755584717\n",
            "24219 1.4462598301179241e-06 4.0786278987070546e-05 Traning Loss: 4.223253927193582e-05 17.99005889892578 2.011658191680908\n",
            "24220 2.1320508949429495e-06 4.21217737311963e-05 Traning Loss: 4.4253825763007626e-05 17.990137100219727 2.0116052627563477\n",
            "24221 1.8570144675322808e-06 4.541951420833357e-05 Traning Loss: 4.7276527766371146e-05 17.9902286529541 2.0115814208984375\n",
            "24222 3.0172686820151284e-06 4.879795233136974e-05 Traning Loss: 5.1815222832374275e-05 17.990299224853516 2.0115230083465576\n",
            "24223 2.876283360819798e-06 5.58222527615726e-05 Traning Loss: 5.86985370318871e-05 17.990388870239258 2.011507511138916\n",
            "24224 4.97576320412918e-06 6.419418059522286e-05 Traning Loss: 6.91699460730888e-05 17.990449905395508 2.0114402770996094\n",
            "24225 5.377752131607849e-06 7.98986220615916e-05 Traning Loss: 8.527637692168355e-05 17.99053955078125 2.0114376544952393\n",
            "24226 9.462438356422354e-06 0.00010060981003334746 Traning Loss: 0.00011007225111825392 17.990585327148438 2.0113565921783447\n",
            "24227 1.1547666872502305e-05 0.00013715466775465757 Traning Loss: 0.00014870234008412808 17.99067497253418 2.011373996734619\n",
            "24228 2.0048853912157938e-05 0.0001885546080302447 Traning Loss: 0.00020860346558038145 17.99069595336914 2.0112709999084473\n",
            "24229 2.6831039576791227e-05 0.0002757418842520565 Traning Loss: 0.0003025729092769325 17.990779876708984 2.0113203525543213\n",
            "24230 4.5398977817967534e-05 0.0004024791996926069 Traning Loss: 0.00044787817751057446 17.990753173828125 2.011183977127075\n",
            "24231 6.423550803447142e-05 0.0006101339822635055 Traning Loss: 0.0006743695121258497 17.990814208984375 2.0112836360931396\n",
            "24232 0.00010497344919713214 0.0009101460454985499 Traning Loss: 0.0010151194874197245 17.990692138671875 2.0110995769500732\n",
            "24233 0.00015040366270113736 0.0013741102302446961 Traning Loss: 0.0015245139366015792 17.990684509277344 2.011274576187134\n",
            "24234 0.00023145736486185342 0.0019931478891521692 Traning Loss: 0.0022246052976697683 17.99037742614746 2.0110342502593994\n",
            "24235 0.0003142223577015102 0.0028213791083544493 Traning Loss: 0.0031356015242636204 17.99020004272461 2.011305570602417\n",
            "24236 0.0004261089488863945 0.0036570795346051455 Traning Loss: 0.004083188250660896 17.98958396911621 2.011033773422241\n",
            "24237 0.0004842380294576287 0.004331836011260748 Traning Loss: 0.004816073924303055 17.98910903930664 2.0113799571990967\n",
            "24238 0.0005039621610194445 0.00429361779242754 Traning Loss: 0.004797579720616341 17.988178253173828 2.0111727714538574\n",
            "24239 0.00037951432750560343 0.003442762652412057 Traning Loss: 0.0038222770672291517 17.98745346069336 2.0114879608154297\n",
            "24240 0.00023296420113183558 0.0019374156836420298 Traning Loss: 0.0021703799720853567 17.98651123046875 2.0114777088165283\n",
            "24241 6.998742173891515e-05 0.0006663171807304025 Traning Loss: 0.0007363046170212328 17.985794067382812 2.011631965637207\n",
            "24242 2.940093145298306e-05 0.0002585368638392538 Traning Loss: 0.0002879377861972898 17.98512840270996 2.011857748031616\n",
            "24243 9.418886475032195e-05 0.0006949661765247583 Traning Loss: 0.0007891550194472075 17.984455108642578 2.011857271194458\n",
            "24244 0.0001505734835518524 0.0013768838252872229 Traning Loss: 0.0015274572651833296 17.983901977539062 2.0122134685516357\n",
            "24245 0.00019484916992951185 0.001496459124609828 Traning Loss: 0.0016913083381950855 17.983139038085938 2.012233257293701\n",
            "24246 0.00011874805204570293 0.0010600745445117354 Traning Loss: 0.0011788225965574384 17.982498168945312 2.012540817260742\n",
            "24247 6.391880742739886e-05 0.00044118889491073787 Traning Loss: 0.0005051076877862215 17.98177146911621 2.0127267837524414\n",
            "24248 3.6747907870449126e-05 0.00025300856214016676 Traning Loss: 0.00028975645545870066 17.981122970581055 2.0128748416900635\n",
            "24249 5.891488035558723e-05 0.0005299538606777787 Traning Loss: 0.0005888687446713448 17.98052215576172 2.013195514678955\n",
            "24250 0.00010821340401889756 0.0008076640660874546 Traning Loss: 0.0009158774628303945 17.979902267456055 2.01326584815979\n",
            "24251 8.86109410203062e-05 0.0007760199951007962 Traning Loss: 0.0008646309142932296 17.979368209838867 2.0135855674743652\n",
            "24252 5.811580558656715e-05 0.000397555559175089 Traning Loss: 0.00045567136839963496 17.978809356689453 2.013723373413086\n",
            "24253 1.8167575035477057e-05 0.0001135974598582834 Traning Loss: 0.00013176503125578165 17.978361129760742 2.013911485671997\n",
            "24254 2.0799885533051565e-05 0.0001566416904097423 Traning Loss: 0.00017744157230481505 17.977964401245117 2.0141475200653076\n",
            "24255 5.349226194084622e-05 0.00040492694824934006 Traning Loss: 0.00045841920655220747 17.977615356445312 2.0142102241516113\n",
            "24256 6.100556856836192e-05 0.0005694511928595603 Traning Loss: 0.0006304567796178162 17.977338790893555 2.0144500732421875\n",
            "24257 5.688593228114769e-05 0.00043850840302184224 Traning Loss: 0.0004953943425789475 17.977066040039062 2.0144972801208496\n",
            "24258 2.1468864360940643e-05 0.0001937562192324549 Traning Loss: 0.00021522508177440614 17.976825714111328 2.0146353244781494\n",
            "24259 7.3898463597288355e-06 4.38442402810324e-05 Traning Loss: 5.123408482177183e-05 17.97661781311035 2.0147452354431152\n",
            "24260 1.4844737052044366e-05 9.94550937321037e-05 Traning Loss: 0.00011429982987465337 17.976415634155273 2.0147693157196045\n",
            "24261 2.4738088541198522e-05 0.0002524394076317549 Traning Loss: 0.0002771774888969958 17.976268768310547 2.014904260635376\n",
            "24262 3.9067403122317046e-05 0.000306405097944662 Traning Loss: 0.00034547250834293664 17.97614097595215 2.014875888824463\n",
            "24263 2.255269828310702e-05 0.0002442395780235529 Traning Loss: 0.0002667922817636281 17.976062774658203 2.0149500370025635\n",
            "24264 1.4207417734724004e-05 0.00012098955630790442 Traning Loss: 0.00013519696949515492 17.97602081298828 2.0149471759796143\n",
            "24265 7.586868377984501e-06 7.361852476606146e-05 Traning Loss: 8.120539132505655e-05 17.975980758666992 2.014932632446289\n",
            "24266 1.015483485389268e-05 0.00011724375508492813 Traning Loss: 0.0001273985835723579 17.976003646850586 2.0149736404418945\n",
            "24267 2.1974263290758245e-05 0.00017211482918355614 Traning Loss: 0.0001940890942933038 17.976003646850586 2.014906406402588\n",
            "24268 1.5786128642503172e-05 0.0001820471661631018 Traning Loss: 0.00019783328752964735 17.97607421875 2.0149359703063965\n",
            "24269 1.4689925592392683e-05 0.00011736622400349006 Traning Loss: 0.00013205615687184036 17.976146697998047 2.014876365661621\n",
            "24270 3.835281859210227e-06 5.821096056024544e-05 Traning Loss: 6.204624514793977e-05 17.976247787475586 2.0148515701293945\n",
            "24271 3.2240216114587383e-06 4.4346179493004456e-05 Traning Loss: 4.757019996759482e-05 17.976375579833984 2.014831781387329\n",
            "24272 9.32932107389206e-06 7.889776316005737e-05 Traning Loss: 8.822708332445472e-05 17.976482391357422 2.0147597789764404\n",
            "24273 1.055508528224891e-05 0.00012314424384385347 Traning Loss: 0.00013369933003559709 17.976633071899414 2.0147602558135986\n",
            "24274 1.4996103345765732e-05 0.00012409129703883082 Traning Loss: 0.00013908739492762834 17.976755142211914 2.014681816101074\n",
            "24275 7.6349351729732e-06 9.633837908040732e-05 Traning Loss: 0.00010397331789135933 17.976932525634766 2.014660596847534\n",
            "24276 5.703774604626233e-06 5.7603239838499576e-05 Traning Loss: 6.330701580736786e-05 17.97710609436035 2.0146093368530273\n",
            "24277 3.638614998635603e-06 4.576638093567453e-05 Traning Loss: 4.940499638905749e-05 17.9773006439209 2.014549732208252\n",
            "24278 4.368332611193182e-06 5.95307101320941e-05 Traning Loss: 6.389904592651874e-05 17.977506637573242 2.0145299434661865\n",
            "24279 8.26414816401666e-06 7.438055035891011e-05 Traning Loss: 8.264469943242148e-05 17.97770118713379 2.0144565105438232\n",
            "24280 6.060003215679899e-06 7.75840671849437e-05 Traning Loss: 8.364407403860241e-05 17.97791290283203 2.014442205429077\n",
            "24281 6.34606749372324e-06 5.9639482060447335e-05 Traning Loss: 6.598555046366528e-05 17.978116989135742 2.014382839202881\n",
            "24282 2.8707290766760707e-06 4.4211108615854755e-05 Traning Loss: 4.7081837692530826e-05 17.978342056274414 2.0143446922302246\n",
            "24283 2.818710072460817e-06 3.981841655331664e-05 Traning Loss: 4.2637126171030104e-05 17.97856903076172 2.0143094062805176\n",
            "24284 4.413306669448502e-06 4.8781286750454456e-05 Traning Loss: 5.3194591600913554e-05 17.97879981994629 2.0142478942871094\n",
            "24285 4.5350798245635815e-06 6.174752343213186e-05 Traning Loss: 6.628260598517954e-05 17.979034423828125 2.0142276287078857\n",
            "24286 6.3320981098513585e-06 6.290706369327381e-05 Traning Loss: 6.923916225787252e-05 17.97926139831543 2.014164686203003\n",
            "24287 3.985960120189702e-06 5.6528304412495345e-05 Traning Loss: 6.0514263168442994e-05 17.9794921875 2.0141384601593018\n",
            "24288 3.6501025988400215e-06 4.466997415875085e-05 Traning Loss: 4.83200783492066e-05 17.97972869873047 2.0140929222106934\n",
            "24289 2.6979132599080913e-06 3.945552452933043e-05 Traning Loss: 4.2153438698733225e-05 17.979961395263672 2.0140492916107178\n",
            "24290 2.53542043537891e-06 4.18010531575419e-05 Traning Loss: 4.4336473365547135e-05 17.980209350585938 2.014021635055542\n",
            "24291 4.065025223098928e-06 4.5824057451682165e-05 Traning Loss: 4.9889084039023146e-05 17.980445861816406 2.013967514038086\n",
            "24292 3.136073928544647e-06 4.909978815703653e-05 Traning Loss: 5.223586049396545e-05 17.98069190979004 2.013946294784546\n",
            "24293 3.911617568519432e-06 4.504156459006481e-05 Traning Loss: 4.895318124908954e-05 17.98092269897461 2.0138981342315674\n",
            "24294 2.4013870643102564e-06 4.06342041969765e-05 Traning Loss: 4.303559035179205e-05 17.981157302856445 2.013869285583496\n",
            "24295 2.3664761101827025e-06 3.6888799513690174e-05 Traning Loss: 3.9255275623872876e-05 17.98138999938965 2.0138344764709473\n",
            "24296 2.458832113916287e-06 3.7459260056493804e-05 Traning Loss: 3.9918093534652144e-05 17.981616973876953 2.0137925148010254\n",
            "24297 2.3017444163997425e-06 4.1142149711959064e-05 Traning Loss: 4.3443895265227184e-05 17.98185157775879 2.013767719268799\n",
            "24298 3.3432611417083535e-06 4.30515683547128e-05 Traning Loss: 4.6394830860663205e-05 17.982070922851562 2.013721227645874\n",
            "24299 2.4995717922138283e-06 4.398149030748755e-05 Traning Loss: 4.64810618723277e-05 17.982297897338867 2.013697385787964\n",
            "24300 2.9309344427019823e-06 4.110649024369195e-05 Traning Loss: 4.403742423164658e-05 17.982511520385742 2.0136566162109375\n",
            "24301 2.068578851321945e-06 3.915917477570474e-05 Traning Loss: 4.122775499126874e-05 17.982730865478516 2.013624668121338\n",
            "24302 2.1012406250520144e-06 3.7929770769551396e-05 Traning Loss: 4.0031012758845463e-05 17.98294448852539 2.01359224319458\n",
            "24303 2.2044978322810493e-06 3.856096009258181e-05 Traning Loss: 4.076545883435756e-05 17.98316192626953 2.0135505199432373\n",
            "24304 2.035620354945422e-06 4.019631160190329e-05 Traning Loss: 4.2231931729475036e-05 17.983379364013672 2.013521909713745\n",
            "24305 2.5285323772550328e-06 4.039272607769817e-05 Traning Loss: 4.292125959182158e-05 17.983592987060547 2.0134756565093994\n",
            "24306 2.0029231109219836e-06 4.0196115151047707e-05 Traning Loss: 4.219903712510131e-05 17.983802795410156 2.013444423675537\n",
            "24307 2.1802602532261517e-06 3.837094845948741e-05 Traning Loss: 4.055120734847151e-05 17.9840087890625 2.0134005546569824\n",
            "24308 1.7830810747909709e-06 3.728305455297232e-05 Traning Loss: 3.906613710569218e-05 17.984210968017578 2.0133612155914307\n",
            "24309 1.7301894104093662e-06 3.6780897062271833e-05 Traning Loss: 3.851108704111539e-05 17.984413146972656 2.0133211612701416\n",
            "24310 1.89674381090299e-06 3.697377906064503e-05 Traning Loss: 3.8870522985234857e-05 17.984609603881836 2.0132734775543213\n",
            "24311 1.6597840613030712e-06 3.7872705433983356e-05 Traning Loss: 3.9532489608973265e-05 17.98480796813965 2.0132346153259277\n",
            "24312 2.043841050181072e-06 3.778039535973221e-05 Traning Loss: 3.9824237319407985e-05 17.98499870300293 2.013183355331421\n",
            "24313 1.6257841934930184e-06 3.7850779335713014e-05 Traning Loss: 3.9476562960771844e-05 17.985191345214844 2.0131421089172363\n",
            "24314 1.832023940551153e-06 3.688068318297155e-05 Traning Loss: 3.871270746458322e-05 17.985376358032227 2.013091802597046\n",
            "24315 1.5335753005274455e-06 3.647433914011344e-05 Traning Loss: 3.8007914554327726e-05 17.985563278198242 2.013045072555542\n",
            "24316 1.5633803513992461e-06 3.614671004470438e-05 Traning Loss: 3.771009141928516e-05 17.985748291015625 2.01299786567688\n",
            "24317 1.615691985534795e-06 3.623083102866076e-05 Traning Loss: 3.784652290050872e-05 17.985931396484375 2.012946128845215\n",
            "24318 1.505107206867251e-06 3.667026248876937e-05 Traning Loss: 3.817537071881816e-05 17.986112594604492 2.0129010677337646\n",
            "24319 1.7320213601124124e-06 3.665394979179837e-05 Traning Loss: 3.838596967398189e-05 17.98628807067871 2.012847900390625\n",
            "24320 1.5006094145064708e-06 3.681254747789353e-05 Traning Loss: 3.831315552815795e-05 17.986461639404297 2.012802839279175\n",
            "24321 1.655771370678849e-06 3.634594759205356e-05 Traning Loss: 3.800171907641925e-05 17.986629486083984 2.012751340866089\n",
            "24322 1.4654896176580223e-06 3.617751644924283e-05 Traning Loss: 3.764300709008239e-05 17.986793518066406 2.012704372406006\n",
            "24323 1.4985630514274817e-06 3.591625500121154e-05 Traning Loss: 3.741481850738637e-05 17.986955642700195 2.0126559734344482\n",
            "24324 1.5005108480181661e-06 3.589120751712471e-05 Traning Loss: 3.739171734196134e-05 17.98711395263672 2.012606382369995\n",
            "24325 1.4261109981816844e-06 3.6088560591451824e-05 Traning Loss: 3.751467011170462e-05 17.98727035522461 2.012561082839966\n",
            "24326 1.5648668068024563e-06 3.609284249250777e-05 Traning Loss: 3.765770816244185e-05 17.9874210357666 2.0125112533569336\n",
            "24327 1.4031663795321947e-06 3.630757055361755e-05 Traning Loss: 3.771073534153402e-05 17.987571716308594 2.012467622756958\n",
            "24328 1.5425097217303119e-06 3.6104735045228153e-05 Traning Loss: 3.7647245335392654e-05 17.987716674804688 2.0124192237854004\n",
            "24329 1.3775304523733212e-06 3.613706212490797e-05 Traning Loss: 3.751459371414967e-05 17.98786163330078 2.0123753547668457\n",
            "24330 1.4566936670235009e-06 3.594161898945458e-05 Traning Loss: 3.7398312997538596e-05 17.988000869750977 2.012329578399658\n",
            "24331 1.3888336525269551e-06 3.5979643143946305e-05 Traning Loss: 3.736847793334164e-05 17.988140106201172 2.012284755706787\n",
            "24332 1.4013347708896617e-06 3.605101665016264e-05 Traning Loss: 3.745235153473914e-05 17.9882755279541 2.0122416019439697\n",
            "24333 1.4375717682924005e-06 3.6198100133333355e-05 Traning Loss: 3.7635672924807295e-05 17.9884090423584 2.0121963024139404\n",
            "24334 1.3973299246572424e-06 3.648850179160945e-05 Traning Loss: 3.788583126151934e-05 17.988536834716797 2.012155055999756\n",
            "24335 1.4784138784307288e-06 3.671486410894431e-05 Traning Loss: 3.819327685050666e-05 17.988664627075195 2.0121099948883057\n",
            "24336 1.4412103155336808e-06 3.715143975568935e-05 Traning Loss: 3.8592650525970384e-05 17.988786697387695 2.0120692253112793\n",
            "24337 1.5274064253389952e-06 3.764111897908151e-05 Traning Loss: 3.916852438123897e-05 17.98891258239746 2.0120253562927246\n",
            "24338 1.5856030586292036e-06 3.847819971269928e-05 Traning Loss: 4.006380186183378e-05 17.989028930664062 2.011984348297119\n",
            "24339 1.6857305809026002e-06 3.978414315497503e-05 Traning Loss: 4.1469873394817114e-05 17.989151000976562 2.0119423866271973\n",
            "24340 1.9426122435106663e-06 4.172582339379005e-05 Traning Loss: 4.366843495517969e-05 17.989259719848633 2.0119011402130127\n",
            "24341 2.1464300061779795e-06 4.491276195039973e-05 Traning Loss: 4.705919127445668e-05 17.9893798828125 2.0118608474731445\n",
            "24342 2.7618277727015084e-06 4.954245741828345e-05 Traning Loss: 5.230428359936923e-05 17.98948097229004 2.01181960105896\n",
            "24343 3.3306635032204213e-06 5.7063338317675516e-05 Traning Loss: 6.0394002503016964e-05 17.989599227905273 2.011780023574829\n",
            "24344 4.7012163122417405e-06 6.837433465989307e-05 Traning Loss: 7.30755491531454e-05 17.989688873291016 2.011739730834961\n",
            "24345 6.294514150795294e-06 8.662378240842372e-05 Traning Loss: 9.291829337598756e-05 17.989805221557617 2.011700391769409\n",
            "24346 9.488801879342645e-06 0.00011510109470691532 Traning Loss: 0.00012458988931030035 17.989877700805664 2.0116615295410156\n",
            "24347 1.3791362107440364e-05 0.00016083726950455457 Traning Loss: 0.00017462862888351083 17.989990234375 2.0116217136383057\n",
            "24348 2.163687713618856e-05 0.00023390891146846116 Traning Loss: 0.0002555457758717239 17.99003028869629 2.011584758758545\n",
            "24349 3.300053140264936e-05 0.00035046160337515175 Traning Loss: 0.00038346214569173753 17.990131378173828 2.0115437507629395\n",
            "24350 5.279621109366417e-05 0.0005386396078392863 Traning Loss: 0.0005914358189329505 17.990110397338867 2.0115082263946533\n",
            "24351 8.190572407329455e-05 0.000832443474791944 Traning Loss: 0.0009143492206931114 17.990171432495117 2.011465549468994\n",
            "24352 0.00013078050687909126 0.0013006178196519613 Traning Loss: 0.0014313983265310526 17.990020751953125 2.011430501937866\n",
            "24353 0.00019899371545761824 0.001984944334253669 Traning Loss: 0.002183937933295965 17.98996925354004 2.011385679244995\n",
            "24354 0.0003048789221793413 0.0029977671802043915 Traning Loss: 0.003302646102383733 17.989547729492188 2.011347532272339\n",
            "24355 0.00042572300299070776 0.0042122891172766685 Traning Loss: 0.004638012032955885 17.98922348022461 2.0113017559051514\n",
            "24356 0.0005722740897908807 0.005582596641033888 Traning Loss: 0.006154870614409447 17.988353729248047 2.011251449584961\n",
            "24357 0.0006377596291713417 0.006274845916777849 Traning Loss: 0.006912605371326208 17.987598419189453 2.0112154483795166\n",
            "24358 0.0006193413864821196 0.005948647856712341 Traning Loss: 0.0065679894760251045 17.986351013183594 2.0111441612243652\n",
            "24359 0.00041359211900271475 0.003996249288320541 Traning Loss: 0.004409841261804104 17.985370635986328 2.011143207550049\n",
            "24360 0.00018614403961692005 0.0015986767830327153 Traning Loss: 0.0017848208080977201 17.98431396484375 2.011073112487793\n",
            "24361 3.732419645530172e-05 0.00020055017375852913 Traning Loss: 0.00023787436657585204 17.983510971069336 2.011112689971924\n",
            "24362 7.79723995947279e-05 0.0005227671354077756 Traning Loss: 0.0006007395568303764 17.982873916625977 2.0110998153686523\n",
            "24363 0.00020410341676324606 0.0017550254706293344 Traning Loss: 0.0019591287709772587 17.982067108154297 2.011138677597046\n",
            "24364 0.0002666135842446238 0.002368635032325983 Traning Loss: 0.002635248703882098 17.981403350830078 2.0112202167510986\n",
            "24365 0.00022023505880497396 0.001787710003554821 Traning Loss: 0.0020079449750483036 17.98046112060547 2.0112431049346924\n",
            "24366 9.805479930946603e-05 0.0006914932164363563 Traning Loss: 0.0007895480375736952 17.97966766357422 2.011414051055908\n",
            "24367 6.289123120950535e-05 0.00021212476713117212 Traning Loss: 0.0002750160056166351 17.978870391845703 2.011476993560791\n",
            "24368 8.783975499682128e-05 0.0006165627273730934 Traning Loss: 0.0007044024532660842 17.978038787841797 2.011655569076538\n",
            "24369 0.00014638049469795078 0.0011055516079068184 Traning Loss: 0.0012519321171566844 17.977346420288086 2.0118095874786377\n",
            "24370 0.00013985074474476278 0.0010715191019698977 Traning Loss: 0.001211369875818491 17.97651481628418 2.0119383335113525\n",
            "24371 8.708863606443629e-05 0.0006014238460920751 Traning Loss: 0.0006885124603286386 17.975852966308594 2.012200117111206\n",
            "24372 6.270026642596349e-05 0.0002773617161437869 Traning Loss: 0.000340061989845708 17.97518539428711 2.012338161468506\n",
            "24373 5.438045263872482e-05 0.0003726499853655696 Traning Loss: 0.0004270304343663156 17.974578857421875 2.0126309394836426\n",
            "24374 8.099665865302086e-05 0.0005595276015810668 Traning Loss: 0.0006405242602340877 17.974088668823242 2.0128421783447266\n",
            "24375 7.330557855311781e-05 0.0005684104398824275 Traning Loss: 0.00064171600388363 17.97354507446289 2.0130624771118164\n",
            "24376 5.2432165830396116e-05 0.0003909508232027292 Traning Loss: 0.00044338300358504057 17.97315216064453 2.013357400894165\n",
            "24377 4.737952258437872e-05 0.0002639145532157272 Traning Loss: 0.00031129407580010593 17.97273063659668 2.0135180950164795\n",
            "24378 3.670659498311579e-05 0.0003052542160730809 Traning Loss: 0.0003419608110561967 17.972373962402344 2.013822078704834\n",
            "24379 4.95353524456732e-05 0.0003445552138146013 Traning Loss: 0.0003940905735362321 17.972070693969727 2.0139851570129395\n",
            "24380 3.503451443975791e-05 0.00030913285445421934 Traning Loss: 0.00034416737616993487 17.97174644470215 2.0141937732696533\n",
            "24381 2.503375981177669e-05 0.0002093259827233851 Traning Loss: 0.0002343597443541512 17.971519470214844 2.0144033432006836\n",
            "24382 2.6770387194119394e-05 0.00017397187184542418 Traning Loss: 0.00020074225903954357 17.971277236938477 2.014512538909912\n",
            "24383 2.3210928702610545e-05 0.00023942289408296347 Traning Loss: 0.0002626338100526482 17.971111297607422 2.014740467071533\n",
            "24384 3.745439971680753e-05 0.00027247934485785663 Traning Loss: 0.00030993373366072774 17.97096824645996 2.0148096084594727\n",
            "24385 2.202051655331161e-05 0.00023672531824558973 Traning Loss: 0.00025874582934193313 17.97085952758789 2.0149710178375244\n",
            "24386 1.531941961729899e-05 0.00012900416913907975 Traning Loss: 0.00014432359603233635 17.970813751220703 2.0150668621063232\n",
            "24387 9.012009286379907e-06 7.504226960008964e-05 Traning Loss: 8.405427797697484e-05 17.970781326293945 2.015129327774048\n",
            "24388 9.836649951466825e-06 0.00011665911733871326 Traning Loss: 0.00012649576819967479 17.970823287963867 2.015254497528076\n",
            "24389 2.42385267483769e-05 0.00018262345110997558 Traning Loss: 0.00020686197967734188 17.970876693725586 2.0152456760406494\n",
            "24390 1.8772729163174517e-05 0.0002099458361044526 Traning Loss: 0.00022871856344863772 17.970979690551758 2.015340805053711\n",
            "24391 1.8372020349488594e-05 0.00014451610331889242 Traning Loss: 0.00016288812912534922 17.97109603881836 2.0153300762176514\n",
            "24392 5.825544576509856e-06 7.024449587333947e-05 Traning Loss: 7.607004226883873e-05 17.971233367919922 2.0153563022613525\n",
            "24393 3.7248851185722742e-06 4.1013754525920376e-05 Traning Loss: 4.47386410087347e-05 17.97139549255371 2.0153822898864746\n",
            "24394 9.4896504378994e-06 7.326832565013319e-05 Traning Loss: 8.27579788165167e-05 17.971559524536133 2.0153424739837646\n",
            "24395 1.0884296898439061e-05 0.00012606309610418975 Traning Loss: 0.0001369473902741447 17.971755981445312 2.015382766723633\n",
            "24396 1.674016675679013e-05 0.00013135104381944984 Traning Loss: 0.00014809120330028236 17.971946716308594 2.015324354171753\n",
            "24397 8.573151717428118e-06 0.0001012371649267152 Traning Loss: 0.00010981031664414331 17.972169876098633 2.0153369903564453\n",
            "24398 6.622452474402962e-06 5.4358177294488996e-05 Traning Loss: 6.098063022363931e-05 17.97239112854004 2.015301465988159\n",
            "24399 3.6160313356958795e-06 3.972994818468578e-05 Traning Loss: 4.334597906563431e-05 17.972633361816406 2.0152642726898193\n",
            "24400 4.656892087950837e-06 5.6439352192683145e-05 Traning Loss: 6.109624519012868e-05 17.972881317138672 2.015256881713867\n",
            "24401 9.077758477360476e-06 7.709058263571933e-05 Traning Loss: 8.616834384156391e-05 17.97313117980957 2.015190362930298\n",
            "24402 6.896890226926189e-06 8.497879025526345e-05 Traning Loss: 9.187567775370553e-05 17.97340202331543 2.015185594558716\n",
            "24403 8.12924190540798e-06 6.75887567922473e-05 Traning Loss: 7.571799505967647e-05 17.973663330078125 2.015122413635254\n",
            "24404 3.902004209521692e-06 5.232779949437827e-05 Traning Loss: 5.622980461339466e-05 17.973949432373047 2.01509428024292\n",
            "24405 4.372448529466055e-06 4.499179340200499e-05 Traning Loss: 4.9364243750460446e-05 17.97422218322754 2.0150561332702637\n",
            "24406 4.626166628440842e-06 5.036457514506765e-05 Traning Loss: 5.499074177350849e-05 17.974510192871094 2.015005111694336\n",
            "24407 4.587939656630624e-06 5.707925447495654e-05 Traning Loss: 6.166719686007127e-05 17.97479820251465 2.014984607696533\n",
            "24408 5.814070846099639e-06 5.486791633302346e-05 Traning Loss: 6.068198854336515e-05 17.975088119506836 2.014925956726074\n",
            "24409 3.7024592529633082e-06 5.026496364735067e-05 Traning Loss: 5.3967421990819275e-05 17.975393295288086 2.0149030685424805\n",
            "24410 4.5371666601567995e-06 4.433077265275642e-05 Traning Loss: 4.886793976766057e-05 17.97568702697754 2.0148558616638184\n",
            "24411 3.5974042020825436e-06 4.581604662234895e-05 Traning Loss: 4.941345105180517e-05 17.975996017456055 2.014822244644165\n",
            "24412 4.334346613177331e-06 4.832651393371634e-05 Traning Loss: 5.266086009214632e-05 17.976285934448242 2.0147907733917236\n",
            "24413 4.355997134553036e-06 4.871641067438759e-05 Traning Loss: 5.307240644469857e-05 17.97658348083496 2.0147488117218018\n",
            "24414 3.573415369828581e-06 4.531936428975314e-05 Traning Loss: 4.8892779886955395e-05 17.976871490478516 2.0147247314453125\n",
            "24415 3.5617206322058337e-06 3.9851081965025514e-05 Traning Loss: 4.341280146036297e-05 17.977153778076172 2.0146849155426025\n",
            "24416 2.7287055672786664e-06 3.8496844354085624e-05 Traning Loss: 4.122555037611164e-05 17.977441787719727 2.0146584510803223\n",
            "24417 3.353809916006867e-06 4.017103128717281e-05 Traning Loss: 4.3524840293684974e-05 17.977718353271484 2.0146238803863525\n",
            "24418 3.293785084679257e-06 4.405206709634513e-05 Traning Loss: 4.7345853090519086e-05 17.97800636291504 2.0145909786224365\n",
            "24419 3.5490620575728826e-06 4.515270484262146e-05 Traning Loss: 4.8701767809689045e-05 17.978281021118164 2.0145599842071533\n",
            "24420 3.1535153084405465e-06 4.299485226511024e-05 Traning Loss: 4.614836871041916e-05 17.978567123413086 2.014524459838867\n",
            "24421 2.710348553591757e-06 3.901460877386853e-05 Traning Loss: 4.1724957554833964e-05 17.97884750366211 2.014491558074951\n",
            "24422 2.400828179816017e-06 3.629154525697231e-05 Traning Loss: 3.869237480103038e-05 17.979129791259766 2.014455556869507\n",
            "24423 2.3416482690663543e-06 3.6410252505447716e-05 Traning Loss: 3.875189941027202e-05 17.979413986206055 2.014418125152588\n",
            "24424 2.562756208135397e-06 3.840852150460705e-05 Traning Loss: 4.097127748536877e-05 17.97968864440918 2.0143818855285645\n",
            "24425 2.676414624147583e-06 4.0346210880670696e-05 Traning Loss: 4.302262459532358e-05 17.979969024658203 2.0143420696258545\n",
            "24426 2.7700464215740794e-06 4.0462447941536084e-05 Traning Loss: 4.323249595472589e-05 17.980236053466797 2.0143020153045654\n",
            "24427 2.457065875205444e-06 3.919823211617768e-05 Traning Loss: 4.165529753663577e-05 17.980506896972656 2.01426100730896\n",
            "24428 2.424314970994601e-06 3.7216083001112565e-05 Traning Loss: 3.964039933634922e-05 17.98076629638672 2.0142147541046143\n",
            "24429 2.0557881725835614e-06 3.6442099371924996e-05 Traning Loss: 3.849788845400326e-05 17.98102569580078 2.0141732692718506\n",
            "24430 2.2660458398604533e-06 3.629925777204335e-05 Traning Loss: 3.8565303839277476e-05 17.981281280517578 2.014122724533081\n",
            "24431 2.0810437035834184e-06 3.715286584338173e-05 Traning Loss: 3.9233909774338827e-05 17.981531143188477 2.014078140258789\n",
            "24432 2.2415424609789625e-06 3.7422043533297256e-05 Traning Loss: 3.966358781326562e-05 17.981782913208008 2.0140268802642822\n",
            "24433 2.133137058990542e-06 3.734155325219035e-05 Traning Loss: 3.947468940168619e-05 17.982025146484375 2.0139777660369873\n",
            "24434 2.0099337234569248e-06 3.688317883643322e-05 Traning Loss: 3.889311119564809e-05 17.982269287109375 2.0139291286468506\n",
            "24435 2.0980166937079048e-06 3.6308869312051684e-05 Traning Loss: 3.840688441414386e-05 17.982505798339844 2.0138750076293945\n",
            "24436 1.8391593812339124e-06 3.645004107966088e-05 Traning Loss: 3.828919943771325e-05 17.982744216918945 2.0138278007507324\n",
            "24437 2.13220255318447e-06 3.6304751120042056e-05 Traning Loss: 3.843695230898447e-05 17.982975006103516 2.013770580291748\n",
            "24438 1.8355852944296203e-06 3.669719444587827e-05 Traning Loss: 3.8532780308742076e-05 17.983203887939453 2.0137226581573486\n",
            "24439 2.0712441255454905e-06 3.6272140278015286e-05 Traning Loss: 3.8343383494066074e-05 17.983427047729492 2.0136659145355225\n",
            "24440 1.8237776657770155e-06 3.6086330510443076e-05 Traning Loss: 3.7910107494099066e-05 17.983644485473633 2.0136144161224365\n",
            "24441 1.864282353380986e-06 3.560912227840163e-05 Traning Loss: 3.7473404518095776e-05 17.983858108520508 2.0135602951049805\n",
            "24442 1.8308713833903312e-06 3.543851198628545e-05 Traning Loss: 3.7269383028615266e-05 17.984066009521484 2.01350474357605\n",
            "24443 1.721761350381712e-06 3.5634282539831474e-05 Traning Loss: 3.7356043321779e-05 17.984270095825195 2.013453483581543\n",
            "24444 1.9057017652812647e-06 3.568896136130206e-05 Traning Loss: 3.759466198971495e-05 17.984468460083008 2.0133955478668213\n",
            "24445 1.6850708561833017e-06 3.608385304687545e-05 Traning Loss: 3.776892481255345e-05 17.984664916992188 2.01334547996521\n",
            "24446 1.8986648910868098e-06 3.5832363209920004e-05 Traning Loss: 3.7731027987319976e-05 17.98485565185547 2.0132875442504883\n",
            "24447 1.6424314708274323e-06 3.585107333492488e-05 Traning Loss: 3.749350435100496e-05 17.98504638671875 2.0132369995117188\n",
            "24448 1.768783590705425e-06 3.541151090757921e-05 Traning Loss: 3.7180296203587204e-05 17.985231399536133 2.013181447982788\n",
            "24449 1.615827613932197e-06 3.532167465891689e-05 Traning Loss: 3.693750113598071e-05 17.985414505004883 2.0131287574768066\n",
            "24450 1.6453800526505802e-06 3.519391975714825e-05 Traning Loss: 3.6839301174040884e-05 17.985593795776367 2.013076066970825\n",
            "24451 1.6473704818054102e-06 3.521458347677253e-05 Traning Loss: 3.6861954868072644e-05 17.98577117919922 2.013021945953369\n",
            "24452 1.5835795466045965e-06 3.5339733585715294e-05 Traning Loss: 3.692331301863305e-05 17.985944747924805 2.0129716396331787\n",
            "24453 1.6712855313016917e-06 3.527383887558244e-05 Traning Loss: 3.694512270158157e-05 17.986114501953125 2.0129175186157227\n",
            "24454 1.5400958091049688e-06 3.5358320019440725e-05 Traning Loss: 3.689841469167732e-05 17.98628044128418 2.0128684043884277\n",
            "24455 1.6323040199495154e-06 3.517060031299479e-05 Traning Loss: 3.680290319607593e-05 17.986440658569336 2.012815475463867\n",
            "24456 1.5013622487458633e-06 3.520539394230582e-05 Traning Loss: 3.6706755054183304e-05 17.98659896850586 2.0127665996551514\n",
            "24457 1.564455715197255e-06 3.5077056963928044e-05 Traning Loss: 3.664151154225692e-05 17.986753463745117 2.0127158164978027\n",
            "24458 1.4844285942672286e-06 3.5130757169099525e-05 Traning Loss: 3.6615187127608806e-05 17.986906051635742 2.012666940689087\n",
            "24459 1.50514517827105e-06 3.5098470107186586e-05 Traning Loss: 3.660361471702345e-05 17.9870548248291 2.012617826461792\n",
            "24460 1.4690843954667798e-06 3.510736860334873e-05 Traning Loss: 3.65764535672497e-05 17.987201690673828 2.012568950653076\n",
            "24461 1.4504455521091586e-06 3.507120345602743e-05 Traning Loss: 3.6521647416520864e-05 17.98734474182129 2.012521266937256\n",
            "24462 1.434984483239532e-06 3.500816092127934e-05 Traning Loss: 3.644314710982144e-05 17.98748779296875 2.0124733448028564\n",
            "24463 1.4019001355336513e-06 3.495535929687321e-05 Traning Loss: 3.635725806816481e-05 17.987627029418945 2.0124263763427734\n",
            "24464 1.3937544736108975e-06 3.4895125281764194e-05 Traning Loss: 3.6288878618506715e-05 17.987764358520508 2.0123791694641113\n",
            "24465 1.3685422572962125e-06 3.4878576116170734e-05 Traning Loss: 3.624711825978011e-05 17.987899780273438 2.0123326778411865\n",
            "24466 1.3603973911813227e-06 3.486857531243004e-05 Traning Loss: 3.6228972021490335e-05 17.9880313873291 2.012286424636841\n",
            "24467 1.344122892987798e-06 3.487887806841172e-05 Traning Loss: 3.62230020982679e-05 17.988161087036133 2.012240409851074\n",
            "24468 1.333529894509411e-06 3.4880475141108036e-05 Traning Loss: 3.621400537667796e-05 17.9882869720459 2.012194871902466\n",
            "24469 1.3147156323611853e-06 3.4877622965723276e-05 Traning Loss: 3.619233757490292e-05 17.988412857055664 2.0121493339538574\n",
            "24470 1.3084328429613379e-06 3.4847329516196623e-05 Traning Loss: 3.615576133597642e-05 17.988534927368164 2.012104034423828\n",
            "24471 1.279049342883809e-06 3.483228283585049e-05 Traning Loss: 3.611133070080541e-05 17.988656997680664 2.012059450149536\n",
            "24472 1.2859570688306121e-06 3.4779601264745e-05 Traning Loss: 3.606555765145458e-05 17.9887752532959 2.012014389038086\n",
            "24473 1.2437369605322601e-06 3.478018697933294e-05 Traning Loss: 3.6023924621986225e-05 17.9888916015625 2.0119707584381104\n",
            "24474 1.265568243979942e-06 3.4725875593721867e-05 Traning Loss: 3.599144474719651e-05 17.98900604248047 2.0119259357452393\n",
            "24475 1.2134736380176037e-06 3.475042467471212e-05 Traning Loss: 3.596389797166921e-05 17.989118576049805 2.01188325881958\n",
            "24476 1.2444409094314324e-06 3.469250805210322e-05 Traning Loss: 3.5936947824666277e-05 17.989229202270508 2.011838912963867\n",
            "24477 1.1898675893462496e-06 3.471756645012647e-05 Traning Loss: 3.590743290260434e-05 17.989337921142578 2.011796712875366\n",
            "24478 1.2194118426123168e-06 3.4655753552215174e-05 Traning Loss: 3.587516403058544e-05 17.989444732666016 2.0117528438568115\n",
            "24479 1.170563677987957e-06 3.466753332759254e-05 Traning Loss: 3.583809666451998e-05 17.98954963684082 2.0117108821868896\n",
            "24480 1.1905467545147985e-06 3.460908919805661e-05 Traning Loss: 3.579963595257141e-05 17.989652633666992 2.0116679668426514\n",
            "24481 1.1537424597918289e-06 3.4609030990395695e-05 Traning Loss: 3.576277231331915e-05 17.98975372314453 2.0116262435913086\n",
            "24482 1.1610102319536963e-06 3.456762351561338e-05 Traning Loss: 3.57286335201934e-05 17.989852905273438 2.0115842819213867\n",
            "24483 1.1390231975383358e-06 3.4558721381472424e-05 Traning Loss: 3.569774344214238e-05 17.98995018005371 2.011542797088623\n",
            "24484 1.1345229040671256e-06 3.453713725320995e-05 Traning Loss: 3.5671659134095535e-05 17.99004554748535 2.0115015506744385\n",
            "24485 1.1249761655562907e-06 3.4519580367486924e-05 Traning Loss: 3.56445561919827e-05 17.99013900756836 2.011460542678833\n",
            "24486 1.1120190492874826e-06 3.450824078754522e-05 Traning Loss: 3.5620258131530136e-05 17.990230560302734 2.0114200115203857\n",
            "24487 1.1088839073636336e-06 3.448384450166486e-05 Traning Loss: 3.559272954589687e-05 17.99032211303711 2.0113792419433594\n",
            "24488 1.0920859949692385e-06 3.4472355764592066e-05 Traning Loss: 3.556444062269293e-05 17.99041175842285 2.0113394260406494\n",
            "24489 1.0901169389399001e-06 3.444520552875474e-05 Traning Loss: 3.5535322240320966e-05 17.99049949645996 2.0112993717193604\n",
            "24490 1.0743120810730034e-06 3.443114474066533e-05 Traning Loss: 3.550545807229355e-05 17.990585327148438 2.0112597942352295\n",
            "24491 1.0707240107876714e-06 3.4406308259349316e-05 Traning Loss: 3.5477030905894935e-05 17.99066925048828 2.0112202167510986\n",
            "24492 1.0584136589386617e-06 3.4390559449093416e-05 Traning Loss: 3.5448974813334644e-05 17.990751266479492 2.011181116104126\n",
            "24493 1.0518764383959933e-06 3.437065606703982e-05 Traning Loss: 3.54225339833647e-05 17.990833282470703 2.0111422538757324\n",
            "24494 1.0434306432216545e-06 3.435272446949966e-05 Traning Loss: 3.539615499903448e-05 17.99091339111328 2.011103630065918\n",
            "24495 1.0342278073949274e-06 3.433728852542117e-05 Traning Loss: 3.537151496857405e-05 17.990991592407227 2.0110652446746826\n",
            "24496 1.0284322797815548e-06 3.431832737987861e-05 Traning Loss: 3.534675852279179e-05 17.99106788635254 2.0110270977020264\n",
            "24497 1.018231273519632e-06 3.43025749316439e-05 Traning Loss: 3.5320805181981996e-05 17.99114227294922 2.010989189147949\n",
            "24498 1.013189375953516e-06 3.4282533306395635e-05 Traning Loss: 3.52957213181071e-05 17.9912166595459 2.010951519012451\n",
            "24499 1.0038664868261549e-06 3.426611510803923e-05 Traning Loss: 3.5269982618046924e-05 17.991289138793945 2.0109140872955322\n",
            "24500 9.974453405448003e-07 3.424665192142129e-05 Traning Loss: 3.524409839883447e-05 17.99135971069336 2.0108768939971924\n",
            "24501 9.903664022203884e-07 3.4227956348331645e-05 Traning Loss: 3.521832331898622e-05 17.991430282592773 2.0108399391174316\n",
            "24502 9.81019525170268e-07 3.42115163221024e-05 Traning Loss: 3.5192537325201556e-05 17.991498947143555 2.010803461074829\n",
            "24503 9.778193543752423e-07 3.41905870300252e-05 Traning Loss: 3.516840661177412e-05 17.991565704345703 2.0107669830322266\n",
            "24504 9.64838250183675e-07 3.417849802644923e-05 Traning Loss: 3.514333729981445e-05 17.99163246154785 2.0107309818267822\n",
            "24505 9.662909405960818e-07 3.415328319533728e-05 Traning Loss: 3.5119574022246525e-05 17.991697311401367 2.010694742202759\n",
            "24506 9.488871910434682e-07 3.4147909900639206e-05 Traning Loss: 3.5096796636935323e-05 17.99176025390625 2.0106594562530518\n",
            "24507 9.560594662616495e-07 3.411798024899326e-05 Traning Loss: 3.5074041079496965e-05 17.991823196411133 2.0106236934661865\n",
            "24508 9.333852517556807e-07 3.411981379031204e-05 Traning Loss: 3.5053199098911136e-05 17.991884231567383 2.010589122772217\n",
            "24509 9.474209150539536e-07 3.4085791412508115e-05 Traning Loss: 3.5033212043344975e-05 17.991943359375 2.0105535984039307\n",
            "24510 9.183162887893559e-07 3.40985570801422e-05 Traning Loss: 3.5016873880522326e-05 17.992002487182617 2.0105197429656982\n",
            "24511 9.416520470040268e-07 3.4061173209920526e-05 Traning Loss: 3.500282400636934e-05 17.9920597076416 2.010484457015991\n",
            "24512 9.03855038814072e-07 3.409301280044019e-05 Traning Loss: 3.499686863506213e-05 17.992116928100586 2.010451316833496\n",
            "24513 9.406256253896572e-07 3.4056185540976e-05 Traning Loss: 3.4996810427401215e-05 17.992172241210938 2.010416269302368\n",
            "24514 8.90683509169321e-07 3.412083242437802e-05 Traning Loss: 3.501151513773948e-05 17.99222755432129 2.0103840827941895\n",
            "24515 9.493880952504696e-07 3.4099390177289024e-05 Traning Loss: 3.504877895466052e-05 17.992280960083008 2.0103490352630615\n",
            "24516 8.822771633276716e-07 3.423862290219404e-05 Traning Loss: 3.512090188451111e-05 17.99233627319336 2.0103182792663574\n",
            "24517 9.793360504772863e-07 3.426882904022932e-05 Traning Loss: 3.5248165659140795e-05 17.992385864257812 2.0102827548980713\n",
            "24518 8.90342107595643e-07 3.457528873695992e-05 Traning Loss: 3.546562948031351e-05 17.99243927001953 2.010253667831421\n",
            "24519 1.0583912626316305e-06 3.47731402143836e-05 Traning Loss: 3.583153011277318e-05 17.99248695373535 2.0102171897888184\n",
            "24520 9.485795544605935e-07 3.5488537832861766e-05 Traning Loss: 3.643711897893809e-05 17.99254035949707 2.010190725326538\n",
            "24521 1.2584479236465995e-06 3.618411210482009e-05 Traning Loss: 3.744256173376925e-05 17.992584228515625 2.0101523399353027\n",
            "24522 1.1566327202672255e-06 3.795386874116957e-05 Traning Loss: 3.911050225724466e-05 17.992637634277344 2.010129690170288\n",
            "24523 1.7743508351486525e-06 4.011870623799041e-05 Traning Loss: 4.189305764157325e-05 17.99267578125 2.010087490081787\n",
            "24524 1.8078374068863923e-06 4.474350862437859e-05 Traning Loss: 4.655134762288071e-05 17.99273109436035 2.010071039199829\n",
            "24525 3.1531840249954257e-06 5.1254992285976186e-05 Traning Loss: 5.4408177675213665e-05 17.992759704589844 2.0100221633911133\n",
            "24526 3.775279083129135e-06 6.392245268216357e-05 Traning Loss: 6.769772880943492e-05 17.99281883239746 2.0100159645080566\n",
            "24527 6.9865027398918755e-06 8.34093225421384e-05 Traning Loss: 9.039582801051438e-05 17.99283218383789 2.009955406188965\n",
            "24528 9.69239954429213e-06 0.0001194838696392253 Traning Loss: 0.00012917627464048564 17.992897033691406 2.0099668502807617\n",
            "24529 1.800882091629319e-05 0.00017812081205192953 Traning Loss: 0.00019612963660620153 17.992881774902344 2.009885549545288\n",
            "24530 2.753991611825768e-05 0.0002835675550159067 Traning Loss: 0.00031110746203921735 17.992948532104492 2.009927749633789\n",
            "24531 5.036643779021688e-05 0.00046021502930670977 Traning Loss: 0.0005105814780108631 17.992870330810547 2.0098116397857666\n",
            "24532 8.096762758214027e-05 0.0007691689534112811 Traning Loss: 0.0008501365664415061 17.992918014526367 2.0099077224731445\n",
            "24533 0.000144321151310578 0.00128581200260669 Traning Loss: 0.0014301331248134375 17.992698669433594 2.0097358226776123\n",
            "24534 0.00023217858688440174 0.0021363995037972927 Traning Loss: 0.002368577988818288 17.992650985717773 2.009922742843628\n",
            "24535 0.0003906862693838775 0.0034572691656649113 Traning Loss: 0.0038479554932564497 17.992107391357422 2.009678840637207\n",
            "24536 0.0005776218604296446 0.0052614654414355755 Traning Loss: 0.005839087069034576 17.991750717163086 2.0099945068359375\n",
            "24537 0.000828910619020462 0.007308286149054766 Traning Loss: 0.00813719630241394 17.990623474121094 2.0097179412841797\n",
            "24538 0.0009299404919147491 0.008499669842422009 Traning Loss: 0.009429610334336758 17.989694595336914 2.010129451751709\n",
            "24539 0.0008843448595143855 0.007714101113379002 Traning Loss: 0.008598445914685726 17.988086700439453 2.009995698928833\n",
            "24540 0.000487606565002352 0.004546361044049263 Traning Loss: 0.005033967550843954 17.986839294433594 2.0103204250335693\n",
            "24541 0.00015488718054257333 0.001214546849951148 Traning Loss: 0.0013694340595975518 17.985546112060547 2.0105273723602295\n",
            "24542 6.649314309470356e-05 0.00032314640702679753 Traning Loss: 0.0003896395501215011 17.98442268371582 2.0106191635131836\n",
            "24543 0.00022127421107143164 0.0019392609829083085 Traning Loss: 0.00216053519397974 17.98355484008789 2.011113166809082\n",
            "24544 0.00044572114711627364 0.0033741106744855642 Traning Loss: 0.003819831879809499 17.982295989990234 2.0111887454986572\n",
            "24545 0.00031319516710937023 0.0027822107076644897 Traning Loss: 0.00309540587477386 17.981250762939453 2.011686325073242\n",
            "24546 0.00014627855853177607 0.0009039355791173875 Traning Loss: 0.001050214166752994 17.980051040649414 2.0120577812194824\n",
            "24547 7.80801783548668e-05 0.00026952166808769107 Traning Loss: 0.00034760183189064264 17.978864669799805 2.0123226642608643\n",
            "24548 0.00016169158334378153 0.0012951894896104932 Traning Loss: 0.0014568810584023595 17.97781753540039 2.012908935546875\n",
            "24549 0.0002913384232670069 0.001964992843568325 Traning Loss: 0.002256331266835332 17.976533889770508 2.013143539428711\n",
            "24550 0.0001661427813814953 0.0012901355512440205 Traning Loss: 0.0014562783762812614 17.97538185119629 2.0136852264404297\n",
            "24551 6.430908251786605e-05 0.00023236495326273143 Traning Loss: 0.00029667402850463986 17.97426986694336 2.014165163040161\n",
            "24552 7.94392908574082e-05 0.0003205756947863847 Traning Loss: 0.0004000149783678353 17.973222732543945 2.014476776123047\n",
            "24553 0.0001486914261477068 0.001123882131651044 Traning Loss: 0.0012725736014544964 17.972349166870117 2.0150580406188965\n",
            "24554 0.00017644287436269224 0.0011754283914342523 Traning Loss: 0.001351871294900775 17.971458435058594 2.0153443813323975\n",
            "24555 7.183387060649693e-05 0.00046256554196588695 Traning Loss: 0.0005343994125723839 17.97073745727539 2.015805721282959\n",
            "24556 2.6640889700502157e-05 3.8864596717758104e-05 Traning Loss: 6.550549005623907e-05 17.97011375427246 2.016245126724243\n",
            "24557 7.345759513555095e-05 0.00040814990643411875 Traning Loss: 0.0004816075088456273 17.96952247619629 2.016484022140503\n",
            "24558 0.00010173939517699182 0.0008300914778374135 Traning Loss: 0.0009318308439105749 17.969064712524414 2.0169363021850586\n",
            "24559 9.063061588676646e-05 0.0005901803378947079 Traning Loss: 0.0006808109465055168 17.96855926513672 2.0171666145324707\n",
            "24560 2.213970583397895e-05 0.00014579891285393387 Traning Loss: 0.00016793861868791282 17.968124389648438 2.0174732208251953\n",
            "24561 1.7645868865656666e-05 0.00010497020412003621 Traning Loss: 0.00012261606752872467 17.967796325683594 2.017791748046875\n",
            "24562 6.109190871939063e-05 0.00040105156949721277 Traning Loss: 0.0004621434782166034 17.967432022094727 2.0179216861724854\n",
            "24563 5.530225462280214e-05 0.000517104403115809 Traning Loss: 0.0005724066868424416 17.967164993286133 2.018200635910034\n",
            "24564 3.970791294705123e-05 0.0002559729036875069 Traning Loss: 0.00029568083118647337 17.9669189453125 2.018341541290283\n",
            "24565 7.20198158887797e-06 5.499630424310453e-05 Traning Loss: 6.219828355824575e-05 17.966724395751953 2.0184924602508545\n",
            "24566 1.5454737877007574e-05 0.0001513546594651416 Traning Loss: 0.00016680939006619155 17.96661949157715 2.0186736583709717\n",
            "24567 4.5904202124802396e-05 0.0003263270773459226 Traning Loss: 0.00037223126855678856 17.966520309448242 2.0187060832977295\n",
            "24568 2.9875749532948248e-05 0.0003134674043394625 Traning Loss: 0.00034334315569140017 17.966520309448242 2.0188417434692383\n",
            "24569 1.78987902472727e-05 0.0001207159148179926 Traning Loss: 0.0001386147050652653 17.966516494750977 2.0189006328582764\n",
            "24570 5.726708423026139e-06 4.175691356067546e-05 Traning Loss: 4.748362334794365e-05 17.966569900512695 2.018933057785034\n",
            "24571 1.2588765457621776e-05 0.00013957316696178168 Traning Loss: 0.00015216192696243525 17.96669578552246 2.019017457962036\n",
            "24572 3.102041591773741e-05 0.00022113155864644796 Traning Loss: 0.0002521519782021642 17.96678924560547 2.0189766883850098\n",
            "24573 1.659140980336815e-05 0.00017943629063665867 Traning Loss: 0.00019602770044002682 17.966955184936523 2.0190157890319824\n",
            "24574 9.315641364082694e-06 6.828120240243152e-05 Traning Loss: 7.759684376651421e-05 17.96712875366211 2.0190160274505615\n",
            "24575 7.161357189033879e-06 5.1234004786238074e-05 Traning Loss: 5.8395362430019304e-05 17.967317581176758 2.0189712047576904\n",
            "24576 1.148290448327316e-05 0.00012246989354025573 Traning Loss: 0.00013395279529504478 17.96755599975586 2.0189871788024902\n",
            "24577 2.0954101273673587e-05 0.00014982878929004073 Traning Loss: 0.00017078289238270372 17.967784881591797 2.0189034938812256\n",
            "24578 9.833079275267664e-06 0.00010483260848559439 Traning Loss: 0.00011466568685136735 17.968042373657227 2.018887758255005\n",
            "24579 5.741592758568004e-06 4.2264720832463354e-05 Traning Loss: 4.800631359103136e-05 17.96830177307129 2.0188474655151367\n",
            "24580 6.706878139084438e-06 4.767641439684667e-05 Traning Loss: 5.438329390017316e-05 17.96857261657715 2.018770217895508\n",
            "24581 9.066831808013376e-06 9.505919297225773e-05 Traning Loss: 0.00010412602568976581 17.968875885009766 2.0187456607818604\n",
            "24582 1.4130411727819592e-05 0.00010204606951447204 Traning Loss: 0.00011617648124229163 17.969160079956055 2.0186452865600586\n",
            "24583 6.45955151412636e-06 7.049462146824226e-05 Traning Loss: 7.695417298236862e-05 17.96947479248047 2.0185976028442383\n",
            "24584 4.715545401268173e-06 3.8919159123906866e-05 Traning Loss: 4.363470361568034e-05 17.96979522705078 2.0185327529907227\n",
            "24585 6.414841664081905e-06 4.8884150601224974e-05 Traning Loss: 5.529899135581218e-05 17.970108032226562 2.018437147140503\n",
            "24586 7.041407116048504e-06 7.592446490889415e-05 Traning Loss: 8.296587475342676e-05 17.970460891723633 2.018383502960205\n",
            "24587 9.759154636412859e-06 7.243839354487136e-05 Traning Loss: 8.219754818128422e-05 17.970794677734375 2.0182816982269287\n",
            "24588 4.554150109470356e-06 5.0944083341164514e-05 Traning Loss: 5.549823254114017e-05 17.971145629882812 2.018214225769043\n",
            "24589 3.927769739675568e-06 3.571502384147607e-05 Traning Loss: 3.9642793126404285e-05 17.971506118774414 2.018144130706787\n",
            "24590 5.839711320732022e-06 4.5053413487039506e-05 Traning Loss: 5.089312617201358e-05 17.97185707092285 2.01804780960083\n",
            "24591 5.629241513815941e-06 6.153572758194059e-05 Traning Loss: 6.716496864100918e-05 17.97222328186035 2.017988443374634\n",
            "24592 7.496613761759363e-06 5.697130836779252e-05 Traning Loss: 6.446792394854128e-05 17.972570419311523 2.0178985595703125\n",
            "24593 4.127864485781174e-06 4.3969721446046606e-05 Traning Loss: 4.809758684132248e-05 17.972923278808594 2.017831325531006\n",
            "24594 3.795886868829257e-06 3.628261765697971e-05 Traning Loss: 4.007850293419324e-05 17.973268508911133 2.0177674293518066\n",
            "24595 5.201734438742278e-06 4.210693805362098e-05 Traning Loss: 4.730867294711061e-05 17.97360610961914 2.0176780223846436\n",
            "24596 4.501482635532739e-06 5.130646241013892e-05 Traning Loss: 5.580794459092431e-05 17.973953247070312 2.0176236629486084\n",
            "24597 5.6977155509230215e-06 4.719586650026031e-05 Traning Loss: 5.289358159643598e-05 17.974279403686523 2.0175421237945557\n",
            "24598 3.4166919249400962e-06 3.9475209632655606e-05 Traning Loss: 4.289190110284835e-05 17.974611282348633 2.0174801349639893\n",
            "24599 3.2507173273188528e-06 3.527649096213281e-05 Traning Loss: 3.852720692520961e-05 17.974943161010742 2.0174190998077393\n",
            "24600 4.077397534274496e-06 3.911037492798641e-05 Traning Loss: 4.3187770643271506e-05 17.97526741027832 2.017338514328003\n",
            "24601 3.5346765798749402e-06 4.498900307225995e-05 Traning Loss: 4.852368147112429e-05 17.975603103637695 2.0172855854034424\n",
            "24602 4.426747182151303e-06 4.270490171620622e-05 Traning Loss: 4.7131648898357525e-05 17.97593116760254 2.017209529876709\n",
            "24603 2.933967152785044e-06 3.838953125523403e-05 Traning Loss: 4.132349931751378e-05 17.976261138916016 2.01714825630188\n",
            "24604 2.9287039069458842e-06 3.5464898246573284e-05 Traning Loss: 3.839360215351917e-05 17.976593017578125 2.0170822143554688\n",
            "24605 3.370151716808323e-06 3.7328078178688884e-05 Traning Loss: 4.0698228986002505e-05 17.976917266845703 2.0170040130615234\n",
            "24606 2.976177484015352e-06 4.086251283297315e-05 Traning Loss: 4.3838688725372776e-05 17.977243423461914 2.016942262649536\n",
            "24607 3.667099235826754e-06 3.9611310057807714e-05 Traning Loss: 4.3278410885250196e-05 17.977556228637695 2.0168631076812744\n",
            "24608 2.629056098157889e-06 3.719534652191214e-05 Traning Loss: 3.9824401028454304e-05 17.97787094116211 2.0167922973632812\n",
            "24609 2.622034799060202e-06 3.499328886391595e-05 Traning Loss: 3.7615322071360424e-05 17.978178024291992 2.016716718673706\n",
            "24610 2.8089095849281875e-06 3.574615766410716e-05 Traning Loss: 3.855506656691432e-05 17.978477478027344 2.0166311264038086\n",
            "24611 2.4685919015610125e-06 3.812712384387851e-05 Traning Loss: 4.059571438119747e-05 17.97878074645996 2.0165586471557617\n",
            "24612 3.0501826131512644e-06 3.777508391067386e-05 Traning Loss: 4.082526720594615e-05 17.97907257080078 2.0164711475372314\n",
            "24613 2.301461563547491e-06 3.676023698062636e-05 Traning Loss: 3.9061698771547526e-05 17.979368209838867 2.016390800476074\n",
            "24614 2.3623142624273896e-06 3.5118424420943484e-05 Traning Loss: 3.748073868337087e-05 17.97966194152832 2.016306161880493\n",
            "24615 2.3895120193628827e-06 3.524027852108702e-05 Traning Loss: 3.762979031307623e-05 17.979951858520508 2.0162148475646973\n",
            "24616 2.1674291019735392e-06 3.664613905129954e-05 Traning Loss: 3.881356678903103e-05 17.980241775512695 2.016134738922119\n",
            "24617 2.67994005298533e-06 3.661099253804423e-05 Traning Loss: 3.929093145416118e-05 17.980525970458984 2.0160415172576904\n",
            "24618 2.146810402336996e-06 3.632077277870849e-05 Traning Loss: 3.8467584090540186e-05 17.98080825805664 2.0159575939178467\n",
            "24619 2.313755885552382e-06 3.499677768559195e-05 Traning Loss: 3.731053220690228e-05 17.981082916259766 2.0158677101135254\n",
            "24620 2.192703050241107e-06 3.4791915823007e-05 Traning Loss: 3.698462023749016e-05 17.981351852416992 2.0157783031463623\n",
            "24621 2.087511802528752e-06 3.542229387676343e-05 Traning Loss: 3.7509806134039536e-05 17.981616973876953 2.015695095062256\n",
            "24622 2.407957708783215e-06 3.556957017281093e-05 Traning Loss: 3.797752651735209e-05 17.981870651245117 2.015604019165039\n",
            "24623 2.053128582701902e-06 3.5680666769621894e-05 Traning Loss: 3.773379648919217e-05 17.98212432861328 2.0155205726623535\n",
            "24624 2.1965552150504664e-06 3.483193358988501e-05 Traning Loss: 3.702849062392488e-05 17.982370376586914 2.0154330730438232\n",
            "24625 2.021575710386969e-06 3.456388367339969e-05 Traning Loss: 3.658545756479725e-05 17.98261070251465 2.015347480773926\n",
            "24626 1.9578419596655294e-06 3.474976620054804e-05 Traning Loss: 3.6707606341224164e-05 17.98284912109375 2.0152664184570312\n",
            "24627 2.146466613339726e-06 3.490367453196086e-05 Traning Loss: 3.7050140235805884e-05 17.98307991027832 2.0151798725128174\n",
            "24628 1.9017294334844337e-06 3.5212779039284214e-05 Traning Loss: 3.711450699483976e-05 17.98331069946289 2.0151002407073975\n",
            "24629 2.0570944343489828e-06 3.473796459729783e-05 Traning Loss: 3.679505971376784e-05 17.983535766601562 2.015017032623291\n",
            "24630 1.872867755992047e-06 3.4545260859886184e-05 Traning Loss: 3.641812872956507e-05 17.98375701904297 2.014937162399292\n",
            "24631 1.8653954612091184e-06 3.4446802601451054e-05 Traning Loss: 3.631219806266017e-05 17.98397445678711 2.014859437942505\n",
            "24632 1.959690962394234e-06 3.450584699749015e-05 Traning Loss: 3.646553886937909e-05 17.98418617248535 2.0147781372070312\n",
            "24633 1.7959766864805715e-06 3.48071007465478e-05 Traning Loss: 3.660307629616e-05 17.98439598083496 2.014702796936035\n",
            "24634 1.94696167454822e-06 3.457870479905978e-05 Traning Loss: 3.65256673831027e-05 17.984600067138672 2.0146231651306152\n",
            "24635 1.7565835150890052e-06 3.4535289159975946e-05 Traning Loss: 3.629187267506495e-05 17.984800338745117 2.0145485401153564\n",
            "24636 1.798916400730377e-06 3.43107312801294e-05 Traning Loss: 3.610964631661773e-05 17.984996795654297 2.0144729614257812\n",
            "24637 1.7772829323803307e-06 3.4318898542551324e-05 Traning Loss: 3.609618215705268e-05 17.985187530517578 2.014396905899048\n",
            "24638 1.6981992985165562e-06 3.448204734013416e-05 Traning Loss: 3.618024493334815e-05 17.985376358032227 2.014324188232422\n",
            "24639 1.7821002984419465e-06 3.442858360358514e-05 Traning Loss: 3.6210683902027085e-05 17.985559463500977 2.014249563217163\n",
            "24640 1.6481648117405712e-06 3.4472155675757676e-05 Traning Loss: 3.6120320146437734e-05 17.985742568969727 2.0141782760620117\n",
            "24641 1.686609948592377e-06 3.4288714232388884e-05 Traning Loss: 3.597532486310229e-05 17.985919952392578 2.014106512069702\n",
            "24642 1.6377998690586537e-06 3.424605165491812e-05 Traning Loss: 3.588385152397677e-05 17.986095428466797 2.0140347480773926\n",
            "24643 1.5855230230954476e-06 3.429914067965001e-05 Traning Loss: 3.5884662793250754e-05 17.986268997192383 2.0139660835266113\n",
            "24644 1.6516180494363653e-06 3.427313640713692e-05 Traning Loss: 3.5924753319704905e-05 17.986438751220703 2.013895034790039\n",
            "24645 1.536768650112208e-06 3.439083957346156e-05 Traning Loss: 3.592760913306847e-05 17.98660659790039 2.0138285160064697\n",
            "24646 1.6140998013725039e-06 3.4258366213180125e-05 Traning Loss: 3.5872464650310576e-05 17.986770629882812 2.013759136199951\n",
            "24647 1.514312998551759e-06 3.428871787036769e-05 Traning Loss: 3.5803030186798424e-05 17.98693084716797 2.01369309425354\n",
            "24648 1.5371425661214744e-06 3.423560337978415e-05 Traning Loss: 3.577274765120819e-05 17.987089157104492 2.0136263370513916\n",
            "24649 1.5155217170104152e-06 3.428454147069715e-05 Traning Loss: 3.580006159609184e-05 17.98724365234375 2.0135610103607178\n",
            "24650 1.4887700672261417e-06 3.4369859349681064e-05 Traning Loss: 3.5858629416907206e-05 17.987396240234375 2.0134963989257812\n",
            "24651 1.5074759858180187e-06 3.441055378061719e-05 Traning Loss: 3.59180303348694e-05 17.987545013427734 2.013432502746582\n",
            "24652 1.4740046481165336e-06 3.4499313187552616e-05 Traning Loss: 3.597331669880077e-05 17.98769187927246 2.013368606567383\n",
            "24653 1.4685737141917343e-06 3.45892331097275e-05 Traning Loss: 3.605780511861667e-05 17.987834930419922 2.0133073329925537\n",
            "24654 1.4964938372941106e-06 3.472323078312911e-05 Traning Loss: 3.621972427936271e-05 17.987977981567383 2.0132436752319336\n",
            "24655 1.4475898524324293e-06 3.504207052174024e-05 Traning Loss: 3.6489658668870106e-05 17.988115310668945 2.0131847858428955\n",
            "24656 1.5604432519467082e-06 3.533261042321101e-05 Traning Loss: 3.6893052310915664e-05 17.988252639770508 2.0131208896636963\n",
            "24657 1.4911945527273929e-06 3.597519753384404e-05 Traning Loss: 3.7466390494955704e-05 17.988386154174805 2.013063907623291\n",
            "24658 1.6745830180298071e-06 3.661000664578751e-05 Traning Loss: 3.8284590118564665e-05 17.9885196685791 2.01300048828125\n",
            "24659 1.6471983599330997e-06 3.7838693970115855e-05 Traning Loss: 3.948589073843323e-05 17.988645553588867 2.0129454135894775\n",
            "24660 1.9270116808911553e-06 3.9344908145722e-05 Traning Loss: 4.127192005398683e-05 17.9887752532959 2.0128822326660156\n",
            "24661 2.0278307601984125e-06 4.192184132989496e-05 Traning Loss: 4.394967254484072e-05 17.988895416259766 2.012828826904297\n",
            "24662 2.5282379283453338e-06 4.5419226808007807e-05 Traning Loss: 4.794746564584784e-05 17.98902130126953 2.0127649307250977\n",
            "24663 2.9039424589427654e-06 5.108471304993145e-05 Traning Loss: 5.398865687311627e-05 17.989133834838867 2.0127148628234863\n",
            "24664 3.937554083677242e-06 5.916191730648279e-05 Traning Loss: 6.309946911642328e-05 17.989255905151367 2.012648582458496\n",
            "24665 4.9503551053931005e-06 7.213224307633936e-05 Traning Loss: 7.708260091021657e-05 17.98935890197754 2.012603521347046\n",
            "24666 7.2332668423769064e-06 9.118376328842714e-05 Traning Loss: 9.841703285928816e-05 17.989477157592773 2.0125327110290527\n",
            "24667 9.857524673861917e-06 0.00012178381439298391 Traning Loss: 0.00013164133997634053 17.989564895629883 2.0124950408935547\n",
            "24668 1.505276668467559e-05 0.0001675605308264494 Traning Loss: 0.00018261329387314618 17.989673614501953 2.0124168395996094\n",
            "24669 2.1770349121652544e-05 0.00024100751033984125 Traning Loss: 0.00026277784490957856 17.989734649658203 2.012389659881592\n",
            "24670 3.376980384928174e-05 0.0003514257841743529 Traning Loss: 0.00038519559893757105 17.989824295043945 2.0122995376586914\n",
            "24671 5.054120265413076e-05 0.0005270506953820586 Traning Loss: 0.0005775918834842741 17.989831924438477 2.012286901473999\n",
            "24672 7.775947597110644e-05 0.0007853841525502503 Traning Loss: 0.0008631436503492296 17.98987579345703 2.012178421020508\n",
            "24673 0.0001167489099316299 0.0011824392713606358 Traning Loss: 0.0012991882394999266 17.989774703979492 2.0121853351593018\n",
            "24674 0.00017217690765392035 0.0017197614070028067 Traning Loss: 0.0018919382710009813 17.989704132080078 2.0120506286621094\n",
            "24675 0.0002464182034600526 0.002460073446854949 Traning Loss: 0.002706491621211171 17.989391326904297 2.012078046798706\n",
            "24676 0.0003248316061217338 0.003235063748434186 Traning Loss: 0.0035598953254520893 17.989093780517578 2.011915922164917\n",
            "24677 0.0004005411174148321 0.003964522387832403 Traning Loss: 0.004365063272416592 17.988460540771484 2.0119476318359375\n",
            "24678 0.0004102560633327812 0.004084095358848572 Traning Loss: 0.004494351334869862 17.98786735534668 2.011784553527832\n",
            "24679 0.00035706814378499985 0.0034994278103113174 Traning Loss: 0.0038564959540963173 17.987010955810547 2.01177716255188\n",
            "24680 0.0002147814229829237 0.0021189232356846333 Traning Loss: 0.0023337046150118113 17.98631477355957 2.0116710662841797\n",
            "24681 8.157211414072663e-05 0.0007393974810838699 Traning Loss: 0.0008209695806726813 17.985610961914062 2.01159405708313\n",
            "24682 1.5337858712882735e-05 9.191242861561477e-05 Traning Loss: 0.0001072502855095081 17.985065460205078 2.011569023132324\n",
            "24683 4.9281021347269416e-05 0.00038264968316070735 Traning Loss: 0.00043193070450797677 17.984630584716797 2.0114567279815674\n",
            "24684 0.00012008929479634389 0.001113078324124217 Traning Loss: 0.001233167597092688 17.98410415649414 2.0114598274230957\n",
            "24685 0.0001628876489121467 0.0014965722803026438 Traning Loss: 0.00165945990011096 17.98366355895996 2.011385202407837\n",
            "24686 0.00013630113971885294 0.001234520459547639 Traning Loss: 0.001370821613818407 17.98303985595703 2.0113625526428223\n",
            "24687 7.016121526248753e-05 0.0005772439762949944 Traning Loss: 0.0006474051624536514 17.98252296447754 2.0113730430603027\n",
            "24688 2.924299769802019e-05 0.00013796586426906288 Traning Loss: 0.00016720886924304068 17.981998443603516 2.011331796646118\n",
            "24689 2.971744652313646e-05 0.00020662398310378194 Traning Loss: 0.0002363414241699502 17.98151206970215 2.0113868713378906\n",
            "24690 6.910072988830507e-05 0.0005409056902863085 Traning Loss: 0.0006100063910707831 17.98113441467285 2.011375904083252\n",
            "24691 8.473103662254289e-05 0.0007557980716228485 Traning Loss: 0.0008405291009694338 17.980669021606445 2.011413335800171\n",
            "24692 7.357636059168726e-05 0.0006262030801735818 Traning Loss: 0.0006997794262133539 17.98031997680664 2.0114777088165283\n",
            "24693 4.476709000300616e-05 0.00032462889794260263 Traning Loss: 0.000369396002497524 17.97990608215332 2.011504650115967\n",
            "24694 1.867708306235727e-05 0.00012606944073922932 Traning Loss: 0.00014474651834461838 17.979568481445312 2.0116238594055176\n",
            "24695 2.4250672140624374e-05 0.00014416548947338015 Traning Loss: 0.0001684161543380469 17.97928237915039 2.0116777420043945\n",
            "24696 3.3764496038202196e-05 0.00030093779787421227 Traning Loss: 0.00033470228663645685 17.978973388671875 2.0117764472961426\n",
            "24697 4.496804467635229e-05 0.00039760611252859235 Traning Loss: 0.000442574149928987 17.97877311706543 2.011873245239258\n",
            "24698 4.2241179471602663e-05 0.00035607602330856025 Traning Loss: 0.0003983172064181417 17.978500366210938 2.0119285583496094\n",
            "24699 2.4550254238420166e-05 0.00022477547463495284 Traning Loss: 0.0002493257343303412 17.978313446044922 2.012049913406372\n",
            "24700 1.6550808140891604e-05 0.0001069070931407623 Traning Loss: 0.0001234578958246857 17.97811508178711 2.012094736099243\n",
            "24701 9.013732778839767e-06 8.990353671833873e-05 Traning Loss: 9.89172694971785e-05 17.977941513061523 2.0121891498565674\n",
            "24702 1.5820183762116358e-05 0.00014544068835675716 Traning Loss: 0.00016126087575685233 17.977840423583984 2.0122530460357666\n",
            "24703 2.2845650164526887e-05 0.00021272503363434225 Traning Loss: 0.00023557068197987974 17.977710723876953 2.012293577194214\n",
            "24704 2.1925570763414726e-05 0.00022909890685696155 Traning Loss: 0.00025102446670643985 17.977678298950195 2.0123748779296875\n",
            "24705 2.0899158698739484e-05 0.00017631977971177548 Traning Loss: 0.00019721893477253616 17.97761344909668 2.0123844146728516\n",
            "24706 8.997743861982599e-06 0.00010595536150503904 Traning Loss: 0.00011495310172904283 17.977611541748047 2.012451171875\n",
            "24707 6.494142780866241e-06 5.678741217707284e-05 Traning Loss: 6.328155723167583e-05 17.97762680053711 2.0124671459198\n",
            "24708 5.397921086114366e-06 6.477756687672809e-05 Traning Loss: 7.017549069132656e-05 17.97764778137207 2.0124900341033936\n",
            "24709 9.397693247592542e-06 0.00010672915959730744 Traning Loss: 0.00011612685193540528 17.97773551940918 2.0125226974487305\n",
            "24710 1.5097636605787557e-05 0.00014003126125317067 Traning Loss: 0.00015512890240643173 17.977792739868164 2.0125045776367188\n",
            "24711 1.2379608051560353e-05 0.00013937450421508402 Traning Loss: 0.00015175410953816026 17.97791862487793 2.01253604888916\n",
            "24712 1.1162883311044425e-05 9.842843428486958e-05 Traning Loss: 0.000109591317595914 17.978015899658203 2.0125112533569336\n",
            "24713 4.148913831159007e-06 5.693281127605587e-05 Traning Loss: 6.108172237873077e-05 17.978145599365234 2.012521743774414\n",
            "24714 3.1335105177277e-06 3.7027522921562195e-05 Traning Loss: 4.016103412141092e-05 17.97828483581543 2.0125184059143066\n",
            "24715 4.574765171128092e-06 5.033806883147918e-05 Traning Loss: 5.491283445735462e-05 17.97841453552246 2.0125017166137695\n",
            "24716 6.268840479606297e-06 7.895720773376524e-05 Traning Loss: 8.522604912286624e-05 17.978591918945312 2.012516498565674\n",
            "24717 1.0025770279753488e-05 9.271992166759446e-05 Traning Loss: 0.00010274568921886384 17.97873878479004 2.0124874114990234\n",
            "24718 6.917769951542141e-06 8.651435928186402e-05 Traning Loss: 9.343212877865881e-05 17.978933334350586 2.0124998092651367\n",
            "24719 6.153672529762844e-06 6.047115311957896e-05 Traning Loss: 6.662482337560505e-05 17.979106903076172 2.0124809741973877\n",
            "24720 2.669170271474286e-06 4.021131098852493e-05 Traning Loss: 4.288048148737289e-05 17.979299545288086 2.0124757289886475\n",
            "24721 2.2625929432251723e-06 3.470746014500037e-05 Traning Loss: 3.697005377034657e-05 17.9794979095459 2.0124759674072266\n",
            "24722 3.965750693168957e-06 4.388290471979417e-05 Traning Loss: 4.7848654503468424e-05 17.97968292236328 2.012455463409424\n",
            "24723 4.0825948417477775e-06 5.865905768587254e-05 Traning Loss: 6.274165207287297e-05 17.97989845275879 2.012465000152588\n",
            "24724 6.22189327259548e-06 6.273762846831232e-05 Traning Loss: 6.8959518102929e-05 17.98008918762207 2.0124433040618896\n",
            "24725 4.1134885577776e-06 5.866877836524509e-05 Traning Loss: 6.278226646827534e-05 17.980308532714844 2.012446880340576\n",
            "24726 3.875591573887505e-06 4.63480937469285e-05 Traning Loss: 5.0223687139805406e-05 17.980510711669922 2.012437343597412\n",
            "24727 2.424018475721823e-06 3.7727866583736613e-05 Traning Loss: 4.0151884604711086e-05 17.980724334716797 2.0124292373657227\n",
            "24728 2.058072823274415e-06 3.56372693204321e-05 Traning Loss: 3.7695343053201213e-05 17.980941772460938 2.012432098388672\n",
            "24729 3.006859287779662e-06 3.874130197800696e-05 Traning Loss: 4.1748160583665594e-05 17.981151580810547 2.012415885925293\n",
            "24730 2.7117580430058297e-06 4.471782813197933e-05 Traning Loss: 4.742958481074311e-05 17.981380462646484 2.012420177459717\n",
            "24731 3.808049996223417e-06 4.6452048991341144e-05 Traning Loss: 5.026009966968559e-05 17.981590270996094 2.0124049186706543\n",
            "24732 2.926493380073225e-06 4.585004717228003e-05 Traning Loss: 4.87765391881112e-05 17.9818172454834 2.012401580810547\n",
            "24733 2.937524186563678e-06 4.160327443969436e-05 Traning Loss: 4.454080044524744e-05 17.982027053833008 2.012392997741699\n",
            "24734 2.36481264437316e-06 3.7799902202095836e-05 Traning Loss: 4.01647157559637e-05 17.982242584228516 2.012380599975586\n",
            "24735 1.950530531757977e-06 3.5680975997820497e-05 Traning Loss: 3.7631507439073175e-05 17.982452392578125 2.012376546859741\n",
            "24736 2.242845312139252e-06 3.519791061989963e-05 Traning Loss: 3.7440757296280935e-05 17.982656478881836 2.0123586654663086\n",
            "24737 1.89105094250408e-06 3.6960762372473255e-05 Traning Loss: 3.8851812860229984e-05 17.98287010192871 2.0123512744903564\n",
            "24738 2.442374807287706e-06 3.813934381469153e-05 Traning Loss: 4.0581719076726586e-05 17.98307228088379 2.0123322010040283\n",
            "24739 2.1471842046594247e-06 3.9365968405036256e-05 Traning Loss: 4.151315079070628e-05 17.983287811279297 2.012316942214966\n",
            "24740 2.292064664288773e-06 3.88398693758063e-05 Traning Loss: 4.113193426746875e-05 17.983491897583008 2.012299060821533\n",
            "24741 2.0485749701038003e-06 3.75409763364587e-05 Traning Loss: 3.95895513065625e-05 17.983701705932617 2.0122766494750977\n",
            "24742 1.786593884389731e-06 3.582003773772158e-05 Traning Loss: 3.760663093999028e-05 17.983905792236328 2.012258291244507\n",
            "24743 1.7638377585171838e-06 3.4280958061572164e-05 Traning Loss: 3.604479570640251e-05 17.984111785888672 2.0122318267822266\n",
            "24744 1.5130589190448518e-06 3.397873661015183e-05 Traning Loss: 3.549179382389411e-05 17.984317779541016 2.01220965385437\n",
            "24745 1.7293834844167577e-06 3.424723036005162e-05 Traning Loss: 3.59766127076e-05 17.984516143798828 2.0121819972991943\n",
            "24746 1.6712270962671028e-06 3.532431219355203e-05 Traning Loss: 3.6995537811890244e-05 17.984718322753906 2.0121548175811768\n",
            "24747 1.83195322733809e-06 3.600257696234621e-05 Traning Loss: 3.7834528484381735e-05 17.984909057617188 2.012126922607422\n",
            "24748 1.7854374618764268e-06 3.617205948103219e-05 Traning Loss: 3.795749580604024e-05 17.985103607177734 2.0120956897735596\n",
            "24749 1.7021901612679358e-06 3.559260585461743e-05 Traning Loss: 3.72947943105828e-05 17.985288619995117 2.01206636428833\n",
            "24750 1.6043225059547694e-06 3.4591994335642084e-05 Traning Loss: 3.619631752371788e-05 17.985475540161133 2.012033224105835\n",
            "24751 1.4644610928371549e-06 3.373334766365588e-05 Traning Loss: 3.519780875649303e-05 17.985658645629883 2.0120012760162354\n",
            "24752 1.4295973187472555e-06 3.327176091261208e-05 Traning Loss: 3.4701359254540876e-05 17.98583984375 2.011967897415161\n",
            "24753 1.4213867416401627e-06 3.336832014610991e-05 Traning Loss: 3.47897075698711e-05 17.986019134521484 2.0119335651397705\n",
            "24754 1.447319618819165e-06 3.379628105903976e-05 Traning Loss: 3.5243599995737895e-05 17.98619270324707 2.0119004249572754\n",
            "24755 1.4923128901500604e-06 3.4218137443531305e-05 Traning Loss: 3.5710450902115554e-05 17.98636817932129 2.0118649005889893\n",
            "24756 1.486532028138754e-06 3.4432228858349845e-05 Traning Loss: 3.5918761568609625e-05 17.986536026000977 2.0118308067321777\n",
            "24757 1.4628661801907583e-06 3.431446384638548e-05 Traning Loss: 3.577733150450513e-05 17.986705780029297 2.0117945671081543\n",
            "24758 1.435701960872393e-06 3.395096064195968e-05 Traning Loss: 3.538666351232678e-05 17.986867904663086 2.0117580890655518\n",
            "24759 1.3486552461472456e-06 3.358266985742375e-05 Traning Loss: 3.4931326808873564e-05 17.987030029296875 2.0117218494415283\n",
            "24760 1.3651114159074496e-06 3.3213738788617775e-05 Traning Loss: 3.457885031821206e-05 17.987186431884766 2.011683464050293\n",
            "24761 1.2734774372802349e-06 3.313330307719298e-05 Traning Loss: 3.440678119659424e-05 17.987340927124023 2.0116469860076904\n",
            "24762 1.3239874760984094e-06 3.3077529224101454e-05 Traning Loss: 3.4401517041260377e-05 17.987491607666016 2.0116076469421387\n",
            "24763 1.265170908482105e-06 3.3225209335796535e-05 Traning Loss: 3.4490381949581206e-05 17.987638473510742 2.0115702152252197\n",
            "24764 1.2925421515319613e-06 3.330057370476425e-05 Traning Loss: 3.459311483311467e-05 17.98778533935547 2.011530876159668\n",
            "24765 1.285061102862528e-06 3.3366159186698496e-05 Traning Loss: 3.465122063062154e-05 17.987926483154297 2.0114917755126953\n",
            "24766 1.2509092357504414e-06 3.33907846652437e-05 Traning Loss: 3.4641692764125764e-05 17.988069534301758 2.011453151702881\n",
            "24767 1.2902123671665322e-06 3.327909143990837e-05 Traning Loss: 3.4569304261822253e-05 17.98820686340332 2.0114128589630127\n",
            "24768 1.201226950797718e-06 3.325266152387485e-05 Traning Loss: 3.445388938416727e-05 17.988344192504883 2.0113749504089355\n",
            "24769 1.260944145542453e-06 3.3054722734959796e-05 Traning Loss: 3.4315668017370626e-05 17.988475799560547 2.011334180831909\n",
            "24770 1.1603178791119717e-06 3.3020798582583666e-05 Traning Loss: 3.418111737119034e-05 17.98860740661621 2.011296272277832\n",
            "24771 1.2107761904189829e-06 3.285670391051099e-05 Traning Loss: 3.406748146517202e-05 17.98873519897461 2.011256217956543\n",
            "24772 1.1463312148407567e-06 3.284531339886598e-05 Traning Loss: 3.399164415895939e-05 17.988861083984375 2.0112178325653076\n",
            "24773 1.1628454785750364e-06 3.279631710029207e-05 Traning Loss: 3.3959164284169674e-05 17.988985061645508 2.011178970336914\n",
            "24774 1.154320671048481e-06 3.280630335211754e-05 Traning Loss: 3.396062311367132e-05 17.989105224609375 2.0111403465270996\n",
            "24775 1.124787786466186e-06 3.2856245525181293e-05 Traning Loss: 3.39810321747791e-05 17.98922348022461 2.0111031532287598\n",
            "24776 1.158985583060712e-06 3.284128615632653e-05 Traning Loss: 3.400027344468981e-05 17.989337921142578 2.0110647678375244\n",
            "24777 1.0935599448202993e-06 3.2906529668252915e-05 Traning Loss: 3.400008790777065e-05 17.989452362060547 2.01102876663208\n",
            "24778 1.143842041528842e-06 3.282897159806453e-05 Traning Loss: 3.397281398065388e-05 17.98956298828125 2.010990619659424\n",
            "24779 1.0689267355701304e-06 3.2848627597559243e-05 Traning Loss: 3.391755308257416e-05 17.989673614501953 2.0109550952911377\n",
            "24780 1.1086849553976208e-06 3.273689799243584e-05 Traning Loss: 3.3845582947833464e-05 17.98978042602539 2.010917901992798\n",
            "24781 1.0498719120732858e-06 3.27187663060613e-05 Traning Loss: 3.3768639696063474e-05 17.989887237548828 2.0108823776245117\n",
            "24782 1.063860509020742e-06 3.2638825359754264e-05 Traning Loss: 3.370268677826971e-05 17.989990234375 2.0108463764190674\n",
            "24783 1.0362969078414608e-06 3.261297752032988e-05 Traning Loss: 3.364927397342399e-05 17.99009132385254 2.0108108520507812\n",
            "24784 1.0238396725981147e-06 3.258865763200447e-05 Traning Loss: 3.36124976456631e-05 17.990190505981445 2.0107758045196533\n",
            "24785 1.0270838401993387e-06 3.256126365158707e-05 Traning Loss: 3.358834874234162e-05 17.99028778076172 2.010740041732788\n",
            "24786 9.961364639821113e-07 3.257523712818511e-05 Traning Loss: 3.357137393322773e-05 17.990385055541992 2.0107057094573975\n",
            "24787 1.0158519216929562e-06 3.254008697695099e-05 Traning Loss: 3.355593798914924e-05 17.990478515625 2.0106701850891113\n",
            "24788 9.77231366050546e-07 3.2561943953623995e-05 Traning Loss: 3.3539174182806164e-05 17.990571975708008 2.0106360912323\n",
            "24789 9.977520676329732e-07 3.2522715628147125e-05 Traning Loss: 3.35204676957801e-05 17.990663528442383 2.010601043701172\n",
            "24790 9.62508011070895e-07 3.253599425079301e-05 Traning Loss: 3.349850157974288e-05 17.990755081176758 2.0105671882629395\n",
            "24791 9.755422070156783e-07 3.249984729336575e-05 Traning Loss: 3.3475389500381425e-05 17.990842819213867 2.010532855987549\n",
            "24792 9.511357461633452e-07 3.24979773722589e-05 Traning Loss: 3.34491123794578e-05 17.990930557250977 2.0104990005493164\n",
            "24793 9.54163169808453e-07 3.246696724090725e-05 Traning Loss: 3.342112904647365e-05 17.991016387939453 2.010465145111084\n",
            "24794 9.397871849614603e-07 3.2451804145239294e-05 Traning Loss: 3.3391592296538875e-05 17.991100311279297 2.0104315280914307\n",
            "24795 9.341606528323609e-07 3.242666571168229e-05 Traning Loss: 3.33608259097673e-05 17.991182327270508 2.0103983879089355\n",
            "24796 9.254153496840445e-07 3.240244768676348e-05 Traning Loss: 3.332786218379624e-05 17.99126434326172 2.0103650093078613\n",
            "24797 9.160825698018016e-07 3.2378899049945176e-05 Traning Loss: 3.329498213133775e-05 17.991342544555664 2.0103321075439453\n",
            "24798 9.091920105674944e-07 3.23526983265765e-05 Traning Loss: 3.326189107610844e-05 17.99142074584961 2.0102992057800293\n",
            "24799 9.012974828692677e-07 3.232919334550388e-05 Traning Loss: 3.323049168102443e-05 17.991497039794922 2.0102665424346924\n",
            "24800 8.931469892559107e-07 3.2307634683093056e-05 Traning Loss: 3.3200780308106914e-05 17.991573333740234 2.0102341175079346\n",
            "24801 8.887503781807027e-07 3.228396235499531e-05 Traning Loss: 3.31727133016102e-05 17.991647720336914 2.010201930999756\n",
            "24802 8.777248012847849e-07 3.226898115826771e-05 Traning Loss: 3.31467053911183e-05 17.99172019958496 2.0101699829101562\n",
            "24803 8.764080803302932e-07 3.2244613976217806e-05 Traning Loss: 3.3121021260740235e-05 17.991790771484375 2.0101380348205566\n",
            "24804 8.6363235141107e-07 3.223311068722978e-05 Traning Loss: 3.3096741390181705e-05 17.99186134338379 2.0101065635681152\n",
            "24805 8.639911470709194e-07 3.220968574169092e-05 Traning Loss: 3.3073676604544744e-05 17.99193000793457 2.0100748538970947\n",
            "24806 8.517101264260418e-07 3.2198320695897564e-05 Traning Loss: 3.305002974229865e-05 17.99199676513672 2.0100438594818115\n",
            "24807 8.513057991876849e-07 3.217657649656758e-05 Traning Loss: 3.3027881727321073e-05 17.992063522338867 2.010012626647949\n",
            "24808 8.411973908550863e-07 3.216297773178667e-05 Traning Loss: 3.3004176657414064e-05 17.992128372192383 2.009981870651245\n",
            "24809 8.377477342946804e-07 3.214342359569855e-05 Traning Loss: 3.2981170079438016e-05 17.9921932220459 2.009951114654541\n",
            "24810 8.312630939144583e-07 3.212779847672209e-05 Traning Loss: 3.295906208222732e-05 17.99225616455078 2.009920597076416\n",
            "24811 8.24226788154192e-07 3.211116563761607e-05 Traning Loss: 3.293539339210838e-05 17.99231719970703 2.00989031791687\n",
            "24812 8.22164054170571e-07 3.209123678971082e-05 Traning Loss: 3.291340181021951e-05 17.99237823486328 2.009860038757324\n",
            "24813 8.115286505017139e-07 3.208158886991441e-05 Traning Loss: 3.289311644039117e-05 17.9924373626709 2.0098302364349365\n",
            "24814 8.137053555401508e-07 3.205837128916755e-05 Traning Loss: 3.287207800894976e-05 17.992494583129883 2.0098001956939697\n",
            "24815 7.996727617864963e-07 3.2051750167738646e-05 Traning Loss: 3.285142156528309e-05 17.992551803588867 2.009770631790161\n",
            "24816 8.051116537899361e-07 3.202769221388735e-05 Traning Loss: 3.2832802389748394e-05 17.99260711669922 2.0097408294677734\n",
            "24817 7.886152957325976e-07 3.202505467925221e-05 Traning Loss: 3.28136702592019e-05 17.99266242980957 2.009711742401123\n",
            "24818 7.968039312800101e-07 3.1998926715459675e-05 Traning Loss: 3.279573138570413e-05 17.99271583557129 2.0096824169158936\n",
            "24819 7.787397180436528e-07 3.200166247552261e-05 Traning Loss: 3.2780400943011045e-05 17.992769241333008 2.0096538066864014\n",
            "24820 7.890789675002452e-07 3.197660771547817e-05 Traning Loss: 3.276568531873636e-05 17.992820739746094 2.009624719619751\n",
            "24821 7.699225079704775e-07 3.198465856257826e-05 Traning Loss: 3.2754582207417116e-05 17.99287223815918 2.009596347808838\n",
            "24822 7.825670991223888e-07 3.1963114452082664e-05 Traning Loss: 3.274568007327616e-05 17.992921829223633 2.0095674991607666\n",
            "24823 7.624263389516273e-07 3.198147169314325e-05 Traning Loss: 3.274389746366069e-05 17.992971420288086 2.0095396041870117\n",
            "24824 7.787417644067318e-07 3.196898614987731e-05 Traning Loss: 3.2747728255344555e-05 17.993019104003906 2.0095112323760986\n",
            "24825 7.573588050036051e-07 3.200643914169632e-05 Traning Loss: 3.276379720773548e-05 17.99306869506836 2.009483814239502\n",
            "24826 7.803461130606593e-07 3.2012947485782206e-05 Traning Loss: 3.279329393990338e-05 17.993114471435547 2.009455680847168\n",
            "24827 7.572241429443238e-07 3.208862472092733e-05 Traning Loss: 3.2845848181750625e-05 17.993162155151367 2.0094287395477295\n",
            "24828 7.92013679529191e-07 3.213540185242891e-05 Traning Loss: 3.292741530458443e-05 17.993206024169922 2.0094008445739746\n",
            "24829 7.676069913031824e-07 3.228760397178121e-05 Traning Loss: 3.3055210224119946e-05 17.99325180053711 2.0093743801116943\n",
            "24830 8.241274258580233e-07 3.242661477997899e-05 Traning Loss: 3.3250740671064705e-05 17.99329376220703 2.0093464851379395\n",
            "24831 8.019763413358305e-07 3.2745214411988854e-05 Traning Loss: 3.3547192288096994e-05 17.99333953857422 2.0093207359313965\n",
            "24832 8.995544362733199e-07 3.309594467282295e-05 Traning Loss: 3.3995500416494906e-05 17.993377685546875 2.0092928409576416\n",
            "24833 8.922099254959903e-07 3.3781347156036645e-05 Traning Loss: 3.467355782049708e-05 17.993423461914062 2.009268045425415\n",
            "24834 1.0713282563301618e-06 3.463448956608772e-05 Traning Loss: 3.570581611711532e-05 17.993457794189453 2.009239912033081\n",
            "24835 1.1150050340802409e-06 3.616061803768389e-05 Traning Loss: 3.727562216226943e-05 17.99350357055664 2.009216547012329\n",
            "24836 1.4656716302852146e-06 3.822743383352645e-05 Traning Loss: 3.9693106373306364e-05 17.993534088134766 2.009187698364258\n",
            "24837 1.6611681985523319e-06 4.1757553844945505e-05 Traning Loss: 4.341872045188211e-05 17.993579864501953 2.0091660022735596\n",
            "24838 2.3960037651704624e-06 4.683803490479477e-05 Traning Loss: 4.9234040488954633e-05 17.99360466003418 2.0091359615325928\n",
            "24839 3.0147159577609273e-06 5.5293890909524634e-05 Traning Loss: 5.830860754940659e-05 17.99365234375 2.0091166496276855\n",
            "24840 4.657859335566172e-06 6.80321900290437e-05 Traning Loss: 7.269004709087312e-05 17.99366569519043 2.009084463119507\n",
            "24841 6.42936674921657e-06 8.895144128473476e-05 Traning Loss: 9.538080485071987e-05 17.993717193603516 2.0090689659118652\n",
            "24842 1.031861211231444e-05 0.00012155204603914171 Traning Loss: 0.00013187066360842437 17.993711471557617 2.009032964706421\n",
            "24843 1.5202856957330368e-05 0.00017467491852585226 Traning Loss: 0.00018987777002621442 17.993764877319336 2.009023427963257\n",
            "24844 2.4819599275360815e-05 0.00025946530513465405 Traning Loss: 0.00028428490622900426 17.993724822998047 2.0089809894561768\n",
            "24845 3.799537444137968e-05 0.0003961928014177829 Traning Loss: 0.0004341881722211838 17.993770599365234 2.008981227874756\n",
            "24846 6.226397817954421e-05 0.0006167898536659777 Traning Loss: 0.0006790538318455219 17.993656158447266 2.0089282989501953\n",
            "24847 9.649278945289552e-05 0.0009629792766645551 Traning Loss: 0.001059472095221281 17.99366569519043 2.008943796157837\n",
            "24848 0.00015601638006046414 0.001512587652541697 Traning Loss: 0.001668604090809822 17.993396759033203 2.0088744163513184\n",
            "24849 0.00023578677792102098 0.002310262294486165 Traning Loss: 0.002546049188822508 17.993284225463867 2.00891375541687\n",
            "24850 0.0003611389547586441 0.0034695167560130358 Traning Loss: 0.00383065571077168 17.99270248413086 2.008822441101074\n",
            "24851 0.0004934887401759624 0.004799095448106527 Traning Loss: 0.00529258418828249 17.992294311523438 2.0088939666748047\n",
            "24852 0.0006474033580161631 0.006174142938107252 Traning Loss: 0.006821546237915754 17.991233825683594 2.008787155151367\n",
            "24853 0.000680498022120446 0.006588997319340706 Traning Loss: 0.007269495166838169 17.99039077758789 2.008887767791748\n",
            "24854 0.0006108651868999004 0.005714067257940769 Traning Loss: 0.00632493244484067 17.98902702331543 2.008807897567749\n",
            "24855 0.0003469887305982411 0.003290486056357622 Traning Loss: 0.0036374747287482023 17.98801612854004 2.0089118480682373\n",
            "24856 0.00011846442794194445 0.0008917765226215124 Traning Loss: 0.001010240986943245 17.986984252929688 2.0089337825775146\n",
            "24857 2.8829495931859128e-05 6.60164951113984e-05 Traning Loss: 9.484599286224693e-05 17.986120223999023 2.009016752243042\n",
            "24858 0.0001259291748283431 0.0010050779674202204 Traning Loss: 0.0011310070985928178 17.985462188720703 2.0091679096221924\n",
            "24859 0.0002768860140349716 0.002332584233954549 Traning Loss: 0.002609470160678029 17.984485626220703 2.0092525482177734\n",
            "24860 0.0002808751305565238 0.002494321670383215 Traning Loss: 0.0027751969173550606 17.98369026184082 2.0094826221466064\n",
            "24861 0.00018383409769739956 0.001385098323225975 Traning Loss: 0.0015689324354752898 17.982648849487305 2.0096309185028076\n",
            "24862 5.630422674585134e-05 0.00024634439614601433 Traning Loss: 0.0003026486374437809 17.981719970703125 2.009874105453491\n",
            "24863 5.537479228223674e-05 0.00019438198069110513 Traning Loss: 0.00024975676205940545 17.980886459350586 2.010118007659912\n",
            "24864 0.00013364334881771356 0.0009756690706126392 Traning Loss: 0.0011093124048784375 17.97989845275879 2.0103514194488525\n",
            "24865 0.0001787730143405497 0.0014310763217508793 Traning Loss: 0.001609849277883768 17.979093551635742 2.0106587409973145\n",
            "24866 0.00013796554412692785 0.0010241531999781728 Traning Loss: 0.0011621187441051006 17.978166580200195 2.0109169483184814\n",
            "24867 5.709316610591486e-05 0.0002833362959790975 Traning Loss: 0.00034042945480905473 17.977399826049805 2.011242628097534\n",
            "24868 3.389290941413492e-05 6.502617179648951e-05 Traning Loss: 9.891908121062443e-05 17.976749420166016 2.0115513801574707\n",
            "24869 6.917876453371719e-05 0.00046310952166095376 Traning Loss: 0.0005322882789187133 17.97608184814453 2.011866807937622\n",
            "24870 0.00010611436300678179 0.0008351686992682517 Traning Loss: 0.0009412830695509911 17.9755859375 2.0122005939483643\n",
            "24871 9.082289761863649e-05 0.0007186903967522085 Traning Loss: 0.0008095132652670145 17.97500991821289 2.0125010013580322\n",
            "24872 4.24631180067081e-05 0.00027893937658518553 Traning Loss: 0.00032140250550583005 17.974573135375977 2.0128173828125\n",
            "24873 1.594769855728373e-05 3.650774306152016e-05 Traning Loss: 5.245544161880389e-05 17.97418212890625 2.013105869293213\n",
            "24874 2.8058628231519833e-05 0.00018823335994966328 Traning Loss: 0.0002162919845432043 17.973785400390625 2.0133819580078125\n",
            "24875 5.4138996347319335e-05 0.00046025647316128016 Traning Loss: 0.0005143954767845571 17.973508834838867 2.0136449337005615\n",
            "24876 5.8615845773601905e-05 0.0005137335392646492 Traning Loss: 0.0005723494105041027 17.973148345947266 2.013878107070923\n",
            "24877 3.474099139566533e-05 0.0003040219016838819 Traning Loss: 0.0003387628821656108 17.972896575927734 2.014099359512329\n",
            "24878 1.1370456377335358e-05 7.931573054520413e-05 Traning Loss: 9.06861896510236e-05 17.9726619720459 2.0143024921417236\n",
            "24879 7.966343218868133e-06 5.3088628192199394e-05 Traning Loss: 6.105496868258342e-05 17.97245216369629 2.014483690261841\n",
            "24880 2.081684215227142e-05 0.00019653058552648872 Traning Loss: 0.00021734743495471776 17.97234344482422 2.014655113220215\n",
            "24881 3.428239870117977e-05 0.0003153995203319937 Traning Loss: 0.00034968191175721586 17.972198486328125 2.0147974491119385\n",
            "24882 2.8816217309213243e-05 0.0002835628984030336 Traning Loss: 0.0003123791248071939 17.9721622467041 2.0149242877960205\n",
            "24883 1.542001700727269e-05 0.00014634431863669306 Traning Loss: 0.00016176432836800814 17.972124099731445 2.015033721923828\n",
            "24884 4.881581844529137e-06 4.576069477479905e-05 Traning Loss: 5.0642276619328186e-05 17.972148895263672 2.0151164531707764\n",
            "24885 5.489021987159504e-06 6.109966489020735e-05 Traning Loss: 6.65886836941354e-05 17.972230911254883 2.0151970386505127\n",
            "24886 1.5523191905231215e-05 0.00014291261322796345 Traning Loss: 0.00015843581059016287 17.972291946411133 2.0152435302734375\n",
            "24887 1.891832289402373e-05 0.00019460283510852605 Traning Loss: 0.00021352115436457098 17.972436904907227 2.0152928829193115\n",
            "24888 1.7039970771293156e-05 0.0001615075598238036 Traning Loss: 0.00017854753241408616 17.97254180908203 2.015320062637329\n",
            "24889 8.292255188280251e-06 8.56042024679482e-05 Traning Loss: 9.389645856572315e-05 17.972700119018555 2.015338897705078\n",
            "24890 3.5211548947700066e-06 3.755860961973667e-05 Traning Loss: 4.1079765651375055e-05 17.972871780395508 2.0153563022613525\n",
            "24891 5.638031325361226e-06 5.044383215135895e-05 Traning Loss: 5.608186256722547e-05 17.973033905029297 2.0153493881225586\n",
            "24892 9.105756362259854e-06 9.685406257631257e-05 Traning Loss: 0.00010595982166705653 17.973249435424805 2.015354871749878\n",
            "24893 1.3050175766693428e-05 0.00011980309500358999 Traning Loss: 0.0001328532671323046 17.973426818847656 2.0153329372406006\n",
            "24894 9.506521564617287e-06 0.00010136412311112508 Traning Loss: 0.00011087064194725826 17.97365379333496 2.015319585800171\n",
            "24895 6.138059688964859e-06 5.895973299629986e-05 Traning Loss: 6.509778904728591e-05 17.97386932373047 2.0152902603149414\n",
            "24896 3.039279818040086e-06 3.4352109651081264e-05 Traning Loss: 3.73913899238687e-05 17.974103927612305 2.015256643295288\n",
            "24897 3.586124194043805e-06 4.235957749187946e-05 Traning Loss: 4.594570054905489e-05 17.974361419677734 2.015225648880005\n",
            "24898 6.892575129313627e-06 6.607609248021618e-05 Traning Loss: 7.296866533579305e-05 17.974597930908203 2.015178680419922\n",
            "24899 7.112356342986459e-06 8.060482650762424e-05 Traning Loss: 8.771718421485275e-05 17.974878311157227 2.0151443481445312\n",
            "24900 7.28639906810713e-06 6.969712558202446e-05 Traning Loss: 7.698352419538423e-05 17.975130081176758 2.0150928497314453\n",
            "24901 4.0059912862489e-06 4.875786180491559e-05 Traning Loss: 5.2763854910153896e-05 17.975414276123047 2.015052080154419\n",
            "24902 3.12296856463945e-06 3.4016829886240885e-05 Traning Loss: 3.713979822350666e-05 17.975698471069336 2.0150036811828613\n",
            "24903 3.3132685075543122e-06 3.699301305459812e-05 Traning Loss: 4.030628042528406e-05 17.975976943969727 2.014955997467041\n",
            "24904 4.325942882132949e-06 4.9830079660750926e-05 Traning Loss: 5.4156022088136524e-05 17.97627830505371 2.014911413192749\n",
            "24905 5.754742687713588e-06 5.73513243580237e-05 Traning Loss: 6.310606840997934e-05 17.976551055908203 2.0148603916168213\n",
            "24906 4.693923528975574e-06 5.426645293482579e-05 Traning Loss: 5.896037691854872e-05 17.976842880249023 2.014817714691162\n",
            "24907 4.111373982595978e-06 4.2509822378633544e-05 Traning Loss: 4.662119681597687e-05 17.977115631103516 2.014768600463867\n",
            "24908 2.7451053483673604e-06 3.421328074182384e-05 Traning Loss: 3.6958386772312224e-05 17.97739028930664 2.014725685119629\n",
            "24909 2.808960061884136e-06 3.3791005989769474e-05 Traning Loss: 3.6599965824279934e-05 17.977664947509766 2.0146799087524414\n",
            "24910 3.404717062949203e-06 3.976192238042131e-05 Traning Loss: 4.316664126235992e-05 17.977928161621094 2.0146360397338867\n",
            "24911 3.6556725717673544e-06 4.544526746030897e-05 Traning Loss: 4.9100941396318376e-05 17.978206634521484 2.0145928859710693\n",
            "24912 3.865633516397793e-06 4.509462451096624e-05 Traning Loss: 4.896025711786933e-05 17.97846794128418 2.0145483016967773\n",
            "24913 2.9938744319224497e-06 4.0301667468156666e-05 Traning Loss: 4.329554212745279e-05 17.97874641418457 2.0145046710968018\n",
            "24914 2.5406814074813155e-06 3.452791497693397e-05 Traning Loss: 3.706859570229426e-05 17.979021072387695 2.0144598484039307\n",
            "24915 2.2263980099523906e-06 3.249724250053987e-05 Traning Loss: 3.472364187473431e-05 17.979297637939453 2.014414072036743\n",
            "24916 2.317329290235648e-06 3.453291719779372e-05 Traning Loss: 3.685024785227142e-05 17.979581832885742 2.0143682956695557\n",
            "24917 2.8141737402620493e-06 3.7693727790610865e-05 Traning Loss: 4.050790084875189e-05 17.9798526763916 2.014319896697998\n",
            "24918 2.725322246988071e-06 3.949020538129844e-05 Traning Loss: 4.221552808303386e-05 17.980133056640625 2.0142712593078613\n",
            "24919 2.7744192721002037e-06 3.7950449041090906e-05 Traning Loss: 4.072486990480684e-05 17.980398178100586 2.014220714569092\n",
            "24920 2.2856011128169484e-06 3.515285789035261e-05 Traning Loss: 3.7438458093674853e-05 17.98066520690918 2.0141677856445312\n",
            "24921 2.052451463896432e-06 3.279655720689334e-05 Traning Loss: 3.48490102624055e-05 17.980926513671875 2.0141146183013916\n",
            "24922 2.0376755855977535e-06 3.2408908737124875e-05 Traning Loss: 3.444658432272263e-05 17.981182098388672 2.014056444168091\n",
            "24923 1.9633457668533083e-06 3.3895481465151533e-05 Traning Loss: 3.585882586776279e-05 17.981441497802734 2.0139999389648438\n",
            "24924 2.2981657821219414e-06 3.520275276969187e-05 Traning Loss: 3.750091855181381e-05 17.981689453125 2.0139377117156982\n",
            "24925 2.0534464511001715e-06 3.5904351534554735e-05 Traning Loss: 3.795779775828123e-05 17.981945037841797 2.0138773918151855\n",
            "24926 2.144662403225084e-06 3.4807610063580796e-05 Traning Loss: 3.69522713299375e-05 17.982189178466797 2.013812780380249\n",
            "24927 1.7974203956327983e-06 3.34918950102292e-05 Traning Loss: 3.528931483742781e-05 17.982439041137695 2.0137486457824707\n",
            "24928 1.7472343643021304e-06 3.2336520234821364e-05 Traning Loss: 3.4083754144376144e-05 17.982685089111328 2.0136830806732178\n",
            "24929 1.722086835798109e-06 3.2207677577389404e-05 Traning Loss: 3.3929765777429566e-05 17.982925415039062 2.013615846633911\n",
            "24930 1.7013334172588657e-06 3.2901964004850015e-05 Traning Loss: 3.460329753579572e-05 17.98316764831543 2.013549566268921\n",
            "24931 1.8999832036570297e-06 3.3483207516837865e-05 Traning Loss: 3.5383189242566004e-05 17.9833984375 2.0134799480438232\n",
            "24932 1.7559524394528125e-06 3.387466131243855e-05 Traning Loss: 3.5630615457193926e-05 17.983631134033203 2.0134127140045166\n",
            "24933 1.8643399926077109e-06 3.3327058190479875e-05 Traning Loss: 3.5191398637834936e-05 17.98385238647461 2.0133419036865234\n",
            "24934 1.6356614196411101e-06 3.2756172004155815e-05 Traning Loss: 3.43918327416759e-05 17.984071731567383 2.0132734775543213\n",
            "24935 1.6576971120230155e-06 3.207126064808108e-05 Traning Loss: 3.372895662323572e-05 17.984285354614258 2.013202667236328\n",
            "24936 1.5624516436218983e-06 3.195628960384056e-05 Traning Loss: 3.351873965584673e-05 17.984493255615234 2.0131332874298096\n",
            "24937 1.575979808876582e-06 3.215763354091905e-05 Traning Loss: 3.3733613236108795e-05 17.984699249267578 2.0130631923675537\n",
            "24938 1.6112510365928756e-06 3.248030407121405e-05 Traning Loss: 3.409155397093855e-05 17.98489761352539 2.012993812561035\n",
            "24939 1.5683950778111466e-06 3.271607420174405e-05 Traning Loss: 3.428446871112101e-05 17.985097885131836 2.0129246711730957\n",
            "24940 1.591250793353538e-06 3.2580770493950695e-05 Traning Loss: 3.417202242417261e-05 17.98529052734375 2.0128557682037354\n",
            "24941 1.501329279562924e-06 3.232415110687725e-05 Traning Loss: 3.3825479476945475e-05 17.985483169555664 2.012787342071533\n",
            "24942 1.4822020375504508e-06 3.1962110369931906e-05 Traning Loss: 3.344431388541125e-05 17.98567008972168 2.0127193927764893\n",
            "24943 1.4435546518143383e-06 3.176647442160174e-05 Traning Loss: 3.321002805023454e-05 17.985855102539062 2.0126514434814453\n",
            "24944 1.416517079633195e-06 3.1768497137818485e-05 Traning Loss: 3.318501330795698e-05 17.98603630065918 2.012584686279297\n",
            "24945 1.4508005961033632e-06 3.1853371183387935e-05 Traning Loss: 3.330417166580446e-05 17.9862117767334 2.0125174522399902\n",
            "24946 1.4038715789865819e-06 3.2030598958954215e-05 Traning Loss: 3.343446951475926e-05 17.986385345458984 2.012451648712158\n",
            "24947 1.4449429954765947e-06 3.2024905522121117e-05 Traning Loss: 3.3469848858658224e-05 17.986553192138672 2.012385845184326\n",
            "24948 1.3721363529839437e-06 3.200105493306182e-05 Traning Loss: 3.3373191399732605e-05 17.986719131469727 2.0123212337493896\n",
            "24949 1.3786967656415072e-06 3.181482315994799e-05 Traning Loss: 3.319351890240796e-05 17.986879348754883 2.0122568607330322\n",
            "24950 1.3211595160100842e-06 3.169056071783416e-05 Traning Loss: 3.301172182545997e-05 17.987037658691406 2.012193441390991\n",
            "24951 1.3014604292038712e-06 3.159430707455613e-05 Traning Loss: 3.289576852694154e-05 17.987194061279297 2.0121307373046875\n",
            "24952 1.2915159004478483e-06 3.15736178890802e-05 Traning Loss: 3.286513310740702e-05 17.987346649169922 2.012068271636963\n",
            "24953 1.254996050192858e-06 3.1640491215512156e-05 Traning Loss: 3.289548840257339e-05 17.987497329711914 2.012007236480713\n",
            "24954 1.2786235856765416e-06 3.165977614116855e-05 Traning Loss: 3.293839836260304e-05 17.98764419555664 2.011945962905884\n",
            "24955 1.2238701856404077e-06 3.1722065614303574e-05 Traning Loss: 3.294593625469133e-05 17.987791061401367 2.0118863582611084\n",
            "24956 1.2508742202044232e-06 3.1652278266847134e-05 Traning Loss: 3.2903153623919934e-05 17.987934112548828 2.011826276779175\n",
            "24957 1.1876308008140768e-06 3.1630534067517146e-05 Traning Loss: 3.281816316302866e-05 17.988075256347656 2.011768102645874\n",
            "24958 1.2046681376887136e-06 3.1516930903308094e-05 Traning Loss: 3.272160029155202e-05 17.98821258544922 2.011709451675415\n",
            "24959 1.1525770560183446e-06 3.148846735712141e-05 Traning Loss: 3.264104452682659e-05 17.98834800720215 2.0116524696350098\n",
            "24960 1.1615422863542335e-06 3.1433391995960847e-05 Traning Loss: 3.25949331454467e-05 17.988481521606445 2.0115952491760254\n",
            "24961 1.1285001164651476e-06 3.1450144888367504e-05 Traning Loss: 3.257864591432735e-05 17.98861312866211 2.0115394592285156\n",
            "24962 1.1328318123560166e-06 3.144821312162094e-05 Traning Loss: 3.258104334236123e-05 17.98874282836914 2.0114831924438477\n",
            "24963 1.1078946045017801e-06 3.147567986161448e-05 Traning Loss: 3.258357537561096e-05 17.988868713378906 2.0114285945892334\n",
            "24964 1.1109940487585845e-06 3.146238304907456e-05 Traning Loss: 3.2573378121014684e-05 17.98899269104004 2.011373519897461\n",
            "24965 1.0786020538944285e-06 3.146663584630005e-05 Traning Loss: 3.254523835494183e-05 17.98911476135254 2.0113203525543213\n",
            "24966 1.0901812856900506e-06 3.1414518161909655e-05 Traning Loss: 3.250470035709441e-05 17.989234924316406 2.011265993118286\n",
            "24967 1.041740574692085e-06 3.1423460313817486e-05 Traning Loss: 3.2465199183207005e-05 17.98935317993164 2.011214256286621\n",
            "24968 1.075390741789306e-06 3.135949373245239e-05 Traning Loss: 3.243488390580751e-05 17.989469528198242 2.011160373687744\n",
            "24969 1.0063115496450337e-06 3.141776323900558e-05 Traning Loss: 3.242407547077164e-05 17.98958396911621 2.011110305786133\n",
            "24970 1.0730580015660962e-06 3.1363782909465954e-05 Traning Loss: 3.2436841138405725e-05 17.989694595336914 2.011056661605835\n",
            "24971 9.782684173842426e-07 3.149776239297353e-05 Traning Loss: 3.247602944611572e-05 17.989805221557617 2.0110082626342773\n",
            "24972 1.086311272047169e-06 3.145803930237889e-05 Traning Loss: 3.25443506881129e-05 17.989912033081055 2.0109546184539795\n",
            "24973 9.600490784578142e-07 3.16916084557306e-05 Traning Loss: 3.265165651100688e-05 17.990018844604492 2.0109081268310547\n",
            "24974 1.1223870615140186e-06 3.1687937735114247e-05 Traning Loss: 3.2810323318699375e-05 17.990121841430664 2.0108540058135986\n",
            "24975 9.596187737770379e-07 3.209511123714037e-05 Traning Loss: 3.305473001091741e-05 17.99022674560547 2.010809898376465\n",
            "24976 1.2057405456289416e-06 3.222715895390138e-05 Traning Loss: 3.3432897907914594e-05 17.990325927734375 2.0107548236846924\n",
            "24977 1.0041582072517485e-06 3.3017848181771114e-05 Traning Loss: 3.4022006730083376e-05 17.990427017211914 2.010714054107666\n",
            "24978 1.3970737882118556e-06 3.354332875460386e-05 Traning Loss: 3.494040356599726e-05 17.990522384643555 2.0106565952301025\n",
            "24979 1.168232188319962e-06 3.521201142575592e-05 Traning Loss: 3.6380242818268016e-05 17.990619659423828 2.010620594024658\n",
            "24980 1.8397691974314512e-06 3.679048313642852e-05 Traning Loss: 3.8630252674920484e-05 17.99070930480957 2.01055908203125\n",
            "24981 1.6471016124341986e-06 4.05338651034981e-05 Traning Loss: 4.218096728436649e-05 17.99080467224121 2.0105299949645996\n",
            "24982 2.8878048397018574e-06 4.488107515498996e-05 Traning Loss: 4.776888090418652e-05 17.990888595581055 2.0104615688323975\n",
            "24983 2.9573304800578626e-06 5.374106694944203e-05 Traning Loss: 5.669839811162092e-05 17.99098014831543 2.0104434490203857\n",
            "24984 5.444955149869202e-06 6.543911149492487e-05 Traning Loss: 7.088406709954143e-05 17.99105453491211 2.0103631019592285\n",
            "24985 6.489229690487264e-06 8.738620090298355e-05 Traning Loss: 9.387543104821816e-05 17.99114227294922 2.010362148284912\n",
            "24986 1.1882486433023587e-05 0.00011876667122123763 Traning Loss: 0.00013064916129224002 17.991201400756836 2.0102622509002686\n",
            "24987 1.599960887688212e-05 0.0001749555376591161 Traning Loss: 0.00019095515017397702 17.991281509399414 2.0102896690368652\n",
            "24988 2.8431684768293053e-05 0.00025876934523694217 Traning Loss: 0.0002872010227292776 17.99131202697754 2.010157823562622\n",
            "24989 4.1411152778891847e-05 0.0004040993226226419 Traning Loss: 0.00044551046448759735 17.991369247436523 2.010230779647827\n",
            "24990 7.069115235935897e-05 0.0006212903535924852 Traning Loss: 0.0006919815205037594 17.991336822509766 2.0100486278533936\n",
            "24991 0.00010635943908710033 0.0009819682454690337 Traning Loss: 0.0010883277282118797 17.991329193115234 2.010193109512329\n",
            "24992 0.0001705556205706671 0.0014852843014523387 Traning Loss: 0.0016558399656787515 17.991159439086914 2.009941816329956\n",
            "24993 0.00024852625210769475 0.0022346540354192257 Traning Loss: 0.00248318025842309 17.99099349975586 2.010183572769165\n",
            "24994 0.0003503518528304994 0.003045442281290889 Traning Loss: 0.003395794192329049 17.990550994873047 2.009868621826172\n",
            "24995 0.0004369546368252486 0.0038786819204688072 Traning Loss: 0.004315636586397886 17.990081787109375 2.010194778442383\n",
            "24996 0.00046515557914972305 0.004022482316941023 Traning Loss: 0.004487637896090746 17.989286422729492 2.0099008083343506\n",
            "24997 0.0003855872491840273 0.0034155906178057194 Traning Loss: 0.003801177954301238 17.988529205322266 2.0102005004882812\n",
            "24998 0.00022829578665550798 0.0019206881988793612 Traning Loss: 0.0021489840000867844 17.98762321472168 2.0100839138031006\n",
            "24999 6.3312218117062e-05 0.0005906966398470104 Traning Loss: 0.0006540088797919452 17.98688507080078 2.010199785232544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0 =  0\n",
        "tf =  1\n",
        "N  =  10000\n",
        "test_time_pinn2 = torch.linspace(t0,tf,N).view(-1,1)\n",
        "y_test_pinn2    = oscilador(d,w0,test_time_pinn2)   #Esta es la solución teórica\n",
        "test_time_eval_pinn2 = test_time_pinn2.requires_grad_(True)\n",
        "y_eval_pinn2 = pinn(test_time_eval_pinn2) #Evaluación de la red"
      ],
      "metadata": {
        "id": "4a11SnuVuEXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivadas\n",
        "der_teo_pinn2 = der_oscilador(d,w0,test_time_pinn2.detach().numpy())\n",
        "der_eval_pinn2 = torch.autograd.grad(y_eval_pinn2, test_time_pinn2, torch.ones_like(y_eval_pinn2), create_graph=True)[0]\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "ax1 = plt.subplot(131)\n",
        "ax1.plot(test_time_pinn2.detach().numpy(),y_test_pinn2.detach().numpy())\n",
        "ax1.plot(test_time_eval_pinn2.detach().numpy(),y_eval_pinn2.detach().numpy())\n",
        "ax1.set_xlabel(r'$t$')\n",
        "ax1.set_ylabel(r'$x(t)$')\n",
        "\n",
        "ax2 = plt.subplot(132)\n",
        "ax2.plot(test_time_pinn2.detach().numpy(),der_teo_pinn2)\n",
        "ax2.plot(test_time_eval_pinn2.detach().numpy(),der_eval_pinn2.detach().numpy())\n",
        "ax2.set_xlabel(r'$t$')\n",
        "ax2.set_ylabel(r'$\\dot x(t)$')\n",
        "\n",
        "ax3 = plt.subplot(133)\n",
        "ax3.plot(y_test_pinn2.detach().numpy(),der_teo_pinn2)\n",
        "ax3.plot(y_eval_pinn2.detach().numpy(),der_eval_pinn2.detach().numpy())\n",
        "ax3.set_xlabel(r'$x$')\n",
        "ax3.set_ylabel(r'$\\dot x$')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "qT-uuUkauFGL",
        "outputId": "69aa1073-5077-48de-b9c8-e0ff14af3d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '$\\\\dot x$')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voy a escribir un integrador para la ecuación de Ginzburg Landau\n",
        "\n",
        "\n",
        "$$\\partial_t A = \\mu A - V_g \\partial_x A + \\xi^2(1 + i c_1) \\partial^2_x A - \\ell_r^2(1 + i c_2)  A |A|^2 $$"
      ],
      "metadata": {
        "id": "a9asl8PBe3TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evol(ru, iu, k, params, dt):\n",
        "    # Evoluciona en el tiempo la PDE usando el método pseudoespectral y Runge-Kutta de segundo orden\n",
        "    rf   = np.fft.rfft(ru)\n",
        "    imf   = np.fft.rfft(iu)\n",
        "    rft  = rf\n",
        "    ift  = imf\n",
        "    for ord in [2,1]:       # Bucle para el Runge-Kutta\n",
        "        D   = dt/ord\n",
        "        rfnl, ifnl = compute_abs(rf, imf)\n",
        "        rfxx = -(k**2)*rf\n",
        "        ifxx = -(k**2)*imf\n",
        "        rfx = 1j*k*rf\n",
        "        ifx = 1j*k*imf\n",
        "        rf   =  rft + D*( params[0] * rf  - params[1] * rfx + params[2] * (rfxx - params[3] * ifxx) - params[4] * (rfnl - params[5]*rfnl))\n",
        "        imf  =  ift + D*( params[0] * imf - params[1] * ifx + params[2] * (ifxx + params[3] * rfxx) - params[4] * (ifnl + params[5]*ifnl))\n",
        "        rf[int(N/3):]  = 0    # Dealiasing\n",
        "        imf[int(N/3):] = 0    # Dealiasing\n",
        "    rout = np.fft.irfft(rf)\n",
        "    iout = np.fft.irfft(imf)\n",
        "    return rout, iout\n",
        "\n",
        "\n",
        "def compute_abs(rf, imf):\n",
        "  ru    = np.fft.irfft(rf)\n",
        "  iu    = np.fft.irfft(imf)\n",
        "  usq  = ru**2 + iu**2\n",
        "  fsq  = np.fft.rfft(usq)\n",
        "  fsq[int(N/3):] = 0\n",
        "  usq = np.fft.irfft(fsq)\n",
        "  rout = usq * ru\n",
        "  iout = usq * iu\n",
        "  rout = np.fft.rfft(rout)\n",
        "  iout = np.fft.rfft(iout)\n",
        "  return rout, iout"
      ],
      "metadata": {
        "id": "1Fr_FQqqfSbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 256\n",
        "dt = 1e-5\n",
        "steps = 0.3//dt\n",
        "x = np.linspace(0,2*np.pi,N,endpoint=False) # Coordenada espacial en [0,2*pi)\n",
        "k = np.arange(0,N/2+1)                      # Números de onda ordenados como en la FFT\n",
        "mu = 0.5\n",
        "vg = 8\n",
        "xisq = 0.8\n",
        "c1 = 2\n",
        "lrsq = 0.8\n",
        "c2 = 1\n",
        "params =  mu, vg, xisq, c1, lrsq, c2"
      ],
      "metadata": {
        "id": "1tMW-lMKgic5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rout = np.zeros((N,int(steps)))\n",
        "iout = np.zeros((N,int(steps)))\n",
        "rout[:,0] = np.real(np.exp(1j * 4 * x))*0.1\n",
        "iout[:,0] = np.imag(np.exp(1j * 2 * x))*0.1\n",
        "\n",
        "for i in range(int(steps)-1): # Evolución temporal\n",
        "    rout[:,i+1], iout[:,i+1] = evol(rout[:,i], iout[:,i], k, params, dt)     #Completa para integrar (mira el bloque anterior)"
      ],
      "metadata": {
        "id": "QMYqbyTKgsoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(rout**2 + iout**2, aspect = 'auto')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "BYaR7VQcn8P4",
        "outputId": "3b53a3be-2176-4c9e-8d44-5772cd1aa56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7bd97ea35d50>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interval = 40\n",
        "testr = torch.tensor(rout[:,::interval]).T\n",
        "testi = torch.tensor(iout[:,::interval]).T\n",
        "data_y = torch.cat((testr[:,:,None], testi[:,:,None]), dim=-1)\n",
        "data_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "573G9-dgjn6W",
        "outputId": "6d95277f-8161-40ea-c179-8554f107bad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([750, 256, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_data = dt*torch.arange(data_y.shape[0]).view(-1)\n",
        "x_data  = torch.linspace(0,2*torch.pi,N)\n",
        "x_grid, t_grid = torch.meshgrid(x_data, t_data, indexing='ij')\n",
        "x_grid = x_grid.T[:,:,None].requires_grad_(True) #Agregamos una dimensión al final para que pueda ser input de la red\n",
        "t_grid = t_grid.T[:,:,None].requires_grad_(True) #Agregamos una dimensión al final para que pueda ser input de la red\n",
        "input_data = torch.cat((x_grid, t_grid), dim=-1)\n",
        "input_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4O10x4EkhDl",
        "outputId": "33d50a49-b6f7-44ac-acca-7986bf67eba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([750, 256, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "dataset = TensorDataset(input_data, data_y)"
      ],
      "metadata": {
        "id": "wlQlIG1sl3vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "w1nRoqztmFcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Multilayer perceptron (MLP) // Perceptríon Multicapa .\n",
        "\n",
        "    Esta clase define una red neuronal feedforward con múltiples capas ocultas\n",
        "    lineales, funciones de activación tangente hiperbólica en  las capas ocultas\n",
        "    y una salida lineal.\n",
        "\n",
        "    Args:\n",
        "        sizes (lista): Lista de enteros que especifica el número de neuronas en\n",
        "        cada capa. El primer elemento debe coincidir con la dimensión de entrada\n",
        "        y el último con la dimensión de salida.\n",
        "\n",
        "    Atributos:\n",
        "        capas (torch.nn.ModuleList): Lista que contiene las capas lineales del MLP.\n",
        "\n",
        "    Métodos:\n",
        "        forward(x): Realiza una pasada hacia adelante a través de la red MLP.\n",
        "\n",
        "    Ejemplo:\n",
        "        tamaños = [entrada_dim, oculta1_dim, oculta2_dim, salida_dim]\n",
        "        mlp = MLP(tamaños)\n",
        "        tensor_entrada = torch.tensor([...])\n",
        "        salida = mlp(tensor_entrada)\n",
        "    \"\"\"\n",
        "    def __init__(self,sizes):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        self.p1  = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
        "        self.p2  = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
        "        self.p3  = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
        "        self.p4  = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
        "        self.p5  = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
        "        self.p6  = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
        "        self.a   = torch.nn.Parameter(data=torch.rand(1), requires_grad=True) #for improved convergence\n",
        "        for i in range(len(sizes)-1):\n",
        "            self.layers.append(torch.nn.Linear(sizes[i],sizes[i+1]))\n",
        "    def forward(self,x):\n",
        "        h = x\n",
        "        for hidden in self.layers[:-1]:\n",
        "            h = torch.tanh(hidden(h))\n",
        "        output = self.layers[-1]\n",
        "        y = output(h)\n",
        "        return y"
      ],
      "metadata": {
        "id": "YztbqOfUvIzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating the data\n",
        "interval = 40\n",
        "testr = torch.tensor(rout[:,::interval]).T\n",
        "testi = torch.tensor(iout[:,::interval]).T\n",
        "data_y = torch.cat((testr[:,:,None], testi[:,:,None]), dim=-1)\n",
        "\n",
        "#t_data = dt*torch.arange(data_y.shape[0]).view(-1)\n",
        "#x_data  = torch.linspace(0,2*torch.pi,N)\n",
        "t_data = torch.linspace(-1,1,data_y.shape[0]).view(-1)\n",
        "x_data  = torch.linspace(-1,1,N)\n",
        "x_grid, t_grid = torch.meshgrid(x_data, t_data, indexing='ij')\n",
        "x_grid = x_grid.T[:,:,None].requires_grad_(True) #Agregamos una dimensión al final para que pueda ser input de la red\n",
        "t_grid = t_grid.T[:,:,None].requires_grad_(True) #Agregamos una dimensión al final para que pueda ser input de la red\n",
        "input_data = torch.cat((x_grid, t_grid), dim=-1)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "dataset = TensorDataset(input_data.to(torch.float32), data_y.to(torch.float32))\n",
        "\n",
        "batch_size = 100\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "K0JUWZfaTbt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the network\n",
        "pinn = MLP([2] + [128]*6 +[2])\n",
        "optimizer = torch.optim.Adam(pinn.parameters(),lr=1e-4)\n",
        "lossfun = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "l6dF4vZ9xruM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loop para el caso SIN FISICA\n",
        "iterations = 15000 #Numero de iteraciones\n",
        "l = 0.5   #Peso relativo lambda\n",
        "\n",
        "for epoch in range(iterations):\n",
        "    for input, target in dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      #Asimilación de los datos\n",
        "      yh    = pinn(input)\n",
        "      loss = lossfun(target, yh)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    with torch.autograd.no_grad():\n",
        "      print(epoch,\"\\tTraning Loss:\",float(loss.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6TGzIqLrWYol",
        "outputId": "fe0e5845-810a-4cf7-af79-7ce91d497750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 \tTraning Loss: 0.0014825188554823399\n",
            "1 \tTraning Loss: 0.001454775920137763\n",
            "2 \tTraning Loss: 0.0015811293851584196\n",
            "3 \tTraning Loss: 0.0017959479009732604\n",
            "4 \tTraning Loss: 0.001356984954327345\n",
            "5 \tTraning Loss: 0.0015650719869881868\n",
            "6 \tTraning Loss: 0.0015811786288395524\n",
            "7 \tTraning Loss: 0.0015481050359085202\n",
            "8 \tTraning Loss: 0.001527340617030859\n",
            "9 \tTraning Loss: 0.001640957547351718\n",
            "10 \tTraning Loss: 0.0017208292847499251\n",
            "11 \tTraning Loss: 0.0014912693295627832\n",
            "12 \tTraning Loss: 0.001593016437254846\n",
            "13 \tTraning Loss: 0.0016052876599133015\n",
            "14 \tTraning Loss: 0.0014339620247483253\n",
            "15 \tTraning Loss: 0.0014060410903766751\n",
            "16 \tTraning Loss: 0.0016980513464659452\n",
            "17 \tTraning Loss: 0.0018540907185524702\n",
            "18 \tTraning Loss: 0.001574179856106639\n",
            "19 \tTraning Loss: 0.0017983268480747938\n",
            "20 \tTraning Loss: 0.0015392906498163939\n",
            "21 \tTraning Loss: 0.001531289773993194\n",
            "22 \tTraning Loss: 0.001502437749877572\n",
            "23 \tTraning Loss: 0.001567191444337368\n",
            "24 \tTraning Loss: 0.0015377316158264875\n",
            "25 \tTraning Loss: 0.0014463274274021387\n",
            "26 \tTraning Loss: 0.0015188648831099272\n",
            "27 \tTraning Loss: 0.0013878695899620652\n",
            "28 \tTraning Loss: 0.001484711654484272\n",
            "29 \tTraning Loss: 0.0015376320807263255\n",
            "30 \tTraning Loss: 0.0014575659297406673\n",
            "31 \tTraning Loss: 0.0014361059293150902\n",
            "32 \tTraning Loss: 0.001542380778118968\n",
            "33 \tTraning Loss: 0.0012800247641280293\n",
            "34 \tTraning Loss: 0.0013664929429069161\n",
            "35 \tTraning Loss: 0.0017102533020079136\n",
            "36 \tTraning Loss: 0.001417953404597938\n",
            "37 \tTraning Loss: 0.0014362887013703585\n",
            "38 \tTraning Loss: 0.0013695009984076023\n",
            "39 \tTraning Loss: 0.0014745410298928618\n",
            "40 \tTraning Loss: 0.00175347866024822\n",
            "41 \tTraning Loss: 0.0012646771501749754\n",
            "42 \tTraning Loss: 0.0016236633528023958\n",
            "43 \tTraning Loss: 0.0017020920058712363\n",
            "44 \tTraning Loss: 0.001547732506878674\n",
            "45 \tTraning Loss: 0.0016388854710385203\n",
            "46 \tTraning Loss: 0.0015210085548460484\n",
            "47 \tTraning Loss: 0.0012078548315912485\n",
            "48 \tTraning Loss: 0.0014291689731180668\n",
            "49 \tTraning Loss: 0.0015229410491883755\n",
            "50 \tTraning Loss: 0.0015009933849796653\n",
            "51 \tTraning Loss: 0.001594222616404295\n",
            "52 \tTraning Loss: 0.0015305763809010386\n",
            "53 \tTraning Loss: 0.0014863457763567567\n",
            "54 \tTraning Loss: 0.0014897072687745094\n",
            "55 \tTraning Loss: 0.0013807588256895542\n",
            "56 \tTraning Loss: 0.0013099330244585872\n",
            "57 \tTraning Loss: 0.0014895781641826034\n",
            "58 \tTraning Loss: 0.0016987966373562813\n",
            "59 \tTraning Loss: 0.0015337142394855618\n",
            "60 \tTraning Loss: 0.0013601259561255574\n",
            "61 \tTraning Loss: 0.001691412297077477\n",
            "62 \tTraning Loss: 0.0013955597532913089\n",
            "63 \tTraning Loss: 0.0013276864774525166\n",
            "64 \tTraning Loss: 0.0013552190503105521\n",
            "65 \tTraning Loss: 0.0015150712570175529\n",
            "66 \tTraning Loss: 0.0012432222720235586\n",
            "67 \tTraning Loss: 0.0013998934300616384\n",
            "68 \tTraning Loss: 0.0014461248647421598\n",
            "69 \tTraning Loss: 0.0013977432390674949\n",
            "70 \tTraning Loss: 0.0014601722359657288\n",
            "71 \tTraning Loss: 0.0014941521221771836\n",
            "72 \tTraning Loss: 0.0012602205388247967\n",
            "73 \tTraning Loss: 0.0012041715672239661\n",
            "74 \tTraning Loss: 0.0012846541358157992\n",
            "75 \tTraning Loss: 0.001389742479659617\n",
            "76 \tTraning Loss: 0.0012768523301929235\n",
            "77 \tTraning Loss: 0.0012930312659591436\n",
            "78 \tTraning Loss: 0.0012226314283907413\n",
            "79 \tTraning Loss: 0.0012217100011184812\n",
            "80 \tTraning Loss: 0.0011844886466860771\n",
            "81 \tTraning Loss: 0.0012502488680183887\n",
            "82 \tTraning Loss: 0.0012270339066162705\n",
            "83 \tTraning Loss: 0.001244837767444551\n",
            "84 \tTraning Loss: 0.001115169725380838\n",
            "85 \tTraning Loss: 0.0010625888826325536\n",
            "86 \tTraning Loss: 0.001076035900041461\n",
            "87 \tTraning Loss: 0.0011599864810705185\n",
            "88 \tTraning Loss: 0.0012605178635567427\n",
            "89 \tTraning Loss: 0.001151706324890256\n",
            "90 \tTraning Loss: 0.0012848239857703447\n",
            "91 \tTraning Loss: 0.0010939358035102487\n",
            "92 \tTraning Loss: 0.000951404043007642\n",
            "93 \tTraning Loss: 0.0009689608123153448\n",
            "94 \tTraning Loss: 0.0010789548978209496\n",
            "95 \tTraning Loss: 0.0010090003488585353\n",
            "96 \tTraning Loss: 0.0009693796746432781\n",
            "97 \tTraning Loss: 0.0009671833831816912\n",
            "98 \tTraning Loss: 0.0009216301259584725\n",
            "99 \tTraning Loss: 0.0009094623383134604\n",
            "100 \tTraning Loss: 0.0012143072672188282\n",
            "101 \tTraning Loss: 0.0011215650010854006\n",
            "102 \tTraning Loss: 0.0010777596617117524\n",
            "103 \tTraning Loss: 0.0010651249904185534\n",
            "104 \tTraning Loss: 0.0009055174887180328\n",
            "105 \tTraning Loss: 0.0010451917769387364\n",
            "106 \tTraning Loss: 0.0009292691829614341\n",
            "107 \tTraning Loss: 0.0008059974061325192\n",
            "108 \tTraning Loss: 0.0008327631512656808\n",
            "109 \tTraning Loss: 0.0006989003159105778\n",
            "110 \tTraning Loss: 0.0007892409921623766\n",
            "111 \tTraning Loss: 0.0006101806648075581\n",
            "112 \tTraning Loss: 0.0008816667832434177\n",
            "113 \tTraning Loss: 0.0009340816177427769\n",
            "114 \tTraning Loss: 0.0008103630971163511\n",
            "115 \tTraning Loss: 0.0008352214936167002\n",
            "116 \tTraning Loss: 0.0007680868147872388\n",
            "117 \tTraning Loss: 0.0009768771706148982\n",
            "118 \tTraning Loss: 0.0011178143322467804\n",
            "119 \tTraning Loss: 0.0007205310394056141\n",
            "120 \tTraning Loss: 0.0009758610976859927\n",
            "121 \tTraning Loss: 0.0009126142249442637\n",
            "122 \tTraning Loss: 0.0007567727006971836\n",
            "123 \tTraning Loss: 0.0010223945137113333\n",
            "124 \tTraning Loss: 0.0008336617611348629\n",
            "125 \tTraning Loss: 0.0007166692521423101\n",
            "126 \tTraning Loss: 0.0008638837025500834\n",
            "127 \tTraning Loss: 0.0006696948548778892\n",
            "128 \tTraning Loss: 0.0008480300894007087\n",
            "129 \tTraning Loss: 0.0008866393472999334\n",
            "130 \tTraning Loss: 0.0007268933113664389\n",
            "131 \tTraning Loss: 0.0006303722620941699\n",
            "132 \tTraning Loss: 0.0006856208783574402\n",
            "133 \tTraning Loss: 0.000963216123636812\n",
            "134 \tTraning Loss: 0.0009956893045455217\n",
            "135 \tTraning Loss: 0.0007154697086662054\n",
            "136 \tTraning Loss: 0.0007028726977296174\n",
            "137 \tTraning Loss: 0.0007901200442574918\n",
            "138 \tTraning Loss: 0.0009166786912828684\n",
            "139 \tTraning Loss: 0.0009465379407629371\n",
            "140 \tTraning Loss: 0.0010025566443800926\n",
            "141 \tTraning Loss: 0.0007414704305119812\n",
            "142 \tTraning Loss: 0.0008863179245963693\n",
            "143 \tTraning Loss: 0.0006442791200242937\n",
            "144 \tTraning Loss: 0.0005416299100033939\n",
            "145 \tTraning Loss: 0.0006952511030249298\n",
            "146 \tTraning Loss: 0.0006645372486673295\n",
            "147 \tTraning Loss: 0.0006462727324105799\n",
            "148 \tTraning Loss: 0.0007178158848546445\n",
            "149 \tTraning Loss: 0.0008966867462731898\n",
            "150 \tTraning Loss: 0.0006091208197176456\n",
            "151 \tTraning Loss: 0.0008582150330767035\n",
            "152 \tTraning Loss: 0.0009052269742824137\n",
            "153 \tTraning Loss: 0.0007548164576292038\n",
            "154 \tTraning Loss: 0.0009559419704601169\n",
            "155 \tTraning Loss: 0.0005773771554231644\n",
            "156 \tTraning Loss: 0.0008310480625368655\n",
            "157 \tTraning Loss: 0.0007481907377950847\n",
            "158 \tTraning Loss: 0.0006689403671771288\n",
            "159 \tTraning Loss: 0.000665666360873729\n",
            "160 \tTraning Loss: 0.0007769088260829449\n",
            "161 \tTraning Loss: 0.0008123577572405338\n",
            "162 \tTraning Loss: 0.0005721041816286743\n",
            "163 \tTraning Loss: 0.0007523234817199409\n",
            "164 \tTraning Loss: 0.0009979824535548687\n",
            "165 \tTraning Loss: 0.000837876636069268\n",
            "166 \tTraning Loss: 0.0010242256103083491\n",
            "167 \tTraning Loss: 0.0009453555685468018\n",
            "168 \tTraning Loss: 0.0007056122994981706\n",
            "169 \tTraning Loss: 0.0006424702587537467\n",
            "170 \tTraning Loss: 0.0008823539246805012\n",
            "171 \tTraning Loss: 0.0009380115079693496\n",
            "172 \tTraning Loss: 0.0009602762875147164\n",
            "173 \tTraning Loss: 0.0007688205223530531\n",
            "174 \tTraning Loss: 0.0009105545468628407\n",
            "175 \tTraning Loss: 0.0007817478617653251\n",
            "176 \tTraning Loss: 0.0011238817824050784\n",
            "177 \tTraning Loss: 0.0007229606853798032\n",
            "178 \tTraning Loss: 0.0010076228063553572\n",
            "179 \tTraning Loss: 0.0009496256243437529\n",
            "180 \tTraning Loss: 0.0007513891323469579\n",
            "181 \tTraning Loss: 0.0011745181400328875\n",
            "182 \tTraning Loss: 0.0007734085083939135\n",
            "183 \tTraning Loss: 0.0007464169175364077\n",
            "184 \tTraning Loss: 0.0008766975370235741\n",
            "185 \tTraning Loss: 0.0006624084780924022\n",
            "186 \tTraning Loss: 0.0006409178022295237\n",
            "187 \tTraning Loss: 0.0007509537972509861\n",
            "188 \tTraning Loss: 0.0009795264340937138\n",
            "189 \tTraning Loss: 0.0007596035138703883\n",
            "190 \tTraning Loss: 0.0004965696134604514\n",
            "191 \tTraning Loss: 0.0006890797521919012\n",
            "192 \tTraning Loss: 0.0008665561908856034\n",
            "193 \tTraning Loss: 0.0007000031182542443\n",
            "194 \tTraning Loss: 0.000731124309822917\n",
            "195 \tTraning Loss: 0.0009057862916961312\n",
            "196 \tTraning Loss: 0.0006648160051554441\n",
            "197 \tTraning Loss: 0.0006119587342254817\n",
            "198 \tTraning Loss: 0.0007517316844314337\n",
            "199 \tTraning Loss: 0.001003764569759369\n",
            "200 \tTraning Loss: 0.0005767649272456765\n",
            "201 \tTraning Loss: 0.000882927153725177\n",
            "202 \tTraning Loss: 0.0004905576352030039\n",
            "203 \tTraning Loss: 0.0005941177369095385\n",
            "204 \tTraning Loss: 0.0007336363196372986\n",
            "205 \tTraning Loss: 0.0006680682417936623\n",
            "206 \tTraning Loss: 0.0007612761110067368\n",
            "207 \tTraning Loss: 0.0007328338688239455\n",
            "208 \tTraning Loss: 0.0005576725816354156\n",
            "209 \tTraning Loss: 0.0006602324428968132\n",
            "210 \tTraning Loss: 0.000957486336119473\n",
            "211 \tTraning Loss: 0.0006953603005968034\n",
            "212 \tTraning Loss: 0.0009302414255216718\n",
            "213 \tTraning Loss: 0.0007217935635708272\n",
            "214 \tTraning Loss: 0.0005371072329580784\n",
            "215 \tTraning Loss: 0.0006600404740311205\n",
            "216 \tTraning Loss: 0.0007487331167794764\n",
            "217 \tTraning Loss: 0.0007713357335887849\n",
            "218 \tTraning Loss: 0.0006170555716380477\n",
            "219 \tTraning Loss: 0.0006197064649313688\n",
            "220 \tTraning Loss: 0.0006817951216362417\n",
            "221 \tTraning Loss: 0.0008036207291297615\n",
            "222 \tTraning Loss: 0.0007779950392432511\n",
            "223 \tTraning Loss: 0.0008602551533840597\n",
            "224 \tTraning Loss: 0.0007397513254545629\n",
            "225 \tTraning Loss: 0.0006369729526340961\n",
            "226 \tTraning Loss: 0.0008325553499162197\n",
            "227 \tTraning Loss: 0.0007165403803810477\n",
            "228 \tTraning Loss: 0.0006606044480577111\n",
            "229 \tTraning Loss: 0.00046165671665221453\n",
            "230 \tTraning Loss: 0.0006289381999522448\n",
            "231 \tTraning Loss: 0.0006860417779535055\n",
            "232 \tTraning Loss: 0.0007108254940249026\n",
            "233 \tTraning Loss: 0.0005959935369901359\n",
            "234 \tTraning Loss: 0.0008957066456787288\n",
            "235 \tTraning Loss: 0.0007669833139516413\n",
            "236 \tTraning Loss: 0.0005956481909379363\n",
            "237 \tTraning Loss: 0.0006427133921533823\n",
            "238 \tTraning Loss: 0.0007089396822266281\n",
            "239 \tTraning Loss: 0.0008623929461464286\n",
            "240 \tTraning Loss: 0.0008867077995091677\n",
            "241 \tTraning Loss: 0.0007047776598483324\n",
            "242 \tTraning Loss: 0.0008941013365983963\n",
            "243 \tTraning Loss: 0.000761932460591197\n",
            "244 \tTraning Loss: 0.0006517778965644538\n",
            "245 \tTraning Loss: 0.0005590319633483887\n",
            "246 \tTraning Loss: 0.0006719162920489907\n",
            "247 \tTraning Loss: 0.0006583628128282726\n",
            "248 \tTraning Loss: 0.0007243140717037022\n",
            "249 \tTraning Loss: 0.0006716724601574242\n",
            "250 \tTraning Loss: 0.0006326017319224775\n",
            "251 \tTraning Loss: 0.0007298382115550339\n",
            "252 \tTraning Loss: 0.000846642127726227\n",
            "253 \tTraning Loss: 0.0008105307351797819\n",
            "254 \tTraning Loss: 0.0006625352543778718\n",
            "255 \tTraning Loss: 0.0006395154632627964\n",
            "256 \tTraning Loss: 0.0005112451035529375\n",
            "257 \tTraning Loss: 0.0006281724781729281\n",
            "258 \tTraning Loss: 0.0007145896088331938\n",
            "259 \tTraning Loss: 0.0007945418474264443\n",
            "260 \tTraning Loss: 0.0006711123860441148\n",
            "261 \tTraning Loss: 0.0006937454454600811\n",
            "262 \tTraning Loss: 0.0007574866176582873\n",
            "263 \tTraning Loss: 0.0006389460759237409\n",
            "264 \tTraning Loss: 0.0005020945682190359\n",
            "265 \tTraning Loss: 0.0007827543304301798\n",
            "266 \tTraning Loss: 0.000700337637681514\n",
            "267 \tTraning Loss: 0.0005674069398082793\n",
            "268 \tTraning Loss: 0.0007253903895616531\n",
            "269 \tTraning Loss: 0.0005315776215866208\n",
            "270 \tTraning Loss: 0.0008113386575132608\n",
            "271 \tTraning Loss: 0.0006006098701618612\n",
            "272 \tTraning Loss: 0.0006422966835089028\n",
            "273 \tTraning Loss: 0.0008188438951037824\n",
            "274 \tTraning Loss: 0.0006343051791191101\n",
            "275 \tTraning Loss: 0.0007204890716820955\n",
            "276 \tTraning Loss: 0.0006412623915821314\n",
            "277 \tTraning Loss: 0.0007012577261775732\n",
            "278 \tTraning Loss: 0.0004632374329958111\n",
            "279 \tTraning Loss: 0.000613409501966089\n",
            "280 \tTraning Loss: 0.0005992066580802202\n",
            "281 \tTraning Loss: 0.0005639747250825167\n",
            "282 \tTraning Loss: 0.0007650463376194239\n",
            "283 \tTraning Loss: 0.0005689799436368048\n",
            "284 \tTraning Loss: 0.0006316197104752064\n",
            "285 \tTraning Loss: 0.0007707828772254288\n",
            "286 \tTraning Loss: 0.0005606897175312042\n",
            "287 \tTraning Loss: 0.0007426806259900331\n",
            "288 \tTraning Loss: 0.000717151677235961\n",
            "289 \tTraning Loss: 0.0008362227817997336\n",
            "290 \tTraning Loss: 0.0006088282680138946\n",
            "291 \tTraning Loss: 0.0008306322270072997\n",
            "292 \tTraning Loss: 0.0006542736082337797\n",
            "293 \tTraning Loss: 0.0007806955254636705\n",
            "294 \tTraning Loss: 0.0007213786011561751\n",
            "295 \tTraning Loss: 0.0005390289588831365\n",
            "296 \tTraning Loss: 0.0005748025723733008\n",
            "297 \tTraning Loss: 0.0005269380053505301\n",
            "298 \tTraning Loss: 0.0009543196065351367\n",
            "299 \tTraning Loss: 0.0005441402900032699\n",
            "300 \tTraning Loss: 0.000680335913784802\n",
            "301 \tTraning Loss: 0.0005599947180598974\n",
            "302 \tTraning Loss: 0.0007076271576806903\n",
            "303 \tTraning Loss: 0.0007005580700933933\n",
            "304 \tTraning Loss: 0.0006877160049043596\n",
            "305 \tTraning Loss: 0.0006399042904376984\n",
            "306 \tTraning Loss: 0.0006717074429616332\n",
            "307 \tTraning Loss: 0.00040716692456044257\n",
            "308 \tTraning Loss: 0.0005773451994173229\n",
            "309 \tTraning Loss: 0.0007281893631443381\n",
            "310 \tTraning Loss: 0.0006049397052265704\n",
            "311 \tTraning Loss: 0.0007175734499469399\n",
            "312 \tTraning Loss: 0.0007899600896053016\n",
            "313 \tTraning Loss: 0.0007077240152284503\n",
            "314 \tTraning Loss: 0.0006331719923764467\n",
            "315 \tTraning Loss: 0.0005673941341228783\n",
            "316 \tTraning Loss: 0.00041310000233352184\n",
            "317 \tTraning Loss: 0.0005222384352236986\n",
            "318 \tTraning Loss: 0.0005773398443125188\n",
            "319 \tTraning Loss: 0.0005161594017408788\n",
            "320 \tTraning Loss: 0.0006248998106457293\n",
            "321 \tTraning Loss: 0.0006410434143617749\n",
            "322 \tTraning Loss: 0.0005691283731721342\n",
            "323 \tTraning Loss: 0.0005930690676905215\n",
            "324 \tTraning Loss: 0.0007391138351522386\n",
            "325 \tTraning Loss: 0.0006304314592853189\n",
            "326 \tTraning Loss: 0.0005594731192104518\n",
            "327 \tTraning Loss: 0.0004851508419960737\n",
            "328 \tTraning Loss: 0.0005514368531294167\n",
            "329 \tTraning Loss: 0.0007086652331054211\n",
            "330 \tTraning Loss: 0.0004933832096867263\n",
            "331 \tTraning Loss: 0.00037005721242167056\n",
            "332 \tTraning Loss: 0.0004921791260130703\n",
            "333 \tTraning Loss: 0.0007649703184142709\n",
            "334 \tTraning Loss: 0.0006547786761075258\n",
            "335 \tTraning Loss: 0.0007137936190702021\n",
            "336 \tTraning Loss: 0.0004653564828913659\n",
            "337 \tTraning Loss: 0.000629013346042484\n",
            "338 \tTraning Loss: 0.0007975309854373336\n",
            "339 \tTraning Loss: 0.0006242934614419937\n",
            "340 \tTraning Loss: 0.0006584530929103494\n",
            "341 \tTraning Loss: 0.0006361760315485299\n",
            "342 \tTraning Loss: 0.0006314135389402509\n",
            "343 \tTraning Loss: 0.0007245655870065093\n",
            "344 \tTraning Loss: 0.0006302015390247107\n",
            "345 \tTraning Loss: 0.0005145787727087736\n",
            "346 \tTraning Loss: 0.0006803667638450861\n",
            "347 \tTraning Loss: 0.0005191233940422535\n",
            "348 \tTraning Loss: 0.0004683020815718919\n",
            "349 \tTraning Loss: 0.0006290519377216697\n",
            "350 \tTraning Loss: 0.0006316471844911575\n",
            "351 \tTraning Loss: 0.0007199468673206866\n",
            "352 \tTraning Loss: 0.0005105922464281321\n",
            "353 \tTraning Loss: 0.0005283704958856106\n",
            "354 \tTraning Loss: 0.0007299333228729665\n",
            "355 \tTraning Loss: 0.0005924317520111799\n",
            "356 \tTraning Loss: 0.0004441516357474029\n",
            "357 \tTraning Loss: 0.000495537300594151\n",
            "358 \tTraning Loss: 0.0006104449275881052\n",
            "359 \tTraning Loss: 0.0006316365906968713\n",
            "360 \tTraning Loss: 0.0004864684888161719\n",
            "361 \tTraning Loss: 0.0007006492232903838\n",
            "362 \tTraning Loss: 0.000567315611988306\n",
            "363 \tTraning Loss: 0.000525155570358038\n",
            "364 \tTraning Loss: 0.0005663097254000604\n",
            "365 \tTraning Loss: 0.0006368345930241048\n",
            "366 \tTraning Loss: 0.0006426807958632708\n",
            "367 \tTraning Loss: 0.0004886527894996107\n",
            "368 \tTraning Loss: 0.0006509335362352431\n",
            "369 \tTraning Loss: 0.00042249556281603873\n",
            "370 \tTraning Loss: 0.0007213490316644311\n",
            "371 \tTraning Loss: 0.0005894232308492064\n",
            "372 \tTraning Loss: 0.0008537984103895724\n",
            "373 \tTraning Loss: 0.000690131273586303\n",
            "374 \tTraning Loss: 0.000616197707131505\n",
            "375 \tTraning Loss: 0.0004521841474343091\n",
            "376 \tTraning Loss: 0.0005527769098989666\n",
            "377 \tTraning Loss: 0.0005254707066342235\n",
            "378 \tTraning Loss: 0.00045564037282019854\n",
            "379 \tTraning Loss: 0.0007014024304226041\n",
            "380 \tTraning Loss: 0.0006831986247561872\n",
            "381 \tTraning Loss: 0.0005418010405264795\n",
            "382 \tTraning Loss: 0.0005147180054336786\n",
            "383 \tTraning Loss: 0.0006048145005479455\n",
            "384 \tTraning Loss: 0.0005516968085430562\n",
            "385 \tTraning Loss: 0.0006159674376249313\n",
            "386 \tTraning Loss: 0.0006489173392765224\n",
            "387 \tTraning Loss: 0.0006206269026733935\n",
            "388 \tTraning Loss: 0.00045003564446233213\n",
            "389 \tTraning Loss: 0.0007351769017986953\n",
            "390 \tTraning Loss: 0.0005189328803680837\n",
            "391 \tTraning Loss: 0.0004065162211190909\n",
            "392 \tTraning Loss: 0.0006339721148833632\n",
            "393 \tTraning Loss: 0.0004021323402412236\n",
            "394 \tTraning Loss: 0.000644124869722873\n",
            "395 \tTraning Loss: 0.0005211895913816988\n",
            "396 \tTraning Loss: 0.0006952357362024486\n",
            "397 \tTraning Loss: 0.0005023638368584216\n",
            "398 \tTraning Loss: 0.000546341179870069\n",
            "399 \tTraning Loss: 0.0005824294639751315\n",
            "400 \tTraning Loss: 0.0005527528119273484\n",
            "401 \tTraning Loss: 0.0004780349845532328\n",
            "402 \tTraning Loss: 0.00046447874046862125\n",
            "403 \tTraning Loss: 0.0007225280860438943\n",
            "404 \tTraning Loss: 0.0005843196413479745\n",
            "405 \tTraning Loss: 0.0005178757128305733\n",
            "406 \tTraning Loss: 0.0007670755148865283\n",
            "407 \tTraning Loss: 0.0006582406931556761\n",
            "408 \tTraning Loss: 0.0005037773516960442\n",
            "409 \tTraning Loss: 0.0007385124918073416\n",
            "410 \tTraning Loss: 0.0005832772585563362\n",
            "411 \tTraning Loss: 0.00044267327757552266\n",
            "412 \tTraning Loss: 0.0006230223807506263\n",
            "413 \tTraning Loss: 0.000800823443569243\n",
            "414 \tTraning Loss: 0.0007035571616142988\n",
            "415 \tTraning Loss: 0.0006845821626484394\n",
            "416 \tTraning Loss: 0.00046842050505802035\n",
            "417 \tTraning Loss: 0.0007126936106942594\n",
            "418 \tTraning Loss: 0.0004664192965719849\n",
            "419 \tTraning Loss: 0.0004971427842974663\n",
            "420 \tTraning Loss: 0.0005216188146732748\n",
            "421 \tTraning Loss: 0.0005605753976851702\n",
            "422 \tTraning Loss: 0.0005095062660984695\n",
            "423 \tTraning Loss: 0.0005329580162651837\n",
            "424 \tTraning Loss: 0.00047192987403832376\n",
            "425 \tTraning Loss: 0.0006379202241078019\n",
            "426 \tTraning Loss: 0.00037279294338077307\n",
            "427 \tTraning Loss: 0.0004252396756783128\n",
            "428 \tTraning Loss: 0.000528766424395144\n",
            "429 \tTraning Loss: 0.0005714695435017347\n",
            "430 \tTraning Loss: 0.0006519095622934401\n",
            "431 \tTraning Loss: 0.0005527469329535961\n",
            "432 \tTraning Loss: 0.000446633726824075\n",
            "433 \tTraning Loss: 0.0002652098482940346\n",
            "434 \tTraning Loss: 0.0005995078827254474\n",
            "435 \tTraning Loss: 0.0005398867069743574\n",
            "436 \tTraning Loss: 0.0005548574263229966\n",
            "437 \tTraning Loss: 0.000485884549561888\n",
            "438 \tTraning Loss: 0.0005122466245666146\n",
            "439 \tTraning Loss: 0.000637105549685657\n",
            "440 \tTraning Loss: 0.00042938717524521053\n",
            "441 \tTraning Loss: 0.000629523943644017\n",
            "442 \tTraning Loss: 0.0007281120051629841\n",
            "443 \tTraning Loss: 0.0005569946952164173\n",
            "444 \tTraning Loss: 0.0005493452190421522\n",
            "445 \tTraning Loss: 0.0005306269740685821\n",
            "446 \tTraning Loss: 0.00041042070370167494\n",
            "447 \tTraning Loss: 0.0005411523161455989\n",
            "448 \tTraning Loss: 0.0003880776057485491\n",
            "449 \tTraning Loss: 0.00038922467501834035\n",
            "450 \tTraning Loss: 0.0006493944092653692\n",
            "451 \tTraning Loss: 0.0004747968050651252\n",
            "452 \tTraning Loss: 0.0005835386691614985\n",
            "453 \tTraning Loss: 0.0005644380580633879\n",
            "454 \tTraning Loss: 0.0006999573088251054\n",
            "455 \tTraning Loss: 0.0005188182112760842\n",
            "456 \tTraning Loss: 0.0005212135147303343\n",
            "457 \tTraning Loss: 0.00045282478095032275\n",
            "458 \tTraning Loss: 0.0006286046118475497\n",
            "459 \tTraning Loss: 0.0004494081949815154\n",
            "460 \tTraning Loss: 0.0005431619938462973\n",
            "461 \tTraning Loss: 0.0006649995339103043\n",
            "462 \tTraning Loss: 0.0006129241664893925\n",
            "463 \tTraning Loss: 0.0006280702073127031\n",
            "464 \tTraning Loss: 0.0004964658292010427\n",
            "465 \tTraning Loss: 0.0005833817413076758\n",
            "466 \tTraning Loss: 0.000638251134660095\n",
            "467 \tTraning Loss: 0.0004014460137113929\n",
            "468 \tTraning Loss: 0.0004932262818329036\n",
            "469 \tTraning Loss: 0.0005234666750766337\n",
            "470 \tTraning Loss: 0.000751352112274617\n",
            "471 \tTraning Loss: 0.0009156548767350614\n",
            "472 \tTraning Loss: 0.00039924270822666585\n",
            "473 \tTraning Loss: 0.0005913064815104008\n",
            "474 \tTraning Loss: 0.0005407622666098177\n",
            "475 \tTraning Loss: 0.0006062300526537001\n",
            "476 \tTraning Loss: 0.000614985590800643\n",
            "477 \tTraning Loss: 0.0005704352515749633\n",
            "478 \tTraning Loss: 0.0006569023244082928\n",
            "479 \tTraning Loss: 0.00045195731217972934\n",
            "480 \tTraning Loss: 0.0005321984062902629\n",
            "481 \tTraning Loss: 0.0004563624388538301\n",
            "482 \tTraning Loss: 0.0004395287833176553\n",
            "483 \tTraning Loss: 0.0005708849639631808\n",
            "484 \tTraning Loss: 0.0003413282975088805\n",
            "485 \tTraning Loss: 0.00043101675692014396\n",
            "486 \tTraning Loss: 0.00043605268001556396\n",
            "487 \tTraning Loss: 0.0005655668210238218\n",
            "488 \tTraning Loss: 0.000480602087918669\n",
            "489 \tTraning Loss: 0.0006549784447997808\n",
            "490 \tTraning Loss: 0.0003804660227615386\n",
            "491 \tTraning Loss: 0.00057857675710693\n",
            "492 \tTraning Loss: 0.0005192662938497961\n",
            "493 \tTraning Loss: 0.0005754954181611538\n",
            "494 \tTraning Loss: 0.000490717648062855\n",
            "495 \tTraning Loss: 0.0005194192053750157\n",
            "496 \tTraning Loss: 0.0005172098171897233\n",
            "497 \tTraning Loss: 0.0005482867127284408\n",
            "498 \tTraning Loss: 0.0006062754546292126\n",
            "499 \tTraning Loss: 0.0006490133819170296\n",
            "500 \tTraning Loss: 0.0004226628225296736\n",
            "501 \tTraning Loss: 0.0004889831761829555\n",
            "502 \tTraning Loss: 0.00039638616726733744\n",
            "503 \tTraning Loss: 0.0004015941231045872\n",
            "504 \tTraning Loss: 0.0003732456243596971\n",
            "505 \tTraning Loss: 0.0006023349706083536\n",
            "506 \tTraning Loss: 0.0004878687032032758\n",
            "507 \tTraning Loss: 0.0004188803141005337\n",
            "508 \tTraning Loss: 0.0004995802300982177\n",
            "509 \tTraning Loss: 0.0006375478114932775\n",
            "510 \tTraning Loss: 0.000539615866728127\n",
            "511 \tTraning Loss: 0.0004679287667386234\n",
            "512 \tTraning Loss: 0.0005663158954121172\n",
            "513 \tTraning Loss: 0.0004671896167565137\n",
            "514 \tTraning Loss: 0.00036238881875760853\n",
            "515 \tTraning Loss: 0.00028423109324648976\n",
            "516 \tTraning Loss: 0.000513038074132055\n",
            "517 \tTraning Loss: 0.0004211950290482491\n",
            "518 \tTraning Loss: 0.00045477470848709345\n",
            "519 \tTraning Loss: 0.00041422859067097306\n",
            "520 \tTraning Loss: 0.00042110192589461803\n",
            "521 \tTraning Loss: 0.0005396357155404985\n",
            "522 \tTraning Loss: 0.0004393935087136924\n",
            "523 \tTraning Loss: 0.00030094635440036654\n",
            "524 \tTraning Loss: 0.0004403828352224082\n",
            "525 \tTraning Loss: 0.0004156698996666819\n",
            "526 \tTraning Loss: 0.00043505048961378634\n",
            "527 \tTraning Loss: 0.000549019081518054\n",
            "528 \tTraning Loss: 0.0004583023546729237\n",
            "529 \tTraning Loss: 0.0005038134986534715\n",
            "530 \tTraning Loss: 0.0004497934423852712\n",
            "531 \tTraning Loss: 0.0003949019010178745\n",
            "532 \tTraning Loss: 0.0005315358866937459\n",
            "533 \tTraning Loss: 0.0004632578929886222\n",
            "534 \tTraning Loss: 0.0006763964192941785\n",
            "535 \tTraning Loss: 0.0005888392333872616\n",
            "536 \tTraning Loss: 0.0005086301825940609\n",
            "537 \tTraning Loss: 0.00038302462780848145\n",
            "538 \tTraning Loss: 0.00044909061398357153\n",
            "539 \tTraning Loss: 0.00038611923810094595\n",
            "540 \tTraning Loss: 0.0006344967987388372\n",
            "541 \tTraning Loss: 0.0004671555361710489\n",
            "542 \tTraning Loss: 0.00040509720565751195\n",
            "543 \tTraning Loss: 0.0005072642816230655\n",
            "544 \tTraning Loss: 0.0005797812482342124\n",
            "545 \tTraning Loss: 0.0004967502318322659\n",
            "546 \tTraning Loss: 0.00043595387251116335\n",
            "547 \tTraning Loss: 0.00032294864649884403\n",
            "548 \tTraning Loss: 0.00035695760743692517\n",
            "549 \tTraning Loss: 0.0004937727353535593\n",
            "550 \tTraning Loss: 0.00046290340833365917\n",
            "551 \tTraning Loss: 0.0006151108536869287\n",
            "552 \tTraning Loss: 0.0004546800337266177\n",
            "553 \tTraning Loss: 0.0006422999431379139\n",
            "554 \tTraning Loss: 0.0004200738330837339\n",
            "555 \tTraning Loss: 0.0004461960925254971\n",
            "556 \tTraning Loss: 0.0005504775326699018\n",
            "557 \tTraning Loss: 0.00042373183532617986\n",
            "558 \tTraning Loss: 0.000510738929733634\n",
            "559 \tTraning Loss: 0.0006730229943059385\n",
            "560 \tTraning Loss: 0.0004962467937730253\n",
            "561 \tTraning Loss: 0.0006866494077257812\n",
            "562 \tTraning Loss: 0.0006371814524754882\n",
            "563 \tTraning Loss: 0.00061267294222489\n",
            "564 \tTraning Loss: 0.0004821941547561437\n",
            "565 \tTraning Loss: 0.0006503083277493715\n",
            "566 \tTraning Loss: 0.0005664750351570547\n",
            "567 \tTraning Loss: 0.0006929307710379362\n",
            "568 \tTraning Loss: 0.0003490654635243118\n",
            "569 \tTraning Loss: 0.00047773984260857105\n",
            "570 \tTraning Loss: 0.0005122341099195182\n",
            "571 \tTraning Loss: 0.00046448232023976743\n",
            "572 \tTraning Loss: 0.00041448799311183393\n",
            "573 \tTraning Loss: 0.0004255717503838241\n",
            "574 \tTraning Loss: 0.0003333046333864331\n",
            "575 \tTraning Loss: 0.0005468071321956813\n",
            "576 \tTraning Loss: 0.00040885433554649353\n",
            "577 \tTraning Loss: 0.00038752102409489453\n",
            "578 \tTraning Loss: 0.0005540570127777755\n",
            "579 \tTraning Loss: 0.0006021622684784234\n",
            "580 \tTraning Loss: 0.0005866691353730857\n",
            "581 \tTraning Loss: 0.0005918824463151395\n",
            "582 \tTraning Loss: 0.0006642368971370161\n",
            "583 \tTraning Loss: 0.0005209815572015941\n",
            "584 \tTraning Loss: 0.00047087096027098596\n",
            "585 \tTraning Loss: 0.0005240628379397094\n",
            "586 \tTraning Loss: 0.0006337735685519874\n",
            "587 \tTraning Loss: 0.000631427566986531\n",
            "588 \tTraning Loss: 0.0006250597652979195\n",
            "589 \tTraning Loss: 0.0005752136930823326\n",
            "590 \tTraning Loss: 0.0006994499126449227\n",
            "591 \tTraning Loss: 0.0003549863467924297\n",
            "592 \tTraning Loss: 0.000307605107082054\n",
            "593 \tTraning Loss: 0.00038556510116904974\n",
            "594 \tTraning Loss: 0.0004281854198779911\n",
            "595 \tTraning Loss: 0.0003780878323595971\n",
            "596 \tTraning Loss: 0.0005676576984114945\n",
            "597 \tTraning Loss: 0.0005975245148874819\n",
            "598 \tTraning Loss: 0.00038727823994122446\n",
            "599 \tTraning Loss: 0.0005732050049118698\n",
            "600 \tTraning Loss: 0.0003490157541818917\n",
            "601 \tTraning Loss: 0.00047917041229084134\n",
            "602 \tTraning Loss: 0.0006041864980943501\n",
            "603 \tTraning Loss: 0.0005175768746994436\n",
            "604 \tTraning Loss: 0.00040010997327044606\n",
            "605 \tTraning Loss: 0.000508435710798949\n",
            "606 \tTraning Loss: 0.0005392672028392553\n",
            "607 \tTraning Loss: 0.0005282703205011785\n",
            "608 \tTraning Loss: 0.0004979573423042893\n",
            "609 \tTraning Loss: 0.00037484828499145806\n",
            "610 \tTraning Loss: 0.000445325713371858\n",
            "611 \tTraning Loss: 0.0003299154050182551\n",
            "612 \tTraning Loss: 0.00040354218799620867\n",
            "613 \tTraning Loss: 0.0005233876290731132\n",
            "614 \tTraning Loss: 0.0005424442351795733\n",
            "615 \tTraning Loss: 0.000330287788528949\n",
            "616 \tTraning Loss: 0.0003474780241958797\n",
            "617 \tTraning Loss: 0.0005188349168747663\n",
            "618 \tTraning Loss: 0.0005455986247397959\n",
            "619 \tTraning Loss: 0.0006475481204688549\n",
            "620 \tTraning Loss: 0.0005872431793250144\n",
            "621 \tTraning Loss: 0.0005240963655523956\n",
            "622 \tTraning Loss: 0.0005983012379147112\n",
            "623 \tTraning Loss: 0.0004945489927195013\n",
            "624 \tTraning Loss: 0.0004902040818706155\n",
            "625 \tTraning Loss: 0.00040700830868445337\n",
            "626 \tTraning Loss: 0.00035575349465943873\n",
            "627 \tTraning Loss: 0.0005106957978568971\n",
            "628 \tTraning Loss: 0.00047368014929816127\n",
            "629 \tTraning Loss: 0.00044688311754725873\n",
            "630 \tTraning Loss: 0.0005236446741037071\n",
            "631 \tTraning Loss: 0.0004881802888121456\n",
            "632 \tTraning Loss: 0.0004525987897068262\n",
            "633 \tTraning Loss: 0.0004894036101177335\n",
            "634 \tTraning Loss: 0.0003912219835910946\n",
            "635 \tTraning Loss: 0.00033217770396731794\n",
            "636 \tTraning Loss: 0.0003872782690450549\n",
            "637 \tTraning Loss: 0.0005415826453827322\n",
            "638 \tTraning Loss: 0.0005498717655427754\n",
            "639 \tTraning Loss: 0.0003846749023068696\n",
            "640 \tTraning Loss: 0.0005519961705431342\n",
            "641 \tTraning Loss: 0.0004865870869252831\n",
            "642 \tTraning Loss: 0.0005327309481799603\n",
            "643 \tTraning Loss: 0.0006729733431711793\n",
            "644 \tTraning Loss: 0.000593131931964308\n",
            "645 \tTraning Loss: 0.0003437447885517031\n",
            "646 \tTraning Loss: 0.0003275176277384162\n",
            "647 \tTraning Loss: 0.0004422414640430361\n",
            "648 \tTraning Loss: 0.0005134976818226278\n",
            "649 \tTraning Loss: 0.0005092210485599935\n",
            "650 \tTraning Loss: 0.0004656165838241577\n",
            "651 \tTraning Loss: 0.00043131489655934274\n",
            "652 \tTraning Loss: 0.00042212396510876715\n",
            "653 \tTraning Loss: 0.00036041176645085216\n",
            "654 \tTraning Loss: 0.00039459840627387166\n",
            "655 \tTraning Loss: 0.0004346714122220874\n",
            "656 \tTraning Loss: 0.000565512862522155\n",
            "657 \tTraning Loss: 0.0006357950624078512\n",
            "658 \tTraning Loss: 0.0003674943873193115\n",
            "659 \tTraning Loss: 0.0005542018916457891\n",
            "660 \tTraning Loss: 0.00038760958705097437\n",
            "661 \tTraning Loss: 0.0005018362426199019\n",
            "662 \tTraning Loss: 0.0004590072203427553\n",
            "663 \tTraning Loss: 0.00046019203728064895\n",
            "664 \tTraning Loss: 0.00047988301957957447\n",
            "665 \tTraning Loss: 0.00029234911198727787\n",
            "666 \tTraning Loss: 0.000546701077837497\n",
            "667 \tTraning Loss: 0.0004921346553601325\n",
            "668 \tTraning Loss: 0.0004844491195399314\n",
            "669 \tTraning Loss: 0.0005701840855181217\n",
            "670 \tTraning Loss: 0.0004755500704050064\n",
            "671 \tTraning Loss: 0.000424246652983129\n",
            "672 \tTraning Loss: 0.00038590430631302297\n",
            "673 \tTraning Loss: 0.00034307967871427536\n",
            "674 \tTraning Loss: 0.0004216715751681477\n",
            "675 \tTraning Loss: 0.00043807271867990494\n",
            "676 \tTraning Loss: 0.00032058253418654203\n",
            "677 \tTraning Loss: 0.0005112153012305498\n",
            "678 \tTraning Loss: 0.00041865409002639353\n",
            "679 \tTraning Loss: 0.00040870215161703527\n",
            "680 \tTraning Loss: 0.0003715888306032866\n",
            "681 \tTraning Loss: 0.00037942311610095203\n",
            "682 \tTraning Loss: 0.00046444215695373714\n",
            "683 \tTraning Loss: 0.0006115229916758835\n",
            "684 \tTraning Loss: 0.000600329483859241\n",
            "685 \tTraning Loss: 0.0005172773962840438\n",
            "686 \tTraning Loss: 0.0003634671156760305\n",
            "687 \tTraning Loss: 0.00043463162728585303\n",
            "688 \tTraning Loss: 0.0004226707969792187\n",
            "689 \tTraning Loss: 0.0005578179261647165\n",
            "690 \tTraning Loss: 0.00036472483770921826\n",
            "691 \tTraning Loss: 0.000349251989973709\n",
            "692 \tTraning Loss: 0.0003539510362315923\n",
            "693 \tTraning Loss: 0.00040671281749382615\n",
            "694 \tTraning Loss: 0.00039059482514858246\n",
            "695 \tTraning Loss: 0.000528165022842586\n",
            "696 \tTraning Loss: 0.00046853951062075794\n",
            "697 \tTraning Loss: 0.00045133032836019993\n",
            "698 \tTraning Loss: 0.0004470763960853219\n",
            "699 \tTraning Loss: 0.000512105762027204\n",
            "700 \tTraning Loss: 0.0005010419990867376\n",
            "701 \tTraning Loss: 0.00044909026473760605\n",
            "702 \tTraning Loss: 0.00029486443963833153\n",
            "703 \tTraning Loss: 0.0004670578346122056\n",
            "704 \tTraning Loss: 0.0003902665921486914\n",
            "705 \tTraning Loss: 0.0005975972744636238\n",
            "706 \tTraning Loss: 0.0004530256846919656\n",
            "707 \tTraning Loss: 0.0005087813478894532\n",
            "708 \tTraning Loss: 0.00043533105053938925\n",
            "709 \tTraning Loss: 0.0005735374288633466\n",
            "710 \tTraning Loss: 0.000532883801497519\n",
            "711 \tTraning Loss: 0.0004828821402043104\n",
            "712 \tTraning Loss: 0.0005412216996774077\n",
            "713 \tTraning Loss: 0.0003984353388659656\n",
            "714 \tTraning Loss: 0.0005077098030596972\n",
            "715 \tTraning Loss: 0.0005269012763164937\n",
            "716 \tTraning Loss: 0.0006187894032336771\n",
            "717 \tTraning Loss: 0.0003680727968458086\n",
            "718 \tTraning Loss: 0.00047560862731188536\n",
            "719 \tTraning Loss: 0.0003903397882822901\n",
            "720 \tTraning Loss: 0.00036912449286319315\n",
            "721 \tTraning Loss: 0.0004517093184404075\n",
            "722 \tTraning Loss: 0.0005311878630891442\n",
            "723 \tTraning Loss: 0.0005582789890468121\n",
            "724 \tTraning Loss: 0.0005894959904253483\n",
            "725 \tTraning Loss: 0.00031988686532713473\n",
            "726 \tTraning Loss: 0.0004982428508810699\n",
            "727 \tTraning Loss: 0.0002580850850790739\n",
            "728 \tTraning Loss: 0.0005302428035065532\n",
            "729 \tTraning Loss: 0.000379391189198941\n",
            "730 \tTraning Loss: 0.0004339648294262588\n",
            "731 \tTraning Loss: 0.00046309284516610205\n",
            "732 \tTraning Loss: 0.00044496479677036405\n",
            "733 \tTraning Loss: 0.00030432618223130703\n",
            "734 \tTraning Loss: 0.0004622001142706722\n",
            "735 \tTraning Loss: 0.0004621926345862448\n",
            "736 \tTraning Loss: 0.0005268698441796005\n",
            "737 \tTraning Loss: 0.00046028915676288307\n",
            "738 \tTraning Loss: 0.0004966053529642522\n",
            "739 \tTraning Loss: 0.000551877950783819\n",
            "740 \tTraning Loss: 0.0004626406589522958\n",
            "741 \tTraning Loss: 0.00027958411374129355\n",
            "742 \tTraning Loss: 0.0006162868812680244\n",
            "743 \tTraning Loss: 0.00040180128416977823\n",
            "744 \tTraning Loss: 0.0004476331523619592\n",
            "745 \tTraning Loss: 0.0005121042486280203\n",
            "746 \tTraning Loss: 0.00037567250546999276\n",
            "747 \tTraning Loss: 0.0003439175779931247\n",
            "748 \tTraning Loss: 0.0004457299364730716\n",
            "749 \tTraning Loss: 0.00048260498442687094\n",
            "750 \tTraning Loss: 0.00037686823634430766\n",
            "751 \tTraning Loss: 0.0005668447120115161\n",
            "752 \tTraning Loss: 0.0003539242607075721\n",
            "753 \tTraning Loss: 0.0004242536670062691\n",
            "754 \tTraning Loss: 0.00030929624335840344\n",
            "755 \tTraning Loss: 0.0003389847988728434\n",
            "756 \tTraning Loss: 0.0004297968989703804\n",
            "757 \tTraning Loss: 0.0004411059489939362\n",
            "758 \tTraning Loss: 0.0003988730604760349\n",
            "759 \tTraning Loss: 0.000376093084923923\n",
            "760 \tTraning Loss: 0.0004992980393581092\n",
            "761 \tTraning Loss: 0.00036255375016480684\n",
            "762 \tTraning Loss: 0.0004510901926551014\n",
            "763 \tTraning Loss: 0.00034716411028057337\n",
            "764 \tTraning Loss: 0.00045213953126221895\n",
            "765 \tTraning Loss: 0.00040854059625416994\n",
            "766 \tTraning Loss: 0.0005777417682111263\n",
            "767 \tTraning Loss: 0.0005278640892356634\n",
            "768 \tTraning Loss: 0.0004612357879523188\n",
            "769 \tTraning Loss: 0.0003918487927876413\n",
            "770 \tTraning Loss: 0.00027109196525998414\n",
            "771 \tTraning Loss: 0.0005278735188767314\n",
            "772 \tTraning Loss: 0.0003299725940451026\n",
            "773 \tTraning Loss: 0.00046143782674334943\n",
            "774 \tTraning Loss: 0.00042055564699694514\n",
            "775 \tTraning Loss: 0.0005189430667087436\n",
            "776 \tTraning Loss: 0.0005590219516307116\n",
            "777 \tTraning Loss: 0.00036883982829749584\n",
            "778 \tTraning Loss: 0.00044216823880560696\n",
            "779 \tTraning Loss: 0.00037635848275385797\n",
            "780 \tTraning Loss: 0.0003334934590384364\n",
            "781 \tTraning Loss: 0.00042139366269111633\n",
            "782 \tTraning Loss: 0.0004402149352245033\n",
            "783 \tTraning Loss: 0.0005071753403171897\n",
            "784 \tTraning Loss: 0.000547215633559972\n",
            "785 \tTraning Loss: 0.00039559550350531936\n",
            "786 \tTraning Loss: 0.0004971178714185953\n",
            "787 \tTraning Loss: 0.0005868101143278182\n",
            "788 \tTraning Loss: 0.0004895871970802546\n",
            "789 \tTraning Loss: 0.00040515794535167515\n",
            "790 \tTraning Loss: 0.0004239305853843689\n",
            "791 \tTraning Loss: 0.0004464288067538291\n",
            "792 \tTraning Loss: 0.00038402833160944283\n",
            "793 \tTraning Loss: 0.0004114451294299215\n",
            "794 \tTraning Loss: 0.000385268620448187\n",
            "795 \tTraning Loss: 0.0005249149980954826\n",
            "796 \tTraning Loss: 0.0005308160907588899\n",
            "797 \tTraning Loss: 0.00039992944221012294\n",
            "798 \tTraning Loss: 0.0004106032138224691\n",
            "799 \tTraning Loss: 0.0004900163621641695\n",
            "800 \tTraning Loss: 0.00045000799582339823\n",
            "801 \tTraning Loss: 0.000365547341061756\n",
            "802 \tTraning Loss: 0.00048749137204140425\n",
            "803 \tTraning Loss: 0.00046693210606463253\n",
            "804 \tTraning Loss: 0.0003799063269980252\n",
            "805 \tTraning Loss: 0.00033241327037103474\n",
            "806 \tTraning Loss: 0.00045005467836745083\n",
            "807 \tTraning Loss: 0.0005391573649831116\n",
            "808 \tTraning Loss: 0.00043253417243249714\n",
            "809 \tTraning Loss: 0.00042188697261735797\n",
            "810 \tTraning Loss: 0.0005957713583484292\n",
            "811 \tTraning Loss: 0.00040973175782710314\n",
            "812 \tTraning Loss: 0.00038977342774160206\n",
            "813 \tTraning Loss: 0.0005749752162955701\n",
            "814 \tTraning Loss: 0.0005374504253268242\n",
            "815 \tTraning Loss: 0.0003811197238974273\n",
            "816 \tTraning Loss: 0.0004649078182410449\n",
            "817 \tTraning Loss: 0.0005280966870486736\n",
            "818 \tTraning Loss: 0.000447892613010481\n",
            "819 \tTraning Loss: 0.0004222842399030924\n",
            "820 \tTraning Loss: 0.00042992658563889563\n",
            "821 \tTraning Loss: 0.000350818270817399\n",
            "822 \tTraning Loss: 0.0005586068728007376\n",
            "823 \tTraning Loss: 0.0005450521130114794\n",
            "824 \tTraning Loss: 0.0002749099221546203\n",
            "825 \tTraning Loss: 0.0004478806222323328\n",
            "826 \tTraning Loss: 0.0005391419981606305\n",
            "827 \tTraning Loss: 0.0005356648471206427\n",
            "828 \tTraning Loss: 0.00031349569326266646\n",
            "829 \tTraning Loss: 0.0004326495109125972\n",
            "830 \tTraning Loss: 0.0005650326493196189\n",
            "831 \tTraning Loss: 0.0004044621018692851\n",
            "832 \tTraning Loss: 0.0005056363879702985\n",
            "833 \tTraning Loss: 0.0005162922898307443\n",
            "834 \tTraning Loss: 0.00043851815280504525\n",
            "835 \tTraning Loss: 0.00044566422002390027\n",
            "836 \tTraning Loss: 0.0003837896219920367\n",
            "837 \tTraning Loss: 0.00036744450335390866\n",
            "838 \tTraning Loss: 0.00031964268418960273\n",
            "839 \tTraning Loss: 0.0003818525292444974\n",
            "840 \tTraning Loss: 0.0002890340983867645\n",
            "841 \tTraning Loss: 0.0005072290077805519\n",
            "842 \tTraning Loss: 0.00040390153299085796\n",
            "843 \tTraning Loss: 0.0003656590706668794\n",
            "844 \tTraning Loss: 0.00041315038106404245\n",
            "845 \tTraning Loss: 0.0004332278040237725\n",
            "846 \tTraning Loss: 0.0004556395870167762\n",
            "847 \tTraning Loss: 0.0003490739327389747\n",
            "848 \tTraning Loss: 0.0004990265588276088\n",
            "849 \tTraning Loss: 0.0006012676167301834\n",
            "850 \tTraning Loss: 0.0006978965830057859\n",
            "851 \tTraning Loss: 0.00033933084341697395\n",
            "852 \tTraning Loss: 0.00041844212682917714\n",
            "853 \tTraning Loss: 0.0003793853975366801\n",
            "854 \tTraning Loss: 0.0005947828176431358\n",
            "855 \tTraning Loss: 0.0004804632917512208\n",
            "856 \tTraning Loss: 0.00046911765821278095\n",
            "857 \tTraning Loss: 0.0005819884827360511\n",
            "858 \tTraning Loss: 0.0005245310603640974\n",
            "859 \tTraning Loss: 0.00044899500790052116\n",
            "860 \tTraning Loss: 0.0004561043460853398\n",
            "861 \tTraning Loss: 0.0005101783317513764\n",
            "862 \tTraning Loss: 0.000481471506645903\n",
            "863 \tTraning Loss: 0.0004953786847181618\n",
            "864 \tTraning Loss: 0.0003869796928483993\n",
            "865 \tTraning Loss: 0.000761716568376869\n",
            "866 \tTraning Loss: 0.0004280306166037917\n",
            "867 \tTraning Loss: 0.00041653934749774635\n",
            "868 \tTraning Loss: 0.0004343579348642379\n",
            "869 \tTraning Loss: 0.0003984929935541004\n",
            "870 \tTraning Loss: 0.00047630429617129266\n",
            "871 \tTraning Loss: 0.0004142912512179464\n",
            "872 \tTraning Loss: 0.0003665264812298119\n",
            "873 \tTraning Loss: 0.0004457714385353029\n",
            "874 \tTraning Loss: 0.0004062211373820901\n",
            "875 \tTraning Loss: 0.00042268403922207654\n",
            "876 \tTraning Loss: 0.000391815701732412\n",
            "877 \tTraning Loss: 0.0004530379083007574\n",
            "878 \tTraning Loss: 0.0004586311406455934\n",
            "879 \tTraning Loss: 0.0005580672877840698\n",
            "880 \tTraning Loss: 0.0004639478283934295\n",
            "881 \tTraning Loss: 0.0005532051436603069\n",
            "882 \tTraning Loss: 0.000591220217756927\n",
            "883 \tTraning Loss: 0.0005905336001887918\n",
            "884 \tTraning Loss: 0.0004352103278506547\n",
            "885 \tTraning Loss: 0.0005205307970754802\n",
            "886 \tTraning Loss: 0.0003396288666408509\n",
            "887 \tTraning Loss: 0.0004512256709858775\n",
            "888 \tTraning Loss: 0.00044488071580417454\n",
            "889 \tTraning Loss: 0.0005459563690237701\n",
            "890 \tTraning Loss: 0.00048263740609399974\n",
            "891 \tTraning Loss: 0.000498118344694376\n",
            "892 \tTraning Loss: 0.0005194279365241528\n",
            "893 \tTraning Loss: 0.000405721744755283\n",
            "894 \tTraning Loss: 0.00036496936809271574\n",
            "895 \tTraning Loss: 0.00046427876804955304\n",
            "896 \tTraning Loss: 0.0003817807009909302\n",
            "897 \tTraning Loss: 0.0003318095696158707\n",
            "898 \tTraning Loss: 0.00040015726699493825\n",
            "899 \tTraning Loss: 0.00044337083818390965\n",
            "900 \tTraning Loss: 0.0005730402772314847\n",
            "901 \tTraning Loss: 0.00034303334541618824\n",
            "902 \tTraning Loss: 0.0005518632242456079\n",
            "903 \tTraning Loss: 0.0005009302403777838\n",
            "904 \tTraning Loss: 0.0004040061612613499\n",
            "905 \tTraning Loss: 0.00039686067611910403\n",
            "906 \tTraning Loss: 0.0005787239060737193\n",
            "907 \tTraning Loss: 0.00033656557206995785\n",
            "908 \tTraning Loss: 0.0004148912848904729\n",
            "909 \tTraning Loss: 0.00042301800567656755\n",
            "910 \tTraning Loss: 0.000417724484577775\n",
            "911 \tTraning Loss: 0.0005851302994415164\n",
            "912 \tTraning Loss: 0.0003967402153648436\n",
            "913 \tTraning Loss: 0.00038660867721773684\n",
            "914 \tTraning Loss: 0.0004244095762260258\n",
            "915 \tTraning Loss: 0.00036122879828326404\n",
            "916 \tTraning Loss: 0.00022493023425340652\n",
            "917 \tTraning Loss: 0.0005629942752420902\n",
            "918 \tTraning Loss: 0.00034362261067144573\n",
            "919 \tTraning Loss: 0.0004559179360512644\n",
            "920 \tTraning Loss: 0.0003297958755865693\n",
            "921 \tTraning Loss: 0.0005197998834773898\n",
            "922 \tTraning Loss: 0.000569721101783216\n",
            "923 \tTraning Loss: 0.0005358911585062742\n",
            "924 \tTraning Loss: 0.00035549921449273825\n",
            "925 \tTraning Loss: 0.00040189572609961033\n",
            "926 \tTraning Loss: 0.0004477657494135201\n",
            "927 \tTraning Loss: 0.0004980082740075886\n",
            "928 \tTraning Loss: 0.0003635825705714524\n",
            "929 \tTraning Loss: 0.0003369112382642925\n",
            "930 \tTraning Loss: 0.0005424683913588524\n",
            "931 \tTraning Loss: 0.0003505133790895343\n",
            "932 \tTraning Loss: 0.000415508053265512\n",
            "933 \tTraning Loss: 0.0003828785556834191\n",
            "934 \tTraning Loss: 0.00028697389643639326\n",
            "935 \tTraning Loss: 0.0003862795128952712\n",
            "936 \tTraning Loss: 0.0005533826188184321\n",
            "937 \tTraning Loss: 0.00046590075362473726\n",
            "938 \tTraning Loss: 0.0004164170240983367\n",
            "939 \tTraning Loss: 0.00045453273924067616\n",
            "940 \tTraning Loss: 0.00040485625504516065\n",
            "941 \tTraning Loss: 0.00041398764005862176\n",
            "942 \tTraning Loss: 0.00044373664422892034\n",
            "943 \tTraning Loss: 0.0003735521459020674\n",
            "944 \tTraning Loss: 0.0005170941585674882\n",
            "945 \tTraning Loss: 0.0004347267677076161\n",
            "946 \tTraning Loss: 0.00044644493027590215\n",
            "947 \tTraning Loss: 0.0004151427128817886\n",
            "948 \tTraning Loss: 0.0004142950347159058\n",
            "949 \tTraning Loss: 0.0003877720155287534\n",
            "950 \tTraning Loss: 0.0004589970922097564\n",
            "951 \tTraning Loss: 0.0004648745816666633\n",
            "952 \tTraning Loss: 0.0004574511549435556\n",
            "953 \tTraning Loss: 0.0003050964733120054\n",
            "954 \tTraning Loss: 0.0005443054833449423\n",
            "955 \tTraning Loss: 0.00046707273577339947\n",
            "956 \tTraning Loss: 0.0004253608058206737\n",
            "957 \tTraning Loss: 0.0003626833204180002\n",
            "958 \tTraning Loss: 0.00043122019269503653\n",
            "959 \tTraning Loss: 0.00037108297692611814\n",
            "960 \tTraning Loss: 0.0003552073903847486\n",
            "961 \tTraning Loss: 0.0004928092821501195\n",
            "962 \tTraning Loss: 0.0005385220283642411\n",
            "963 \tTraning Loss: 0.0005741892964579165\n",
            "964 \tTraning Loss: 0.00046435213880613446\n",
            "965 \tTraning Loss: 0.0003639138594735414\n",
            "966 \tTraning Loss: 0.0005215262062847614\n",
            "967 \tTraning Loss: 0.00047152547631412745\n",
            "968 \tTraning Loss: 0.0003644815878942609\n",
            "969 \tTraning Loss: 0.0005049777682870626\n",
            "970 \tTraning Loss: 0.00033682549837976694\n",
            "971 \tTraning Loss: 0.00040753561188466847\n",
            "972 \tTraning Loss: 0.0004929270362481475\n",
            "973 \tTraning Loss: 0.00027101763407699764\n",
            "974 \tTraning Loss: 0.0004345061315689236\n",
            "975 \tTraning Loss: 0.0003757961967494339\n",
            "976 \tTraning Loss: 0.0005392029415816069\n",
            "977 \tTraning Loss: 0.00041255823452956975\n",
            "978 \tTraning Loss: 0.0005852063186466694\n",
            "979 \tTraning Loss: 0.00039032503264024854\n",
            "980 \tTraning Loss: 0.000457482150522992\n",
            "981 \tTraning Loss: 0.0003894364053849131\n",
            "982 \tTraning Loss: 0.0004077117482665926\n",
            "983 \tTraning Loss: 0.00040919907041825354\n",
            "984 \tTraning Loss: 0.0003461054293438792\n",
            "985 \tTraning Loss: 0.0004724358150269836\n",
            "986 \tTraning Loss: 0.00041609033360145986\n",
            "987 \tTraning Loss: 0.0003709631273522973\n",
            "988 \tTraning Loss: 0.0005045332363806665\n",
            "989 \tTraning Loss: 0.000342764105880633\n",
            "990 \tTraning Loss: 0.0003777973761316389\n",
            "991 \tTraning Loss: 0.0003416594408918172\n",
            "992 \tTraning Loss: 0.00046328577445819974\n",
            "993 \tTraning Loss: 0.00035757775185629725\n",
            "994 \tTraning Loss: 0.0003897529386449605\n",
            "995 \tTraning Loss: 0.0005761071224696934\n",
            "996 \tTraning Loss: 0.00039568584179505706\n",
            "997 \tTraning Loss: 0.00039059764822013676\n",
            "998 \tTraning Loss: 0.00019524607341736555\n",
            "999 \tTraning Loss: 0.0005011864122934639\n",
            "1000 \tTraning Loss: 0.0003405237221159041\n",
            "1001 \tTraning Loss: 0.00044494227040559053\n",
            "1002 \tTraning Loss: 0.000344963395036757\n",
            "1003 \tTraning Loss: 0.00043082181946374476\n",
            "1004 \tTraning Loss: 0.0005254758289083838\n",
            "1005 \tTraning Loss: 0.00037468946538865566\n",
            "1006 \tTraning Loss: 0.0002433807385386899\n",
            "1007 \tTraning Loss: 0.00044685305329039693\n",
            "1008 \tTraning Loss: 0.00035792833659797907\n",
            "1009 \tTraning Loss: 0.0006292089237831533\n",
            "1010 \tTraning Loss: 0.0003503315965645015\n",
            "1011 \tTraning Loss: 0.0004395124560687691\n",
            "1012 \tTraning Loss: 0.00048019507084973156\n",
            "1013 \tTraning Loss: 0.0005795846227556467\n",
            "1014 \tTraning Loss: 0.00034367170883342624\n",
            "1015 \tTraning Loss: 0.0005114225205034018\n",
            "1016 \tTraning Loss: 0.00039616922731511295\n",
            "1017 \tTraning Loss: 0.0003812696086242795\n",
            "1018 \tTraning Loss: 0.00046448406646959484\n",
            "1019 \tTraning Loss: 0.0004750341468024999\n",
            "1020 \tTraning Loss: 0.00029336445732042193\n",
            "1021 \tTraning Loss: 0.000329762464389205\n",
            "1022 \tTraning Loss: 0.00038821675116196275\n",
            "1023 \tTraning Loss: 0.0004595829814206809\n",
            "1024 \tTraning Loss: 0.0004846063966397196\n",
            "1025 \tTraning Loss: 0.0003743178094737232\n",
            "1026 \tTraning Loss: 0.0005415076157078147\n",
            "1027 \tTraning Loss: 0.00044639798579737544\n",
            "1028 \tTraning Loss: 0.00046718178782612085\n",
            "1029 \tTraning Loss: 0.00035284028854221106\n",
            "1030 \tTraning Loss: 0.00032648578053340316\n",
            "1031 \tTraning Loss: 0.0005834896001033485\n",
            "1032 \tTraning Loss: 0.0005707846721634269\n",
            "1033 \tTraning Loss: 0.00036926104803569615\n",
            "1034 \tTraning Loss: 0.0004207658057566732\n",
            "1035 \tTraning Loss: 0.0005090403137728572\n",
            "1036 \tTraning Loss: 0.0004050002316944301\n",
            "1037 \tTraning Loss: 0.0005840210360474885\n",
            "1038 \tTraning Loss: 0.0003013524110428989\n",
            "1039 \tTraning Loss: 0.00033008441096171737\n",
            "1040 \tTraning Loss: 0.00035504859988577664\n",
            "1041 \tTraning Loss: 0.0004685674502979964\n",
            "1042 \tTraning Loss: 0.00042840599780902267\n",
            "1043 \tTraning Loss: 0.00034371152287349105\n",
            "1044 \tTraning Loss: 0.00024251840659417212\n",
            "1045 \tTraning Loss: 0.0005657984875142574\n",
            "1046 \tTraning Loss: 0.00044530004379339516\n",
            "1047 \tTraning Loss: 0.0004501151561271399\n",
            "1048 \tTraning Loss: 0.0004922712105326355\n",
            "1049 \tTraning Loss: 0.0003989944525528699\n",
            "1050 \tTraning Loss: 0.00037604477256536484\n",
            "1051 \tTraning Loss: 0.0004257416585460305\n",
            "1052 \tTraning Loss: 0.0005388552672229707\n",
            "1053 \tTraning Loss: 0.00042571715312078595\n",
            "1054 \tTraning Loss: 0.0004552207246888429\n",
            "1055 \tTraning Loss: 0.00028765786555595696\n",
            "1056 \tTraning Loss: 0.0004511958686634898\n",
            "1057 \tTraning Loss: 0.0005277888849377632\n",
            "1058 \tTraning Loss: 0.0004951594164595008\n",
            "1059 \tTraning Loss: 0.0004350743838585913\n",
            "1060 \tTraning Loss: 0.0004151216126047075\n",
            "1061 \tTraning Loss: 0.00039617999573238194\n",
            "1062 \tTraning Loss: 0.00048206656356342137\n",
            "1063 \tTraning Loss: 0.000422510172938928\n",
            "1064 \tTraning Loss: 0.0004727566847577691\n",
            "1065 \tTraning Loss: 0.00027867237804457545\n",
            "1066 \tTraning Loss: 0.00039186037611216307\n",
            "1067 \tTraning Loss: 0.00045362472883425653\n",
            "1068 \tTraning Loss: 0.0004351089883130044\n",
            "1069 \tTraning Loss: 0.0003337762609589845\n",
            "1070 \tTraning Loss: 0.0003552955749910325\n",
            "1071 \tTraning Loss: 0.00045155291445553303\n",
            "1072 \tTraning Loss: 0.0003656726039480418\n",
            "1073 \tTraning Loss: 0.0003606395039241761\n",
            "1074 \tTraning Loss: 0.0004476158937904984\n",
            "1075 \tTraning Loss: 0.00043669299338944256\n",
            "1076 \tTraning Loss: 0.0004714232636615634\n",
            "1077 \tTraning Loss: 0.0004100360965821892\n",
            "1078 \tTraning Loss: 0.00047372467815876007\n",
            "1079 \tTraning Loss: 0.0003041211748495698\n",
            "1080 \tTraning Loss: 0.0003749453171622008\n",
            "1081 \tTraning Loss: 0.0003981486370321363\n",
            "1082 \tTraning Loss: 0.00035881713847629726\n",
            "1083 \tTraning Loss: 0.00034485640935599804\n",
            "1084 \tTraning Loss: 0.0005531918723136187\n",
            "1085 \tTraning Loss: 0.0004377645964268595\n",
            "1086 \tTraning Loss: 0.0004440415359567851\n",
            "1087 \tTraning Loss: 0.0005579664139077067\n",
            "1088 \tTraning Loss: 0.00047427730169147253\n",
            "1089 \tTraning Loss: 0.0003487837966531515\n",
            "1090 \tTraning Loss: 0.0005244642379693687\n",
            "1091 \tTraning Loss: 0.00028518663020804524\n",
            "1092 \tTraning Loss: 0.00038908811984583735\n",
            "1093 \tTraning Loss: 0.00050681660650298\n",
            "1094 \tTraning Loss: 0.0005266618682071567\n",
            "1095 \tTraning Loss: 0.0004621948173735291\n",
            "1096 \tTraning Loss: 0.0002764568489510566\n",
            "1097 \tTraning Loss: 0.00039160504820756614\n",
            "1098 \tTraning Loss: 0.0005066936137154698\n",
            "1099 \tTraning Loss: 0.00033784008701331913\n",
            "1100 \tTraning Loss: 0.0003184230881743133\n",
            "1101 \tTraning Loss: 0.0005139546119607985\n",
            "1102 \tTraning Loss: 0.00028322040452621877\n",
            "1103 \tTraning Loss: 0.000304399203741923\n",
            "1104 \tTraning Loss: 0.0003892664972227067\n",
            "1105 \tTraning Loss: 0.00043148250551894307\n",
            "1106 \tTraning Loss: 0.0003827863256447017\n",
            "1107 \tTraning Loss: 0.0004972383612766862\n",
            "1108 \tTraning Loss: 0.00039238377939909697\n",
            "1109 \tTraning Loss: 0.00035676537663675845\n",
            "1110 \tTraning Loss: 0.0003792278585024178\n",
            "1111 \tTraning Loss: 0.00041783187771216035\n",
            "1112 \tTraning Loss: 0.0003942173789255321\n",
            "1113 \tTraning Loss: 0.00027942107408307493\n",
            "1114 \tTraning Loss: 0.00047424546210095286\n",
            "1115 \tTraning Loss: 0.0004555555060505867\n",
            "1116 \tTraning Loss: 0.0004630183393601328\n",
            "1117 \tTraning Loss: 0.0003336725349072367\n",
            "1118 \tTraning Loss: 0.00048320842324756086\n",
            "1119 \tTraning Loss: 0.0006301540415734053\n",
            "1120 \tTraning Loss: 0.0005548209883272648\n",
            "1121 \tTraning Loss: 0.0004267185868229717\n",
            "1122 \tTraning Loss: 0.00037075483123771846\n",
            "1123 \tTraning Loss: 0.0003692902682814747\n",
            "1124 \tTraning Loss: 0.0003219562058802694\n",
            "1125 \tTraning Loss: 0.0004272418736945838\n",
            "1126 \tTraning Loss: 0.00035894839675165713\n",
            "1127 \tTraning Loss: 0.00039605944766663015\n",
            "1128 \tTraning Loss: 0.00043328528408892453\n",
            "1129 \tTraning Loss: 0.0004247461329214275\n",
            "1130 \tTraning Loss: 0.0005696475272998214\n",
            "1131 \tTraning Loss: 0.0005191050586290658\n",
            "1132 \tTraning Loss: 0.0003067537327297032\n",
            "1133 \tTraning Loss: 0.0004801563045475632\n",
            "1134 \tTraning Loss: 0.0006519652088172734\n",
            "1135 \tTraning Loss: 0.0004190122417639941\n",
            "1136 \tTraning Loss: 0.00040480797179043293\n",
            "1137 \tTraning Loss: 0.00034712295746430755\n",
            "1138 \tTraning Loss: 0.0004096151969861239\n",
            "1139 \tTraning Loss: 0.00037854682886973023\n",
            "1140 \tTraning Loss: 0.00033417539088986814\n",
            "1141 \tTraning Loss: 0.0002956360694952309\n",
            "1142 \tTraning Loss: 0.0005014671478420496\n",
            "1143 \tTraning Loss: 0.0004901312058791518\n",
            "1144 \tTraning Loss: 0.00033457300742156804\n",
            "1145 \tTraning Loss: 0.00028870158712379634\n",
            "1146 \tTraning Loss: 0.00039849159657023847\n",
            "1147 \tTraning Loss: 0.0006052418611943722\n",
            "1148 \tTraning Loss: 0.0004127540159970522\n",
            "1149 \tTraning Loss: 0.000418317795265466\n",
            "1150 \tTraning Loss: 0.000271142489509657\n",
            "1151 \tTraning Loss: 0.00029075981001369655\n",
            "1152 \tTraning Loss: 0.00045324466191232204\n",
            "1153 \tTraning Loss: 0.0003978353051934391\n",
            "1154 \tTraning Loss: 0.00044117559446021914\n",
            "1155 \tTraning Loss: 0.000526656222064048\n",
            "1156 \tTraning Loss: 0.00035485700936987996\n",
            "1157 \tTraning Loss: 0.00037492287810891867\n",
            "1158 \tTraning Loss: 0.0005033481284044683\n",
            "1159 \tTraning Loss: 0.0004932106239721179\n",
            "1160 \tTraning Loss: 0.00046691580791957676\n",
            "1161 \tTraning Loss: 0.00038222019793465734\n",
            "1162 \tTraning Loss: 0.00038887743721716106\n",
            "1163 \tTraning Loss: 0.0004587358853314072\n",
            "1164 \tTraning Loss: 0.0003851052315440029\n",
            "1165 \tTraning Loss: 0.000427960796514526\n",
            "1166 \tTraning Loss: 0.00046627677511423826\n",
            "1167 \tTraning Loss: 0.00038315128767862916\n",
            "1168 \tTraning Loss: 0.0005155964754521847\n",
            "1169 \tTraning Loss: 0.00022725894814357162\n",
            "1170 \tTraning Loss: 0.0004140477394685149\n",
            "1171 \tTraning Loss: 0.00036466604797169566\n",
            "1172 \tTraning Loss: 0.00037657679058611393\n",
            "1173 \tTraning Loss: 0.00033525022445246577\n",
            "1174 \tTraning Loss: 0.0004259567067492753\n",
            "1175 \tTraning Loss: 0.00020414378377608955\n",
            "1176 \tTraning Loss: 0.00037873361725360155\n",
            "1177 \tTraning Loss: 0.00033867749152705073\n",
            "1178 \tTraning Loss: 0.0006713235634379089\n",
            "1179 \tTraning Loss: 0.00037724236608482897\n",
            "1180 \tTraning Loss: 0.0003827928740065545\n",
            "1181 \tTraning Loss: 0.0002688917447812855\n",
            "1182 \tTraning Loss: 0.000287108268821612\n",
            "1183 \tTraning Loss: 0.0004682347644120455\n",
            "1184 \tTraning Loss: 0.00043001410085707903\n",
            "1185 \tTraning Loss: 0.0003624164965003729\n",
            "1186 \tTraning Loss: 0.0004942606901749969\n",
            "1187 \tTraning Loss: 0.00035685658804140985\n",
            "1188 \tTraning Loss: 0.00035107877920381725\n",
            "1189 \tTraning Loss: 0.00040832144441083074\n",
            "1190 \tTraning Loss: 0.0005227350047789514\n",
            "1191 \tTraning Loss: 0.00046168715925887227\n",
            "1192 \tTraning Loss: 0.00033343720133416355\n",
            "1193 \tTraning Loss: 0.00038832452264614403\n",
            "1194 \tTraning Loss: 0.0005176302511245012\n",
            "1195 \tTraning Loss: 0.0004862982896156609\n",
            "1196 \tTraning Loss: 0.0004297003033570945\n",
            "1197 \tTraning Loss: 0.00037523044738918543\n",
            "1198 \tTraning Loss: 0.0005025888094678521\n",
            "1199 \tTraning Loss: 0.00042593490798026323\n",
            "1200 \tTraning Loss: 0.00039890120388008654\n",
            "1201 \tTraning Loss: 0.00042623665649443865\n",
            "1202 \tTraning Loss: 0.00038597232196480036\n",
            "1203 \tTraning Loss: 0.0005111089558340609\n",
            "1204 \tTraning Loss: 0.0006008744239807129\n",
            "1205 \tTraning Loss: 0.0005898684030398726\n",
            "1206 \tTraning Loss: 0.0004785153141710907\n",
            "1207 \tTraning Loss: 0.0004976907512173057\n",
            "1208 \tTraning Loss: 0.0003059140872210264\n",
            "1209 \tTraning Loss: 0.00042328075505793095\n",
            "1210 \tTraning Loss: 0.00028741429559886456\n",
            "1211 \tTraning Loss: 0.0003630697028711438\n",
            "1212 \tTraning Loss: 0.00044169818283990026\n",
            "1213 \tTraning Loss: 0.0004683926235884428\n",
            "1214 \tTraning Loss: 0.00033405606518499553\n",
            "1215 \tTraning Loss: 0.00035502694663591683\n",
            "1216 \tTraning Loss: 0.00036601469037123024\n",
            "1217 \tTraning Loss: 0.00048715222510509193\n",
            "1218 \tTraning Loss: 0.0003888579085469246\n",
            "1219 \tTraning Loss: 0.00044529043952934444\n",
            "1220 \tTraning Loss: 0.00043205954716540873\n",
            "1221 \tTraning Loss: 0.0003550171968527138\n",
            "1222 \tTraning Loss: 0.0002877288206946105\n",
            "1223 \tTraning Loss: 0.00034110594424419105\n",
            "1224 \tTraning Loss: 0.00033265003003180027\n",
            "1225 \tTraning Loss: 0.0004099124053027481\n",
            "1226 \tTraning Loss: 0.0003578978357836604\n",
            "1227 \tTraning Loss: 0.0003545965300872922\n",
            "1228 \tTraning Loss: 0.00041291944216936827\n",
            "1229 \tTraning Loss: 0.00032303587067872286\n",
            "1230 \tTraning Loss: 0.00040494793211109936\n",
            "1231 \tTraning Loss: 0.0003189945127815008\n",
            "1232 \tTraning Loss: 0.000401440222049132\n",
            "1233 \tTraning Loss: 0.00026920929667539895\n",
            "1234 \tTraning Loss: 0.00029112043557688594\n",
            "1235 \tTraning Loss: 0.0002528592594899237\n",
            "1236 \tTraning Loss: 0.00040646165143698454\n",
            "1237 \tTraning Loss: 0.0005455727805383503\n",
            "1238 \tTraning Loss: 0.000510034617036581\n",
            "1239 \tTraning Loss: 0.0005137716070748866\n",
            "1240 \tTraning Loss: 0.00048342059017159045\n",
            "1241 \tTraning Loss: 0.0003796746314037591\n",
            "1242 \tTraning Loss: 0.0004977407515980303\n",
            "1243 \tTraning Loss: 0.0004566549032460898\n",
            "1244 \tTraning Loss: 0.00035595783265307546\n",
            "1245 \tTraning Loss: 0.00041727005736902356\n",
            "1246 \tTraning Loss: 0.00048653612611815333\n",
            "1247 \tTraning Loss: 0.0005113232764415443\n",
            "1248 \tTraning Loss: 0.0004084929823875427\n",
            "1249 \tTraning Loss: 0.00034208502620458603\n",
            "1250 \tTraning Loss: 0.00045301730278879404\n",
            "1251 \tTraning Loss: 0.0002924982982221991\n",
            "1252 \tTraning Loss: 0.0004305462643969804\n",
            "1253 \tTraning Loss: 0.0004989465815015137\n",
            "1254 \tTraning Loss: 0.0003601186617743224\n",
            "1255 \tTraning Loss: 0.0005085543380118906\n",
            "1256 \tTraning Loss: 0.00034606881672516465\n",
            "1257 \tTraning Loss: 0.00044731033267453313\n",
            "1258 \tTraning Loss: 0.0003291225293651223\n",
            "1259 \tTraning Loss: 0.0003939279413316399\n",
            "1260 \tTraning Loss: 0.00048329163109883666\n",
            "1261 \tTraning Loss: 0.00037812412483617663\n",
            "1262 \tTraning Loss: 0.0005734532605856657\n",
            "1263 \tTraning Loss: 0.00045974820386618376\n",
            "1264 \tTraning Loss: 0.00038528002914972603\n",
            "1265 \tTraning Loss: 0.0004849638498853892\n",
            "1266 \tTraning Loss: 0.0004134766641072929\n",
            "1267 \tTraning Loss: 0.0004102755046915263\n",
            "1268 \tTraning Loss: 0.0004030178242828697\n",
            "1269 \tTraning Loss: 0.00037809216883033514\n",
            "1270 \tTraning Loss: 0.0005341179203242064\n",
            "1271 \tTraning Loss: 0.0003272084577474743\n",
            "1272 \tTraning Loss: 0.0003730455064214766\n",
            "1273 \tTraning Loss: 0.0003615072346292436\n",
            "1274 \tTraning Loss: 0.00036686036037281156\n",
            "1275 \tTraning Loss: 0.00035697795101441443\n",
            "1276 \tTraning Loss: 0.000497725501190871\n",
            "1277 \tTraning Loss: 0.0002669157402124256\n",
            "1278 \tTraning Loss: 0.0003432452795095742\n",
            "1279 \tTraning Loss: 0.0003351918130647391\n",
            "1280 \tTraning Loss: 0.0003672117309179157\n",
            "1281 \tTraning Loss: 0.0003469105577096343\n",
            "1282 \tTraning Loss: 0.00033341191010549664\n",
            "1283 \tTraning Loss: 0.000448752281954512\n",
            "1284 \tTraning Loss: 0.0003619580820668489\n",
            "1285 \tTraning Loss: 0.0005155571852810681\n",
            "1286 \tTraning Loss: 0.00041239141137339175\n",
            "1287 \tTraning Loss: 0.0004068521084263921\n",
            "1288 \tTraning Loss: 0.000510459765791893\n",
            "1289 \tTraning Loss: 0.00042418998782522976\n",
            "1290 \tTraning Loss: 0.0004980324301868677\n",
            "1291 \tTraning Loss: 0.00033913328661583364\n",
            "1292 \tTraning Loss: 0.00031205188133753836\n",
            "1293 \tTraning Loss: 0.0002625916968099773\n",
            "1294 \tTraning Loss: 0.0004983711405657232\n",
            "1295 \tTraning Loss: 0.000338231329806149\n",
            "1296 \tTraning Loss: 0.00045057389070279896\n",
            "1297 \tTraning Loss: 0.00028671647305600345\n",
            "1298 \tTraning Loss: 0.0004268030170351267\n",
            "1299 \tTraning Loss: 0.0004872171557508409\n",
            "1300 \tTraning Loss: 0.0002858227235265076\n",
            "1301 \tTraning Loss: 0.00042630996904335916\n",
            "1302 \tTraning Loss: 0.00029206715407781303\n",
            "1303 \tTraning Loss: 0.00034561651409603655\n",
            "1304 \tTraning Loss: 0.00042697263415902853\n",
            "1305 \tTraning Loss: 0.0005926871090196073\n",
            "1306 \tTraning Loss: 0.00023909196897875518\n",
            "1307 \tTraning Loss: 0.0003679649962577969\n",
            "1308 \tTraning Loss: 0.0005030832253396511\n",
            "1309 \tTraning Loss: 0.00045264794607646763\n",
            "1310 \tTraning Loss: 0.00046072728582657874\n",
            "1311 \tTraning Loss: 0.00026848429115489125\n",
            "1312 \tTraning Loss: 0.0004225728625897318\n",
            "1313 \tTraning Loss: 0.0006092111580073833\n",
            "1314 \tTraning Loss: 0.00043709733290597796\n",
            "1315 \tTraning Loss: 0.00040389143396168947\n",
            "1316 \tTraning Loss: 0.00037689844612032175\n",
            "1317 \tTraning Loss: 0.00034448690712451935\n",
            "1318 \tTraning Loss: 0.0003966779913753271\n",
            "1319 \tTraning Loss: 0.0003389458288438618\n",
            "1320 \tTraning Loss: 0.0001865996018750593\n",
            "1321 \tTraning Loss: 0.00037341099232435226\n",
            "1322 \tTraning Loss: 0.0003452441596891731\n",
            "1323 \tTraning Loss: 0.00041564545244909823\n",
            "1324 \tTraning Loss: 0.0005697439191862941\n",
            "1325 \tTraning Loss: 0.00041400230838917196\n",
            "1326 \tTraning Loss: 0.00033405155409127474\n",
            "1327 \tTraning Loss: 0.00036646780790761113\n",
            "1328 \tTraning Loss: 0.0003881104930769652\n",
            "1329 \tTraning Loss: 0.00034420235897414386\n",
            "1330 \tTraning Loss: 0.00048791582230478525\n",
            "1331 \tTraning Loss: 0.0003723290574271232\n",
            "1332 \tTraning Loss: 0.00037210635491646826\n",
            "1333 \tTraning Loss: 0.0004425944644026458\n",
            "1334 \tTraning Loss: 0.00030940340366214514\n",
            "1335 \tTraning Loss: 0.0003821304126176983\n",
            "1336 \tTraning Loss: 0.0002927073510363698\n",
            "1337 \tTraning Loss: 0.00033864311990328133\n",
            "1338 \tTraning Loss: 0.0004962100647389889\n",
            "1339 \tTraning Loss: 0.0004802076146006584\n",
            "1340 \tTraning Loss: 0.0003854838141705841\n",
            "1341 \tTraning Loss: 0.00024010232300497591\n",
            "1342 \tTraning Loss: 0.0003258588840253651\n",
            "1343 \tTraning Loss: 0.0002998383715748787\n",
            "1344 \tTraning Loss: 0.00017489471065346152\n",
            "1345 \tTraning Loss: 0.00040952107519842684\n",
            "1346 \tTraning Loss: 0.00036257063038647175\n",
            "1347 \tTraning Loss: 0.00048737871111370623\n",
            "1348 \tTraning Loss: 0.00028249656315892935\n",
            "1349 \tTraning Loss: 0.00031135266181081533\n",
            "1350 \tTraning Loss: 0.0002856449573300779\n",
            "1351 \tTraning Loss: 0.00038319217856042087\n",
            "1352 \tTraning Loss: 0.0004514495958574116\n",
            "1353 \tTraning Loss: 0.0003047800564672798\n",
            "1354 \tTraning Loss: 0.00035557872615754604\n",
            "1355 \tTraning Loss: 0.00029800983611494303\n",
            "1356 \tTraning Loss: 0.00041759759187698364\n",
            "1357 \tTraning Loss: 0.00032615274540148675\n",
            "1358 \tTraning Loss: 0.0003322242700960487\n",
            "1359 \tTraning Loss: 0.0004362095787655562\n",
            "1360 \tTraning Loss: 0.0003241795056965202\n",
            "1361 \tTraning Loss: 0.0003889707149937749\n",
            "1362 \tTraning Loss: 0.0004939051577821374\n",
            "1363 \tTraning Loss: 0.000481859955471009\n",
            "1364 \tTraning Loss: 0.0004555477644316852\n",
            "1365 \tTraning Loss: 0.0003636915935203433\n",
            "1366 \tTraning Loss: 0.0002996455004904419\n",
            "1367 \tTraning Loss: 0.0004101387457922101\n",
            "1368 \tTraning Loss: 0.0003053895488847047\n",
            "1369 \tTraning Loss: 0.00027383246924728155\n",
            "1370 \tTraning Loss: 0.00027189397951588035\n",
            "1371 \tTraning Loss: 0.0005264673964120448\n",
            "1372 \tTraning Loss: 0.00039677039603702724\n",
            "1373 \tTraning Loss: 0.0003718718944583088\n",
            "1374 \tTraning Loss: 0.0005223272019065917\n",
            "1375 \tTraning Loss: 0.0004233456857036799\n",
            "1376 \tTraning Loss: 0.0003173834702465683\n",
            "1377 \tTraning Loss: 0.0004045158566441387\n",
            "1378 \tTraning Loss: 0.00031254917848855257\n",
            "1379 \tTraning Loss: 0.0003343680582474917\n",
            "1380 \tTraning Loss: 0.0004933063173666596\n",
            "1381 \tTraning Loss: 0.00048049393808469176\n",
            "1382 \tTraning Loss: 0.00031520985066890717\n",
            "1383 \tTraning Loss: 0.0002872814075089991\n",
            "1384 \tTraning Loss: 0.0004764589248225093\n",
            "1385 \tTraning Loss: 0.00039691932033747435\n",
            "1386 \tTraning Loss: 0.0003832461661659181\n",
            "1387 \tTraning Loss: 0.00023121006961446255\n",
            "1388 \tTraning Loss: 0.0002916379598900676\n",
            "1389 \tTraning Loss: 0.0003498520818538964\n",
            "1390 \tTraning Loss: 0.00030473334481939673\n",
            "1391 \tTraning Loss: 0.0004445365921128541\n",
            "1392 \tTraning Loss: 0.00027896524989046156\n",
            "1393 \tTraning Loss: 0.000568013871088624\n",
            "1394 \tTraning Loss: 0.0002635906566865742\n",
            "1395 \tTraning Loss: 0.00047056059702299535\n",
            "1396 \tTraning Loss: 0.00043870549416169524\n",
            "1397 \tTraning Loss: 0.0003135338192805648\n",
            "1398 \tTraning Loss: 0.00047978389193303883\n",
            "1399 \tTraning Loss: 0.00034706309088505805\n",
            "1400 \tTraning Loss: 0.0004280841676518321\n",
            "1401 \tTraning Loss: 0.000523319176863879\n",
            "1402 \tTraning Loss: 0.0004949289141222835\n",
            "1403 \tTraning Loss: 0.00043940168689005077\n",
            "1404 \tTraning Loss: 0.0004184514982625842\n",
            "1405 \tTraning Loss: 0.00046265541459433734\n",
            "1406 \tTraning Loss: 0.0004767704813275486\n",
            "1407 \tTraning Loss: 0.00029668162460438907\n",
            "1408 \tTraning Loss: 0.0003119624743703753\n",
            "1409 \tTraning Loss: 0.00041569594759494066\n",
            "1410 \tTraning Loss: 0.0004161971155554056\n",
            "1411 \tTraning Loss: 0.0005222383188083768\n",
            "1412 \tTraning Loss: 0.00040371858631260693\n",
            "1413 \tTraning Loss: 0.0003535143914632499\n",
            "1414 \tTraning Loss: 0.00037493836134672165\n",
            "1415 \tTraning Loss: 0.00035452950396575034\n",
            "1416 \tTraning Loss: 0.0003515077696647495\n",
            "1417 \tTraning Loss: 0.00033651330159045756\n",
            "1418 \tTraning Loss: 0.00037768433685414493\n",
            "1419 \tTraning Loss: 0.0004781219467986375\n",
            "1420 \tTraning Loss: 0.00041491107549518347\n",
            "1421 \tTraning Loss: 0.0002820177760440856\n",
            "1422 \tTraning Loss: 0.0005278201424516737\n",
            "1423 \tTraning Loss: 0.00041590631008148193\n",
            "1424 \tTraning Loss: 0.0003809171903412789\n",
            "1425 \tTraning Loss: 0.00031808551284484565\n",
            "1426 \tTraning Loss: 0.0004743973258882761\n",
            "1427 \tTraning Loss: 0.00040553894359618425\n",
            "1428 \tTraning Loss: 0.0005021465476602316\n",
            "1429 \tTraning Loss: 0.0004009016265626997\n",
            "1430 \tTraning Loss: 0.0004587889416143298\n",
            "1431 \tTraning Loss: 0.00040530820842832327\n",
            "1432 \tTraning Loss: 0.00033074626117013395\n",
            "1433 \tTraning Loss: 0.00020971978665329516\n",
            "1434 \tTraning Loss: 0.0004506129480432719\n",
            "1435 \tTraning Loss: 0.00035736936843022704\n",
            "1436 \tTraning Loss: 0.0003199975471943617\n",
            "1437 \tTraning Loss: 0.0004821555339731276\n",
            "1438 \tTraning Loss: 0.0004014566948171705\n",
            "1439 \tTraning Loss: 0.00035725170164369047\n",
            "1440 \tTraning Loss: 0.0004132945032324642\n",
            "1441 \tTraning Loss: 0.00023382622748613358\n",
            "1442 \tTraning Loss: 0.0002009694289881736\n",
            "1443 \tTraning Loss: 0.0003164173976983875\n",
            "1444 \tTraning Loss: 0.00042135253897868097\n",
            "1445 \tTraning Loss: 0.0005744000663980842\n",
            "1446 \tTraning Loss: 0.0004081219376530498\n",
            "1447 \tTraning Loss: 0.00040859333239495754\n",
            "1448 \tTraning Loss: 0.00038759413291700184\n",
            "1449 \tTraning Loss: 0.00037042968324385583\n",
            "1450 \tTraning Loss: 0.0003566863015294075\n",
            "1451 \tTraning Loss: 0.00034123167279176414\n",
            "1452 \tTraning Loss: 0.0002654092095326632\n",
            "1453 \tTraning Loss: 0.00031259472598321736\n",
            "1454 \tTraning Loss: 0.00034391588997095823\n",
            "1455 \tTraning Loss: 0.00036154917324893177\n",
            "1456 \tTraning Loss: 0.0004898456390947104\n",
            "1457 \tTraning Loss: 0.0002432702895021066\n",
            "1458 \tTraning Loss: 0.000377117918105796\n",
            "1459 \tTraning Loss: 0.00038230165955610573\n",
            "1460 \tTraning Loss: 0.0002327969705220312\n",
            "1461 \tTraning Loss: 0.0002562109148129821\n",
            "1462 \tTraning Loss: 0.0003800709673669189\n",
            "1463 \tTraning Loss: 0.0003890672232955694\n",
            "1464 \tTraning Loss: 0.00035816460149362683\n",
            "1465 \tTraning Loss: 0.0005134789389558136\n",
            "1466 \tTraning Loss: 0.0003950848476961255\n",
            "1467 \tTraning Loss: 0.0004743908066302538\n",
            "1468 \tTraning Loss: 0.0003791861527133733\n",
            "1469 \tTraning Loss: 0.0003080706810578704\n",
            "1470 \tTraning Loss: 0.00034188589779660106\n",
            "1471 \tTraning Loss: 0.0003582868375815451\n",
            "1472 \tTraning Loss: 0.0003929539816454053\n",
            "1473 \tTraning Loss: 0.00030186568619683385\n",
            "1474 \tTraning Loss: 0.0005427425494417548\n",
            "1475 \tTraning Loss: 0.0003057770663872361\n",
            "1476 \tTraning Loss: 0.0003718503285199404\n",
            "1477 \tTraning Loss: 0.0003482071915641427\n",
            "1478 \tTraning Loss: 0.0004765783669427037\n",
            "1479 \tTraning Loss: 0.0003993614809587598\n",
            "1480 \tTraning Loss: 0.00036651757545769215\n",
            "1481 \tTraning Loss: 0.00030935273389331996\n",
            "1482 \tTraning Loss: 0.0003412206715438515\n",
            "1483 \tTraning Loss: 0.0003580267366487533\n",
            "1484 \tTraning Loss: 0.0003967331431340426\n",
            "1485 \tTraning Loss: 0.0003633207525126636\n",
            "1486 \tTraning Loss: 0.0003847128537017852\n",
            "1487 \tTraning Loss: 0.0003229136345908046\n",
            "1488 \tTraning Loss: 0.0003024468314833939\n",
            "1489 \tTraning Loss: 0.0003233693423680961\n",
            "1490 \tTraning Loss: 0.000451241503469646\n",
            "1491 \tTraning Loss: 0.0003689478326123208\n",
            "1492 \tTraning Loss: 0.00030819495441392064\n",
            "1493 \tTraning Loss: 0.0003935415006708354\n",
            "1494 \tTraning Loss: 0.0004796760913450271\n",
            "1495 \tTraning Loss: 0.00035255838884040713\n",
            "1496 \tTraning Loss: 0.00037292263004928827\n",
            "1497 \tTraning Loss: 0.00039846450090408325\n",
            "1498 \tTraning Loss: 0.0003925241471733898\n",
            "1499 \tTraning Loss: 0.00048770805005915463\n",
            "1500 \tTraning Loss: 0.0003397201362531632\n",
            "1501 \tTraning Loss: 0.00036077731056138873\n",
            "1502 \tTraning Loss: 0.00030593026895076036\n",
            "1503 \tTraning Loss: 0.00045767813571728766\n",
            "1504 \tTraning Loss: 0.00042251148261129856\n",
            "1505 \tTraning Loss: 0.0004497874469961971\n",
            "1506 \tTraning Loss: 0.000453010608907789\n",
            "1507 \tTraning Loss: 0.0004263509763404727\n",
            "1508 \tTraning Loss: 0.0003760811814572662\n",
            "1509 \tTraning Loss: 0.00021344597917050123\n",
            "1510 \tTraning Loss: 0.000332208612235263\n",
            "1511 \tTraning Loss: 0.000333236763253808\n",
            "1512 \tTraning Loss: 0.000547904463019222\n",
            "1513 \tTraning Loss: 0.00041163936839438975\n",
            "1514 \tTraning Loss: 0.00028954955632798374\n",
            "1515 \tTraning Loss: 0.0003755335637833923\n",
            "1516 \tTraning Loss: 0.0003662934759631753\n",
            "1517 \tTraning Loss: 0.00034991788561455905\n",
            "1518 \tTraning Loss: 0.00046554484288208187\n",
            "1519 \tTraning Loss: 0.0003926991776097566\n",
            "1520 \tTraning Loss: 0.00031310145277529955\n",
            "1521 \tTraning Loss: 0.00030207642703317106\n",
            "1522 \tTraning Loss: 0.0003282323887106031\n",
            "1523 \tTraning Loss: 0.0004601031250786036\n",
            "1524 \tTraning Loss: 0.0003796178789343685\n",
            "1525 \tTraning Loss: 0.00027602390036918223\n",
            "1526 \tTraning Loss: 0.00027179071912541986\n",
            "1527 \tTraning Loss: 0.00029298398294486105\n",
            "1528 \tTraning Loss: 0.000560430926270783\n",
            "1529 \tTraning Loss: 0.0003906766069121659\n",
            "1530 \tTraning Loss: 0.00032890267902985215\n",
            "1531 \tTraning Loss: 0.00023335502191912383\n",
            "1532 \tTraning Loss: 0.0002966558386106044\n",
            "1533 \tTraning Loss: 0.0004143683472648263\n",
            "1534 \tTraning Loss: 0.0003479411534499377\n",
            "1535 \tTraning Loss: 0.00021504779579117894\n",
            "1536 \tTraning Loss: 0.000481818919070065\n",
            "1537 \tTraning Loss: 0.0004485342651605606\n",
            "1538 \tTraning Loss: 0.0002951835922431201\n",
            "1539 \tTraning Loss: 0.0003134185099042952\n",
            "1540 \tTraning Loss: 0.00043966257362626493\n",
            "1541 \tTraning Loss: 0.0003383111907169223\n",
            "1542 \tTraning Loss: 0.00025060249026864767\n",
            "1543 \tTraning Loss: 0.0003752069897018373\n",
            "1544 \tTraning Loss: 0.00044885973329655826\n",
            "1545 \tTraning Loss: 0.00032672591623850167\n",
            "1546 \tTraning Loss: 0.0002805459371302277\n",
            "1547 \tTraning Loss: 0.0004929423448629677\n",
            "1548 \tTraning Loss: 0.0002816044434439391\n",
            "1549 \tTraning Loss: 0.0003921999305021018\n",
            "1550 \tTraning Loss: 0.00041412870632484555\n",
            "1551 \tTraning Loss: 0.00032599002588540316\n",
            "1552 \tTraning Loss: 0.0002971645735669881\n",
            "1553 \tTraning Loss: 0.0005629810621030629\n",
            "1554 \tTraning Loss: 0.0004176081856712699\n",
            "1555 \tTraning Loss: 0.0005282749189063907\n",
            "1556 \tTraning Loss: 0.0005324503290466964\n",
            "1557 \tTraning Loss: 0.00040893160621635616\n",
            "1558 \tTraning Loss: 0.000489040045067668\n",
            "1559 \tTraning Loss: 0.00036370038287714124\n",
            "1560 \tTraning Loss: 0.00036290581920184195\n",
            "1561 \tTraning Loss: 0.0002800037036649883\n",
            "1562 \tTraning Loss: 0.0004595969512593001\n",
            "1563 \tTraning Loss: 0.0002487909805495292\n",
            "1564 \tTraning Loss: 0.0004405815270729363\n",
            "1565 \tTraning Loss: 0.0004345730703789741\n",
            "1566 \tTraning Loss: 0.00032513318001292646\n",
            "1567 \tTraning Loss: 0.00028810088406316936\n",
            "1568 \tTraning Loss: 0.00033421863918192685\n",
            "1569 \tTraning Loss: 0.0003768892493098974\n",
            "1570 \tTraning Loss: 0.00032452226150780916\n",
            "1571 \tTraning Loss: 0.0004913862212561071\n",
            "1572 \tTraning Loss: 0.0004122981335967779\n",
            "1573 \tTraning Loss: 0.00047470227582380176\n",
            "1574 \tTraning Loss: 0.0004181322583463043\n",
            "1575 \tTraning Loss: 0.0002405572304269299\n",
            "1576 \tTraning Loss: 0.000348253088304773\n",
            "1577 \tTraning Loss: 0.0004505165561567992\n",
            "1578 \tTraning Loss: 0.0003385163436178118\n",
            "1579 \tTraning Loss: 0.000427099468652159\n",
            "1580 \tTraning Loss: 0.00026297467411495745\n",
            "1581 \tTraning Loss: 0.00020173058146610856\n",
            "1582 \tTraning Loss: 0.000337827717885375\n",
            "1583 \tTraning Loss: 0.00028917755116708577\n",
            "1584 \tTraning Loss: 0.0002162116434192285\n",
            "1585 \tTraning Loss: 0.0003194129385519773\n",
            "1586 \tTraning Loss: 0.00038252456579357386\n",
            "1587 \tTraning Loss: 0.0004025892121717334\n",
            "1588 \tTraning Loss: 0.00036129236104898155\n",
            "1589 \tTraning Loss: 0.00041275747935287654\n",
            "1590 \tTraning Loss: 0.0002375414624111727\n",
            "1591 \tTraning Loss: 0.00033684406662359834\n",
            "1592 \tTraning Loss: 0.00044548686128109694\n",
            "1593 \tTraning Loss: 0.0002680363832041621\n",
            "1594 \tTraning Loss: 0.00032251846278086305\n",
            "1595 \tTraning Loss: 0.00033406104194000363\n",
            "1596 \tTraning Loss: 0.0004471360589377582\n",
            "1597 \tTraning Loss: 0.0003995815641246736\n",
            "1598 \tTraning Loss: 0.00023025186965242028\n",
            "1599 \tTraning Loss: 0.0002904439461417496\n",
            "1600 \tTraning Loss: 0.0003275361959822476\n",
            "1601 \tTraning Loss: 0.00040385685861110687\n",
            "1602 \tTraning Loss: 0.0003882058954332024\n",
            "1603 \tTraning Loss: 0.0003990093246102333\n",
            "1604 \tTraning Loss: 0.00030279075144790113\n",
            "1605 \tTraning Loss: 0.0003067599900532514\n",
            "1606 \tTraning Loss: 0.0003303393314126879\n",
            "1607 \tTraning Loss: 0.0003207035770174116\n",
            "1608 \tTraning Loss: 0.00031982758082449436\n",
            "1609 \tTraning Loss: 0.0003810739435721189\n",
            "1610 \tTraning Loss: 0.00023704426712356508\n",
            "1611 \tTraning Loss: 0.0003449538489803672\n",
            "1612 \tTraning Loss: 0.00035064812982454896\n",
            "1613 \tTraning Loss: 0.0004083506064489484\n",
            "1614 \tTraning Loss: 0.00044196672388352454\n",
            "1615 \tTraning Loss: 0.00036953508970327675\n",
            "1616 \tTraning Loss: 0.0003284157137386501\n",
            "1617 \tTraning Loss: 0.0004093184834346175\n",
            "1618 \tTraning Loss: 0.00032426128746010363\n",
            "1619 \tTraning Loss: 0.0003821313730441034\n",
            "1620 \tTraning Loss: 0.00032110867323353887\n",
            "1621 \tTraning Loss: 0.0002905320725403726\n",
            "1622 \tTraning Loss: 0.00037851236993446946\n",
            "1623 \tTraning Loss: 0.000569110969081521\n",
            "1624 \tTraning Loss: 0.00048705682274885476\n",
            "1625 \tTraning Loss: 0.00041803918429650366\n",
            "1626 \tTraning Loss: 0.00027286887052468956\n",
            "1627 \tTraning Loss: 0.0003578477480914444\n",
            "1628 \tTraning Loss: 0.000479020964121446\n",
            "1629 \tTraning Loss: 0.00036827471922151744\n",
            "1630 \tTraning Loss: 0.0003233026945963502\n",
            "1631 \tTraning Loss: 0.00037344160955399275\n",
            "1632 \tTraning Loss: 0.00027570713427849114\n",
            "1633 \tTraning Loss: 0.0002828755241353065\n",
            "1634 \tTraning Loss: 0.0002537445689085871\n",
            "1635 \tTraning Loss: 0.0004458752227947116\n",
            "1636 \tTraning Loss: 0.00040531851118430495\n",
            "1637 \tTraning Loss: 0.0003435618127696216\n",
            "1638 \tTraning Loss: 0.00048506789607927203\n",
            "1639 \tTraning Loss: 0.00017613037198316306\n",
            "1640 \tTraning Loss: 0.00034669876913540065\n",
            "1641 \tTraning Loss: 0.0004081818333361298\n",
            "1642 \tTraning Loss: 0.0004667998000513762\n",
            "1643 \tTraning Loss: 0.00037368093035183847\n",
            "1644 \tTraning Loss: 0.0005225241766311228\n",
            "1645 \tTraning Loss: 0.0002807818236760795\n",
            "1646 \tTraning Loss: 0.00037084906944073737\n",
            "1647 \tTraning Loss: 0.0004769067163579166\n",
            "1648 \tTraning Loss: 0.0003968565142713487\n",
            "1649 \tTraning Loss: 0.0002939036930911243\n",
            "1650 \tTraning Loss: 0.0004904059460386634\n",
            "1651 \tTraning Loss: 0.0002538878470659256\n",
            "1652 \tTraning Loss: 0.00030428997706621885\n",
            "1653 \tTraning Loss: 0.00017521133122500032\n",
            "1654 \tTraning Loss: 0.00021623140492010862\n",
            "1655 \tTraning Loss: 0.0004158992087468505\n",
            "1656 \tTraning Loss: 0.0004552991013042629\n",
            "1657 \tTraning Loss: 0.00030416512163355947\n",
            "1658 \tTraning Loss: 0.00047236529644578695\n",
            "1659 \tTraning Loss: 0.00036785961128771305\n",
            "1660 \tTraning Loss: 0.0004319967410992831\n",
            "1661 \tTraning Loss: 0.00036027803434990346\n",
            "1662 \tTraning Loss: 0.0004117251664865762\n",
            "1663 \tTraning Loss: 0.0004657191748265177\n",
            "1664 \tTraning Loss: 0.00031672048498876393\n",
            "1665 \tTraning Loss: 0.00037950699334032834\n",
            "1666 \tTraning Loss: 0.00034916450385935605\n",
            "1667 \tTraning Loss: 0.0003713356563821435\n",
            "1668 \tTraning Loss: 0.00028564277454279363\n",
            "1669 \tTraning Loss: 0.0002326918620383367\n",
            "1670 \tTraning Loss: 0.000354511576006189\n",
            "1671 \tTraning Loss: 0.00027120980666950345\n",
            "1672 \tTraning Loss: 0.00043227392598055303\n",
            "1673 \tTraning Loss: 0.00033007783349603415\n",
            "1674 \tTraning Loss: 0.0004537469649221748\n",
            "1675 \tTraning Loss: 0.00041270320070907474\n",
            "1676 \tTraning Loss: 0.0002606778871268034\n",
            "1677 \tTraning Loss: 0.0002472182968631387\n",
            "1678 \tTraning Loss: 0.00045394926564767957\n",
            "1679 \tTraning Loss: 0.00029085666756145656\n",
            "1680 \tTraning Loss: 0.00033050100319087505\n",
            "1681 \tTraning Loss: 0.00042005404247902334\n",
            "1682 \tTraning Loss: 0.0003928939113393426\n",
            "1683 \tTraning Loss: 0.00034507326199673116\n",
            "1684 \tTraning Loss: 0.0003747393493540585\n",
            "1685 \tTraning Loss: 0.00030921940924599767\n",
            "1686 \tTraning Loss: 0.0002623085165396333\n",
            "1687 \tTraning Loss: 0.0005014950875192881\n",
            "1688 \tTraning Loss: 0.0004223453579470515\n",
            "1689 \tTraning Loss: 0.00025785312755033374\n",
            "1690 \tTraning Loss: 0.00031108653638511896\n",
            "1691 \tTraning Loss: 0.0003124917857348919\n",
            "1692 \tTraning Loss: 0.0004222856368869543\n",
            "1693 \tTraning Loss: 0.0003911831881850958\n",
            "1694 \tTraning Loss: 0.00047099002404138446\n",
            "1695 \tTraning Loss: 0.00031476831645704806\n",
            "1696 \tTraning Loss: 0.0002592892269603908\n",
            "1697 \tTraning Loss: 0.0001983269612537697\n",
            "1698 \tTraning Loss: 0.00043343615834601223\n",
            "1699 \tTraning Loss: 0.00039539497811347246\n",
            "1700 \tTraning Loss: 0.00043655576882883906\n",
            "1701 \tTraning Loss: 0.00046469003427773714\n",
            "1702 \tTraning Loss: 0.0002692949492484331\n",
            "1703 \tTraning Loss: 0.0003257395583204925\n",
            "1704 \tTraning Loss: 0.0002457340306136757\n",
            "1705 \tTraning Loss: 0.00026852189330384135\n",
            "1706 \tTraning Loss: 0.00047050206921994686\n",
            "1707 \tTraning Loss: 0.0003993225400336087\n",
            "1708 \tTraning Loss: 0.0003020814328920096\n",
            "1709 \tTraning Loss: 0.0003626830584835261\n",
            "1710 \tTraning Loss: 0.0003241377999074757\n",
            "1711 \tTraning Loss: 0.00020459441293496639\n",
            "1712 \tTraning Loss: 0.0003936048597097397\n",
            "1713 \tTraning Loss: 0.0004500877985265106\n",
            "1714 \tTraning Loss: 0.0002796472981572151\n",
            "1715 \tTraning Loss: 0.000507785938680172\n",
            "1716 \tTraning Loss: 0.0002871036995202303\n",
            "1717 \tTraning Loss: 0.00044591620098799467\n",
            "1718 \tTraning Loss: 0.00022786614135839045\n",
            "1719 \tTraning Loss: 0.0004088048299308866\n",
            "1720 \tTraning Loss: 0.00041492600576020777\n",
            "1721 \tTraning Loss: 0.00027102036983706057\n",
            "1722 \tTraning Loss: 0.0003075131098739803\n",
            "1723 \tTraning Loss: 0.00025147615815512836\n",
            "1724 \tTraning Loss: 0.0003195418103132397\n",
            "1725 \tTraning Loss: 0.00038930148002691567\n",
            "1726 \tTraning Loss: 0.00024568926892243326\n",
            "1727 \tTraning Loss: 0.0004160097159910947\n",
            "1728 \tTraning Loss: 0.0004004575021099299\n",
            "1729 \tTraning Loss: 0.000343429041095078\n",
            "1730 \tTraning Loss: 0.00030469876946881413\n",
            "1731 \tTraning Loss: 0.0004532833117991686\n",
            "1732 \tTraning Loss: 0.00038826209492981434\n",
            "1733 \tTraning Loss: 0.000423651683377102\n",
            "1734 \tTraning Loss: 0.00029781501507386565\n",
            "1735 \tTraning Loss: 0.00044191282358951867\n",
            "1736 \tTraning Loss: 0.00040985082159750164\n",
            "1737 \tTraning Loss: 0.000502267386764288\n",
            "1738 \tTraning Loss: 0.0002336962934350595\n",
            "1739 \tTraning Loss: 0.0003709531156346202\n",
            "1740 \tTraning Loss: 0.00024011429923120886\n",
            "1741 \tTraning Loss: 0.00040144682861864567\n",
            "1742 \tTraning Loss: 0.0002980807039421052\n",
            "1743 \tTraning Loss: 0.00037755462108179927\n",
            "1744 \tTraning Loss: 0.00040368904592469335\n",
            "1745 \tTraning Loss: 0.0003493807453196496\n",
            "1746 \tTraning Loss: 0.0004146376741118729\n",
            "1747 \tTraning Loss: 0.0004336757119745016\n",
            "1748 \tTraning Loss: 0.0003243820392526686\n",
            "1749 \tTraning Loss: 0.00037894141860306263\n",
            "1750 \tTraning Loss: 0.00028065210790373385\n",
            "1751 \tTraning Loss: 0.00021845260926056653\n",
            "1752 \tTraning Loss: 0.00013344315811991692\n",
            "1753 \tTraning Loss: 0.0003650456201285124\n",
            "1754 \tTraning Loss: 0.0004360327438917011\n",
            "1755 \tTraning Loss: 0.0003585372178349644\n",
            "1756 \tTraning Loss: 0.00036904349690303206\n",
            "1757 \tTraning Loss: 0.00045777196646668017\n",
            "1758 \tTraning Loss: 0.00029181662830524147\n",
            "1759 \tTraning Loss: 0.0003578159958124161\n",
            "1760 \tTraning Loss: 0.00039905536687001586\n",
            "1761 \tTraning Loss: 0.00033689348492771387\n",
            "1762 \tTraning Loss: 0.0003064920601900667\n",
            "1763 \tTraning Loss: 0.0003020667936652899\n",
            "1764 \tTraning Loss: 0.0003974007850047201\n",
            "1765 \tTraning Loss: 0.0003229899739380926\n",
            "1766 \tTraning Loss: 0.0003718402294907719\n",
            "1767 \tTraning Loss: 0.0003966132062487304\n",
            "1768 \tTraning Loss: 0.00029191942303441465\n",
            "1769 \tTraning Loss: 0.00048151882947422564\n",
            "1770 \tTraning Loss: 0.000319237558869645\n",
            "1771 \tTraning Loss: 0.0003173183649778366\n",
            "1772 \tTraning Loss: 0.0003289741580374539\n",
            "1773 \tTraning Loss: 0.00029260708834044635\n",
            "1774 \tTraning Loss: 0.0003883411700371653\n",
            "1775 \tTraning Loss: 0.0003294711059425026\n",
            "1776 \tTraning Loss: 0.0003206216497346759\n",
            "1777 \tTraning Loss: 0.0003631219151429832\n",
            "1778 \tTraning Loss: 0.0004870722477789968\n",
            "1779 \tTraning Loss: 0.0002702144847717136\n",
            "1780 \tTraning Loss: 0.00022497083409689367\n",
            "1781 \tTraning Loss: 0.00032348610693588853\n",
            "1782 \tTraning Loss: 0.0004803790943697095\n",
            "1783 \tTraning Loss: 0.00035923669929616153\n",
            "1784 \tTraning Loss: 0.00044291935046203434\n",
            "1785 \tTraning Loss: 0.00034047276130877435\n",
            "1786 \tTraning Loss: 0.0004612025513779372\n",
            "1787 \tTraning Loss: 0.00023520170361734927\n",
            "1788 \tTraning Loss: 0.0003938645531889051\n",
            "1789 \tTraning Loss: 0.00032043972169049084\n",
            "1790 \tTraning Loss: 0.00039425119757652283\n",
            "1791 \tTraning Loss: 0.0003803626459557563\n",
            "1792 \tTraning Loss: 0.00039830527384765446\n",
            "1793 \tTraning Loss: 0.00043238181388005614\n",
            "1794 \tTraning Loss: 0.00037246753345243633\n",
            "1795 \tTraning Loss: 0.00031989143462851644\n",
            "1796 \tTraning Loss: 0.0004312969685997814\n",
            "1797 \tTraning Loss: 0.00033996443380601704\n",
            "1798 \tTraning Loss: 0.00040330574847757816\n",
            "1799 \tTraning Loss: 0.00038811404374428093\n",
            "1800 \tTraning Loss: 0.0003885506885126233\n",
            "1801 \tTraning Loss: 0.0003456103731878102\n",
            "1802 \tTraning Loss: 0.0003262051322963089\n",
            "1803 \tTraning Loss: 0.00024604317150078714\n",
            "1804 \tTraning Loss: 0.0003781646955758333\n",
            "1805 \tTraning Loss: 0.0004458713228814304\n",
            "1806 \tTraning Loss: 0.000264281959971413\n",
            "1807 \tTraning Loss: 0.00045110328937880695\n",
            "1808 \tTraning Loss: 0.00046342931455001235\n",
            "1809 \tTraning Loss: 0.00029383436776697636\n",
            "1810 \tTraning Loss: 0.00029403893859125674\n",
            "1811 \tTraning Loss: 0.0003725613350979984\n",
            "1812 \tTraning Loss: 0.00027538678841665387\n",
            "1813 \tTraning Loss: 0.0003461747255641967\n",
            "1814 \tTraning Loss: 0.0003221462538931519\n",
            "1815 \tTraning Loss: 0.0002487163001205772\n",
            "1816 \tTraning Loss: 0.00032347484375350177\n",
            "1817 \tTraning Loss: 0.00023238219728227705\n",
            "1818 \tTraning Loss: 0.00029516962240450084\n",
            "1819 \tTraning Loss: 0.0003422671870794147\n",
            "1820 \tTraning Loss: 0.00038925014086999\n",
            "1821 \tTraning Loss: 0.00033459492260590196\n",
            "1822 \tTraning Loss: 0.0004479266353882849\n",
            "1823 \tTraning Loss: 0.0003908574872184545\n",
            "1824 \tTraning Loss: 0.00043540593469515443\n",
            "1825 \tTraning Loss: 0.00037520137266255915\n",
            "1826 \tTraning Loss: 0.0004949651774950325\n",
            "1827 \tTraning Loss: 0.0004203800926916301\n",
            "1828 \tTraning Loss: 0.0003389689663890749\n",
            "1829 \tTraning Loss: 0.00038693324313499033\n",
            "1830 \tTraning Loss: 0.0003536107833497226\n",
            "1831 \tTraning Loss: 0.00034655642230063677\n",
            "1832 \tTraning Loss: 0.0002709606196731329\n",
            "1833 \tTraning Loss: 0.00020477782527450472\n",
            "1834 \tTraning Loss: 0.0003943323390558362\n",
            "1835 \tTraning Loss: 0.00046411558287218213\n",
            "1836 \tTraning Loss: 0.0004356408608146012\n",
            "1837 \tTraning Loss: 0.0003838125558104366\n",
            "1838 \tTraning Loss: 0.0003729745512828231\n",
            "1839 \tTraning Loss: 0.0003555356233846396\n",
            "1840 \tTraning Loss: 0.0003307378210593015\n",
            "1841 \tTraning Loss: 0.00029105995781719685\n",
            "1842 \tTraning Loss: 0.000263043271843344\n",
            "1843 \tTraning Loss: 0.00046500787721015513\n",
            "1844 \tTraning Loss: 0.0004187504528090358\n",
            "1845 \tTraning Loss: 0.00043091687257401645\n",
            "1846 \tTraning Loss: 0.00033208151580765843\n",
            "1847 \tTraning Loss: 0.0002722593198996037\n",
            "1848 \tTraning Loss: 0.0003813750809058547\n",
            "1849 \tTraning Loss: 0.0003658364003058523\n",
            "1850 \tTraning Loss: 0.0003502469917293638\n",
            "1851 \tTraning Loss: 0.0001929706777445972\n",
            "1852 \tTraning Loss: 0.0003940106835216284\n",
            "1853 \tTraning Loss: 0.00034449531813152134\n",
            "1854 \tTraning Loss: 0.0003508457448333502\n",
            "1855 \tTraning Loss: 0.00031263765413314104\n",
            "1856 \tTraning Loss: 0.00033144716871902347\n",
            "1857 \tTraning Loss: 0.00026521788095124066\n",
            "1858 \tTraning Loss: 0.00031872381805442274\n",
            "1859 \tTraning Loss: 0.0003900804731529206\n",
            "1860 \tTraning Loss: 0.00033299069036729634\n",
            "1861 \tTraning Loss: 0.00030139953014440835\n",
            "1862 \tTraning Loss: 0.0003807330795098096\n",
            "1863 \tTraning Loss: 0.0003384201554581523\n",
            "1864 \tTraning Loss: 0.0003161804052069783\n",
            "1865 \tTraning Loss: 0.00026938290102407336\n",
            "1866 \tTraning Loss: 0.00037704978603869677\n",
            "1867 \tTraning Loss: 0.0002984805323649198\n",
            "1868 \tTraning Loss: 0.00043949842802248895\n",
            "1869 \tTraning Loss: 0.00040408727363683283\n",
            "1870 \tTraning Loss: 0.0003254801267758012\n",
            "1871 \tTraning Loss: 0.00035120933898724616\n",
            "1872 \tTraning Loss: 0.0004556383064482361\n",
            "1873 \tTraning Loss: 0.00034322048304602504\n",
            "1874 \tTraning Loss: 0.00036249987897463143\n",
            "1875 \tTraning Loss: 0.00038490683073177934\n",
            "1876 \tTraning Loss: 0.00036459320108406246\n",
            "1877 \tTraning Loss: 0.0004196613735985011\n",
            "1878 \tTraning Loss: 0.0003307782462798059\n",
            "1879 \tTraning Loss: 0.00041906916885636747\n",
            "1880 \tTraning Loss: 0.00026517914375290275\n",
            "1881 \tTraning Loss: 0.0003063462791033089\n",
            "1882 \tTraning Loss: 0.0003243825340177864\n",
            "1883 \tTraning Loss: 0.00028249688330106437\n",
            "1884 \tTraning Loss: 0.00021550178644247353\n",
            "1885 \tTraning Loss: 0.0004954047035425901\n",
            "1886 \tTraning Loss: 0.0003934685082640499\n",
            "1887 \tTraning Loss: 0.0002869091695174575\n",
            "1888 \tTraning Loss: 0.0003542292397469282\n",
            "1889 \tTraning Loss: 0.00042629282688722014\n",
            "1890 \tTraning Loss: 0.000242895184783265\n",
            "1891 \tTraning Loss: 0.0004286476469133049\n",
            "1892 \tTraning Loss: 0.00030328991124406457\n",
            "1893 \tTraning Loss: 0.00037518516182899475\n",
            "1894 \tTraning Loss: 0.0003056744171772152\n",
            "1895 \tTraning Loss: 0.000274000340141356\n",
            "1896 \tTraning Loss: 0.0002986607141792774\n",
            "1897 \tTraning Loss: 0.0003764852008316666\n",
            "1898 \tTraning Loss: 0.00030715102911926806\n",
            "1899 \tTraning Loss: 0.00022312829969450831\n",
            "1900 \tTraning Loss: 0.0003353914653416723\n",
            "1901 \tTraning Loss: 0.0004105631378479302\n",
            "1902 \tTraning Loss: 0.0002248548116767779\n",
            "1903 \tTraning Loss: 0.0004143119731452316\n",
            "1904 \tTraning Loss: 0.00030277183395810425\n",
            "1905 \tTraning Loss: 0.00031612880411557853\n",
            "1906 \tTraning Loss: 0.0003244941763114184\n",
            "1907 \tTraning Loss: 0.0003373002982698381\n",
            "1908 \tTraning Loss: 0.00043480965541675687\n",
            "1909 \tTraning Loss: 0.0001788991503417492\n",
            "1910 \tTraning Loss: 0.000283893576124683\n",
            "1911 \tTraning Loss: 0.00023209778009913862\n",
            "1912 \tTraning Loss: 0.00036283666850067675\n",
            "1913 \tTraning Loss: 0.00031943729845806956\n",
            "1914 \tTraning Loss: 0.00034281861735507846\n",
            "1915 \tTraning Loss: 0.00028005658532492816\n",
            "1916 \tTraning Loss: 0.00032305248896591365\n",
            "1917 \tTraning Loss: 0.000362491759005934\n",
            "1918 \tTraning Loss: 0.0003990117402281612\n",
            "1919 \tTraning Loss: 0.0004183369455859065\n",
            "1920 \tTraning Loss: 0.0003276031056884676\n",
            "1921 \tTraning Loss: 0.00046282316907308996\n",
            "1922 \tTraning Loss: 0.0003040420706383884\n",
            "1923 \tTraning Loss: 0.00031793408561497927\n",
            "1924 \tTraning Loss: 0.0003308247833047062\n",
            "1925 \tTraning Loss: 0.00028882839251309633\n",
            "1926 \tTraning Loss: 0.00019636511569842696\n",
            "1927 \tTraning Loss: 0.0003090089885517955\n",
            "1928 \tTraning Loss: 0.00036543316673487425\n",
            "1929 \tTraning Loss: 0.00023110558686312288\n",
            "1930 \tTraning Loss: 0.0004825960495509207\n",
            "1931 \tTraning Loss: 0.0003819417906925082\n",
            "1932 \tTraning Loss: 0.00038114047492854297\n",
            "1933 \tTraning Loss: 0.0003467963251750916\n",
            "1934 \tTraning Loss: 0.0003506376233417541\n",
            "1935 \tTraning Loss: 0.00044771411921828985\n",
            "1936 \tTraning Loss: 0.0003142148198094219\n",
            "1937 \tTraning Loss: 0.00026877300115302205\n",
            "1938 \tTraning Loss: 0.0003421891888137907\n",
            "1939 \tTraning Loss: 0.00028237287187948823\n",
            "1940 \tTraning Loss: 0.0003627958649303764\n",
            "1941 \tTraning Loss: 0.0004305171314626932\n",
            "1942 \tTraning Loss: 0.00031814418616704643\n",
            "1943 \tTraning Loss: 0.0003179515479132533\n",
            "1944 \tTraning Loss: 0.0003197479236405343\n",
            "1945 \tTraning Loss: 0.000287834758637473\n",
            "1946 \tTraning Loss: 0.00043048892985098064\n",
            "1947 \tTraning Loss: 0.00037508306559175253\n",
            "1948 \tTraning Loss: 0.0003265056002419442\n",
            "1949 \tTraning Loss: 0.0004919390194118023\n",
            "1950 \tTraning Loss: 0.0003329728206153959\n",
            "1951 \tTraning Loss: 0.00041676947148516774\n",
            "1952 \tTraning Loss: 0.00033969804644584656\n",
            "1953 \tTraning Loss: 0.00034418757422827184\n",
            "1954 \tTraning Loss: 0.0003701713285408914\n",
            "1955 \tTraning Loss: 0.0003948589146602899\n",
            "1956 \tTraning Loss: 0.00024630161351524293\n",
            "1957 \tTraning Loss: 0.00033272788277827203\n",
            "1958 \tTraning Loss: 0.0004039041814394295\n",
            "1959 \tTraning Loss: 0.00045681133633479476\n",
            "1960 \tTraning Loss: 0.0003028573701158166\n",
            "1961 \tTraning Loss: 0.0002982219448313117\n",
            "1962 \tTraning Loss: 0.0002718737814575434\n",
            "1963 \tTraning Loss: 0.00031912082340568304\n",
            "1964 \tTraning Loss: 0.0001944231626112014\n",
            "1965 \tTraning Loss: 0.0003983864444307983\n",
            "1966 \tTraning Loss: 0.00026694461121223867\n",
            "1967 \tTraning Loss: 0.0003410725621506572\n",
            "1968 \tTraning Loss: 0.00034429316292516887\n",
            "1969 \tTraning Loss: 0.0003383732109796256\n",
            "1970 \tTraning Loss: 0.00019786562188528478\n",
            "1971 \tTraning Loss: 0.0003511965915095061\n",
            "1972 \tTraning Loss: 0.0004980990197509527\n",
            "1973 \tTraning Loss: 0.0004307489434722811\n",
            "1974 \tTraning Loss: 0.00041972563485614955\n",
            "1975 \tTraning Loss: 0.0003607529797591269\n",
            "1976 \tTraning Loss: 0.000332424184307456\n",
            "1977 \tTraning Loss: 0.000278128165518865\n",
            "1978 \tTraning Loss: 0.00036137699498794973\n",
            "1979 \tTraning Loss: 0.00022957475448492914\n",
            "1980 \tTraning Loss: 0.0002532782091293484\n",
            "1981 \tTraning Loss: 0.00043770886259153485\n",
            "1982 \tTraning Loss: 0.00038693135138601065\n",
            "1983 \tTraning Loss: 0.00027731709997169673\n",
            "1984 \tTraning Loss: 0.0005011872272007167\n",
            "1985 \tTraning Loss: 0.000369595130905509\n",
            "1986 \tTraning Loss: 0.00046788505278527737\n",
            "1987 \tTraning Loss: 0.0004638241371139884\n",
            "1988 \tTraning Loss: 0.0002836112689692527\n",
            "1989 \tTraning Loss: 0.000278387131402269\n",
            "1990 \tTraning Loss: 0.0002833072212524712\n",
            "1991 \tTraning Loss: 0.000320299353916198\n",
            "1992 \tTraning Loss: 0.00033419474493712187\n",
            "1993 \tTraning Loss: 0.0003379154659342021\n",
            "1994 \tTraning Loss: 0.00037232559407129884\n",
            "1995 \tTraning Loss: 0.00036158732837066054\n",
            "1996 \tTraning Loss: 0.0002938169345725328\n",
            "1997 \tTraning Loss: 0.0003980579203926027\n",
            "1998 \tTraning Loss: 0.0004052824224345386\n",
            "1999 \tTraning Loss: 0.00034846042399294674\n",
            "2000 \tTraning Loss: 0.00033598096342757344\n",
            "2001 \tTraning Loss: 0.0002747562248259783\n",
            "2002 \tTraning Loss: 0.0003766294103115797\n",
            "2003 \tTraning Loss: 0.0002732692810241133\n",
            "2004 \tTraning Loss: 0.00032672222005203366\n",
            "2005 \tTraning Loss: 0.0004455550806596875\n",
            "2006 \tTraning Loss: 0.00034634865005500615\n",
            "2007 \tTraning Loss: 0.0003608344413805753\n",
            "2008 \tTraning Loss: 0.00042908903560601175\n",
            "2009 \tTraning Loss: 0.00020230149675626308\n",
            "2010 \tTraning Loss: 0.00030765615520067513\n",
            "2011 \tTraning Loss: 0.000301148189464584\n",
            "2012 \tTraning Loss: 0.0004930177819915116\n",
            "2013 \tTraning Loss: 0.00046250238665379584\n",
            "2014 \tTraning Loss: 0.0002740847412496805\n",
            "2015 \tTraning Loss: 0.00045075378147885203\n",
            "2016 \tTraning Loss: 0.00037450806121341884\n",
            "2017 \tTraning Loss: 0.0004465530510060489\n",
            "2018 \tTraning Loss: 0.0002744952798821032\n",
            "2019 \tTraning Loss: 0.0002967067703139037\n",
            "2020 \tTraning Loss: 0.00033207397791557014\n",
            "2021 \tTraning Loss: 0.00030665635131299496\n",
            "2022 \tTraning Loss: 0.0004035373858641833\n",
            "2023 \tTraning Loss: 0.00032463320530951023\n",
            "2024 \tTraning Loss: 0.00038461803342215717\n",
            "2025 \tTraning Loss: 0.00027405619039200246\n",
            "2026 \tTraning Loss: 0.0003462923050392419\n",
            "2027 \tTraning Loss: 0.0003649407299235463\n",
            "2028 \tTraning Loss: 0.0004347700742073357\n",
            "2029 \tTraning Loss: 0.00028954536537639797\n",
            "2030 \tTraning Loss: 0.0002578501880634576\n",
            "2031 \tTraning Loss: 0.0002612530079204589\n",
            "2032 \tTraning Loss: 0.00021310962620191276\n",
            "2033 \tTraning Loss: 0.00031083141220733523\n",
            "2034 \tTraning Loss: 0.0003701791283674538\n",
            "2035 \tTraning Loss: 0.0002121044381055981\n",
            "2036 \tTraning Loss: 0.00026872806483879685\n",
            "2037 \tTraning Loss: 0.000308623886667192\n",
            "2038 \tTraning Loss: 0.00026878566131927073\n",
            "2039 \tTraning Loss: 0.0004269875935278833\n",
            "2040 \tTraning Loss: 0.0004019517218694091\n",
            "2041 \tTraning Loss: 0.0002811275189742446\n",
            "2042 \tTraning Loss: 0.00018799326790031046\n",
            "2043 \tTraning Loss: 0.0003922356991097331\n",
            "2044 \tTraning Loss: 0.0004035178280901164\n",
            "2045 \tTraning Loss: 0.0002915769873652607\n",
            "2046 \tTraning Loss: 0.0004583163536153734\n",
            "2047 \tTraning Loss: 0.0003216115292161703\n",
            "2048 \tTraning Loss: 0.000307534821331501\n",
            "2049 \tTraning Loss: 0.0003368968900758773\n",
            "2050 \tTraning Loss: 0.0004857266612816602\n",
            "2051 \tTraning Loss: 0.00037118818727321923\n",
            "2052 \tTraning Loss: 0.00026133551727980375\n",
            "2053 \tTraning Loss: 0.0003327492158859968\n",
            "2054 \tTraning Loss: 0.0003345247241668403\n",
            "2055 \tTraning Loss: 0.00044156028889119625\n",
            "2056 \tTraning Loss: 0.0003315047943033278\n",
            "2057 \tTraning Loss: 0.00033476200769655406\n",
            "2058 \tTraning Loss: 0.0002731017884798348\n",
            "2059 \tTraning Loss: 0.0003325424622744322\n",
            "2060 \tTraning Loss: 0.0002622906176839024\n",
            "2061 \tTraning Loss: 0.00048500136472284794\n",
            "2062 \tTraning Loss: 0.0003635000321082771\n",
            "2063 \tTraning Loss: 0.00028770320932380855\n",
            "2064 \tTraning Loss: 0.00026942419935949147\n",
            "2065 \tTraning Loss: 0.0002426554128760472\n",
            "2066 \tTraning Loss: 0.00040860020089894533\n",
            "2067 \tTraning Loss: 0.00023535847140010446\n",
            "2068 \tTraning Loss: 0.00034764246083796024\n",
            "2069 \tTraning Loss: 0.0003059618466068059\n",
            "2070 \tTraning Loss: 0.0004691723734140396\n",
            "2071 \tTraning Loss: 0.0002862889086827636\n",
            "2072 \tTraning Loss: 0.00030256385798566043\n",
            "2073 \tTraning Loss: 0.00037084249197505414\n",
            "2074 \tTraning Loss: 0.0002854296180885285\n",
            "2075 \tTraning Loss: 0.0003502452454995364\n",
            "2076 \tTraning Loss: 0.00023776391753926873\n",
            "2077 \tTraning Loss: 0.00035088337608613074\n",
            "2078 \tTraning Loss: 0.0004866208473686129\n",
            "2079 \tTraning Loss: 0.00035548664163798094\n",
            "2080 \tTraning Loss: 0.00039750896394252777\n",
            "2081 \tTraning Loss: 0.0002882240805774927\n",
            "2082 \tTraning Loss: 0.00028860277961939573\n",
            "2083 \tTraning Loss: 0.00041326560312882066\n",
            "2084 \tTraning Loss: 0.00030633655842393637\n",
            "2085 \tTraning Loss: 0.0002654000127222389\n",
            "2086 \tTraning Loss: 0.00038037062040530145\n",
            "2087 \tTraning Loss: 0.0003176727914251387\n",
            "2088 \tTraning Loss: 0.0002902265696320683\n",
            "2089 \tTraning Loss: 0.00045905716251581907\n",
            "2090 \tTraning Loss: 0.00034444659831933677\n",
            "2091 \tTraning Loss: 0.0003147193056065589\n",
            "2092 \tTraning Loss: 0.0003655307227745652\n",
            "2093 \tTraning Loss: 0.0002715133596211672\n",
            "2094 \tTraning Loss: 0.0004783784970641136\n",
            "2095 \tTraning Loss: 0.00032666855258867145\n",
            "2096 \tTraning Loss: 0.00036029063630849123\n",
            "2097 \tTraning Loss: 0.00039322502561844885\n",
            "2098 \tTraning Loss: 0.0003422716981731355\n",
            "2099 \tTraning Loss: 0.00046219449723139405\n",
            "2100 \tTraning Loss: 0.00035995853249914944\n",
            "2101 \tTraning Loss: 0.0003183259104844183\n",
            "2102 \tTraning Loss: 0.0003616719623096287\n",
            "2103 \tTraning Loss: 0.0004956846823915839\n",
            "2104 \tTraning Loss: 0.00014308470417745411\n",
            "2105 \tTraning Loss: 0.0002444166166242212\n",
            "2106 \tTraning Loss: 0.0006400223937816918\n",
            "2107 \tTraning Loss: 0.000369209359632805\n",
            "2108 \tTraning Loss: 0.0003896091948263347\n",
            "2109 \tTraning Loss: 0.0004023872606921941\n",
            "2110 \tTraning Loss: 0.0002817401255015284\n",
            "2111 \tTraning Loss: 0.0002469648898113519\n",
            "2112 \tTraning Loss: 0.0003068521909881383\n",
            "2113 \tTraning Loss: 0.0002519766567274928\n",
            "2114 \tTraning Loss: 0.0002273398422403261\n",
            "2115 \tTraning Loss: 0.00018071624799631536\n",
            "2116 \tTraning Loss: 0.00038162298733368516\n",
            "2117 \tTraning Loss: 0.00038335908902809024\n",
            "2118 \tTraning Loss: 0.000495120941195637\n",
            "2119 \tTraning Loss: 0.00023467166465707123\n",
            "2120 \tTraning Loss: 0.00030286406399682164\n",
            "2121 \tTraning Loss: 0.00035892080632038414\n",
            "2122 \tTraning Loss: 0.0003403125738259405\n",
            "2123 \tTraning Loss: 0.000376923504518345\n",
            "2124 \tTraning Loss: 0.0002548309275880456\n",
            "2125 \tTraning Loss: 0.000472382758744061\n",
            "2126 \tTraning Loss: 0.0002449812600389123\n",
            "2127 \tTraning Loss: 0.00033327325945720077\n",
            "2128 \tTraning Loss: 0.0003565686638467014\n",
            "2129 \tTraning Loss: 0.00029007901321165264\n",
            "2130 \tTraning Loss: 0.00034329082700423896\n",
            "2131 \tTraning Loss: 0.00034112788853235543\n",
            "2132 \tTraning Loss: 0.0003539333993103355\n",
            "2133 \tTraning Loss: 0.0002608581562526524\n",
            "2134 \tTraning Loss: 0.0004036254540551454\n",
            "2135 \tTraning Loss: 0.00035577078233473003\n",
            "2136 \tTraning Loss: 0.00031915996805764735\n",
            "2137 \tTraning Loss: 0.00040046070353128016\n",
            "2138 \tTraning Loss: 0.0002384221734246239\n",
            "2139 \tTraning Loss: 0.00033820420503616333\n",
            "2140 \tTraning Loss: 0.00029445873224176466\n",
            "2141 \tTraning Loss: 0.00045293400762602687\n",
            "2142 \tTraning Loss: 0.0003322498232591897\n",
            "2143 \tTraning Loss: 0.00039365419070236385\n",
            "2144 \tTraning Loss: 0.0003110982070211321\n",
            "2145 \tTraning Loss: 0.0003560874320100993\n",
            "2146 \tTraning Loss: 0.0004325977060943842\n",
            "2147 \tTraning Loss: 0.00032600777922198176\n",
            "2148 \tTraning Loss: 0.0004903070512227714\n",
            "2149 \tTraning Loss: 0.00027264546952210367\n",
            "2150 \tTraning Loss: 0.00039320337236858904\n",
            "2151 \tTraning Loss: 0.00037709742900915444\n",
            "2152 \tTraning Loss: 0.0003283756959717721\n",
            "2153 \tTraning Loss: 0.00035508512519299984\n",
            "2154 \tTraning Loss: 0.0002843603433575481\n",
            "2155 \tTraning Loss: 0.0002901257248595357\n",
            "2156 \tTraning Loss: 0.00029407598776742816\n",
            "2157 \tTraning Loss: 0.00020411016885191202\n",
            "2158 \tTraning Loss: 0.00028946067322976887\n",
            "2159 \tTraning Loss: 0.00037237053038552403\n",
            "2160 \tTraning Loss: 0.00034655205672606826\n",
            "2161 \tTraning Loss: 0.000280362757621333\n",
            "2162 \tTraning Loss: 0.0002797576307784766\n",
            "2163 \tTraning Loss: 0.0004066349647473544\n",
            "2164 \tTraning Loss: 0.00034178869100287557\n",
            "2165 \tTraning Loss: 0.0002544686431065202\n",
            "2166 \tTraning Loss: 0.0002862461842596531\n",
            "2167 \tTraning Loss: 0.0003536914009600878\n",
            "2168 \tTraning Loss: 0.0004857293388340622\n",
            "2169 \tTraning Loss: 0.0004686512111220509\n",
            "2170 \tTraning Loss: 0.00036222970811650157\n",
            "2171 \tTraning Loss: 0.0003947324585169554\n",
            "2172 \tTraning Loss: 0.00031489477260038257\n",
            "2173 \tTraning Loss: 0.0002211361424997449\n",
            "2174 \tTraning Loss: 0.0003045446355827153\n",
            "2175 \tTraning Loss: 0.00020965920703019947\n",
            "2176 \tTraning Loss: 0.00032819018815644085\n",
            "2177 \tTraning Loss: 0.00039440710679627955\n",
            "2178 \tTraning Loss: 0.0002988808846566826\n",
            "2179 \tTraning Loss: 0.0003506626235321164\n",
            "2180 \tTraning Loss: 0.0002858942316379398\n",
            "2181 \tTraning Loss: 0.000458600145066157\n",
            "2182 \tTraning Loss: 0.00030691063147969544\n",
            "2183 \tTraning Loss: 0.00044474523747339845\n",
            "2184 \tTraning Loss: 0.0003562499477993697\n",
            "2185 \tTraning Loss: 0.00035618062247522175\n",
            "2186 \tTraning Loss: 0.00033999537117779255\n",
            "2187 \tTraning Loss: 0.00032465546973980963\n",
            "2188 \tTraning Loss: 0.00025604257825762033\n",
            "2189 \tTraning Loss: 0.0003964524657931179\n",
            "2190 \tTraning Loss: 0.00036956515396013856\n",
            "2191 \tTraning Loss: 0.0004897982580587268\n",
            "2192 \tTraning Loss: 0.00033263853401876986\n",
            "2193 \tTraning Loss: 0.000386350613553077\n",
            "2194 \tTraning Loss: 0.0003167179529555142\n",
            "2195 \tTraning Loss: 0.00043988306424580514\n",
            "2196 \tTraning Loss: 0.00036277450271882117\n",
            "2197 \tTraning Loss: 0.0005021736142225564\n",
            "2198 \tTraning Loss: 0.0003000560391228646\n",
            "2199 \tTraning Loss: 0.00031945816590450704\n",
            "2200 \tTraning Loss: 0.0002973969967570156\n",
            "2201 \tTraning Loss: 0.00023134012008085847\n",
            "2202 \tTraning Loss: 0.0002867535222321749\n",
            "2203 \tTraning Loss: 0.0003587997052818537\n",
            "2204 \tTraning Loss: 0.00041224260348826647\n",
            "2205 \tTraning Loss: 0.00022948969854041934\n",
            "2206 \tTraning Loss: 0.0002697113959584385\n",
            "2207 \tTraning Loss: 0.00032086577266454697\n",
            "2208 \tTraning Loss: 0.00047105285921134055\n",
            "2209 \tTraning Loss: 0.0003217690682504326\n",
            "2210 \tTraning Loss: 0.0004052604781463742\n",
            "2211 \tTraning Loss: 0.00032963798730634153\n",
            "2212 \tTraning Loss: 0.00029217227711342275\n",
            "2213 \tTraning Loss: 0.000245528673985973\n",
            "2214 \tTraning Loss: 0.00046490077511407435\n",
            "2215 \tTraning Loss: 0.0002543010050430894\n",
            "2216 \tTraning Loss: 0.0003981331828981638\n",
            "2217 \tTraning Loss: 0.0004101349040865898\n",
            "2218 \tTraning Loss: 0.0004648240574169904\n",
            "2219 \tTraning Loss: 0.00031888685771264136\n",
            "2220 \tTraning Loss: 0.00029754219576716423\n",
            "2221 \tTraning Loss: 0.0003344085707794875\n",
            "2222 \tTraning Loss: 0.00023756803420837969\n",
            "2223 \tTraning Loss: 0.00022729118063580245\n",
            "2224 \tTraning Loss: 0.0003122425696346909\n",
            "2225 \tTraning Loss: 0.00046175328316166997\n",
            "2226 \tTraning Loss: 0.0002659614838194102\n",
            "2227 \tTraning Loss: 0.00029849246493540704\n",
            "2228 \tTraning Loss: 0.0004678085388150066\n",
            "2229 \tTraning Loss: 0.0003735179780051112\n",
            "2230 \tTraning Loss: 0.0003538389573805034\n",
            "2231 \tTraning Loss: 0.00053611327894032\n",
            "2232 \tTraning Loss: 0.0003418436390347779\n",
            "2233 \tTraning Loss: 0.0002992076624650508\n",
            "2234 \tTraning Loss: 0.00047974527115002275\n",
            "2235 \tTraning Loss: 0.00038006421527825296\n",
            "2236 \tTraning Loss: 0.0003435075923334807\n",
            "2237 \tTraning Loss: 0.0003680844674818218\n",
            "2238 \tTraning Loss: 0.0002492674975655973\n",
            "2239 \tTraning Loss: 0.00039755902253091335\n",
            "2240 \tTraning Loss: 0.0003616339236032218\n",
            "2241 \tTraning Loss: 0.00035841696080751717\n",
            "2242 \tTraning Loss: 0.0003336425870656967\n",
            "2243 \tTraning Loss: 0.00029201217694208026\n",
            "2244 \tTraning Loss: 0.0003739712992683053\n",
            "2245 \tTraning Loss: 0.00025284691946581006\n",
            "2246 \tTraning Loss: 0.00041644045268185437\n",
            "2247 \tTraning Loss: 0.00023727247025817633\n",
            "2248 \tTraning Loss: 0.00033943625749088824\n",
            "2249 \tTraning Loss: 0.0003136643790639937\n",
            "2250 \tTraning Loss: 0.00022548572451341897\n",
            "2251 \tTraning Loss: 0.00033173043630085886\n",
            "2252 \tTraning Loss: 0.0002768954436760396\n",
            "2253 \tTraning Loss: 0.00038873060839250684\n",
            "2254 \tTraning Loss: 0.00026065108249895275\n",
            "2255 \tTraning Loss: 0.00027583393966779113\n",
            "2256 \tTraning Loss: 0.0001569648302393034\n",
            "2257 \tTraning Loss: 0.00028504719375632703\n",
            "2258 \tTraning Loss: 0.000447307713329792\n",
            "2259 \tTraning Loss: 0.00029462497332133353\n",
            "2260 \tTraning Loss: 0.0002935960947070271\n",
            "2261 \tTraning Loss: 0.0003260877274442464\n",
            "2262 \tTraning Loss: 0.0002654875279404223\n",
            "2263 \tTraning Loss: 0.00032968056621029973\n",
            "2264 \tTraning Loss: 0.0004108290304429829\n",
            "2265 \tTraning Loss: 0.0004295016697142273\n",
            "2266 \tTraning Loss: 0.00021693877351935953\n",
            "2267 \tTraning Loss: 0.0002830112644005567\n",
            "2268 \tTraning Loss: 0.00033434084616601467\n",
            "2269 \tTraning Loss: 0.0003645263204816729\n",
            "2270 \tTraning Loss: 0.00036544480826705694\n",
            "2271 \tTraning Loss: 0.0003501647734083235\n",
            "2272 \tTraning Loss: 0.00033494189847260714\n",
            "2273 \tTraning Loss: 0.00036866997834295034\n",
            "2274 \tTraning Loss: 0.0003502874751575291\n",
            "2275 \tTraning Loss: 0.0002959407866001129\n",
            "2276 \tTraning Loss: 0.000271505763521418\n",
            "2277 \tTraning Loss: 0.00038735117414034903\n",
            "2278 \tTraning Loss: 0.00037153487210161984\n",
            "2279 \tTraning Loss: 0.0003370387712493539\n",
            "2280 \tTraning Loss: 0.00031086133094504476\n",
            "2281 \tTraning Loss: 0.00024502567248418927\n",
            "2282 \tTraning Loss: 0.00027900750865228474\n",
            "2283 \tTraning Loss: 0.00033887504832819104\n",
            "2284 \tTraning Loss: 0.00035087487776763737\n",
            "2285 \tTraning Loss: 0.00043078127782791853\n",
            "2286 \tTraning Loss: 0.0003028622886631638\n",
            "2287 \tTraning Loss: 0.00027123730978928506\n",
            "2288 \tTraning Loss: 0.000268475036136806\n",
            "2289 \tTraning Loss: 0.00044242822332307696\n",
            "2290 \tTraning Loss: 0.0002497830428183079\n",
            "2291 \tTraning Loss: 0.00020338133617769927\n",
            "2292 \tTraning Loss: 0.0002944905136246234\n",
            "2293 \tTraning Loss: 0.0003153291763737798\n",
            "2294 \tTraning Loss: 0.0003238109638914466\n",
            "2295 \tTraning Loss: 0.00027873044018633664\n",
            "2296 \tTraning Loss: 0.00027120395679958165\n",
            "2297 \tTraning Loss: 0.0002777323534246534\n",
            "2298 \tTraning Loss: 0.00030683877412229776\n",
            "2299 \tTraning Loss: 0.00020711385877802968\n",
            "2300 \tTraning Loss: 0.00023444004182238132\n",
            "2301 \tTraning Loss: 0.00036830976023338735\n",
            "2302 \tTraning Loss: 0.0003109303070232272\n",
            "2303 \tTraning Loss: 0.0003648296988103539\n",
            "2304 \tTraning Loss: 0.00033705911482684314\n",
            "2305 \tTraning Loss: 0.0003497664292808622\n",
            "2306 \tTraning Loss: 0.00028551192372106016\n",
            "2307 \tTraning Loss: 0.0003138719475828111\n",
            "2308 \tTraning Loss: 0.00023870034783612937\n",
            "2309 \tTraning Loss: 0.00035211798967793584\n",
            "2310 \tTraning Loss: 0.0003091139660682529\n",
            "2311 \tTraning Loss: 0.0003162754001095891\n",
            "2312 \tTraning Loss: 0.00038806776865385473\n",
            "2313 \tTraning Loss: 0.0004404272185638547\n",
            "2314 \tTraning Loss: 0.00029852648731321096\n",
            "2315 \tTraning Loss: 0.0003430221986491233\n",
            "2316 \tTraning Loss: 0.0002450533502269536\n",
            "2317 \tTraning Loss: 0.00030538206920027733\n",
            "2318 \tTraning Loss: 0.0003209092828910798\n",
            "2319 \tTraning Loss: 0.00033986358903348446\n",
            "2320 \tTraning Loss: 0.0002642443869262934\n",
            "2321 \tTraning Loss: 0.00048819713992998004\n",
            "2322 \tTraning Loss: 0.0004092529125045985\n",
            "2323 \tTraning Loss: 0.0002932299976237118\n",
            "2324 \tTraning Loss: 0.0003861575969494879\n",
            "2325 \tTraning Loss: 0.00044871363206766546\n",
            "2326 \tTraning Loss: 0.00041575738578103483\n",
            "2327 \tTraning Loss: 0.0002905682194977999\n",
            "2328 \tTraning Loss: 0.0003608438419178128\n",
            "2329 \tTraning Loss: 0.0001779830636223778\n",
            "2330 \tTraning Loss: 0.00030953818350099027\n",
            "2331 \tTraning Loss: 0.00042243156349286437\n",
            "2332 \tTraning Loss: 0.00039750905125401914\n",
            "2333 \tTraning Loss: 0.000398327101720497\n",
            "2334 \tTraning Loss: 0.00027870392659679055\n",
            "2335 \tTraning Loss: 0.0002791545994114131\n",
            "2336 \tTraning Loss: 0.0003866310289595276\n",
            "2337 \tTraning Loss: 0.0003772772033698857\n",
            "2338 \tTraning Loss: 0.0002927363966591656\n",
            "2339 \tTraning Loss: 0.000434612826211378\n",
            "2340 \tTraning Loss: 0.00043429050128906965\n",
            "2341 \tTraning Loss: 0.00028614618349820375\n",
            "2342 \tTraning Loss: 0.0004276499093975872\n",
            "2343 \tTraning Loss: 0.0003542640770319849\n",
            "2344 \tTraning Loss: 0.0004056618781760335\n",
            "2345 \tTraning Loss: 0.00027926944312639534\n",
            "2346 \tTraning Loss: 0.0003508307854644954\n",
            "2347 \tTraning Loss: 0.0003284937993157655\n",
            "2348 \tTraning Loss: 0.00041174606303684413\n",
            "2349 \tTraning Loss: 0.0004141581885050982\n",
            "2350 \tTraning Loss: 0.00040696896030567586\n",
            "2351 \tTraning Loss: 0.0004005194059573114\n",
            "2352 \tTraning Loss: 0.0002734060399234295\n",
            "2353 \tTraning Loss: 0.0003938395529985428\n",
            "2354 \tTraning Loss: 0.00029273077961988747\n",
            "2355 \tTraning Loss: 0.0002516809618100524\n",
            "2356 \tTraning Loss: 0.0004503712698351592\n",
            "2357 \tTraning Loss: 0.0002735902671702206\n",
            "2358 \tTraning Loss: 0.000404581252951175\n",
            "2359 \tTraning Loss: 0.0002544227463658899\n",
            "2360 \tTraning Loss: 0.0003704816335812211\n",
            "2361 \tTraning Loss: 0.00026598936528898776\n",
            "2362 \tTraning Loss: 0.00044300369336269796\n",
            "2363 \tTraning Loss: 0.0002918193058576435\n",
            "2364 \tTraning Loss: 0.00029274856206029654\n",
            "2365 \tTraning Loss: 0.00038710396620444953\n",
            "2366 \tTraning Loss: 0.00035819632466882467\n",
            "2367 \tTraning Loss: 0.0002944039588328451\n",
            "2368 \tTraning Loss: 0.0003051537787541747\n",
            "2369 \tTraning Loss: 0.0003626906836871058\n",
            "2370 \tTraning Loss: 0.0005171019001863897\n",
            "2371 \tTraning Loss: 0.00041743519250303507\n",
            "2372 \tTraning Loss: 0.00030600622994825244\n",
            "2373 \tTraning Loss: 0.00023590544878970832\n",
            "2374 \tTraning Loss: 0.0003301876422483474\n",
            "2375 \tTraning Loss: 0.00033139847801066935\n",
            "2376 \tTraning Loss: 0.00032543495763093233\n",
            "2377 \tTraning Loss: 0.0004322495951782912\n",
            "2378 \tTraning Loss: 0.00040205559344030917\n",
            "2379 \tTraning Loss: 0.00028049733373336494\n",
            "2380 \tTraning Loss: 0.00038765676436014473\n",
            "2381 \tTraning Loss: 0.0003989177930634469\n",
            "2382 \tTraning Loss: 0.0003918164293281734\n",
            "2383 \tTraning Loss: 0.00035220806603319943\n",
            "2384 \tTraning Loss: 0.0003047151258215308\n",
            "2385 \tTraning Loss: 0.00037822843296453357\n",
            "2386 \tTraning Loss: 0.00030446593882516026\n",
            "2387 \tTraning Loss: 0.0002611535892356187\n",
            "2388 \tTraning Loss: 0.0002978151896968484\n",
            "2389 \tTraning Loss: 0.00047035320312716067\n",
            "2390 \tTraning Loss: 0.00029401801293715835\n",
            "2391 \tTraning Loss: 0.00023114938812796026\n",
            "2392 \tTraning Loss: 0.00036328574060462415\n",
            "2393 \tTraning Loss: 0.00030319878715090454\n",
            "2394 \tTraning Loss: 0.00039580833981744945\n",
            "2395 \tTraning Loss: 0.0002879952662624419\n",
            "2396 \tTraning Loss: 0.0002387229906162247\n",
            "2397 \tTraning Loss: 0.0002637520374264568\n",
            "2398 \tTraning Loss: 0.00020183471497148275\n",
            "2399 \tTraning Loss: 0.00026821676874533296\n",
            "2400 \tTraning Loss: 0.0002994626993313432\n",
            "2401 \tTraning Loss: 0.0002961070276796818\n",
            "2402 \tTraning Loss: 0.0001701258443063125\n",
            "2403 \tTraning Loss: 0.0003067209618166089\n",
            "2404 \tTraning Loss: 0.00020536372903734446\n",
            "2405 \tTraning Loss: 0.00038815371226519346\n",
            "2406 \tTraning Loss: 0.00035005059908144176\n",
            "2407 \tTraning Loss: 0.000423233286710456\n",
            "2408 \tTraning Loss: 0.000213069113669917\n",
            "2409 \tTraning Loss: 0.0004545056726783514\n",
            "2410 \tTraning Loss: 0.0003096106229349971\n",
            "2411 \tTraning Loss: 0.00017306832887697965\n",
            "2412 \tTraning Loss: 0.0002416309725958854\n",
            "2413 \tTraning Loss: 0.0002965486783068627\n",
            "2414 \tTraning Loss: 0.00030603428604081273\n",
            "2415 \tTraning Loss: 0.0003226518747396767\n",
            "2416 \tTraning Loss: 0.00039394371560774744\n",
            "2417 \tTraning Loss: 0.000325918837916106\n",
            "2418 \tTraning Loss: 0.00046310818288475275\n",
            "2419 \tTraning Loss: 0.00034206625423394144\n",
            "2420 \tTraning Loss: 0.0005238458979874849\n",
            "2421 \tTraning Loss: 0.0003056950226891786\n",
            "2422 \tTraning Loss: 0.00034617132041603327\n",
            "2423 \tTraning Loss: 0.0003201161453034729\n",
            "2424 \tTraning Loss: 0.00042189619853161275\n",
            "2425 \tTraning Loss: 0.0003136089362669736\n",
            "2426 \tTraning Loss: 0.00032514974009245634\n",
            "2427 \tTraning Loss: 0.000500962371006608\n",
            "2428 \tTraning Loss: 0.0002797188935801387\n",
            "2429 \tTraning Loss: 0.00037635088665410876\n",
            "2430 \tTraning Loss: 0.0004229953046888113\n",
            "2431 \tTraning Loss: 0.00022556832118425518\n",
            "2432 \tTraning Loss: 0.0005013701738789678\n",
            "2433 \tTraning Loss: 0.00028079963522031903\n",
            "2434 \tTraning Loss: 0.0002551981306169182\n",
            "2435 \tTraning Loss: 0.0002454657224006951\n",
            "2436 \tTraning Loss: 0.00037319413968361914\n",
            "2437 \tTraning Loss: 0.0002582193410489708\n",
            "2438 \tTraning Loss: 0.0003940162423532456\n",
            "2439 \tTraning Loss: 0.0003901737800333649\n",
            "2440 \tTraning Loss: 0.0004175621725153178\n",
            "2441 \tTraning Loss: 0.0002688180538825691\n",
            "2442 \tTraning Loss: 0.0003673322207760066\n",
            "2443 \tTraning Loss: 0.00032104633282870054\n",
            "2444 \tTraning Loss: 0.000341029284754768\n",
            "2445 \tTraning Loss: 0.0003768680617213249\n",
            "2446 \tTraning Loss: 0.00023865226830821484\n",
            "2447 \tTraning Loss: 0.0002458900271449238\n",
            "2448 \tTraning Loss: 0.0003389471094124019\n",
            "2449 \tTraning Loss: 0.0003199239436071366\n",
            "2450 \tTraning Loss: 0.00028288341127336025\n",
            "2451 \tTraning Loss: 0.00016644338029436767\n",
            "2452 \tTraning Loss: 0.0002766996913123876\n",
            "2453 \tTraning Loss: 0.00041368044912815094\n",
            "2454 \tTraning Loss: 0.0005211677635088563\n",
            "2455 \tTraning Loss: 0.0003807567118201405\n",
            "2456 \tTraning Loss: 0.00036347832065075636\n",
            "2457 \tTraning Loss: 0.00039953403756953776\n",
            "2458 \tTraning Loss: 0.0003731326723936945\n",
            "2459 \tTraning Loss: 0.0003896135021932423\n",
            "2460 \tTraning Loss: 0.00027476862305775285\n",
            "2461 \tTraning Loss: 0.0002885996364057064\n",
            "2462 \tTraning Loss: 0.00023683145991526544\n",
            "2463 \tTraning Loss: 0.0002940930426120758\n",
            "2464 \tTraning Loss: 0.0002327943657292053\n",
            "2465 \tTraning Loss: 0.0003774650685954839\n",
            "2466 \tTraning Loss: 0.00021135971473995596\n",
            "2467 \tTraning Loss: 0.0002954545198008418\n",
            "2468 \tTraning Loss: 0.00029871115111745894\n",
            "2469 \tTraning Loss: 0.00036219245521351695\n",
            "2470 \tTraning Loss: 0.0004970948793925345\n",
            "2471 \tTraning Loss: 0.00021212569845374674\n",
            "2472 \tTraning Loss: 0.0003531169786583632\n",
            "2473 \tTraning Loss: 0.0002420384407741949\n",
            "2474 \tTraning Loss: 0.0004540091031230986\n",
            "2475 \tTraning Loss: 0.00030910756322555244\n",
            "2476 \tTraning Loss: 0.00048347326810471714\n",
            "2477 \tTraning Loss: 0.0003244195249862969\n",
            "2478 \tTraning Loss: 0.00023436853371094912\n",
            "2479 \tTraning Loss: 0.0003487726207822561\n",
            "2480 \tTraning Loss: 0.0002718236646614969\n",
            "2481 \tTraning Loss: 0.0002446316066198051\n",
            "2482 \tTraning Loss: 0.00021398894023150206\n",
            "2483 \tTraning Loss: 0.000362509599654004\n",
            "2484 \tTraning Loss: 0.0001296371192438528\n",
            "2485 \tTraning Loss: 0.00037485346547327936\n",
            "2486 \tTraning Loss: 0.0003191955329384655\n",
            "2487 \tTraning Loss: 0.0004591698234435171\n",
            "2488 \tTraning Loss: 0.0003558112366590649\n",
            "2489 \tTraning Loss: 0.00026384645025245845\n",
            "2490 \tTraning Loss: 0.0002638384758029133\n",
            "2491 \tTraning Loss: 0.00035094900522381067\n",
            "2492 \tTraning Loss: 0.00025681190891191363\n",
            "2493 \tTraning Loss: 0.00036964769242331386\n",
            "2494 \tTraning Loss: 0.00038791916449554265\n",
            "2495 \tTraning Loss: 0.00033938177512027323\n",
            "2496 \tTraning Loss: 0.0004173664783593267\n",
            "2497 \tTraning Loss: 0.0002981854195240885\n",
            "2498 \tTraning Loss: 0.0003050064842682332\n",
            "2499 \tTraning Loss: 0.00022328426712192595\n",
            "2500 \tTraning Loss: 0.0004477009060792625\n",
            "2501 \tTraning Loss: 0.00016750450595282018\n",
            "2502 \tTraning Loss: 0.0002385044499533251\n",
            "2503 \tTraning Loss: 0.0004033472796436399\n",
            "2504 \tTraning Loss: 0.00037717761006206274\n",
            "2505 \tTraning Loss: 0.0002793156018014997\n",
            "2506 \tTraning Loss: 0.00042127989581786096\n",
            "2507 \tTraning Loss: 0.00031586471595801413\n",
            "2508 \tTraning Loss: 0.0002901267143897712\n",
            "2509 \tTraning Loss: 0.00029657655977644026\n",
            "2510 \tTraning Loss: 0.0003568720130715519\n",
            "2511 \tTraning Loss: 0.00024558461154811084\n",
            "2512 \tTraning Loss: 0.00031754790688864887\n",
            "2513 \tTraning Loss: 0.00040078230085782707\n",
            "2514 \tTraning Loss: 0.00036162190372124314\n",
            "2515 \tTraning Loss: 0.00029735229327343404\n",
            "2516 \tTraning Loss: 0.00031482218764722347\n",
            "2517 \tTraning Loss: 0.0003514251729939133\n",
            "2518 \tTraning Loss: 0.00031341443536803126\n",
            "2519 \tTraning Loss: 0.0003287518338765949\n",
            "2520 \tTraning Loss: 0.0003800888662226498\n",
            "2521 \tTraning Loss: 0.0003448912175372243\n",
            "2522 \tTraning Loss: 0.00031584477983415127\n",
            "2523 \tTraning Loss: 0.00014988482871558517\n",
            "2524 \tTraning Loss: 0.00021914337412454188\n",
            "2525 \tTraning Loss: 0.00046876780106686056\n",
            "2526 \tTraning Loss: 0.00025020926841534674\n",
            "2527 \tTraning Loss: 0.0003911175299435854\n",
            "2528 \tTraning Loss: 0.0004346197529230267\n",
            "2529 \tTraning Loss: 0.0005014828057028353\n",
            "2530 \tTraning Loss: 0.00039362043025903404\n",
            "2531 \tTraning Loss: 0.00040977972093969584\n",
            "2532 \tTraning Loss: 0.0003580006887204945\n",
            "2533 \tTraning Loss: 0.0003255513438489288\n",
            "2534 \tTraning Loss: 0.0002739809569902718\n",
            "2535 \tTraning Loss: 0.0004991536261513829\n",
            "2536 \tTraning Loss: 0.00031034406856633723\n",
            "2537 \tTraning Loss: 0.0002714372531045228\n",
            "2538 \tTraning Loss: 0.0004434729344211519\n",
            "2539 \tTraning Loss: 0.0003406738396733999\n",
            "2540 \tTraning Loss: 0.00027293796301819384\n",
            "2541 \tTraning Loss: 0.0002518728142604232\n",
            "2542 \tTraning Loss: 0.0003132082929369062\n",
            "2543 \tTraning Loss: 0.0004309875948820263\n",
            "2544 \tTraning Loss: 0.00034627359127625823\n",
            "2545 \tTraning Loss: 0.00041928549762815237\n",
            "2546 \tTraning Loss: 0.00039790067239664495\n",
            "2547 \tTraning Loss: 0.00048696211888454854\n",
            "2548 \tTraning Loss: 0.00029985528090037405\n",
            "2549 \tTraning Loss: 0.00035942436079494655\n",
            "2550 \tTraning Loss: 0.0003040226292796433\n",
            "2551 \tTraning Loss: 0.0003497841826174408\n",
            "2552 \tTraning Loss: 0.000353617244400084\n",
            "2553 \tTraning Loss: 0.00041701633017510176\n",
            "2554 \tTraning Loss: 0.0002506592427380383\n",
            "2555 \tTraning Loss: 0.00047726870980113745\n",
            "2556 \tTraning Loss: 0.00033481765422038734\n",
            "2557 \tTraning Loss: 0.0003195471945218742\n",
            "2558 \tTraning Loss: 0.00042911124182865024\n",
            "2559 \tTraning Loss: 0.00038843616493977606\n",
            "2560 \tTraning Loss: 0.0002680881298147142\n",
            "2561 \tTraning Loss: 0.0003001958830282092\n",
            "2562 \tTraning Loss: 0.00033323344541713595\n",
            "2563 \tTraning Loss: 0.0003334515786264092\n",
            "2564 \tTraning Loss: 0.00031698786187916994\n",
            "2565 \tTraning Loss: 0.00020718801533803344\n",
            "2566 \tTraning Loss: 0.00038058258360251784\n",
            "2567 \tTraning Loss: 0.0004223333380650729\n",
            "2568 \tTraning Loss: 0.00040856486884877086\n",
            "2569 \tTraning Loss: 0.0002759777125902474\n",
            "2570 \tTraning Loss: 0.00023262962349690497\n",
            "2571 \tTraning Loss: 0.00022936493041925132\n",
            "2572 \tTraning Loss: 0.0001934953616000712\n",
            "2573 \tTraning Loss: 0.00035103035042993724\n",
            "2574 \tTraning Loss: 0.00031511406996287405\n",
            "2575 \tTraning Loss: 0.0004171403998043388\n",
            "2576 \tTraning Loss: 0.0003115306899417192\n",
            "2577 \tTraning Loss: 0.0002751009596977383\n",
            "2578 \tTraning Loss: 0.00023366410459857434\n",
            "2579 \tTraning Loss: 0.00037619509384967387\n",
            "2580 \tTraning Loss: 0.00020149460760876536\n",
            "2581 \tTraning Loss: 0.0004965642583556473\n",
            "2582 \tTraning Loss: 0.00030059716664254665\n",
            "2583 \tTraning Loss: 0.0003873557725455612\n",
            "2584 \tTraning Loss: 0.0002581844455562532\n",
            "2585 \tTraning Loss: 0.0002812182647176087\n",
            "2586 \tTraning Loss: 0.00032528649899177253\n",
            "2587 \tTraning Loss: 0.0003339330432936549\n",
            "2588 \tTraning Loss: 0.0002492240455467254\n",
            "2589 \tTraning Loss: 0.0003986576630268246\n",
            "2590 \tTraning Loss: 0.0003715543425641954\n",
            "2591 \tTraning Loss: 0.00027462159050628543\n",
            "2592 \tTraning Loss: 0.0002962400612886995\n",
            "2593 \tTraning Loss: 0.0003223970124963671\n",
            "2594 \tTraning Loss: 0.000512543716467917\n",
            "2595 \tTraning Loss: 0.0003456698323134333\n",
            "2596 \tTraning Loss: 0.00041259435238316655\n",
            "2597 \tTraning Loss: 0.00024157582083716989\n",
            "2598 \tTraning Loss: 0.0003276846546214074\n",
            "2599 \tTraning Loss: 0.00042799138464033604\n",
            "2600 \tTraning Loss: 0.00042226992081850767\n",
            "2601 \tTraning Loss: 0.00018145490321330726\n",
            "2602 \tTraning Loss: 0.00033434652141295373\n",
            "2603 \tTraning Loss: 0.00034508746466599405\n",
            "2604 \tTraning Loss: 0.00027281971415504813\n",
            "2605 \tTraning Loss: 0.00030954336398281157\n",
            "2606 \tTraning Loss: 0.00038664709427393973\n",
            "2607 \tTraning Loss: 0.0002470784820616245\n",
            "2608 \tTraning Loss: 0.00029632088262587786\n",
            "2609 \tTraning Loss: 0.00030590887763537467\n",
            "2610 \tTraning Loss: 0.00031784200109541416\n",
            "2611 \tTraning Loss: 0.0005540449637919664\n",
            "2612 \tTraning Loss: 0.00024796801153570414\n",
            "2613 \tTraning Loss: 0.00045075477100908756\n",
            "2614 \tTraning Loss: 0.00042124674655497074\n",
            "2615 \tTraning Loss: 0.0004063356900587678\n",
            "2616 \tTraning Loss: 0.00018199553596787155\n",
            "2617 \tTraning Loss: 0.0003070987295359373\n",
            "2618 \tTraning Loss: 0.00027743956889025867\n",
            "2619 \tTraning Loss: 0.00043799224658869207\n",
            "2620 \tTraning Loss: 0.0003178785846102983\n",
            "2621 \tTraning Loss: 0.0003435460093896836\n",
            "2622 \tTraning Loss: 0.00020500113896559924\n",
            "2623 \tTraning Loss: 0.00034524701186455786\n",
            "2624 \tTraning Loss: 0.00030386869912035763\n",
            "2625 \tTraning Loss: 0.0003547833766788244\n",
            "2626 \tTraning Loss: 0.00030587782384827733\n",
            "2627 \tTraning Loss: 0.00034056560252793133\n",
            "2628 \tTraning Loss: 0.0003331210173200816\n",
            "2629 \tTraning Loss: 0.00026657647686079144\n",
            "2630 \tTraning Loss: 0.0004503623058553785\n",
            "2631 \tTraning Loss: 0.0002687376691028476\n",
            "2632 \tTraning Loss: 0.0003166329115629196\n",
            "2633 \tTraning Loss: 0.00032413299777545035\n",
            "2634 \tTraning Loss: 0.00025488660321570933\n",
            "2635 \tTraning Loss: 0.0003943440387956798\n",
            "2636 \tTraning Loss: 0.0003691118326969445\n",
            "2637 \tTraning Loss: 0.00041245241300202906\n",
            "2638 \tTraning Loss: 0.0004006895760539919\n",
            "2639 \tTraning Loss: 0.00042097008554264903\n",
            "2640 \tTraning Loss: 0.0003949783858843148\n",
            "2641 \tTraning Loss: 0.0003197171026840806\n",
            "2642 \tTraning Loss: 0.00034451682586222887\n",
            "2643 \tTraning Loss: 0.00032361785997636616\n",
            "2644 \tTraning Loss: 0.00020618585404008627\n",
            "2645 \tTraning Loss: 0.0004522409290075302\n",
            "2646 \tTraning Loss: 0.00025303466827608645\n",
            "2647 \tTraning Loss: 0.00033497967524454\n",
            "2648 \tTraning Loss: 0.0003198864287696779\n",
            "2649 \tTraning Loss: 0.0002569053613115102\n",
            "2650 \tTraning Loss: 0.00038279767613857985\n",
            "2651 \tTraning Loss: 0.0003670023288577795\n",
            "2652 \tTraning Loss: 0.0003568028623703867\n",
            "2653 \tTraning Loss: 0.0002452439221087843\n",
            "2654 \tTraning Loss: 0.00034557306207716465\n",
            "2655 \tTraning Loss: 0.0003198490012437105\n",
            "2656 \tTraning Loss: 0.0002537771943025291\n",
            "2657 \tTraning Loss: 0.00034370188950560987\n",
            "2658 \tTraning Loss: 0.0004320926091168076\n",
            "2659 \tTraning Loss: 0.0002847210271283984\n",
            "2660 \tTraning Loss: 0.0003038353461306542\n",
            "2661 \tTraning Loss: 0.0003994717844761908\n",
            "2662 \tTraning Loss: 0.00027442281134426594\n",
            "2663 \tTraning Loss: 0.00027663569198921323\n",
            "2664 \tTraning Loss: 0.00031918135937303305\n",
            "2665 \tTraning Loss: 0.0003115705621894449\n",
            "2666 \tTraning Loss: 0.00044763184268958867\n",
            "2667 \tTraning Loss: 0.0004213778011035174\n",
            "2668 \tTraning Loss: 0.0003036127891391516\n",
            "2669 \tTraning Loss: 0.00044388946844264865\n",
            "2670 \tTraning Loss: 0.000294162193313241\n",
            "2671 \tTraning Loss: 0.00030307433917187154\n",
            "2672 \tTraning Loss: 0.0003269709413871169\n",
            "2673 \tTraning Loss: 0.0002680690377019346\n",
            "2674 \tTraning Loss: 0.00038578029489144683\n",
            "2675 \tTraning Loss: 0.00028350201318971813\n",
            "2676 \tTraning Loss: 0.00035592078347690403\n",
            "2677 \tTraning Loss: 0.00027967471396550536\n",
            "2678 \tTraning Loss: 0.00040605859248898923\n",
            "2679 \tTraning Loss: 0.000520371540915221\n",
            "2680 \tTraning Loss: 0.00033730632276274264\n",
            "2681 \tTraning Loss: 0.000396509567508474\n",
            "2682 \tTraning Loss: 0.0003213640011381358\n",
            "2683 \tTraning Loss: 0.00019900202460121363\n",
            "2684 \tTraning Loss: 0.00042345476686023176\n",
            "2685 \tTraning Loss: 0.00042108845082111657\n",
            "2686 \tTraning Loss: 0.0002361529623158276\n",
            "2687 \tTraning Loss: 0.00033327893470413983\n",
            "2688 \tTraning Loss: 0.0002866633585654199\n",
            "2689 \tTraning Loss: 0.00040349707705900073\n",
            "2690 \tTraning Loss: 0.0003226548433303833\n",
            "2691 \tTraning Loss: 0.0003736089274752885\n",
            "2692 \tTraning Loss: 0.0003122718189842999\n",
            "2693 \tTraning Loss: 0.00030704852542839944\n",
            "2694 \tTraning Loss: 0.0002492270723450929\n",
            "2695 \tTraning Loss: 0.0003022692690137774\n",
            "2696 \tTraning Loss: 0.0002759113267529756\n",
            "2697 \tTraning Loss: 0.0004967679851688445\n",
            "2698 \tTraning Loss: 0.00028329703491181135\n",
            "2699 \tTraning Loss: 0.0002945369924418628\n",
            "2700 \tTraning Loss: 0.00029438809724524617\n",
            "2701 \tTraning Loss: 0.00041204606532119215\n",
            "2702 \tTraning Loss: 0.00031651477911509573\n",
            "2703 \tTraning Loss: 0.00023982487618923187\n",
            "2704 \tTraning Loss: 0.0005013885674998164\n",
            "2705 \tTraning Loss: 0.0003643358941189945\n",
            "2706 \tTraning Loss: 0.0004312689998187125\n",
            "2707 \tTraning Loss: 0.0003399637062102556\n",
            "2708 \tTraning Loss: 0.00039380084490403533\n",
            "2709 \tTraning Loss: 0.00028972659492865205\n",
            "2710 \tTraning Loss: 0.0003308302548248321\n",
            "2711 \tTraning Loss: 0.0002252042613690719\n",
            "2712 \tTraning Loss: 0.0004000714689027518\n",
            "2713 \tTraning Loss: 0.0002780286886263639\n",
            "2714 \tTraning Loss: 0.0004915503086522222\n",
            "2715 \tTraning Loss: 0.00030996507848612964\n",
            "2716 \tTraning Loss: 0.0003106126678176224\n",
            "2717 \tTraning Loss: 0.0003256100171711296\n",
            "2718 \tTraning Loss: 0.0002695521106943488\n",
            "2719 \tTraning Loss: 0.0002710017142817378\n",
            "2720 \tTraning Loss: 0.0002872175828088075\n",
            "2721 \tTraning Loss: 0.0002724675287026912\n",
            "2722 \tTraning Loss: 0.00038823916111141443\n",
            "2723 \tTraning Loss: 0.00029817703762091696\n",
            "2724 \tTraning Loss: 0.00033961687586270273\n",
            "2725 \tTraning Loss: 0.00039828885928727686\n",
            "2726 \tTraning Loss: 0.0004001033375971019\n",
            "2727 \tTraning Loss: 0.0004374101699795574\n",
            "2728 \tTraning Loss: 0.00032517765066586435\n",
            "2729 \tTraning Loss: 0.0002959349367301911\n",
            "2730 \tTraning Loss: 0.000306358328089118\n",
            "2731 \tTraning Loss: 0.00048388243885710835\n",
            "2732 \tTraning Loss: 0.00023211543157231063\n",
            "2733 \tTraning Loss: 0.00034257920924574137\n",
            "2734 \tTraning Loss: 0.0002853750193025917\n",
            "2735 \tTraning Loss: 0.0003538275486789644\n",
            "2736 \tTraning Loss: 0.0002493301872164011\n",
            "2737 \tTraning Loss: 0.0003425471077207476\n",
            "2738 \tTraning Loss: 0.00032265347545035183\n",
            "2739 \tTraning Loss: 0.0002609901421237737\n",
            "2740 \tTraning Loss: 0.0003663059032987803\n",
            "2741 \tTraning Loss: 0.0003681159869302064\n",
            "2742 \tTraning Loss: 0.0003677929053083062\n",
            "2743 \tTraning Loss: 0.00045718703768216074\n",
            "2744 \tTraning Loss: 0.0002859363448806107\n",
            "2745 \tTraning Loss: 0.00034576086909510195\n",
            "2746 \tTraning Loss: 0.00028845283668488264\n",
            "2747 \tTraning Loss: 0.0003786208981182426\n",
            "2748 \tTraning Loss: 0.00039474962977692485\n",
            "2749 \tTraning Loss: 0.00038079346995800734\n",
            "2750 \tTraning Loss: 0.00038534632767550647\n",
            "2751 \tTraning Loss: 0.0004333758261054754\n",
            "2752 \tTraning Loss: 0.0003804675361607224\n",
            "2753 \tTraning Loss: 0.0002822116366587579\n",
            "2754 \tTraning Loss: 0.0002284171641804278\n",
            "2755 \tTraning Loss: 0.00046344808652065694\n",
            "2756 \tTraning Loss: 0.0002955537929665297\n",
            "2757 \tTraning Loss: 0.00042264899821020663\n",
            "2758 \tTraning Loss: 0.0004057568439748138\n",
            "2759 \tTraning Loss: 0.0002985314349643886\n",
            "2760 \tTraning Loss: 0.0002126172767020762\n",
            "2761 \tTraning Loss: 0.0004920637584291399\n",
            "2762 \tTraning Loss: 0.00029024918330833316\n",
            "2763 \tTraning Loss: 0.0002856975479517132\n",
            "2764 \tTraning Loss: 0.0002977849799208343\n",
            "2765 \tTraning Loss: 0.0002488438622094691\n",
            "2766 \tTraning Loss: 0.0003118680906482041\n",
            "2767 \tTraning Loss: 0.00031249900348484516\n",
            "2768 \tTraning Loss: 0.00029282274772413075\n",
            "2769 \tTraning Loss: 0.00024346529971808195\n",
            "2770 \tTraning Loss: 0.00036788376746699214\n",
            "2771 \tTraning Loss: 0.0005365167744457722\n",
            "2772 \tTraning Loss: 0.00023811135906726122\n",
            "2773 \tTraning Loss: 0.0004109177680220455\n",
            "2774 \tTraning Loss: 0.0002983395825140178\n",
            "2775 \tTraning Loss: 0.0003387673932593316\n",
            "2776 \tTraning Loss: 0.00022736896062269807\n",
            "2777 \tTraning Loss: 0.0002830570447258651\n",
            "2778 \tTraning Loss: 0.0003936513385269791\n",
            "2779 \tTraning Loss: 0.0004196423396933824\n",
            "2780 \tTraning Loss: 0.0002637595171108842\n",
            "2781 \tTraning Loss: 0.00025797358830459416\n",
            "2782 \tTraning Loss: 0.0003538668097462505\n",
            "2783 \tTraning Loss: 0.0003004323225468397\n",
            "2784 \tTraning Loss: 0.0002752029977273196\n",
            "2785 \tTraning Loss: 0.00029007240664213896\n",
            "2786 \tTraning Loss: 0.00023939699167385697\n",
            "2787 \tTraning Loss: 0.0002785364631563425\n",
            "2788 \tTraning Loss: 0.0003156523162033409\n",
            "2789 \tTraning Loss: 0.00039431199547834694\n",
            "2790 \tTraning Loss: 0.00043064472265541553\n",
            "2791 \tTraning Loss: 0.00028681132243946195\n",
            "2792 \tTraning Loss: 0.00019088007684331387\n",
            "2793 \tTraning Loss: 0.00022135517792776227\n",
            "2794 \tTraning Loss: 0.00028860708698630333\n",
            "2795 \tTraning Loss: 0.0003204616950824857\n",
            "2796 \tTraning Loss: 0.00024188257521018386\n",
            "2797 \tTraning Loss: 0.00033799189259298146\n",
            "2798 \tTraning Loss: 0.0002409267472103238\n",
            "2799 \tTraning Loss: 0.0002774522581603378\n",
            "2800 \tTraning Loss: 0.00024327509163413197\n",
            "2801 \tTraning Loss: 0.00026451938902027905\n",
            "2802 \tTraning Loss: 0.0003580671036615968\n",
            "2803 \tTraning Loss: 0.0003071678802371025\n",
            "2804 \tTraning Loss: 0.00030990055529400706\n",
            "2805 \tTraning Loss: 0.000348958681570366\n",
            "2806 \tTraning Loss: 0.0002686688967514783\n",
            "2807 \tTraning Loss: 0.00028524213121272624\n",
            "2808 \tTraning Loss: 0.0002836590865626931\n",
            "2809 \tTraning Loss: 0.0003531660186126828\n",
            "2810 \tTraning Loss: 0.0002801308874040842\n",
            "2811 \tTraning Loss: 0.0003629889106377959\n",
            "2812 \tTraning Loss: 0.00048547229380346835\n",
            "2813 \tTraning Loss: 0.0004548945580609143\n",
            "2814 \tTraning Loss: 0.0003112392732873559\n",
            "2815 \tTraning Loss: 0.00026447209529578686\n",
            "2816 \tTraning Loss: 0.00019200838869437575\n",
            "2817 \tTraning Loss: 0.00029626445029862225\n",
            "2818 \tTraning Loss: 0.0003649615100584924\n",
            "2819 \tTraning Loss: 0.0003008542989846319\n",
            "2820 \tTraning Loss: 0.00022636368521489203\n",
            "2821 \tTraning Loss: 0.00024809353635646403\n",
            "2822 \tTraning Loss: 0.000291725795250386\n",
            "2823 \tTraning Loss: 0.0003141550696454942\n",
            "2824 \tTraning Loss: 0.00043071730760857463\n",
            "2825 \tTraning Loss: 0.00043731395271606743\n",
            "2826 \tTraning Loss: 0.00035516024217940867\n",
            "2827 \tTraning Loss: 0.0002993824891746044\n",
            "2828 \tTraning Loss: 0.0003856383264064789\n",
            "2829 \tTraning Loss: 0.0002340964274480939\n",
            "2830 \tTraning Loss: 0.0003691001911647618\n",
            "2831 \tTraning Loss: 0.00035524286795407534\n",
            "2832 \tTraning Loss: 0.00020696323190350085\n",
            "2833 \tTraning Loss: 0.00024603039491921663\n",
            "2834 \tTraning Loss: 0.00038538611261174083\n",
            "2835 \tTraning Loss: 0.0003201914660166949\n",
            "2836 \tTraning Loss: 0.0004350679228082299\n",
            "2837 \tTraning Loss: 0.00035692803794518113\n",
            "2838 \tTraning Loss: 0.00031079689506441355\n",
            "2839 \tTraning Loss: 0.0002442922268528491\n",
            "2840 \tTraning Loss: 0.00031141203362494707\n",
            "2841 \tTraning Loss: 0.00034964937367476523\n",
            "2842 \tTraning Loss: 0.00044736149720847607\n",
            "2843 \tTraning Loss: 0.0002712293644435704\n",
            "2844 \tTraning Loss: 0.0002757118199951947\n",
            "2845 \tTraning Loss: 0.00023857969790697098\n",
            "2846 \tTraning Loss: 0.00027230760315433145\n",
            "2847 \tTraning Loss: 0.0004014535516034812\n",
            "2848 \tTraning Loss: 0.00035622858558781445\n",
            "2849 \tTraning Loss: 0.00026752392295747995\n",
            "2850 \tTraning Loss: 0.0002221970644313842\n",
            "2851 \tTraning Loss: 0.00029217847622931004\n",
            "2852 \tTraning Loss: 0.0003648950660135597\n",
            "2853 \tTraning Loss: 0.0004143948608543724\n",
            "2854 \tTraning Loss: 0.00035428631235845387\n",
            "2855 \tTraning Loss: 0.00031831380329094827\n",
            "2856 \tTraning Loss: 0.0003733833145815879\n",
            "2857 \tTraning Loss: 0.0002839431108441204\n",
            "2858 \tTraning Loss: 0.0002696543524507433\n",
            "2859 \tTraning Loss: 0.000294101657345891\n",
            "2860 \tTraning Loss: 0.0003015961265191436\n",
            "2861 \tTraning Loss: 0.0003255214833188802\n",
            "2862 \tTraning Loss: 0.0004135324852541089\n",
            "2863 \tTraning Loss: 0.00029404935776256025\n",
            "2864 \tTraning Loss: 0.0002687188098207116\n",
            "2865 \tTraning Loss: 0.00030993076507002115\n",
            "2866 \tTraning Loss: 0.0003006783954333514\n",
            "2867 \tTraning Loss: 0.0004657863755710423\n",
            "2868 \tTraning Loss: 0.0004382410552352667\n",
            "2869 \tTraning Loss: 0.0003589892294257879\n",
            "2870 \tTraning Loss: 0.00040132077992893755\n",
            "2871 \tTraning Loss: 0.0003209255519323051\n",
            "2872 \tTraning Loss: 0.00032646843465045094\n",
            "2873 \tTraning Loss: 0.0003568670654203743\n",
            "2874 \tTraning Loss: 0.00042478731484152377\n",
            "2875 \tTraning Loss: 0.00020856600895058364\n",
            "2876 \tTraning Loss: 0.00022861560864839703\n",
            "2877 \tTraning Loss: 0.0002625708293635398\n",
            "2878 \tTraning Loss: 0.0002713551511988044\n",
            "2879 \tTraning Loss: 0.000350243499269709\n",
            "2880 \tTraning Loss: 0.0002804364776238799\n",
            "2881 \tTraning Loss: 0.00030140162562020123\n",
            "2882 \tTraning Loss: 0.00037728913594037294\n",
            "2883 \tTraning Loss: 0.00028243439737707376\n",
            "2884 \tTraning Loss: 0.00026812226860783994\n",
            "2885 \tTraning Loss: 0.00021802652918267995\n",
            "2886 \tTraning Loss: 0.0003096415603067726\n",
            "2887 \tTraning Loss: 0.00030333601171150804\n",
            "2888 \tTraning Loss: 0.00037937983870506287\n",
            "2889 \tTraning Loss: 0.00028316478710621595\n",
            "2890 \tTraning Loss: 0.0003665145195554942\n",
            "2891 \tTraning Loss: 0.0002565392351243645\n",
            "2892 \tTraning Loss: 0.0003772485360968858\n",
            "2893 \tTraning Loss: 0.00044825844815932214\n",
            "2894 \tTraning Loss: 0.0002728356630541384\n",
            "2895 \tTraning Loss: 0.00026274306583218277\n",
            "2896 \tTraning Loss: 0.00029538109083659947\n",
            "2897 \tTraning Loss: 0.00034332371433265507\n",
            "2898 \tTraning Loss: 0.00024236332683358341\n",
            "2899 \tTraning Loss: 0.0003277119540143758\n",
            "2900 \tTraning Loss: 0.0003552997950464487\n",
            "2901 \tTraning Loss: 0.00029304169584065676\n",
            "2902 \tTraning Loss: 0.0002020174142671749\n",
            "2903 \tTraning Loss: 0.0002778917842078954\n",
            "2904 \tTraning Loss: 0.00041819759644567966\n",
            "2905 \tTraning Loss: 0.0002691876725293696\n",
            "2906 \tTraning Loss: 0.0002922742860391736\n",
            "2907 \tTraning Loss: 0.00040493541746400297\n",
            "2908 \tTraning Loss: 0.0003896690614055842\n",
            "2909 \tTraning Loss: 0.00020062098337803036\n",
            "2910 \tTraning Loss: 0.0001986782590392977\n",
            "2911 \tTraning Loss: 0.00028294167714193463\n",
            "2912 \tTraning Loss: 0.00031667418079450727\n",
            "2913 \tTraning Loss: 0.00026275136042386293\n",
            "2914 \tTraning Loss: 0.00034287202288396657\n",
            "2915 \tTraning Loss: 0.0002595776750240475\n",
            "2916 \tTraning Loss: 0.0003963042690884322\n",
            "2917 \tTraning Loss: 0.0004226207092870027\n",
            "2918 \tTraning Loss: 0.00037256404175423086\n",
            "2919 \tTraning Loss: 0.00037910425453446805\n",
            "2920 \tTraning Loss: 0.0004507187695708126\n",
            "2921 \tTraning Loss: 0.00038176748785190284\n",
            "2922 \tTraning Loss: 0.00048149959184229374\n",
            "2923 \tTraning Loss: 0.00021706825646106154\n",
            "2924 \tTraning Loss: 0.0003078647132497281\n",
            "2925 \tTraning Loss: 0.00027112054522149265\n",
            "2926 \tTraning Loss: 0.0003571717534214258\n",
            "2927 \tTraning Loss: 0.0003205902175977826\n",
            "2928 \tTraning Loss: 0.0004669116751756519\n",
            "2929 \tTraning Loss: 0.00023072566546034068\n",
            "2930 \tTraning Loss: 0.00036140423617325723\n",
            "2931 \tTraning Loss: 0.0002686748339328915\n",
            "2932 \tTraning Loss: 0.0003570345288608223\n",
            "2933 \tTraning Loss: 0.00031748597393743694\n",
            "2934 \tTraning Loss: 0.0001845511724241078\n",
            "2935 \tTraning Loss: 0.00026856851764023304\n",
            "2936 \tTraning Loss: 0.00025613102479837835\n",
            "2937 \tTraning Loss: 0.00020379145280458033\n",
            "2938 \tTraning Loss: 0.00028967001708224416\n",
            "2939 \tTraning Loss: 0.0003841155848931521\n",
            "2940 \tTraning Loss: 0.000372835696907714\n",
            "2941 \tTraning Loss: 0.0003620755160227418\n",
            "2942 \tTraning Loss: 0.0003060864401049912\n",
            "2943 \tTraning Loss: 0.0003768673923332244\n",
            "2944 \tTraning Loss: 0.0003107020747847855\n",
            "2945 \tTraning Loss: 0.00026120932307094336\n",
            "2946 \tTraning Loss: 0.00026395468739792705\n",
            "2947 \tTraning Loss: 0.00022543856175616384\n",
            "2948 \tTraning Loss: 0.00022580074437428266\n",
            "2949 \tTraning Loss: 0.00040271200123243034\n",
            "2950 \tTraning Loss: 0.0003874549875035882\n",
            "2951 \tTraning Loss: 0.0002595348923932761\n",
            "2952 \tTraning Loss: 0.0003747107111848891\n",
            "2953 \tTraning Loss: 0.00031914719147607684\n",
            "2954 \tTraning Loss: 0.00034216619678772986\n",
            "2955 \tTraning Loss: 0.00030500206048600376\n",
            "2956 \tTraning Loss: 0.00032411038409918547\n",
            "2957 \tTraning Loss: 0.0003705556446220726\n",
            "2958 \tTraning Loss: 0.0002449990715831518\n",
            "2959 \tTraning Loss: 0.00040774515946395695\n",
            "2960 \tTraning Loss: 0.00033148235525004566\n",
            "2961 \tTraning Loss: 0.00039167344220913947\n",
            "2962 \tTraning Loss: 0.0003343381395097822\n",
            "2963 \tTraning Loss: 0.0002385368716204539\n",
            "2964 \tTraning Loss: 0.00030122962198220193\n",
            "2965 \tTraning Loss: 0.00031750526977702975\n",
            "2966 \tTraning Loss: 0.0003383312141522765\n",
            "2967 \tTraning Loss: 0.0003961987094953656\n",
            "2968 \tTraning Loss: 0.00035422947257757187\n",
            "2969 \tTraning Loss: 0.00031767840846441686\n",
            "2970 \tTraning Loss: 0.0003557742456905544\n",
            "2971 \tTraning Loss: 0.0003517175209708512\n",
            "2972 \tTraning Loss: 0.0003258718643337488\n",
            "2973 \tTraning Loss: 0.00031845123157836497\n",
            "2974 \tTraning Loss: 0.00028867210494354367\n",
            "2975 \tTraning Loss: 0.00019712577341124415\n",
            "2976 \tTraning Loss: 0.0004991407040506601\n",
            "2977 \tTraning Loss: 0.0002886036818381399\n",
            "2978 \tTraning Loss: 0.00033912487560883164\n",
            "2979 \tTraning Loss: 0.0004024948284495622\n",
            "2980 \tTraning Loss: 0.00030942843295633793\n",
            "2981 \tTraning Loss: 0.0003822628641501069\n",
            "2982 \tTraning Loss: 0.00023714643612038344\n",
            "2983 \tTraning Loss: 0.00034622271778061986\n",
            "2984 \tTraning Loss: 0.0003446933696977794\n",
            "2985 \tTraning Loss: 0.00041495339246466756\n",
            "2986 \tTraning Loss: 0.0003334694483783096\n",
            "2987 \tTraning Loss: 0.00036429319879971445\n",
            "2988 \tTraning Loss: 0.00033056121901609004\n",
            "2989 \tTraning Loss: 0.0002765438111964613\n",
            "2990 \tTraning Loss: 0.0002814911713358015\n",
            "2991 \tTraning Loss: 0.0002841117966454476\n",
            "2992 \tTraning Loss: 0.0002144150494132191\n",
            "2993 \tTraning Loss: 0.0003541706537362188\n",
            "2994 \tTraning Loss: 0.000230477744480595\n",
            "2995 \tTraning Loss: 0.00015639077173545957\n",
            "2996 \tTraning Loss: 0.00026345046353526413\n",
            "2997 \tTraning Loss: 0.00036867414019070566\n",
            "2998 \tTraning Loss: 0.0004743794270325452\n",
            "2999 \tTraning Loss: 0.0003423814778216183\n",
            "3000 \tTraning Loss: 0.00026673416141420603\n",
            "3001 \tTraning Loss: 0.0003479988081380725\n",
            "3002 \tTraning Loss: 0.00040260105743072927\n",
            "3003 \tTraning Loss: 0.0003804174775723368\n",
            "3004 \tTraning Loss: 0.00022521043138112873\n",
            "3005 \tTraning Loss: 0.0002690425608307123\n",
            "3006 \tTraning Loss: 0.000421978096710518\n",
            "3007 \tTraning Loss: 0.0001975725608645007\n",
            "3008 \tTraning Loss: 0.0003830769273918122\n",
            "3009 \tTraning Loss: 0.0002348298003198579\n",
            "3010 \tTraning Loss: 0.00019426812650635839\n",
            "3011 \tTraning Loss: 0.000297547405352816\n",
            "3012 \tTraning Loss: 0.00033406808506697416\n",
            "3013 \tTraning Loss: 0.00027176132425665855\n",
            "3014 \tTraning Loss: 0.00017717208538670093\n",
            "3015 \tTraning Loss: 0.00024408813624177128\n",
            "3016 \tTraning Loss: 0.0003274149203207344\n",
            "3017 \tTraning Loss: 0.0002443970297463238\n",
            "3018 \tTraning Loss: 0.0002920598490163684\n",
            "3019 \tTraning Loss: 0.00038428130210377276\n",
            "3020 \tTraning Loss: 0.0002321235806448385\n",
            "3021 \tTraning Loss: 0.0003479558217804879\n",
            "3022 \tTraning Loss: 0.0003580562479328364\n",
            "3023 \tTraning Loss: 0.00024355041387025267\n",
            "3024 \tTraning Loss: 0.00030983355827629566\n",
            "3025 \tTraning Loss: 0.00018211272254120559\n",
            "3026 \tTraning Loss: 0.0002606537891551852\n",
            "3027 \tTraning Loss: 0.00027531516388989985\n",
            "3028 \tTraning Loss: 0.00044432756840251386\n",
            "3029 \tTraning Loss: 0.00021711930457968265\n",
            "3030 \tTraning Loss: 0.00031409424263983965\n",
            "3031 \tTraning Loss: 0.00030974563560448587\n",
            "3032 \tTraning Loss: 0.0002878993982449174\n",
            "3033 \tTraning Loss: 0.0002630575036164373\n",
            "3034 \tTraning Loss: 0.00034924186184071004\n",
            "3035 \tTraning Loss: 0.00022016471484676003\n",
            "3036 \tTraning Loss: 0.0002381661906838417\n",
            "3037 \tTraning Loss: 0.00032372615532949567\n",
            "3038 \tTraning Loss: 0.00026962137781083584\n",
            "3039 \tTraning Loss: 0.0003015678084921092\n",
            "3040 \tTraning Loss: 0.00022414718114305288\n",
            "3041 \tTraning Loss: 0.0002988588821608573\n",
            "3042 \tTraning Loss: 0.0002823702816385776\n",
            "3043 \tTraning Loss: 0.00035928672878071666\n",
            "3044 \tTraning Loss: 0.00039835929055698216\n",
            "3045 \tTraning Loss: 0.00044479541247710586\n",
            "3046 \tTraning Loss: 0.0003720166569110006\n",
            "3047 \tTraning Loss: 0.0003197316837031394\n",
            "3048 \tTraning Loss: 0.00020979750843252987\n",
            "3049 \tTraning Loss: 0.00038570945616811514\n",
            "3050 \tTraning Loss: 0.00033618949237279594\n",
            "3051 \tTraning Loss: 0.00030986109049990773\n",
            "3052 \tTraning Loss: 0.00027394501375965774\n",
            "3053 \tTraning Loss: 0.0002516045933589339\n",
            "3054 \tTraning Loss: 0.00017715823196340352\n",
            "3055 \tTraning Loss: 0.000339530932251364\n",
            "3056 \tTraning Loss: 0.00029997245292179286\n",
            "3057 \tTraning Loss: 0.00034633598988875747\n",
            "3058 \tTraning Loss: 0.00023889687145128846\n",
            "3059 \tTraning Loss: 0.0003148545802105218\n",
            "3060 \tTraning Loss: 0.0003258005017414689\n",
            "3061 \tTraning Loss: 0.00041758641600608826\n",
            "3062 \tTraning Loss: 0.00035324005875736475\n",
            "3063 \tTraning Loss: 0.0002504979493096471\n",
            "3064 \tTraning Loss: 0.0003236275224480778\n",
            "3065 \tTraning Loss: 0.0002385069674346596\n",
            "3066 \tTraning Loss: 0.0001719605061225593\n",
            "3067 \tTraning Loss: 0.00026259850710630417\n",
            "3068 \tTraning Loss: 0.00016153111937455833\n",
            "3069 \tTraning Loss: 0.00045187780051492155\n",
            "3070 \tTraning Loss: 0.0003437463892623782\n",
            "3071 \tTraning Loss: 0.00031648774165660143\n",
            "3072 \tTraning Loss: 0.0002856064820662141\n",
            "3073 \tTraning Loss: 0.0003234304895158857\n",
            "3074 \tTraning Loss: 0.0002769271959550679\n",
            "3075 \tTraning Loss: 0.00032563816057518125\n",
            "3076 \tTraning Loss: 0.00034820332075469196\n",
            "3077 \tTraning Loss: 0.00017585800378583372\n",
            "3078 \tTraning Loss: 0.00035119455424137414\n",
            "3079 \tTraning Loss: 0.0003044804616365582\n",
            "3080 \tTraning Loss: 0.00034346035681664944\n",
            "3081 \tTraning Loss: 0.000321989442454651\n",
            "3082 \tTraning Loss: 0.00028017585282213986\n",
            "3083 \tTraning Loss: 0.00032412970904260874\n",
            "3084 \tTraning Loss: 0.0003206562250852585\n",
            "3085 \tTraning Loss: 0.0002468528982717544\n",
            "3086 \tTraning Loss: 0.00048008240992203355\n",
            "3087 \tTraning Loss: 0.00023381190840154886\n",
            "3088 \tTraning Loss: 0.00027920716092921793\n",
            "3089 \tTraning Loss: 0.0003551847185008228\n",
            "3090 \tTraning Loss: 0.00028790245414711535\n",
            "3091 \tTraning Loss: 0.0003346156736370176\n",
            "3092 \tTraning Loss: 0.0003209138521924615\n",
            "3093 \tTraning Loss: 0.00035918678622692823\n",
            "3094 \tTraning Loss: 0.000341955223120749\n",
            "3095 \tTraning Loss: 0.00021732460299972445\n",
            "3096 \tTraning Loss: 0.0002442790428176522\n",
            "3097 \tTraning Loss: 0.0003096938016824424\n",
            "3098 \tTraning Loss: 0.00030921862344257534\n",
            "3099 \tTraning Loss: 0.0001782335457392037\n",
            "3100 \tTraning Loss: 0.00028887760709039867\n",
            "3101 \tTraning Loss: 0.00018727178394328803\n",
            "3102 \tTraning Loss: 0.000328068999806419\n",
            "3103 \tTraning Loss: 0.00022702170826960355\n",
            "3104 \tTraning Loss: 0.0002742176584433764\n",
            "3105 \tTraning Loss: 0.0004080907965544611\n",
            "3106 \tTraning Loss: 0.00024840340483933687\n",
            "3107 \tTraning Loss: 0.0003516500582918525\n",
            "3108 \tTraning Loss: 0.00026631372747942805\n",
            "3109 \tTraning Loss: 0.0003673290484584868\n",
            "3110 \tTraning Loss: 0.00027685024542734027\n",
            "3111 \tTraning Loss: 0.00033486151369288564\n",
            "3112 \tTraning Loss: 0.00026600551791489124\n",
            "3113 \tTraning Loss: 0.0002831096644513309\n",
            "3114 \tTraning Loss: 0.00032828160328790545\n",
            "3115 \tTraning Loss: 0.00029861953225918114\n",
            "3116 \tTraning Loss: 0.00039629076491110027\n",
            "3117 \tTraning Loss: 0.00017444534751120955\n",
            "3118 \tTraning Loss: 0.00021328702860046178\n",
            "3119 \tTraning Loss: 0.0002901880943682045\n",
            "3120 \tTraning Loss: 0.00020250602392479777\n",
            "3121 \tTraning Loss: 0.00027179913013242185\n",
            "3122 \tTraning Loss: 0.00024595862487331033\n",
            "3123 \tTraning Loss: 0.00023313677229452878\n",
            "3124 \tTraning Loss: 0.00021097328863106668\n",
            "3125 \tTraning Loss: 0.00035336922155693173\n",
            "3126 \tTraning Loss: 0.0003966282238252461\n",
            "3127 \tTraning Loss: 0.0004342683532740921\n",
            "3128 \tTraning Loss: 0.0003953625273425132\n",
            "3129 \tTraning Loss: 0.00039190135430544615\n",
            "3130 \tTraning Loss: 0.00018776727665681392\n",
            "3131 \tTraning Loss: 0.0003352751664351672\n",
            "3132 \tTraning Loss: 0.00041483939276076853\n",
            "3133 \tTraning Loss: 0.00030113154207356274\n",
            "3134 \tTraning Loss: 0.00037538877222687006\n",
            "3135 \tTraning Loss: 0.00033666109084151685\n",
            "3136 \tTraning Loss: 0.00033344730036333203\n",
            "3137 \tTraning Loss: 0.00023706960200797766\n",
            "3138 \tTraning Loss: 0.00019262416753917933\n",
            "3139 \tTraning Loss: 0.0003347758902236819\n",
            "3140 \tTraning Loss: 0.00032822584034875035\n",
            "3141 \tTraning Loss: 0.00021141927572898567\n",
            "3142 \tTraning Loss: 0.0003310695756226778\n",
            "3143 \tTraning Loss: 0.00037659931695088744\n",
            "3144 \tTraning Loss: 0.00031034459243528545\n",
            "3145 \tTraning Loss: 0.00022736869868822396\n",
            "3146 \tTraning Loss: 0.0003666056436486542\n",
            "3147 \tTraning Loss: 0.00021202786592766643\n",
            "3148 \tTraning Loss: 0.00036065466701984406\n",
            "3149 \tTraning Loss: 0.00034421563032083213\n",
            "3150 \tTraning Loss: 0.00037207111017778516\n",
            "3151 \tTraning Loss: 0.0002443478733766824\n",
            "3152 \tTraning Loss: 0.00038994578062556684\n",
            "3153 \tTraning Loss: 0.0003118994936812669\n",
            "3154 \tTraning Loss: 0.00022039371833670884\n",
            "3155 \tTraning Loss: 0.00037731992779299617\n",
            "3156 \tTraning Loss: 0.00024664716329425573\n",
            "3157 \tTraning Loss: 0.0003525734064169228\n",
            "3158 \tTraning Loss: 0.00040672568138688803\n",
            "3159 \tTraning Loss: 0.00029818847542628646\n",
            "3160 \tTraning Loss: 0.00037006675847806036\n",
            "3161 \tTraning Loss: 0.00023358712496701628\n",
            "3162 \tTraning Loss: 0.00031661734101362526\n",
            "3163 \tTraning Loss: 0.0002920663100667298\n",
            "3164 \tTraning Loss: 0.00034024761407636106\n",
            "3165 \tTraning Loss: 0.00019802716269623488\n",
            "3166 \tTraning Loss: 0.0004393275885377079\n",
            "3167 \tTraning Loss: 0.0003875858383253217\n",
            "3168 \tTraning Loss: 0.00028897932497784495\n",
            "3169 \tTraning Loss: 0.00016323615272995085\n",
            "3170 \tTraning Loss: 0.00026482826797291636\n",
            "3171 \tTraning Loss: 0.0003735707432497293\n",
            "3172 \tTraning Loss: 0.00030014532967470586\n",
            "3173 \tTraning Loss: 0.0003293971240054816\n",
            "3174 \tTraning Loss: 0.0003608036495279521\n",
            "3175 \tTraning Loss: 0.0003023147874046117\n",
            "3176 \tTraning Loss: 0.0002364422398386523\n",
            "3177 \tTraning Loss: 0.00024431172641925514\n",
            "3178 \tTraning Loss: 0.00031147210393100977\n",
            "3179 \tTraning Loss: 0.0002201426832471043\n",
            "3180 \tTraning Loss: 0.0002600463922135532\n",
            "3181 \tTraning Loss: 0.00022096851898822933\n",
            "3182 \tTraning Loss: 0.0002892454795073718\n",
            "3183 \tTraning Loss: 0.0002732026041485369\n",
            "3184 \tTraning Loss: 0.00028883680352009833\n",
            "3185 \tTraning Loss: 0.00025724564329721034\n",
            "3186 \tTraning Loss: 0.00028129704878665507\n",
            "3187 \tTraning Loss: 0.0002819538058247417\n",
            "3188 \tTraning Loss: 0.00036010175244882703\n",
            "3189 \tTraning Loss: 0.0003445276233833283\n",
            "3190 \tTraning Loss: 0.00024540460435673594\n",
            "3191 \tTraning Loss: 0.0003622589574661106\n",
            "3192 \tTraning Loss: 0.0003230071743018925\n",
            "3193 \tTraning Loss: 0.0001629630132811144\n",
            "3194 \tTraning Loss: 0.00025887315860018134\n",
            "3195 \tTraning Loss: 0.00028894157730974257\n",
            "3196 \tTraning Loss: 0.00030095173860900104\n",
            "3197 \tTraning Loss: 0.00023992259229999036\n",
            "3198 \tTraning Loss: 0.00029352636192925274\n",
            "3199 \tTraning Loss: 0.00032797944732010365\n",
            "3200 \tTraning Loss: 0.00019808851357083768\n",
            "3201 \tTraning Loss: 0.00032192724756896496\n",
            "3202 \tTraning Loss: 0.00017857769853435457\n",
            "3203 \tTraning Loss: 0.00022811112285126\n",
            "3204 \tTraning Loss: 0.000273393583483994\n",
            "3205 \tTraning Loss: 0.0002291784476255998\n",
            "3206 \tTraning Loss: 0.00023499495000578463\n",
            "3207 \tTraning Loss: 0.00030191734549589455\n",
            "3208 \tTraning Loss: 0.00031887792283669114\n",
            "3209 \tTraning Loss: 0.0002475538058206439\n",
            "3210 \tTraning Loss: 0.0002224872587248683\n",
            "3211 \tTraning Loss: 0.000194880849448964\n",
            "3212 \tTraning Loss: 0.0002953991061076522\n",
            "3213 \tTraning Loss: 0.0002142954763257876\n",
            "3214 \tTraning Loss: 0.0004360750026535243\n",
            "3215 \tTraning Loss: 0.00024335506896022707\n",
            "3216 \tTraning Loss: 0.00022009368694853038\n",
            "3217 \tTraning Loss: 0.0003815052332356572\n",
            "3218 \tTraning Loss: 0.00024195727019105107\n",
            "3219 \tTraning Loss: 0.00034276224323548377\n",
            "3220 \tTraning Loss: 0.00027869525365531445\n",
            "3221 \tTraning Loss: 0.00035091134486719966\n",
            "3222 \tTraning Loss: 0.00035225250758230686\n",
            "3223 \tTraning Loss: 0.0003322020929772407\n",
            "3224 \tTraning Loss: 0.00023508969752583653\n",
            "3225 \tTraning Loss: 0.00029527736478485167\n",
            "3226 \tTraning Loss: 0.0002953166258521378\n",
            "3227 \tTraning Loss: 0.000434123765444383\n",
            "3228 \tTraning Loss: 0.00019466881349217147\n",
            "3229 \tTraning Loss: 0.0002795424370560795\n",
            "3230 \tTraning Loss: 0.0003045077610295266\n",
            "3231 \tTraning Loss: 0.00032912741880863905\n",
            "3232 \tTraning Loss: 0.00041742066969163716\n",
            "3233 \tTraning Loss: 0.00020823261002078652\n",
            "3234 \tTraning Loss: 0.0003094445855822414\n",
            "3235 \tTraning Loss: 0.0003243292449042201\n",
            "3236 \tTraning Loss: 0.00023365591187030077\n",
            "3237 \tTraning Loss: 0.0002459546085447073\n",
            "3238 \tTraning Loss: 0.00017056796059478074\n",
            "3239 \tTraning Loss: 0.0003032421227544546\n",
            "3240 \tTraning Loss: 0.00021055048273410648\n",
            "3241 \tTraning Loss: 0.0001507709239376709\n",
            "3242 \tTraning Loss: 0.00024098197172861546\n",
            "3243 \tTraning Loss: 0.0002476793888490647\n",
            "3244 \tTraning Loss: 0.00027464539743959904\n",
            "3245 \tTraning Loss: 0.00020829698769375682\n",
            "3246 \tTraning Loss: 0.000350506859831512\n",
            "3247 \tTraning Loss: 0.00027800071984529495\n",
            "3248 \tTraning Loss: 0.00040816314867697656\n",
            "3249 \tTraning Loss: 0.00026955545763485134\n",
            "3250 \tTraning Loss: 0.00025912810815498233\n",
            "3251 \tTraning Loss: 0.00019522884394973516\n",
            "3252 \tTraning Loss: 0.00016292755026370287\n",
            "3253 \tTraning Loss: 0.00038710053195245564\n",
            "3254 \tTraning Loss: 0.0002813303144648671\n",
            "3255 \tTraning Loss: 0.0001948999270098284\n",
            "3256 \tTraning Loss: 0.00023800396593287587\n",
            "3257 \tTraning Loss: 0.0003184768429491669\n",
            "3258 \tTraning Loss: 0.00025269968318752944\n",
            "3259 \tTraning Loss: 0.000323158863466233\n",
            "3260 \tTraning Loss: 0.00026096575311385095\n",
            "3261 \tTraning Loss: 0.00024841123376972973\n",
            "3262 \tTraning Loss: 0.0002201645984314382\n",
            "3263 \tTraning Loss: 0.00026210802025161684\n",
            "3264 \tTraning Loss: 0.00035034597385674715\n",
            "3265 \tTraning Loss: 0.00022394229017663747\n",
            "3266 \tTraning Loss: 0.00025618946528993547\n",
            "3267 \tTraning Loss: 0.0002951625792775303\n",
            "3268 \tTraning Loss: 0.0003470297669991851\n",
            "3269 \tTraning Loss: 0.00036444776924327016\n",
            "3270 \tTraning Loss: 0.0003497636935207993\n",
            "3271 \tTraning Loss: 0.0002871753240469843\n",
            "3272 \tTraning Loss: 0.0004600752436090261\n",
            "3273 \tTraning Loss: 0.00021087023196741939\n",
            "3274 \tTraning Loss: 0.00023179173876997083\n",
            "3275 \tTraning Loss: 0.0003071014943998307\n",
            "3276 \tTraning Loss: 0.0002576613915152848\n",
            "3277 \tTraning Loss: 0.00036037698737345636\n",
            "3278 \tTraning Loss: 0.0002909806789830327\n",
            "3279 \tTraning Loss: 0.0003103211638517678\n",
            "3280 \tTraning Loss: 0.0003523383056744933\n",
            "3281 \tTraning Loss: 0.0003221615043003112\n",
            "3282 \tTraning Loss: 0.000275336584309116\n",
            "3283 \tTraning Loss: 0.00028747518081218004\n",
            "3284 \tTraning Loss: 0.00023595344100613147\n",
            "3285 \tTraning Loss: 0.00026349566178396344\n",
            "3286 \tTraning Loss: 0.0002954572846647352\n",
            "3287 \tTraning Loss: 0.00021891467622481287\n",
            "3288 \tTraning Loss: 0.00031378594576381147\n",
            "3289 \tTraning Loss: 0.00035514115006662905\n",
            "3290 \tTraning Loss: 0.00018717360217124224\n",
            "3291 \tTraning Loss: 0.0002849735028576106\n",
            "3292 \tTraning Loss: 0.0002707400999497622\n",
            "3293 \tTraning Loss: 0.00020859770302195102\n",
            "3294 \tTraning Loss: 0.00026491028256714344\n",
            "3295 \tTraning Loss: 0.0002818538632709533\n",
            "3296 \tTraning Loss: 0.0004590147000271827\n",
            "3297 \tTraning Loss: 0.0003581825876608491\n",
            "3298 \tTraning Loss: 0.00021061819279566407\n",
            "3299 \tTraning Loss: 0.0003395253443159163\n",
            "3300 \tTraning Loss: 0.00022900728799868375\n",
            "3301 \tTraning Loss: 0.00027083835448138416\n",
            "3302 \tTraning Loss: 0.0002136183757102117\n",
            "3303 \tTraning Loss: 0.00023182958830147982\n",
            "3304 \tTraning Loss: 0.00021501645096577704\n",
            "3305 \tTraning Loss: 0.00024916333495639265\n",
            "3306 \tTraning Loss: 0.00025124181411229074\n",
            "3307 \tTraning Loss: 0.0002747941471170634\n",
            "3308 \tTraning Loss: 0.0002711117558646947\n",
            "3309 \tTraning Loss: 0.00021437830582726747\n",
            "3310 \tTraning Loss: 0.0001811128604458645\n",
            "3311 \tTraning Loss: 0.0002495751832611859\n",
            "3312 \tTraning Loss: 0.00032865838147699833\n",
            "3313 \tTraning Loss: 0.00032434731838293374\n",
            "3314 \tTraning Loss: 0.00023682838946115226\n",
            "3315 \tTraning Loss: 0.00021266886324156076\n",
            "3316 \tTraning Loss: 0.00021803664276376367\n",
            "3317 \tTraning Loss: 0.00022831599926576018\n",
            "3318 \tTraning Loss: 0.00020554187358357012\n",
            "3319 \tTraning Loss: 0.00021668530825991184\n",
            "3320 \tTraning Loss: 0.0002609019575174898\n",
            "3321 \tTraning Loss: 0.00030841014813631773\n",
            "3322 \tTraning Loss: 0.0003178715705871582\n",
            "3323 \tTraning Loss: 0.00022426240320783108\n",
            "3324 \tTraning Loss: 0.00029799158801324666\n",
            "3325 \tTraning Loss: 0.00033768636058084667\n",
            "3326 \tTraning Loss: 0.00036278204061090946\n",
            "3327 \tTraning Loss: 0.00016961766232270747\n",
            "3328 \tTraning Loss: 0.0002601770684123039\n",
            "3329 \tTraning Loss: 0.00030552607495337725\n",
            "3330 \tTraning Loss: 0.000382793543394655\n",
            "3331 \tTraning Loss: 0.00022533730953000486\n",
            "3332 \tTraning Loss: 0.0003787763707805425\n",
            "3333 \tTraning Loss: 0.00018107550567947328\n",
            "3334 \tTraning Loss: 0.00021806130826007575\n",
            "3335 \tTraning Loss: 0.00024726538686081767\n",
            "3336 \tTraning Loss: 0.0002453082415740937\n",
            "3337 \tTraning Loss: 0.0003396151296328753\n",
            "3338 \tTraning Loss: 0.0002908390306401998\n",
            "3339 \tTraning Loss: 0.00023846696421969682\n",
            "3340 \tTraning Loss: 0.00015961570898070931\n",
            "3341 \tTraning Loss: 0.0002319362247362733\n",
            "3342 \tTraning Loss: 0.0002447860315442085\n",
            "3343 \tTraning Loss: 0.00021037692204117775\n",
            "3344 \tTraning Loss: 0.000310535921016708\n",
            "3345 \tTraning Loss: 0.0003279670199844986\n",
            "3346 \tTraning Loss: 0.0002853196347132325\n",
            "3347 \tTraning Loss: 0.00029542631818912923\n",
            "3348 \tTraning Loss: 0.00020717833831440657\n",
            "3349 \tTraning Loss: 0.00025321304565295577\n",
            "3350 \tTraning Loss: 0.00018543402256909758\n",
            "3351 \tTraning Loss: 0.00024948138161562383\n",
            "3352 \tTraning Loss: 0.00033923116279765964\n",
            "3353 \tTraning Loss: 0.0002919003018178046\n",
            "3354 \tTraning Loss: 0.0003021145239472389\n",
            "3355 \tTraning Loss: 0.0003224368847440928\n",
            "3356 \tTraning Loss: 0.00032891816226765513\n",
            "3357 \tTraning Loss: 0.0002632073010317981\n",
            "3358 \tTraning Loss: 0.00033644121140241623\n",
            "3359 \tTraning Loss: 0.00021060839935671538\n",
            "3360 \tTraning Loss: 0.0003691869496833533\n",
            "3361 \tTraning Loss: 0.00022686051670461893\n",
            "3362 \tTraning Loss: 0.0003351381456013769\n",
            "3363 \tTraning Loss: 0.00021646278037223965\n",
            "3364 \tTraning Loss: 0.0003584345104172826\n",
            "3365 \tTraning Loss: 0.00028162874514237046\n",
            "3366 \tTraning Loss: 0.00024116679560393095\n",
            "3367 \tTraning Loss: 0.00018763616390060633\n",
            "3368 \tTraning Loss: 0.00024459706037305295\n",
            "3369 \tTraning Loss: 0.00026215531397610903\n",
            "3370 \tTraning Loss: 0.000250670884270221\n",
            "3371 \tTraning Loss: 0.00016317337576765567\n",
            "3372 \tTraning Loss: 0.0002936749660875648\n",
            "3373 \tTraning Loss: 0.00031960493652150035\n",
            "3374 \tTraning Loss: 0.0002823909162543714\n",
            "3375 \tTraning Loss: 0.000186247838428244\n",
            "3376 \tTraning Loss: 0.00026750299730338156\n",
            "3377 \tTraning Loss: 0.00020057194342371076\n",
            "3378 \tTraning Loss: 0.00018817957607097924\n",
            "3379 \tTraning Loss: 0.0002191238891100511\n",
            "3380 \tTraning Loss: 0.0002618624712340534\n",
            "3381 \tTraning Loss: 0.0002664686762727797\n",
            "3382 \tTraning Loss: 0.00027834236971102655\n",
            "3383 \tTraning Loss: 0.00022712117061018944\n",
            "3384 \tTraning Loss: 0.00023866038827691227\n",
            "3385 \tTraning Loss: 0.0003276834322605282\n",
            "3386 \tTraning Loss: 0.00023097045777831227\n",
            "3387 \tTraning Loss: 0.00024233240401372313\n",
            "3388 \tTraning Loss: 0.00025928454124368727\n",
            "3389 \tTraning Loss: 0.00018886056204792112\n",
            "3390 \tTraning Loss: 0.00019421466276980937\n",
            "3391 \tTraning Loss: 0.00024754638434387743\n",
            "3392 \tTraning Loss: 0.0002116258692694828\n",
            "3393 \tTraning Loss: 0.00022746241302229464\n",
            "3394 \tTraning Loss: 0.0002612506505101919\n",
            "3395 \tTraning Loss: 0.00027125669294036925\n",
            "3396 \tTraning Loss: 0.00036143482429906726\n",
            "3397 \tTraning Loss: 0.00023819605121389031\n",
            "3398 \tTraning Loss: 0.0003463476023171097\n",
            "3399 \tTraning Loss: 0.00028735349769704044\n",
            "3400 \tTraning Loss: 0.00038083831896074116\n",
            "3401 \tTraning Loss: 0.0002500625269021839\n",
            "3402 \tTraning Loss: 0.00033725143293850124\n",
            "3403 \tTraning Loss: 0.0002923792344518006\n",
            "3404 \tTraning Loss: 0.00031516666058450937\n",
            "3405 \tTraning Loss: 0.0003058527363464236\n",
            "3406 \tTraning Loss: 0.00018020282732322812\n",
            "3407 \tTraning Loss: 0.00023988114844541997\n",
            "3408 \tTraning Loss: 0.0002940945269074291\n",
            "3409 \tTraning Loss: 0.0001953575701918453\n",
            "3410 \tTraning Loss: 0.0002105871244566515\n",
            "3411 \tTraning Loss: 0.00026998238172382116\n",
            "3412 \tTraning Loss: 0.00023033801699057221\n",
            "3413 \tTraning Loss: 0.0002564128953963518\n",
            "3414 \tTraning Loss: 0.00018227151304017752\n",
            "3415 \tTraning Loss: 0.00026050227461382747\n",
            "3416 \tTraning Loss: 0.00018453571829013526\n",
            "3417 \tTraning Loss: 0.00020011588640045375\n",
            "3418 \tTraning Loss: 0.00026192504446953535\n",
            "3419 \tTraning Loss: 0.00037405217881314456\n",
            "3420 \tTraning Loss: 0.0003189357230439782\n",
            "3421 \tTraning Loss: 0.0003448244242463261\n",
            "3422 \tTraning Loss: 0.0003166064852848649\n",
            "3423 \tTraning Loss: 0.00015045834879856557\n",
            "3424 \tTraning Loss: 0.0003014317189808935\n",
            "3425 \tTraning Loss: 0.00026198095292784274\n",
            "3426 \tTraning Loss: 0.0001961094094440341\n",
            "3427 \tTraning Loss: 0.00019098799384664744\n",
            "3428 \tTraning Loss: 0.0003037033893633634\n",
            "3429 \tTraning Loss: 0.0003852669906336814\n",
            "3430 \tTraning Loss: 0.00034263680572621524\n",
            "3431 \tTraning Loss: 0.0001882221404230222\n",
            "3432 \tTraning Loss: 0.00021267660486046225\n",
            "3433 \tTraning Loss: 0.00022751737560611218\n",
            "3434 \tTraning Loss: 0.00027630748809315264\n",
            "3435 \tTraning Loss: 0.0003118202439509332\n",
            "3436 \tTraning Loss: 0.00024389363534282893\n",
            "3437 \tTraning Loss: 0.00028800597647204995\n",
            "3438 \tTraning Loss: 0.00016574282199144363\n",
            "3439 \tTraning Loss: 0.00028118479531258345\n",
            "3440 \tTraning Loss: 0.00028876939904876053\n",
            "3441 \tTraning Loss: 0.00019519348279573023\n",
            "3442 \tTraning Loss: 0.00019589510338846594\n",
            "3443 \tTraning Loss: 0.00024200708139687777\n",
            "3444 \tTraning Loss: 0.00018963110051117837\n",
            "3445 \tTraning Loss: 0.0003094675194006413\n",
            "3446 \tTraning Loss: 0.00022854740382172167\n",
            "3447 \tTraning Loss: 0.000212916245800443\n",
            "3448 \tTraning Loss: 0.00016549404244869947\n",
            "3449 \tTraning Loss: 0.0002147826162399724\n",
            "3450 \tTraning Loss: 0.00017979394760914147\n",
            "3451 \tTraning Loss: 0.00027466739993542433\n",
            "3452 \tTraning Loss: 0.00018109672237187624\n",
            "3453 \tTraning Loss: 0.0002866127761080861\n",
            "3454 \tTraning Loss: 0.00043649235158227384\n",
            "3455 \tTraning Loss: 0.0002651027461979538\n",
            "3456 \tTraning Loss: 0.0002686438092496246\n",
            "3457 \tTraning Loss: 0.0002149174688383937\n",
            "3458 \tTraning Loss: 0.0003551534318830818\n",
            "3459 \tTraning Loss: 0.00022814498515799642\n",
            "3460 \tTraning Loss: 0.0002714093716349453\n",
            "3461 \tTraning Loss: 0.0002862187975551933\n",
            "3462 \tTraning Loss: 0.0002866178401745856\n",
            "3463 \tTraning Loss: 0.0003157236787956208\n",
            "3464 \tTraning Loss: 0.00022004294442012906\n",
            "3465 \tTraning Loss: 0.00019231095211580396\n",
            "3466 \tTraning Loss: 0.000316882214974612\n",
            "3467 \tTraning Loss: 0.00026341961347498\n",
            "3468 \tTraning Loss: 0.00018737958453129977\n",
            "3469 \tTraning Loss: 0.00023059207887854427\n",
            "3470 \tTraning Loss: 0.0002105837338604033\n",
            "3471 \tTraning Loss: 0.0004227764147799462\n",
            "3472 \tTraning Loss: 0.0002636084973346442\n",
            "3473 \tTraning Loss: 0.00028919120086357\n",
            "3474 \tTraning Loss: 0.0002410017914371565\n",
            "3475 \tTraning Loss: 0.00021464831661432981\n",
            "3476 \tTraning Loss: 0.0003120478941127658\n",
            "3477 \tTraning Loss: 0.000286026275716722\n",
            "3478 \tTraning Loss: 0.0001998196676140651\n",
            "3479 \tTraning Loss: 0.0001701653964119032\n",
            "3480 \tTraning Loss: 0.00023596020764671266\n",
            "3481 \tTraning Loss: 0.0002271999983349815\n",
            "3482 \tTraning Loss: 0.00023946240253280848\n",
            "3483 \tTraning Loss: 0.00026347371749579906\n",
            "3484 \tTraning Loss: 0.0003203376254532486\n",
            "3485 \tTraning Loss: 0.0002677737211342901\n",
            "3486 \tTraning Loss: 0.00031553200096823275\n",
            "3487 \tTraning Loss: 0.00020161735301371664\n",
            "3488 \tTraning Loss: 0.0001542828103993088\n",
            "3489 \tTraning Loss: 0.0002497161040082574\n",
            "3490 \tTraning Loss: 0.00032651223591528833\n",
            "3491 \tTraning Loss: 0.0003000968717969954\n",
            "3492 \tTraning Loss: 0.0002720841730479151\n",
            "3493 \tTraning Loss: 0.00020094694627914578\n",
            "3494 \tTraning Loss: 0.00025086072855629027\n",
            "3495 \tTraning Loss: 0.00038326397771015763\n",
            "3496 \tTraning Loss: 0.00028001549071632326\n",
            "3497 \tTraning Loss: 0.0003225088003091514\n",
            "3498 \tTraning Loss: 0.0002320934145245701\n",
            "3499 \tTraning Loss: 0.00022864557104185224\n",
            "3500 \tTraning Loss: 0.00019824916671495885\n",
            "3501 \tTraning Loss: 0.00020185686298646033\n",
            "3502 \tTraning Loss: 0.0004469386185519397\n",
            "3503 \tTraning Loss: 0.00020461800158955157\n",
            "3504 \tTraning Loss: 0.00029169858316890895\n",
            "3505 \tTraning Loss: 0.0002679213648661971\n",
            "3506 \tTraning Loss: 0.00019947567488998175\n",
            "3507 \tTraning Loss: 0.00019949501438532025\n",
            "3508 \tTraning Loss: 0.00022449296375270933\n",
            "3509 \tTraning Loss: 0.0002556666440796107\n",
            "3510 \tTraning Loss: 0.00023904624686110765\n",
            "3511 \tTraning Loss: 0.00018207859829999506\n",
            "3512 \tTraning Loss: 0.0003170441777911037\n",
            "3513 \tTraning Loss: 0.0002349570277146995\n",
            "3514 \tTraning Loss: 0.00026917323702946305\n",
            "3515 \tTraning Loss: 0.00014267084770835936\n",
            "3516 \tTraning Loss: 0.0002420786040602252\n",
            "3517 \tTraning Loss: 0.00031007674988359213\n",
            "3518 \tTraning Loss: 0.00017481208487879485\n",
            "3519 \tTraning Loss: 0.00022810731024947017\n",
            "3520 \tTraning Loss: 0.00017307052621617913\n",
            "3521 \tTraning Loss: 0.0002388877037446946\n",
            "3522 \tTraning Loss: 0.00013632012996822596\n",
            "3523 \tTraning Loss: 0.00018568692030385137\n",
            "3524 \tTraning Loss: 0.00023493012122344226\n",
            "3525 \tTraning Loss: 0.0001975254126591608\n",
            "3526 \tTraning Loss: 0.0002612503885757178\n",
            "3527 \tTraning Loss: 0.00020175888494122773\n",
            "3528 \tTraning Loss: 0.00034503143979236484\n",
            "3529 \tTraning Loss: 0.00024685158859938383\n",
            "3530 \tTraning Loss: 0.00024483344168402255\n",
            "3531 \tTraning Loss: 0.0002206105418736115\n",
            "3532 \tTraning Loss: 0.00019793881801888347\n",
            "3533 \tTraning Loss: 0.0002824271214194596\n",
            "3534 \tTraning Loss: 0.00021435771486721933\n",
            "3535 \tTraning Loss: 0.00029462220845744014\n",
            "3536 \tTraning Loss: 0.00018807688320521265\n",
            "3537 \tTraning Loss: 0.00026283529587090015\n",
            "3538 \tTraning Loss: 0.00027357006911188364\n",
            "3539 \tTraning Loss: 0.0002900578547269106\n",
            "3540 \tTraning Loss: 0.00021947525965515524\n",
            "3541 \tTraning Loss: 0.00019541535584721714\n",
            "3542 \tTraning Loss: 0.00020900626259390265\n",
            "3543 \tTraning Loss: 0.00019710129708983004\n",
            "3544 \tTraning Loss: 0.0002699568576645106\n",
            "3545 \tTraning Loss: 0.00022038239694666117\n",
            "3546 \tTraning Loss: 0.0002677178999874741\n",
            "3547 \tTraning Loss: 0.0003416812396608293\n",
            "3548 \tTraning Loss: 0.0003175670572090894\n",
            "3549 \tTraning Loss: 0.00016499502817168832\n",
            "3550 \tTraning Loss: 0.00020243841572664678\n",
            "3551 \tTraning Loss: 0.0003611263236962259\n",
            "3552 \tTraning Loss: 0.00027481591678224504\n",
            "3553 \tTraning Loss: 0.00018674902094062418\n",
            "3554 \tTraning Loss: 0.0002255113940918818\n",
            "3555 \tTraning Loss: 0.0003360075643286109\n",
            "3556 \tTraning Loss: 0.00028532612486742437\n",
            "3557 \tTraning Loss: 0.0002754857705440372\n",
            "3558 \tTraning Loss: 0.00019816563872154802\n",
            "3559 \tTraning Loss: 0.00024157267762348056\n",
            "3560 \tTraning Loss: 0.00016185443382710218\n",
            "3561 \tTraning Loss: 0.00015442399308085442\n",
            "3562 \tTraning Loss: 0.0002988171763718128\n",
            "3563 \tTraning Loss: 0.00015729664301034063\n",
            "3564 \tTraning Loss: 0.0002733062137849629\n",
            "3565 \tTraning Loss: 0.00023654797405470163\n",
            "3566 \tTraning Loss: 0.0002528581244405359\n",
            "3567 \tTraning Loss: 0.00022952191648073494\n",
            "3568 \tTraning Loss: 0.00018669893324840814\n",
            "3569 \tTraning Loss: 0.00018601102055981755\n",
            "3570 \tTraning Loss: 0.000199320784304291\n",
            "3571 \tTraning Loss: 0.00019941100617870688\n",
            "3572 \tTraning Loss: 0.00037390817306004465\n",
            "3573 \tTraning Loss: 0.0002657045843079686\n",
            "3574 \tTraning Loss: 0.00020986085291951895\n",
            "3575 \tTraning Loss: 0.00013352031237445772\n",
            "3576 \tTraning Loss: 0.00013817002763971686\n",
            "3577 \tTraning Loss: 0.00018662522779777646\n",
            "3578 \tTraning Loss: 0.0003075143613386899\n",
            "3579 \tTraning Loss: 0.00018152959819417447\n",
            "3580 \tTraning Loss: 0.0001847632956923917\n",
            "3581 \tTraning Loss: 0.0002168552891816944\n",
            "3582 \tTraning Loss: 0.00021151102555450052\n",
            "3583 \tTraning Loss: 0.00033603032352402806\n",
            "3584 \tTraning Loss: 0.0002121646684827283\n",
            "3585 \tTraning Loss: 0.00022545730462297797\n",
            "3586 \tTraning Loss: 0.00024506018962711096\n",
            "3587 \tTraning Loss: 0.0002270282420795411\n",
            "3588 \tTraning Loss: 0.0002385987463640049\n",
            "3589 \tTraning Loss: 0.00024935315013863146\n",
            "3590 \tTraning Loss: 0.00022613345936406404\n",
            "3591 \tTraning Loss: 0.0003220940998289734\n",
            "3592 \tTraning Loss: 0.0001411607809131965\n",
            "3593 \tTraning Loss: 0.00013583136023953557\n",
            "3594 \tTraning Loss: 0.0002668047964107245\n",
            "3595 \tTraning Loss: 0.00025038563762791455\n",
            "3596 \tTraning Loss: 0.0002754175220616162\n",
            "3597 \tTraning Loss: 0.0002760513743851334\n",
            "3598 \tTraning Loss: 0.0002740836644079536\n",
            "3599 \tTraning Loss: 0.00025100691709667444\n",
            "3600 \tTraning Loss: 0.00019546144176274538\n",
            "3601 \tTraning Loss: 0.0002860644890461117\n",
            "3602 \tTraning Loss: 0.00023506071011070162\n",
            "3603 \tTraning Loss: 0.0002931815979536623\n",
            "3604 \tTraning Loss: 0.00018884155724663287\n",
            "3605 \tTraning Loss: 0.0002012873301282525\n",
            "3606 \tTraning Loss: 0.00022824147890787572\n",
            "3607 \tTraning Loss: 0.00022206382709555328\n",
            "3608 \tTraning Loss: 0.00023906152637209743\n",
            "3609 \tTraning Loss: 0.00022986099065747112\n",
            "3610 \tTraning Loss: 0.00035815074807032943\n",
            "3611 \tTraning Loss: 0.0003119916364084929\n",
            "3612 \tTraning Loss: 0.0002816996129695326\n",
            "3613 \tTraning Loss: 0.00023094700009096414\n",
            "3614 \tTraning Loss: 0.00018004565208684653\n",
            "3615 \tTraning Loss: 0.0003147269308101386\n",
            "3616 \tTraning Loss: 0.00024489080533385277\n",
            "3617 \tTraning Loss: 0.0002658804878592491\n",
            "3618 \tTraning Loss: 0.000224896110012196\n",
            "3619 \tTraning Loss: 0.000154920868226327\n",
            "3620 \tTraning Loss: 0.0002466303121764213\n",
            "3621 \tTraning Loss: 0.00014661143359262496\n",
            "3622 \tTraning Loss: 0.00025443770573474467\n",
            "3623 \tTraning Loss: 0.0002976965915877372\n",
            "3624 \tTraning Loss: 0.00024121301248669624\n",
            "3625 \tTraning Loss: 0.00020689428492914885\n",
            "3626 \tTraning Loss: 0.00027389515889808536\n",
            "3627 \tTraning Loss: 0.0002414857444819063\n",
            "3628 \tTraning Loss: 0.00018406021990813315\n",
            "3629 \tTraning Loss: 0.0001822544145397842\n",
            "3630 \tTraning Loss: 0.00021234009182080626\n",
            "3631 \tTraning Loss: 0.00024269685673061758\n",
            "3632 \tTraning Loss: 0.00021437158284243196\n",
            "3633 \tTraning Loss: 0.0002867071598302573\n",
            "3634 \tTraning Loss: 0.0002310599375050515\n",
            "3635 \tTraning Loss: 0.00019679305842146277\n",
            "3636 \tTraning Loss: 0.00019765198521781713\n",
            "3637 \tTraning Loss: 0.00029586124583147466\n",
            "3638 \tTraning Loss: 0.0001695248793112114\n",
            "3639 \tTraning Loss: 0.00019394209084566683\n",
            "3640 \tTraning Loss: 0.0002410838205832988\n",
            "3641 \tTraning Loss: 0.000204410869628191\n",
            "3642 \tTraning Loss: 0.0002407757128821686\n",
            "3643 \tTraning Loss: 0.00017023507098201662\n",
            "3644 \tTraning Loss: 0.00018021176219917834\n",
            "3645 \tTraning Loss: 0.00018922284652944654\n",
            "3646 \tTraning Loss: 0.00021187095262575895\n",
            "3647 \tTraning Loss: 0.00027793474146164954\n",
            "3648 \tTraning Loss: 0.00017619699065107852\n",
            "3649 \tTraning Loss: 0.00010842296615010127\n",
            "3650 \tTraning Loss: 0.00026010844158008695\n",
            "3651 \tTraning Loss: 0.0003428679774515331\n",
            "3652 \tTraning Loss: 0.0002966567990370095\n",
            "3653 \tTraning Loss: 0.0002769533602986485\n",
            "3654 \tTraning Loss: 0.00016579459770582616\n",
            "3655 \tTraning Loss: 0.00025205864221788943\n",
            "3656 \tTraning Loss: 0.00017704143829178065\n",
            "3657 \tTraning Loss: 0.0001810036483220756\n",
            "3658 \tTraning Loss: 0.000289259129203856\n",
            "3659 \tTraning Loss: 0.0001759774168021977\n",
            "3660 \tTraning Loss: 0.000133869398268871\n",
            "3661 \tTraning Loss: 0.0003309832827653736\n",
            "3662 \tTraning Loss: 0.0002069837792078033\n",
            "3663 \tTraning Loss: 0.00022370587976183742\n",
            "3664 \tTraning Loss: 0.0001789211673894897\n",
            "3665 \tTraning Loss: 0.00017304146604146808\n",
            "3666 \tTraning Loss: 0.0002244400093331933\n",
            "3667 \tTraning Loss: 0.00018162719788961112\n",
            "3668 \tTraning Loss: 0.0002604881301522255\n",
            "3669 \tTraning Loss: 0.00023458692885469645\n",
            "3670 \tTraning Loss: 0.0001723786845104769\n",
            "3671 \tTraning Loss: 0.00030186789808794856\n",
            "3672 \tTraning Loss: 0.0001300010335398838\n",
            "3673 \tTraning Loss: 0.00015136062575038522\n",
            "3674 \tTraning Loss: 0.00020814980962313712\n",
            "3675 \tTraning Loss: 0.00027702946681529284\n",
            "3676 \tTraning Loss: 0.00018437001563142985\n",
            "3677 \tTraning Loss: 0.0001874543377198279\n",
            "3678 \tTraning Loss: 0.0003912057727575302\n",
            "3679 \tTraning Loss: 0.000547705392818898\n",
            "3680 \tTraning Loss: 0.0003399687120690942\n",
            "3681 \tTraning Loss: 0.0002740225463639945\n",
            "3682 \tTraning Loss: 0.00022601881937589496\n",
            "3683 \tTraning Loss: 0.0001445374364266172\n",
            "3684 \tTraning Loss: 0.00020303834753576666\n",
            "3685 \tTraning Loss: 0.0002789500867947936\n",
            "3686 \tTraning Loss: 0.00023490190505981445\n",
            "3687 \tTraning Loss: 0.00024112279061228037\n",
            "3688 \tTraning Loss: 0.0001981950190383941\n",
            "3689 \tTraning Loss: 0.00015806661394890398\n",
            "3690 \tTraning Loss: 0.00018563169578555971\n",
            "3691 \tTraning Loss: 0.00019284531299490482\n",
            "3692 \tTraning Loss: 0.00019158721261192113\n",
            "3693 \tTraning Loss: 0.00022198869555722922\n",
            "3694 \tTraning Loss: 0.00022111828729975969\n",
            "3695 \tTraning Loss: 0.00018963852198794484\n",
            "3696 \tTraning Loss: 0.0002098859695252031\n",
            "3697 \tTraning Loss: 0.00017471304454375058\n",
            "3698 \tTraning Loss: 0.00025607147836126387\n",
            "3699 \tTraning Loss: 0.0002094051451422274\n",
            "3700 \tTraning Loss: 0.00017150586063507944\n",
            "3701 \tTraning Loss: 0.00019231783517170697\n",
            "3702 \tTraning Loss: 0.00020642926392611116\n",
            "3703 \tTraning Loss: 0.00022337803966365755\n",
            "3704 \tTraning Loss: 0.00018225658277515322\n",
            "3705 \tTraning Loss: 0.00025599985383450985\n",
            "3706 \tTraning Loss: 0.00013793594553135335\n",
            "3707 \tTraning Loss: 0.00024389932514168322\n",
            "3708 \tTraning Loss: 0.000166886777151376\n",
            "3709 \tTraning Loss: 0.00023364144726656377\n",
            "3710 \tTraning Loss: 0.0002027588343480602\n",
            "3711 \tTraning Loss: 0.00016393624537158757\n",
            "3712 \tTraning Loss: 0.00026506520225666463\n",
            "3713 \tTraning Loss: 0.00021024924353696406\n",
            "3714 \tTraning Loss: 0.00016210203466471285\n",
            "3715 \tTraning Loss: 0.00018862636352423579\n",
            "3716 \tTraning Loss: 0.00020439269428607076\n",
            "3717 \tTraning Loss: 0.00020297468290664256\n",
            "3718 \tTraning Loss: 0.00016139884246513247\n",
            "3719 \tTraning Loss: 0.0001666059106355533\n",
            "3720 \tTraning Loss: 0.0002460981486365199\n",
            "3721 \tTraning Loss: 0.0003525230276864022\n",
            "3722 \tTraning Loss: 0.0002467783633619547\n",
            "3723 \tTraning Loss: 0.00024246014072559774\n",
            "3724 \tTraning Loss: 0.00027981220046058297\n",
            "3725 \tTraning Loss: 0.0001206528177135624\n",
            "3726 \tTraning Loss: 0.00020772777497768402\n",
            "3727 \tTraning Loss: 0.0002901891712099314\n",
            "3728 \tTraning Loss: 0.0002659153542481363\n",
            "3729 \tTraning Loss: 0.0002008461015066132\n",
            "3730 \tTraning Loss: 0.00022572019952349365\n",
            "3731 \tTraning Loss: 0.00033914376399479806\n",
            "3732 \tTraning Loss: 0.00020229100482538342\n",
            "3733 \tTraning Loss: 0.00018709246069192886\n",
            "3734 \tTraning Loss: 0.00021253526210784912\n",
            "3735 \tTraning Loss: 0.00023089683963917196\n",
            "3736 \tTraning Loss: 0.0002295641606906429\n",
            "3737 \tTraning Loss: 0.00014519199612550437\n",
            "3738 \tTraning Loss: 0.00020258007862139493\n",
            "3739 \tTraning Loss: 0.00023213090025819838\n",
            "3740 \tTraning Loss: 0.00023980015248525888\n",
            "3741 \tTraning Loss: 0.00020210942602716386\n",
            "3742 \tTraning Loss: 0.00014347911928780377\n",
            "3743 \tTraning Loss: 0.00020716745348181576\n",
            "3744 \tTraning Loss: 0.00015626261301804334\n",
            "3745 \tTraning Loss: 0.00028978759655728936\n",
            "3746 \tTraning Loss: 0.0002902522392105311\n",
            "3747 \tTraning Loss: 0.00014645593182649463\n",
            "3748 \tTraning Loss: 0.00018984517373610288\n",
            "3749 \tTraning Loss: 0.00013187079457566142\n",
            "3750 \tTraning Loss: 0.00022541916405316442\n",
            "3751 \tTraning Loss: 0.00017058366211131215\n",
            "3752 \tTraning Loss: 0.0002473041240591556\n",
            "3753 \tTraning Loss: 0.00018496683333069086\n",
            "3754 \tTraning Loss: 0.00021937934798188508\n",
            "3755 \tTraning Loss: 0.0002002292894758284\n",
            "3756 \tTraning Loss: 0.0002310891286469996\n",
            "3757 \tTraning Loss: 0.00017024431144818664\n",
            "3758 \tTraning Loss: 0.00022967008408159018\n",
            "3759 \tTraning Loss: 0.00017378799384459853\n",
            "3760 \tTraning Loss: 0.00021474277309607714\n",
            "3761 \tTraning Loss: 0.0003278252843301743\n",
            "3762 \tTraning Loss: 0.00028275043587200344\n",
            "3763 \tTraning Loss: 0.0002056275843642652\n",
            "3764 \tTraning Loss: 0.00023515417706221342\n",
            "3765 \tTraning Loss: 0.00022047014499548823\n",
            "3766 \tTraning Loss: 0.00020540232071653008\n",
            "3767 \tTraning Loss: 0.0001667180476943031\n",
            "3768 \tTraning Loss: 0.0001927247067214921\n",
            "3769 \tTraning Loss: 0.00025821090093813837\n",
            "3770 \tTraning Loss: 0.00022518496552947909\n",
            "3771 \tTraning Loss: 0.0001543647376820445\n",
            "3772 \tTraning Loss: 0.0001531622401671484\n",
            "3773 \tTraning Loss: 0.00018042964802589267\n",
            "3774 \tTraning Loss: 0.00034200269146822393\n",
            "3775 \tTraning Loss: 0.00014228354848455638\n",
            "3776 \tTraning Loss: 0.00022832256217952818\n",
            "3777 \tTraning Loss: 0.00022290371998678893\n",
            "3778 \tTraning Loss: 0.00029769589309580624\n",
            "3779 \tTraning Loss: 0.00024189795658458024\n",
            "3780 \tTraning Loss: 0.000305660447338596\n",
            "3781 \tTraning Loss: 0.00024241440405603498\n",
            "3782 \tTraning Loss: 0.00021697561896871775\n",
            "3783 \tTraning Loss: 0.0002445960126351565\n",
            "3784 \tTraning Loss: 0.00019233229977544397\n",
            "3785 \tTraning Loss: 0.00026821400388143957\n",
            "3786 \tTraning Loss: 0.0002073464565910399\n",
            "3787 \tTraning Loss: 0.0001475089811719954\n",
            "3788 \tTraning Loss: 0.00015733552572783083\n",
            "3789 \tTraning Loss: 0.00026131447521038353\n",
            "3790 \tTraning Loss: 0.00023671882809139788\n",
            "3791 \tTraning Loss: 0.0002454794885125011\n",
            "3792 \tTraning Loss: 0.00013963840319775045\n",
            "3793 \tTraning Loss: 0.00022635055938735604\n",
            "3794 \tTraning Loss: 0.00017889897571876645\n",
            "3795 \tTraning Loss: 0.00022182120301295072\n",
            "3796 \tTraning Loss: 0.00025140619254671037\n",
            "3797 \tTraning Loss: 0.0002152457891497761\n",
            "3798 \tTraning Loss: 0.00024023570585995913\n",
            "3799 \tTraning Loss: 0.00018821250705514103\n",
            "3800 \tTraning Loss: 0.00014625705080106854\n",
            "3801 \tTraning Loss: 0.0001990214950637892\n",
            "3802 \tTraning Loss: 0.0001303532044403255\n",
            "3803 \tTraning Loss: 0.00020855233015026897\n",
            "3804 \tTraning Loss: 0.00032202136935666203\n",
            "3805 \tTraning Loss: 0.00017199190915562212\n",
            "3806 \tTraning Loss: 0.00019403055193834007\n",
            "3807 \tTraning Loss: 0.00022357287525665015\n",
            "3808 \tTraning Loss: 0.00017521867994219065\n",
            "3809 \tTraning Loss: 0.00016072923608589917\n",
            "3810 \tTraning Loss: 0.0002168624778278172\n",
            "3811 \tTraning Loss: 0.00027961100568063557\n",
            "3812 \tTraning Loss: 0.000167524311109446\n",
            "3813 \tTraning Loss: 0.00015252739831339568\n",
            "3814 \tTraning Loss: 0.0001919005298987031\n",
            "3815 \tTraning Loss: 0.0003233380557503551\n",
            "3816 \tTraning Loss: 0.00023191615764517337\n",
            "3817 \tTraning Loss: 0.0002055041113635525\n",
            "3818 \tTraning Loss: 0.00020343458163551986\n",
            "3819 \tTraning Loss: 0.00018977369472850114\n",
            "3820 \tTraning Loss: 0.00018794566858559847\n",
            "3821 \tTraning Loss: 0.00021872487559448928\n",
            "3822 \tTraning Loss: 0.00010685024608392268\n",
            "3823 \tTraning Loss: 0.00021231833670753986\n",
            "3824 \tTraning Loss: 0.0002893430646508932\n",
            "3825 \tTraning Loss: 0.00020749185932800174\n",
            "3826 \tTraning Loss: 0.00023842112568672746\n",
            "3827 \tTraning Loss: 0.0002516851236578077\n",
            "3828 \tTraning Loss: 0.00017004195251502097\n",
            "3829 \tTraning Loss: 0.000121621138532646\n",
            "3830 \tTraning Loss: 0.0001782392500899732\n",
            "3831 \tTraning Loss: 0.00022143189562484622\n",
            "3832 \tTraning Loss: 0.00020820774079766124\n",
            "3833 \tTraning Loss: 0.000133452020236291\n",
            "3834 \tTraning Loss: 0.00019324611639603972\n",
            "3835 \tTraning Loss: 0.00021082052262499928\n",
            "3836 \tTraning Loss: 0.00020383356604725122\n",
            "3837 \tTraning Loss: 0.00015473667008336633\n",
            "3838 \tTraning Loss: 0.00015016100951470435\n",
            "3839 \tTraning Loss: 0.00020431030134204775\n",
            "3840 \tTraning Loss: 0.00025444570928812027\n",
            "3841 \tTraning Loss: 0.00013663484423886985\n",
            "3842 \tTraning Loss: 0.00027053209487348795\n",
            "3843 \tTraning Loss: 0.0002510122722014785\n",
            "3844 \tTraning Loss: 0.00018441112479195\n",
            "3845 \tTraning Loss: 0.00019083080405835062\n",
            "3846 \tTraning Loss: 0.00024635656154714525\n",
            "3847 \tTraning Loss: 0.000216605156310834\n",
            "3848 \tTraning Loss: 0.00020395845058374107\n",
            "3849 \tTraning Loss: 0.000206463853828609\n",
            "3850 \tTraning Loss: 0.00018827125313691795\n",
            "3851 \tTraning Loss: 0.00021312671015039086\n",
            "3852 \tTraning Loss: 0.00016717232938390225\n",
            "3853 \tTraning Loss: 0.0001364048512186855\n",
            "3854 \tTraning Loss: 0.00019339798018336296\n",
            "3855 \tTraning Loss: 0.000160367286298424\n",
            "3856 \tTraning Loss: 0.00019069340487476438\n",
            "3857 \tTraning Loss: 0.0001321551389992237\n",
            "3858 \tTraning Loss: 0.00016143560060299933\n",
            "3859 \tTraning Loss: 0.00013706723984796554\n",
            "3860 \tTraning Loss: 0.0002452271874062717\n",
            "3861 \tTraning Loss: 0.00023744632198940963\n",
            "3862 \tTraning Loss: 0.00024504278553649783\n",
            "3863 \tTraning Loss: 0.0001775603013811633\n",
            "3864 \tTraning Loss: 0.0001785320055205375\n",
            "3865 \tTraning Loss: 0.00018730387091636658\n",
            "3866 \tTraning Loss: 0.00015940678713377565\n",
            "3867 \tTraning Loss: 0.00023252474784385413\n",
            "3868 \tTraning Loss: 0.00018486069166101515\n",
            "3869 \tTraning Loss: 0.0001759714214131236\n",
            "3870 \tTraning Loss: 0.00016437243903055787\n",
            "3871 \tTraning Loss: 0.0001927822013385594\n",
            "3872 \tTraning Loss: 0.00012285966658964753\n",
            "3873 \tTraning Loss: 0.00018416187958791852\n",
            "3874 \tTraning Loss: 0.00018967640062328428\n",
            "3875 \tTraning Loss: 0.000187825076864101\n",
            "3876 \tTraning Loss: 0.00015952943067532033\n",
            "3877 \tTraning Loss: 0.00016906001837924123\n",
            "3878 \tTraning Loss: 0.0002847618598025292\n",
            "3879 \tTraning Loss: 0.0002983369631692767\n",
            "3880 \tTraning Loss: 0.000295227044261992\n",
            "3881 \tTraning Loss: 0.00020955735817551613\n",
            "3882 \tTraning Loss: 0.00018572948465589434\n",
            "3883 \tTraning Loss: 0.00014471655595116317\n",
            "3884 \tTraning Loss: 0.00016518350457772613\n",
            "3885 \tTraning Loss: 0.0002412647008895874\n",
            "3886 \tTraning Loss: 0.00015716649068053812\n",
            "3887 \tTraning Loss: 0.00018928518693428487\n",
            "3888 \tTraning Loss: 0.00013930740533396602\n",
            "3889 \tTraning Loss: 0.00012436001270543784\n",
            "3890 \tTraning Loss: 0.00017668798682279885\n",
            "3891 \tTraning Loss: 0.00020117321400903165\n",
            "3892 \tTraning Loss: 0.00014364212984219193\n",
            "3893 \tTraning Loss: 0.0001879957562778145\n",
            "3894 \tTraning Loss: 0.00015684531535953283\n",
            "3895 \tTraning Loss: 0.00010604195267660543\n",
            "3896 \tTraning Loss: 0.00013773977116215974\n",
            "3897 \tTraning Loss: 0.00019534019520506263\n",
            "3898 \tTraning Loss: 0.0001906590914586559\n",
            "3899 \tTraning Loss: 0.0001746162015479058\n",
            "3900 \tTraning Loss: 0.00010200054384768009\n",
            "3901 \tTraning Loss: 0.00014092616038396955\n",
            "3902 \tTraning Loss: 0.00010981653758790344\n",
            "3903 \tTraning Loss: 0.00014330691192299128\n",
            "3904 \tTraning Loss: 0.0001646304299356416\n",
            "3905 \tTraning Loss: 0.00020721033797599375\n",
            "3906 \tTraning Loss: 0.00022394118423108011\n",
            "3907 \tTraning Loss: 0.00019994286412838846\n",
            "3908 \tTraning Loss: 0.0001844817161327228\n",
            "3909 \tTraning Loss: 0.00019098444317933172\n",
            "3910 \tTraning Loss: 0.0002225303032901138\n",
            "3911 \tTraning Loss: 0.00014422801905311644\n",
            "3912 \tTraning Loss: 0.0002335357276024297\n",
            "3913 \tTraning Loss: 0.00024234883312601596\n",
            "3914 \tTraning Loss: 0.00021539421868510544\n",
            "3915 \tTraning Loss: 0.00016078879707492888\n",
            "3916 \tTraning Loss: 0.00017835816834121943\n",
            "3917 \tTraning Loss: 0.00018722223467193544\n",
            "3918 \tTraning Loss: 0.00015932906535454094\n",
            "3919 \tTraning Loss: 0.00015196700405795127\n",
            "3920 \tTraning Loss: 0.0002115708339260891\n",
            "3921 \tTraning Loss: 0.00021514827676583081\n",
            "3922 \tTraning Loss: 0.00018090847879648209\n",
            "3923 \tTraning Loss: 0.0001777050201781094\n",
            "3924 \tTraning Loss: 0.00016871654952410609\n",
            "3925 \tTraning Loss: 0.00015940277080517262\n",
            "3926 \tTraning Loss: 0.00020099665562156588\n",
            "3927 \tTraning Loss: 0.00016861336189322174\n",
            "3928 \tTraning Loss: 0.00023277509899344295\n",
            "3929 \tTraning Loss: 0.00023971410701051354\n",
            "3930 \tTraning Loss: 0.00018297815404366702\n",
            "3931 \tTraning Loss: 0.00019975744362454861\n",
            "3932 \tTraning Loss: 0.00017588779155630618\n",
            "3933 \tTraning Loss: 0.00020866229897364974\n",
            "3934 \tTraning Loss: 0.00014251546235755086\n",
            "3935 \tTraning Loss: 0.0002278700703755021\n",
            "3936 \tTraning Loss: 0.000186725941603072\n",
            "3937 \tTraning Loss: 0.00016357855929527432\n",
            "3938 \tTraning Loss: 0.00021388901222962886\n",
            "3939 \tTraning Loss: 0.00015336839715018868\n",
            "3940 \tTraning Loss: 0.00023801662609912455\n",
            "3941 \tTraning Loss: 0.00020282939658500254\n",
            "3942 \tTraning Loss: 0.00025317291147075593\n",
            "3943 \tTraning Loss: 0.00018038309644907713\n",
            "3944 \tTraning Loss: 0.0003653769672382623\n",
            "3945 \tTraning Loss: 0.00019017711747437716\n",
            "3946 \tTraning Loss: 0.00023332462296821177\n",
            "3947 \tTraning Loss: 0.0002008322480833158\n",
            "3948 \tTraning Loss: 0.00021669146372005343\n",
            "3949 \tTraning Loss: 0.00016819739539641887\n",
            "3950 \tTraning Loss: 0.00021399391698651016\n",
            "3951 \tTraning Loss: 0.00018733539036475122\n",
            "3952 \tTraning Loss: 0.00017810608551371843\n",
            "3953 \tTraning Loss: 0.00021676879259757698\n",
            "3954 \tTraning Loss: 0.0002117799740517512\n",
            "3955 \tTraning Loss: 0.0002131010842276737\n",
            "3956 \tTraning Loss: 0.00024120927264448255\n",
            "3957 \tTraning Loss: 0.000203730131033808\n",
            "3958 \tTraning Loss: 0.00020168207993265241\n",
            "3959 \tTraning Loss: 0.00021164710051380098\n",
            "3960 \tTraning Loss: 0.00019069109112024307\n",
            "3961 \tTraning Loss: 0.00016560964286327362\n",
            "3962 \tTraning Loss: 0.00015296113269869238\n",
            "3963 \tTraning Loss: 0.0002234931889688596\n",
            "3964 \tTraning Loss: 0.00013907581160310656\n",
            "3965 \tTraning Loss: 0.00015772075857967138\n",
            "3966 \tTraning Loss: 9.301800309913233e-05\n",
            "3967 \tTraning Loss: 0.00016420381143689156\n",
            "3968 \tTraning Loss: 0.00019467131642159075\n",
            "3969 \tTraning Loss: 0.0001892388245323673\n",
            "3970 \tTraning Loss: 0.0002109136403305456\n",
            "3971 \tTraning Loss: 0.00016367672651540488\n",
            "3972 \tTraning Loss: 0.00024150745593942702\n",
            "3973 \tTraning Loss: 0.00019103540398646146\n",
            "3974 \tTraning Loss: 0.00019630369206424803\n",
            "3975 \tTraning Loss: 0.00024518222198821604\n",
            "3976 \tTraning Loss: 0.00027632497949525714\n",
            "3977 \tTraning Loss: 0.0002079303958453238\n",
            "3978 \tTraning Loss: 0.0002486197918187827\n",
            "3979 \tTraning Loss: 0.0001525411062175408\n",
            "3980 \tTraning Loss: 0.00019441846234258264\n",
            "3981 \tTraning Loss: 0.00011266906949458644\n",
            "3982 \tTraning Loss: 0.00021229749836493284\n",
            "3983 \tTraning Loss: 0.00022296958195511252\n",
            "3984 \tTraning Loss: 0.00014846579870209098\n",
            "3985 \tTraning Loss: 0.00015869738126639277\n",
            "3986 \tTraning Loss: 0.00015285989502444863\n",
            "3987 \tTraning Loss: 0.00015113872359506786\n",
            "3988 \tTraning Loss: 0.00011443166295066476\n",
            "3989 \tTraning Loss: 0.00015218710177578032\n",
            "3990 \tTraning Loss: 0.00014839752111583948\n",
            "3991 \tTraning Loss: 0.00016118284838739783\n",
            "3992 \tTraning Loss: 0.00016783754108473659\n",
            "3993 \tTraning Loss: 0.00015368207823485136\n",
            "3994 \tTraning Loss: 0.00014154509699437767\n",
            "3995 \tTraning Loss: 0.00013645549188368022\n",
            "3996 \tTraning Loss: 0.00015440564311575145\n",
            "3997 \tTraning Loss: 0.00017698033479973674\n",
            "3998 \tTraning Loss: 0.0001117989158956334\n",
            "3999 \tTraning Loss: 0.00021066766930744052\n",
            "4000 \tTraning Loss: 0.0001499137288192287\n",
            "4001 \tTraning Loss: 0.00018146423099096864\n",
            "4002 \tTraning Loss: 0.0001458691112929955\n",
            "4003 \tTraning Loss: 0.00016864135977812111\n",
            "4004 \tTraning Loss: 0.0001526583801023662\n",
            "4005 \tTraning Loss: 9.045840852195397e-05\n",
            "4006 \tTraning Loss: 0.0002004613052122295\n",
            "4007 \tTraning Loss: 0.0001462640502722934\n",
            "4008 \tTraning Loss: 0.00021742336684837937\n",
            "4009 \tTraning Loss: 0.00011971469211857766\n",
            "4010 \tTraning Loss: 0.00014309355174191296\n",
            "4011 \tTraning Loss: 0.00021105800988152623\n",
            "4012 \tTraning Loss: 0.0002290883130626753\n",
            "4013 \tTraning Loss: 0.0002552331716287881\n",
            "4014 \tTraning Loss: 0.00014728566748090088\n",
            "4015 \tTraning Loss: 0.00019346941553521901\n",
            "4016 \tTraning Loss: 0.00016594224143773317\n",
            "4017 \tTraning Loss: 0.0001973820326384157\n",
            "4018 \tTraning Loss: 0.00020173272059764713\n",
            "4019 \tTraning Loss: 0.00016581281670369208\n",
            "4020 \tTraning Loss: 0.00014737392484676093\n",
            "4021 \tTraning Loss: 0.0001582837285241112\n",
            "4022 \tTraning Loss: 0.00012254214379936457\n",
            "4023 \tTraning Loss: 0.00013831480464432389\n",
            "4024 \tTraning Loss: 0.00021200002811383456\n",
            "4025 \tTraning Loss: 0.00019377928401809186\n",
            "4026 \tTraning Loss: 0.00016485848755110055\n",
            "4027 \tTraning Loss: 0.00013119896175339818\n",
            "4028 \tTraning Loss: 0.00011852804891532287\n",
            "4029 \tTraning Loss: 0.0001517515629529953\n",
            "4030 \tTraning Loss: 0.00010582990216789767\n",
            "4031 \tTraning Loss: 0.00023260990565177053\n",
            "4032 \tTraning Loss: 0.0002124892343999818\n",
            "4033 \tTraning Loss: 0.00018002584693022072\n",
            "4034 \tTraning Loss: 0.0001951762242242694\n",
            "4035 \tTraning Loss: 0.00019392471585888416\n",
            "4036 \tTraning Loss: 0.0002021235559368506\n",
            "4037 \tTraning Loss: 0.0001429741532774642\n",
            "4038 \tTraning Loss: 0.0001680234563536942\n",
            "4039 \tTraning Loss: 0.0001617428642930463\n",
            "4040 \tTraning Loss: 0.00014122812717687339\n",
            "4041 \tTraning Loss: 0.00020818038319703192\n",
            "4042 \tTraning Loss: 0.00013826416397932917\n",
            "4043 \tTraning Loss: 0.0002178890281356871\n",
            "4044 \tTraning Loss: 0.00015817274106666446\n",
            "4045 \tTraning Loss: 0.0001272225781576708\n",
            "4046 \tTraning Loss: 0.00010042959911515936\n",
            "4047 \tTraning Loss: 0.00016611264436505735\n",
            "4048 \tTraning Loss: 0.0001415517763234675\n",
            "4049 \tTraning Loss: 0.00015028283814899623\n",
            "4050 \tTraning Loss: 0.00019568743300624192\n",
            "4051 \tTraning Loss: 0.00015374209033325315\n",
            "4052 \tTraning Loss: 0.00012967584189027548\n",
            "4053 \tTraning Loss: 0.000194160922546871\n",
            "4054 \tTraning Loss: 0.000175886059878394\n",
            "4055 \tTraning Loss: 0.00013866140216123313\n",
            "4056 \tTraning Loss: 0.00014836763148196042\n",
            "4057 \tTraning Loss: 0.00021432424546219409\n",
            "4058 \tTraning Loss: 0.0001371865364490077\n",
            "4059 \tTraning Loss: 0.00015097888535819948\n",
            "4060 \tTraning Loss: 0.00022075251035857946\n",
            "4061 \tTraning Loss: 0.00013931044668424875\n",
            "4062 \tTraning Loss: 0.00022349927166942507\n",
            "4063 \tTraning Loss: 0.00014614964311476797\n",
            "4064 \tTraning Loss: 0.00014804409875068814\n",
            "4065 \tTraning Loss: 0.00019415402493905276\n",
            "4066 \tTraning Loss: 0.00018287141574546695\n",
            "4067 \tTraning Loss: 9.078582661459222e-05\n",
            "4068 \tTraning Loss: 0.00014998136612121016\n",
            "4069 \tTraning Loss: 0.00020747046801261604\n",
            "4070 \tTraning Loss: 0.0002069731563096866\n",
            "4071 \tTraning Loss: 0.00017194949032273144\n",
            "4072 \tTraning Loss: 0.00012007669283775613\n",
            "4073 \tTraning Loss: 0.00018557114526629448\n",
            "4074 \tTraning Loss: 0.0003519672027323395\n",
            "4075 \tTraning Loss: 0.00018220081983599812\n",
            "4076 \tTraning Loss: 7.741161243757233e-05\n",
            "4077 \tTraning Loss: 0.00017019080405589193\n",
            "4078 \tTraning Loss: 0.00015945638006087393\n",
            "4079 \tTraning Loss: 0.00011189216456841677\n",
            "4080 \tTraning Loss: 0.0001476228644605726\n",
            "4081 \tTraning Loss: 0.00016731418145354837\n",
            "4082 \tTraning Loss: 0.00016807026986498386\n",
            "4083 \tTraning Loss: 0.0001740049192449078\n",
            "4084 \tTraning Loss: 0.00014204200124368072\n",
            "4085 \tTraning Loss: 0.00015256869664881378\n",
            "4086 \tTraning Loss: 0.0001647425233386457\n",
            "4087 \tTraning Loss: 0.00012532327673397958\n",
            "4088 \tTraning Loss: 0.00016897465684451163\n",
            "4089 \tTraning Loss: 0.000152085063746199\n",
            "4090 \tTraning Loss: 0.00011664381599985063\n",
            "4091 \tTraning Loss: 0.00023328278621193022\n",
            "4092 \tTraning Loss: 0.00017841218505054712\n",
            "4093 \tTraning Loss: 0.00014547676255460829\n",
            "4094 \tTraning Loss: 0.0001234888331964612\n",
            "4095 \tTraning Loss: 0.00020193608361296356\n",
            "4096 \tTraning Loss: 0.00021379422105383128\n",
            "4097 \tTraning Loss: 0.00013394116831477731\n",
            "4098 \tTraning Loss: 0.00029229948995634913\n",
            "4099 \tTraning Loss: 0.00018609169637784362\n",
            "4100 \tTraning Loss: 0.0001946895499713719\n",
            "4101 \tTraning Loss: 0.00014102387649472803\n",
            "4102 \tTraning Loss: 0.00011339360935380682\n",
            "4103 \tTraning Loss: 0.00018763217667583376\n",
            "4104 \tTraning Loss: 0.0001183147105621174\n",
            "4105 \tTraning Loss: 0.0001130105447373353\n",
            "4106 \tTraning Loss: 0.00021454133093357086\n",
            "4107 \tTraning Loss: 0.00019888454698957503\n",
            "4108 \tTraning Loss: 0.00015813799109309912\n",
            "4109 \tTraning Loss: 0.00014996332174632698\n",
            "4110 \tTraning Loss: 0.00015926135529298335\n",
            "4111 \tTraning Loss: 0.00030011433409526944\n",
            "4112 \tTraning Loss: 0.0004545500851236284\n",
            "4113 \tTraning Loss: 0.0002682689519133419\n",
            "4114 \tTraning Loss: 0.00021187987294979393\n",
            "4115 \tTraning Loss: 0.00016875434084795415\n",
            "4116 \tTraning Loss: 0.0001568463194416836\n",
            "4117 \tTraning Loss: 0.00018473848467692733\n",
            "4118 \tTraning Loss: 0.00012520389282144606\n",
            "4119 \tTraning Loss: 0.00016444617358502\n",
            "4120 \tTraning Loss: 7.544666732428595e-05\n",
            "4121 \tTraning Loss: 0.00016810285160318017\n",
            "4122 \tTraning Loss: 0.0001607571612112224\n",
            "4123 \tTraning Loss: 0.00013786478666588664\n",
            "4124 \tTraning Loss: 0.00015365323633886874\n",
            "4125 \tTraning Loss: 0.0001967957941815257\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-9c1bb04a36cc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0myh\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mpinn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neural = pinn(input_data)\n",
        "neural = (neural[:,:,0]**2 + neural[:,:,1]**2).detach().numpy()\n",
        "numerical =(testr**2 + testi**2).detach().numpy()"
      ],
      "metadata": {
        "id": "GBDz499AZvMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtuMmCynUmEu",
        "outputId": "b4c9733a-d626-43a6-8619-74017eb062f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(750, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(neural[700,:], label = 'neural')\n",
        "plt.plot(numerical[700,:], label = 'numerical')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "E4eLQSEXUiE_",
        "outputId": "f4210b34-7c75-4bf5-f744-e549c227af8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7bd97d64be80>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 15000 #Numero de iteraciones\n",
        "l = 0.5   #Peso relativo lambda\n",
        "\n",
        "for epoch in range(iterations):\n",
        "    optimizer.zero_grad()\n",
        "    #Asimilación de los datos\n",
        "    yh = pinn(input_data)               #Evalúo los datos que tengo de la simuación/medicion\n",
        "    loss1 = torch.mean((yh-y_data)**2)  #Calculo el mean square error\n",
        "    #Condiciones de la física sobre la misma grilla\n",
        "    #Parte real\n",
        "    dxr  = torch.autograd.grad(yh[:,:,0], x_grid, torch.ones_like(yh[:,:,0]), create_graph=True)[0]\n",
        "    dxxr = torch.autograd.grad(dxr,        x_grid, torch.ones_like(dxr), create_graph=True)[0]\n",
        "    dtr  = torch.autograd.grad(yh[:,:,0], t_grid, torch.ones_like(yh[:,:,0]), create_graph=True)[0]\n",
        "    dxr  = dxr[:,:,0]\n",
        "    dxxr = dxxr[:,:,0]\n",
        "    dtr  = dtr[:,:,0]\n",
        "    #Parte imaginaria\n",
        "    dxi  = torch.autograd.grad(yh[:,:,1], x_grid, torch.ones_like(yh[:,:,1]), create_graph=True)[0]\n",
        "    dxxi = torch.autograd.grad(dxi,        x_grid, torch.ones_like(dxi), create_graph=True)[0]\n",
        "    dti  = torch.autograd.grad(yh[:,:,1], t_grid, torch.ones_like(yh[:,:,1]), create_graph=True)[0]\n",
        "    dxi  = dxi[:,:,0]\n",
        "    dxxi = dxxi[:,:,0]\n",
        "    dti  = dti[:,:,0]\n",
        "    #|A|²\n",
        "    sqr  = yh[:,:,0]**2 + yh[:,:,1]**2\n",
        "    ######################\n",
        "    physicsr = dtr - pinn.p1*yh[:,:,0] + pinn.p2*dxr + pinn.p3*(dxxr - pinn.p4*dxxi) - pinn.p5*sqr*(yh[:,:,0] - pinn.p6*yh[:,:,1])\n",
        "    physicsi = dti - pinn.p1*yh[:,:,1] + pinn.p2*dxi + pinn.p3*(dxxi + pinn.p4*dxxr) - pinn.p5*sqr*(yh[:,:,1] + pinn.p6*yh[:,:,0])\n",
        "    physics  = physicsr + physicsi\n",
        "    loss2 = l*torch.mean((physics)**2)   #MSE de la física\n",
        "    loss = loss1 + loss2   #Sumamos todos los errores\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.autograd.no_grad():\n",
        "    \tprint(epoch,'Data',float(loss1), 'PDE',float(loss2), float(pinn.p1),float(pinn.p1),float(pinn.p2),float(pinn.p3), float(pinn.p4),float(pinn.p5),float(pinn.p6) ,\"\\tTraning Loss:\",float(loss.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K0jUbw1uxcKY",
        "outputId": "4c6349eb-2def-40d1-8d4f-82751bcd4693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Data 0.0322575367088622 PDE 0.03149552643299103 0.15227869153022766 0.15227869153022766 0.1451098471879959 0.030656570568680763 0.5409872531890869 0.9537032246589661 0.611669659614563 \tTraning Loss: 0.06375306314185322\n",
            "1 Data 0.014038805614384678 PDE 0.037308476865291595 0.1522040069103241 0.1522040069103241 0.14499638974666595 0.03063681721687317 0.5410025119781494 0.9535953402519226 0.6118128299713135 \tTraning Loss: 0.05134728247967627\n",
            "2 Data 0.012212067823535186 PDE 0.038901329040527344 0.15213678777217865 0.15213678777217865 0.14495442807674408 0.03059501387178898 0.5409716963768005 0.9535025358200073 0.6119397282600403 \tTraning Loss: 0.05111339686406253\n",
            "3 Data 0.02021967920108749 PDE 0.03986791521310806 0.15206646919250488 0.15206646919250488 0.14499293267726898 0.030518149957060814 0.5409042239189148 0.9534210562705994 0.6120526790618896 \tTraning Loss: 0.060087594414195554\n",
            "4 Data 0.03245556635041352 PDE 0.041349392384290695 0.1519859880208969 0.1519859880208969 0.14509816467761993 0.030403530225157738 0.5408065319061279 0.9533474445343018 0.6121527552604675 \tTraning Loss: 0.07380495873470422\n",
            "5 Data 0.04419662726831293 PDE 0.04247860237956047 0.15189148485660553 0.15189148485660553 0.14525213837623596 0.030255207791924477 0.5406835675239563 0.953278660774231 0.6122404932975769 \tTraning Loss: 0.0866752296478734\n",
            "6 Data 0.052040029831892654 PDE 0.04143865406513214 0.15178236365318298 0.15178236365318298 0.14544031023979187 0.030080191791057587 0.5405402779579163 0.9532124996185303 0.6123163104057312 \tTraning Loss: 0.0934786838970248\n",
            "7 Data 0.054354161012326575 PDE 0.037051938474178314 0.15166087448596954 0.15166087448596954 0.1456516832113266 0.02988622896373272 0.540381669998169 0.9531482458114624 0.612381100654602 \tTraning Loss: 0.09140609948650488\n",
            "8 Data 0.05129185230473999 PDE 0.029839497059583664 0.1515311449766159 0.1515311449766159 0.14587722718715668 0.029681092128157616 0.540212869644165 0.9530863165855408 0.6124362349510193 \tTraning Loss: 0.08113134936432365\n",
            "9 Data 0.04426596213433309 PDE 0.02159060910344124 0.15139807760715485 0.15139807760715485 0.14610879123210907 0.02947237715125084 0.5400389432907104 0.953027606010437 0.6124833226203918 \tTraning Loss: 0.06585657123777433\n",
            "10 Data 0.035202681886377235 PDE 0.014110567979514599 0.15126635134220123 0.15126635134220123 0.14633892476558685 0.02926715649664402 0.5398646593093872 0.952972948551178 0.6125239729881287 \tTraning Loss: 0.049313249865891834\n",
            "11 Data 0.025929112737243906 PDE 0.008397224359214306 0.1511397510766983 0.1511397510766983 0.14656120538711548 0.029071403667330742 0.5396943688392639 0.9529227614402771 0.61255943775177 \tTraning Loss: 0.034326337096458215\n",
            "12 Data 0.017834849233709607 PDE 0.004611157346516848 0.1510208398103714 0.1510208398103714 0.14677079021930695 0.02888946793973446 0.5395317673683167 0.9528769850730896 0.6125907897949219 \tTraning Loss: 0.022446006580226455\n",
            "13 Data 0.011766734197129794 PDE 0.0024440002162009478 0.15091092884540558 0.15091092884540558 0.14696474373340607 0.02872384525835514 0.5393801331520081 0.9528353214263916 0.6126186847686768 \tTraning Loss: 0.014210734413330742\n",
            "14 Data 0.008060044978656717 PDE 0.0014668592484667897 0.15081024169921875 0.15081024169921875 0.14714202284812927 0.028575267642736435 0.5392422080039978 0.9527974724769592 0.6126435399055481 \tTraning Loss: 0.009526904227123507\n",
            "15 Data 0.006629145441997003 PDE 0.0013100816868245602 0.15071821212768555 0.15071821212768555 0.14730313420295715 0.02844301611185074 0.5391203165054321 0.952763020992279 0.6126656532287598 \tTraning Loss: 0.007939227128821563\n",
            "16 Data 0.007077553953063303 PDE 0.0017038125079125166 0.15063367784023285 0.15063367784023285 0.14744973182678223 0.028325339779257774 0.5390163064002991 0.9527316093444824 0.6126852631568909 \tTraning Loss: 0.008781366460975819\n",
            "17 Data 0.008812336961041396 PDE 0.002450405852869153 0.15055517852306366 0.15055517852306366 0.14758412539958954 0.028219880536198616 0.538931667804718 0.9527029991149902 0.6127025485038757 \tTraning Loss: 0.01126274281391055\n",
            "18 Data 0.01115746592514818 PDE 0.0033823975827544928 0.15048116445541382 0.15048116445541382 0.14770886301994324 0.028124075382947922 0.5388672351837158 0.9526768326759338 0.6127177476882935 \tTraning Loss: 0.014539863507902673\n",
            "19 Data 0.01346112518983674 PDE 0.004336177837103605 0.15041017532348633 0.15041017532348633 0.1478263884782791 0.028035525232553482 0.5388233661651611 0.9526528716087341 0.6127310395240784 \tTraning Loss: 0.017797303026940346\n",
            "20 Data 0.015188851938820785 PDE 0.005153093487024307 0.15034107863903046 0.15034107863903046 0.14793869853019714 0.027952294796705246 0.5387997627258301 0.9526308178901672 0.6127426624298096 \tTraning Loss: 0.020341945425845092\n",
            "21 Data 0.01599151980311288 PDE 0.005703191738575697 0.15027311444282532 0.15027311444282532 0.14804723858833313 0.027873123064637184 0.5387954711914062 0.9526104927062988 0.6127527952194214 \tTraning Loss: 0.021694711541688578\n",
            "22 Data 0.01573770026781737 PDE 0.005913787521421909 0.15020601451396942 0.15020601451396942 0.14815273880958557 0.02779749408364296 0.538809061050415 0.9525917172431946 0.6127616763114929 \tTraning Loss: 0.02165148778923928\n",
            "23 Data 0.014505240704819563 PDE 0.00578294787555933 0.1501399129629135 0.1501399129629135 0.1482553482055664 0.027725601568818092 0.538838803768158 0.9525744318962097 0.612769365310669 \tTraning Loss: 0.020288188580378893\n",
            "24 Data 0.01253628047834287 PDE 0.005369123071432114 0.1500752717256546 0.1500752717256546 0.1483546644449234 0.027658186852931976 0.5388827323913574 0.9525585174560547 0.6127760410308838 \tTraning Loss: 0.017905403549774983\n",
            "25 Data 0.010168670181040973 PDE 0.004763838369399309 0.15001273155212402 0.15001273155212402 0.14844997227191925 0.027596337720751762 0.5389388203620911 0.9525439143180847 0.612781822681427 \tTraning Loss: 0.014932508550440282\n",
            "26 Data 0.007760982131152107 PDE 0.004062457010149956 0.1499529480934143 0.1499529480934143 0.14854039251804352 0.027541249990463257 0.5390051603317261 0.952530562877655 0.6127868890762329 \tTraning Loss: 0.011823439141302062\n",
            "27 Data 0.005626755584638365 PDE 0.0033447749447077513 0.1498965322971344 0.1498965322971344 0.14862510561943054 0.027494028210639954 0.5390798449516296 0.9525183439254761 0.6127912998199463 \tTraning Loss: 0.008971530529346117\n",
            "28 Data 0.003988342044501404 PDE 0.002668386558070779 0.1498439460992813 0.1498439460992813 0.1487034261226654 0.0274555254727602 0.5391612648963928 0.9525071978569031 0.6127952337265015 \tTraning Loss: 0.006656728602572183\n",
            "29 Data 0.0029544836647441373 PDE 0.002070827642455697 0.14979541301727295 0.14979541301727295 0.14877498149871826 0.027426235377788544 0.5392478704452515 0.9524970054626465 0.6127986907958984 \tTraning Loss: 0.005025311307199834\n",
            "30 Data 0.0025207152633256557 PDE 0.0015745990676805377 0.1497509926557541 0.1497509926557541 0.14883971214294434 0.027406249195337296 0.539338231086731 0.9524877667427063 0.6128017902374268 \tTraning Loss: 0.004095314331006194\n",
            "31 Data 0.0025882627889225913 PDE 0.0011909740278497338 0.1497105211019516 0.1497105211019516 0.14889787137508392 0.027395261451601982 0.5394311547279358 0.9524793028831482 0.6128045916557312 \tTraning Loss: 0.003779236816772325\n",
            "32 Data 0.0029953656438467865 PDE 0.0009214028832502663 0.1496736705303192 0.1496736705303192 0.1489500254392624 0.027392614632844925 0.5395255088806152 0.9524716138839722 0.6128070950508118 \tTraning Loss: 0.003916768527097053\n",
            "33 Data 0.003554230276148181 PDE 0.0007576187490485609 0.1496400386095047 0.1496400386095047 0.14899693429470062 0.027397381141781807 0.5396202206611633 0.9524646401405334 0.6128093600273132 \tTraning Loss: 0.004311849025196742\n",
            "34 Data 0.004087032229147168 PDE 0.0006823203875683248 0.14960914850234985 0.14960914850234985 0.14903947710990906 0.027408462017774582 0.5397143363952637 0.9524582624435425 0.6128114461898804 \tTraning Loss: 0.004769352616715493\n",
            "35 Data 0.004455273544958696 PDE 0.0006716998759657145 0.14958056807518005 0.14958056807518005 0.14907851815223694 0.027424685657024384 0.5398069620132446 0.9524524211883545 0.6128133535385132 \tTraning Loss: 0.005126973420924411\n",
            "36 Data 0.004578316534186619 PDE 0.0006996577722020447 0.14955390989780426 0.14955390989780426 0.14911486208438873 0.027444908395409584 0.5398972630500793 0.9524471163749695 0.6128150820732117 \tTraning Loss: 0.005277974306388664\n",
            "37 Data 0.004439023991628148 PDE 0.0007423540810123086 0.1495288908481598 0.1495288908481598 0.14914913475513458 0.027468089014291763 0.5399844646453857 0.9524422883987427 0.6128166317939758 \tTraning Loss: 0.0051813780726404565\n",
            "38 Data 0.004076909042252623 PDE 0.0007815100834704936 0.14950533211231232 0.14950533211231232 0.1491817981004715 0.02749333530664444 0.5400678515434265 0.9524378776550293 0.6128180623054504 \tTraning Loss: 0.004858419125723117\n",
            "39 Data 0.0035715509183366087 PDE 0.000805611489340663 0.14948314428329468 0.14948314428329468 0.14921312034130096 0.027519939467310905 0.5401468873023987 0.9524338841438293 0.6128193736076355 \tTraning Loss: 0.004377162407677272\n",
            "40 Data 0.0030207202587423597 PDE 0.0008092522039078176 0.14946234226226807 0.14946234226226807 0.14924316108226776 0.027547374367713928 0.540221095085144 0.952430248260498 0.612820565700531 \tTraning Loss: 0.0038299724626501773\n",
            "41 Data 0.0025182392436562248 PDE 0.0007915252353996038 0.14944295585155487 0.14944295585155487 0.1492718607187271 0.027575278654694557 0.5402901768684387 0.9524269104003906 0.612821638584137 \tTraning Loss: 0.0033097644790558286\n",
            "42 Data 0.002136003447111849 PDE 0.0007543812389485538 0.14942505955696106 0.14942505955696106 0.14929907023906708 0.027603428810834885 0.5403540134429932 0.9524238705635071 0.6128225922584534 \tTraning Loss: 0.0028903846860604027\n",
            "43 Data 0.0019131016640163685 PDE 0.0007014329312369227 0.1494087129831314 0.1494087129831314 0.14932461082935333 0.02763170748949051 0.5404126048088074 0.9524211287498474 0.612823486328125 \tTraning Loss: 0.002614534595253291\n",
            "44 Data 0.0018531167910491734 PDE 0.0006372271454893053 0.1493939608335495 0.1493939608335495 0.14934827387332916 0.0276600681245327 0.5404660701751709 0.9524186253547668 0.6128242611885071 \tTraning Loss: 0.0024903439365384786\n",
            "45 Data 0.0019288389882639255 PDE 0.0005667508230544627 0.14938078820705414 0.14938078820705414 0.14936994016170502 0.02768850140273571 0.5405147671699524 0.9524163603782654 0.6128249764442444 \tTraning Loss: 0.0024955898113183882\n",
            "46 Data 0.0020922973110298077 PDE 0.0004949553986079991 0.14936916530132294 0.14936916530132294 0.14938953518867493 0.027717022225260735 0.5405591726303101 0.9524142742156982 0.6128256320953369 \tTraning Loss: 0.002587252709637807\n",
            "47 Data 0.002287299508868738 PDE 0.00042625595233403146 0.14935898780822754 0.14935898780822754 0.14940707385540009 0.027745651081204414 0.5405998229980469 0.9524124264717102 0.6128262281417847 \tTraning Loss: 0.0027135554612027694\n",
            "48 Data 0.0024616194109472246 PDE 0.00036409319727681577 0.1493501514196396 0.1493501514196396 0.14942267537117004 0.027774415910243988 0.5406373143196106 0.9524106979370117 0.6128267645835876 \tTraning Loss: 0.0028257126082240403\n",
            "49 Data 0.0025764935780257965 PDE 0.00031069343094713986 0.14934250712394714 0.14934250712394714 0.14943650364875793 0.02780335396528244 0.5406723618507385 0.9524091482162476 0.6128272414207458 \tTraning Loss: 0.0028871870089729364\n",
            "50 Data 0.002611954561175183 PDE 0.0002670839021448046 0.14933590590953827 0.14933590590953827 0.14944878220558167 0.0278325192630291 0.540705680847168 0.9524077773094177 0.6128276586532593 \tTraning Loss: 0.0028790384633199874\n",
            "51 Data 0.002567591397912469 PDE 0.00023331251577474177 0.14933021366596222 0.14933021366596222 0.14945976436138153 0.02786198817193508 0.5407379269599915 0.9524065256118774 0.6128280758857727 \tTraning Loss: 0.002800903913687211\n",
            "52 Data 0.002459272951517369 PDE 0.00020875566406175494 0.14932529628276825 0.14932529628276825 0.14946968853473663 0.027891866862773895 0.5407698154449463 0.9524053931236267 0.6128284335136414 \tTraning Loss: 0.002668028615579124\n",
            "53 Data 0.002313131722173947 PDE 0.00019239954417571425 0.1493210345506668 0.1493210345506668 0.14947879314422607 0.027922287583351135 0.540802001953125 0.9524043202400208 0.61282879114151 \tTraning Loss: 0.002505531266349661\n",
            "54 Data 0.002158468712779932 PDE 0.0001830379042075947 0.14931733906269073 0.14931733906269073 0.14948727190494537 0.027953408658504486 0.5408350229263306 0.9524033665657043 0.6128290891647339 \tTraning Loss: 0.0023415066169875268\n",
            "55 Data 0.0020212177589348933 PDE 0.00017938191012945026 0.14931412041187286 0.14931412041187286 0.14949527382850647 0.027985403314232826 0.540869414806366 0.9524025321006775 0.6128293871879578 \tTraning Loss: 0.0022005996690643436\n",
            "56 Data 0.0019192394880490028 PDE 0.00018011605425272137 0.14931133389472961 0.14931133389472961 0.14950290322303772 0.02801845408976078 0.5409055948257446 0.9524017572402954 0.6128296256065369 \tTraning Loss: 0.002099355542301724\n",
            "57 Data 0.0018601320306158812 PDE 0.0001839356409618631 0.1493089199066162 0.1493089199066162 0.1495102345943451 0.028052734211087227 0.54094398021698 0.9524010419845581 0.612829864025116 \tTraning Loss: 0.002044067671577744\n",
            "58 Data 0.0018415897861265781 PDE 0.00018958831788040698 0.14930683374404907 0.14930683374404907 0.14951729774475098 0.028088396415114403 0.5409848093986511 0.9524003863334656 0.6128301024436951 \tTraning Loss: 0.002031178104006985\n",
            "59 Data 0.0018537943076597455 PDE 0.00019592547323554754 0.149305060505867 0.149305060505867 0.14952409267425537 0.02812555991113186 0.5410283207893372 0.9523997902870178 0.6128302812576294 \tTraning Loss: 0.002049719780895293\n",
            "60 Data 0.00188295148108696 PDE 0.00020195827528368682 0.1493035852909088 0.1493035852909088 0.14953063428401947 0.028164302930235863 0.5410746335983276 0.9523992538452148 0.6128304600715637 \tTraning Loss: 0.002084909756370647\n",
            "61 Data 0.0019150091767615217 PDE 0.00020690634846687317 0.1493023782968521 0.1493023782968521 0.14953690767288208 0.028204651549458504 0.5411238074302673 0.9523987770080566 0.612830638885498 \tTraning Loss: 0.002121915525228395\n",
            "62 Data 0.0019386973768724003 PDE 0.0002102285361615941 0.1493014246225357 0.1493014246225357 0.1495429128408432 0.028246574103832245 0.5411757826805115 0.9523983597755432 0.6128308176994324 \tTraning Loss: 0.002148925913033994\n",
            "63 Data 0.0019473596781870355 PDE 0.0002116279792971909 0.1493007093667984 0.1493007093667984 0.14954864978790283 0.02828998677432537 0.5412304997444153 0.9523979425430298 0.6128309965133667 \tTraning Loss: 0.002158987657484226\n",
            "64 Data 0.0019393968024898171 PDE 0.00021103178733028471 0.149300217628479 0.149300217628479 0.14955414831638336 0.02833474986255169 0.5412877202033997 0.9523975849151611 0.6128311157226562 \tTraning Loss: 0.002150428589820102\n",
            "65 Data 0.0019174952723260477 PDE 0.0002085509622702375 0.14929993450641632 0.14929993450641632 0.14955942332744598 0.028380678966641426 0.5413472652435303 0.9523972868919373 0.6128312349319458 \tTraning Loss: 0.0021260462345962855\n",
            "66 Data 0.0018870676974954393 PDE 0.0002044278080575168 0.14929983019828796 0.14929983019828796 0.14956451952457428 0.028427554294466972 0.5414087772369385 0.9523969888687134 0.6128313541412354 \tTraning Loss: 0.002091495505552956\n",
            "67 Data 0.0018544075806886965 PDE 0.00019898514437954873 0.14929987490177155 0.14929987490177155 0.14956945180892944 0.028475133702158928 0.5414719581604004 0.9523967504501343 0.6128314733505249 \tTraning Loss: 0.0020533927250682453\n",
            "68 Data 0.0018250534953587127 PDE 0.000192582854651846 0.14930005371570587 0.14930005371570587 0.14957426488399506 0.0285231601446867 0.5415363907814026 0.9523965120315552 0.6128315925598145 \tTraning Loss: 0.0020176363500105585\n",
            "69 Data 0.0018026870801312756 PDE 0.0001855877781054005 0.14930035173892975 0.14930035173892975 0.1495789736509323 0.02857137843966484 0.5416017174720764 0.9523962736129761 0.612831711769104 \tTraning Loss: 0.001988274858236676\n",
            "70 Data 0.0017887087422299253 PDE 0.00017835503967944533 0.1493007391691208 0.1493007391691208 0.1495835930109024 0.028619544580578804 0.5416675209999084 0.9523960947990417 0.6128318309783936 \tTraning Loss: 0.0019670637819093704\n",
            "71 Data 0.0017824410750954455 PDE 0.00017121848941314965 0.1493011862039566 0.1493011862039566 0.14958812296390533 0.02866743877530098 0.54173344373703 0.9523959159851074 0.6128319501876831 \tTraning Loss: 0.001953659564508595\n",
            "72 Data 0.0017817668547083947 PDE 0.0001644858275540173 0.1493016928434372 0.1493016928434372 0.1495925635099411 0.028714871034026146 0.5417990684509277 0.9523957371711731 0.6128320693969727 \tTraning Loss: 0.001946252682262412\n",
            "73 Data 0.0017839543572976597 PDE 0.00015843224537093192 0.14930224418640137 0.14930224418640137 0.14959688484668732 0.02876168116927147 0.5418640375137329 0.9523956179618835 0.6128321290016174 \tTraning Loss: 0.0019423866026685916\n",
            "74 Data 0.0017864247253048127 PDE 0.00015328978770412505 0.14930284023284912 0.14930284023284912 0.1496010720729828 0.02880774810910225 0.5419281125068665 0.952395498752594 0.6128321886062622 \tTraning Loss: 0.0019397145130089378\n",
            "75 Data 0.0017872956032991712 PDE 0.00014923261187504977 0.14930346608161926 0.14930346608161926 0.14960509538650513 0.028852984309196472 0.5419909954071045 0.9523953795433044 0.612832248210907 \tTraning Loss: 0.001936528215174221\n",
            "76 Data 0.001785619358097457 PDE 0.00014635837578680366 0.1493041217327118 0.1493041217327118 0.14960893988609314 0.02889733761548996 0.5420524477958679 0.9523952603340149 0.6128323078155518 \tTraning Loss: 0.0019319777338842607\n",
            "77 Data 0.0017813407844199463 PDE 0.0001446713285986334 0.1493048220872879 0.1493048220872879 0.14961257576942444 0.028940780088305473 0.5421123504638672 0.9523951411247253 0.6128323674201965 \tTraning Loss: 0.0019260121130185797\n",
            "78 Data 0.0017750564064197415 PDE 0.00014407324488274753 0.1493055522441864 0.1493055522441864 0.14961600303649902 0.028983309864997864 0.5421705842018127 0.9523950815200806 0.6128324270248413 \tTraning Loss: 0.001919129651302489\n",
            "79 Data 0.0017676891596942346 PDE 0.00014436677156481892 0.1493063122034073 0.1493063122034073 0.1496192216873169 0.029024943709373474 0.5422270894050598 0.9523950219154358 0.6128324866294861 \tTraning Loss: 0.0019120559312590536\n",
            "80 Data 0.001760183696776407 PDE 0.0001452742872061208 0.14930710196495056 0.14930710196495056 0.14962224662303925 0.02906571328639984 0.5422818660736084 0.952394962310791 0.6128325462341309 \tTraning Loss: 0.0019054579839825278\n",
            "81 Data 0.0017532903442823607 PDE 0.00014647081843577325 0.14930790662765503 0.14930790662765503 0.14962507784366608 0.02910565957427025 0.5423349142074585 0.9523949027061462 0.6128326058387756 \tTraning Loss: 0.001899761162718134\n",
            "82 Data 0.001747465644916469 PDE 0.00014762654609512538 0.14930874109268188 0.14930874109268188 0.14962774515151978 0.029144832864403725 0.5423863530158997 0.9523948431015015 0.6128326654434204 \tTraning Loss: 0.0018950921910115945\n",
            "83 Data 0.0017428746999348696 PDE 0.00014845076657366008 0.14930959045886993 0.14930959045886993 0.14963026344776154 0.029183289036154747 0.5424362421035767 0.9523947834968567 0.6128327250480652 \tTraning Loss: 0.0018913254665085296\n",
            "84 Data 0.0017394591727035074 PDE 0.00014873106556478888 0.14931043982505798 0.14931043982505798 0.14963266253471375 0.02922108583152294 0.542484700679779 0.9523947238922119 0.61283278465271 \tTraning Loss: 0.0018881902382682962\n",
            "85 Data 0.001737027128389759 PDE 0.0001483577798353508 0.14931130409240723 0.14931130409240723 0.1496349424123764 0.029258284717798233 0.5425318479537964 0.9523946642875671 0.6128328442573547 \tTraning Loss: 0.0018853849082251099\n",
            "86 Data 0.0017353341541662384 PDE 0.00014733224816154689 0.14931216835975647 0.14931216835975647 0.1496371328830719 0.029294949024915695 0.542577862739563 0.9523946046829224 0.6128329038619995 \tTraning Loss: 0.0018826664023277853\n",
            "87 Data 0.0017341353417733966 PDE 0.00014575592649634928 0.1493130326271057 0.1493130326271057 0.14963924884796143 0.029331140220165253 0.5426229238510132 0.9523945450782776 0.6128329634666443 \tTraning Loss: 0.001879891268269746\n",
            "88 Data 0.0017332117176348408 PDE 0.000143804369145073 0.14931389689445496 0.14931389689445496 0.14964129030704498 0.02936691977083683 0.5426672101020813 0.9523944854736328 0.6128330230712891 \tTraning Loss: 0.0018770160867799138\n",
            "89 Data 0.0017323767033578777 PDE 0.00014169128553476185 0.1493147760629654 0.1493147760629654 0.14964327216148376 0.029402345418930054 0.5427108407020569 0.9523944854736328 0.6128330826759338 \tTraning Loss: 0.0018740679888926395\n",
            "90 Data 0.0017314801001045998 PDE 0.00013962946832180023 0.14931565523147583 0.14931565523147583 0.14964519441127777 0.0294374730437994 0.542754054069519 0.9523944854736328 0.6128331422805786 \tTraning Loss: 0.0018711095684264\n",
            "91 Data 0.0017304146424418799 PDE 0.0001377970038447529 0.14931654930114746 0.14931654930114746 0.1496470719575882 0.0294723529368639 0.5427969694137573 0.9523944854736328 0.6128332018852234 \tTraning Loss: 0.0018682116462866328\n",
            "92 Data 0.0017291265722647819 PDE 0.00013631269393954426 0.1493174433708191 0.1493174433708191 0.14964888989925385 0.029507027938961983 0.542839765548706 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018654392662043261\n",
            "93 Data 0.0017276248232564958 PDE 0.00013522560766432434 0.14931835234165192 0.14931835234165192 0.1496506631374359 0.029541533440351486 0.5428825616836548 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018628504309208202\n",
            "94 Data 0.0017259804360957288 PDE 0.00013451758422888815 0.14931926131248474 0.14931926131248474 0.1496523916721344 0.029575902968645096 0.5429254770278931 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001860498020324617\n",
            "95 Data 0.001724311436931348 PDE 0.0001341170136583969 0.14932018518447876 0.14932018518447876 0.1496540755033493 0.029610158875584602 0.5429685711860657 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018584284505897448\n",
            "96 Data 0.0017227535565397757 PDE 0.0001339197187917307 0.14932110905647278 0.14932110905647278 0.14965572953224182 0.029644321650266647 0.5430119633674622 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018566732753315064\n",
            "97 Data 0.0017214228434578238 PDE 0.00013381207827478647 0.149322047829628 0.149322047829628 0.14965733885765076 0.029678404331207275 0.5430556535720825 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018552349217326103\n",
            "98 Data 0.0017203833217888868 PDE 0.0001336915447609499 0.1493229866027832 0.1493229866027832 0.1496589183807373 0.029712414368987083 0.5430996417999268 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018540748665498367\n",
            "99 Data 0.0017196276547848074 PDE 0.00013348164793569595 0.14932392537593842 0.14932392537593842 0.14966046810150146 0.02974635735154152 0.5431439876556396 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018531093027205033\n",
            "100 Data 0.0017190799926110974 PDE 0.00013313970703165978 0.14932486414909363 0.14932486414909363 0.14966198801994324 0.029780235141515732 0.5431886315345764 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018522196996427571\n",
            "101 Data 0.001718618514754371 PDE 0.0001326567871728912 0.14932580292224884 0.14932580292224884 0.14966346323490143 0.029814045876264572 0.5432335734367371 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018512753019272622\n",
            "102 Data 0.0017181117682265915 PDE 0.00013205203867983073 0.14932675659656525 0.14932675659656525 0.14966490864753723 0.029847782105207443 0.5432787537574768 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018501638069064223\n",
            "103 Data 0.001717457773074477 PDE 0.00013136351481080055 0.14932771027088165 0.14932771027088165 0.14966632425785065 0.0298814345151186 0.5433240532875061 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018488212878852775\n",
            "104 Data 0.001716610730430882 PDE 0.00013063706865068525 0.14932866394519806 0.14932866394519806 0.14966771006584167 0.029914991930127144 0.543369472026825 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018472477990815672\n",
            "105 Data 0.0017155900733998707 PDE 0.0001299172145081684 0.14932961761951447 0.14932961761951447 0.14966906607151031 0.029948439449071884 0.5434149503707886 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018455072879080391\n",
            "106 Data 0.0017144693563524627 PDE 0.000129239764646627 0.14933057129383087 0.14933057129383087 0.14967039227485657 0.029981760308146477 0.5434603691101074 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018437091209990897\n",
            "107 Data 0.0017133487473376052 PDE 0.00012862782750744373 0.14933153986930847 0.14933153986930847 0.14967168867588043 0.03001493588089943 0.5435056686401367 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001841976574845049\n",
            "108 Data 0.001712320882100012 PDE 0.00012809115287382156 0.14933250844478607 0.14933250844478607 0.1496729701757431 0.03004794754087925 0.5435507893562317 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018404120349738335\n",
            "109 Data 0.001711443369245856 PDE 0.00012762761616613716 0.14933347702026367 0.14933347702026367 0.1496742218732834 0.030080776661634445 0.5435956120491028 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018390709854119933\n",
            "110 Data 0.0017107229074803413 PDE 0.00012722652172669768 0.14933444559574127 0.14933444559574127 0.14967544376850128 0.030113404616713524 0.54364013671875 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001837949429207039\n",
            "111 Data 0.0017101189000739295 PDE 0.0001268722116947174 0.14933541417121887 0.14933541417121887 0.1496766358613968 0.030145816504955292 0.5436843037605286 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001836991111768647\n",
            "112 Data 0.0017095593649831035 PDE 0.00012654782040044665 0.14933638274669647 0.14933638274669647 0.1496777981519699 0.030178001150488853 0.5437281131744385 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00183610718538355\n",
            "113 Data 0.0017089681264971209 PDE 0.0001262378500541672 0.14933735132217407 0.14933735132217407 0.14967893064022064 0.030209949240088463 0.543771505355835 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001835205976551288\n",
            "114 Data 0.0017082877247705068 PDE 0.00012593009159900248 0.14933831989765167 0.14933831989765167 0.149680033326149 0.030241655185818672 0.543814480304718 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018342178163695093\n",
            "115 Data 0.0017074968171836365 PDE 0.00012561646872200072 0.14933928847312927 0.14933928847312927 0.14968110620975494 0.03027312085032463 0.5438570976257324 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018331132859056372\n",
            "116 Data 0.0017066120545297968 PDE 0.00012529268860816956 0.14934025704860687 0.14934025704860687 0.1496821492910385 0.030304349958896637 0.5438992977142334 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018319047431379664\n",
            "117 Data 0.0017056788693785637 PDE 0.00012495812552515417 0.14934122562408447 0.14934122562408447 0.1496831625699997 0.030335349962115288 0.5439411401748657 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018306369949037179\n",
            "118 Data 0.0017047534031646105 PDE 0.00012461439473554492 0.14934219419956207 0.14934219419956207 0.1496841460466385 0.03036613203585148 0.5439826250076294 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018293677979001554\n",
            "119 Data 0.001703883463716113 PDE 0.00012426474131643772 0.14934316277503967 0.14934316277503967 0.1496850997209549 0.030396712943911552 0.5440238118171692 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018281482050325508\n",
            "120 Data 0.0017030939352088304 PDE 0.00012391313794068992 0.14934413135051727 0.14934413135051727 0.1496860235929489 0.030427107587456703 0.5440647602081299 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018270070731495204\n",
            "121 Data 0.0017023820339008976 PDE 0.0001235630043083802 0.14934509992599487 0.14934509992599487 0.14968691766262054 0.030457336455583572 0.5441055297851562 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018259450382092778\n",
            "122 Data 0.0017017213396121737 PDE 0.0001232177164638415 0.14934606850147247 0.14934606850147247 0.14968779683113098 0.030487418174743652 0.5441461205482483 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018249390560760152\n",
            "123 Data 0.001701073736815164 PDE 0.00012287934077903628 0.14934703707695007 0.14934703707695007 0.14968864619731903 0.030517371371388435 0.5441865921020508 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018239530775942003\n",
            "124 Data 0.0017004031494366918 PDE 0.00012254917237441987 0.14934800565242767 0.14934800565242767 0.1496894806623459 0.030547214671969414 0.5442270040512085 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018229523218111117\n",
            "125 Data 0.0016996874077804835 PDE 0.00012222751684021205 0.14934897422790527 0.14934897422790527 0.14969030022621155 0.03057696484029293 0.5442673563957214 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018219149246206955\n",
            "126 Data 0.0016989231278115283 PDE 0.0001219139521708712 0.14934994280338287 0.14934994280338287 0.14969110488891602 0.03060663864016533 0.5443077087402344 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018208370799823995\n",
            "127 Data 0.0016981245902780147 PDE 0.00012160748156020418 0.14935091137886047 0.14935091137886047 0.1496918946504593 0.030636250972747803 0.5443481206893921 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018197320718382189\n",
            "128 Data 0.001697316180028194 PDE 0.00012130676623200998 0.14935187995433807 0.14935187995433807 0.14969268441200256 0.030665811151266098 0.5443885922431946 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001818622946260204\n",
            "129 Data 0.0016965229650620177 PDE 0.00012101040192646906 0.14935284852981567 0.14935284852981567 0.14969345927238464 0.030695326626300812 0.5444291234016418 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018175333669884868\n",
            "130 Data 0.001695761539352505 PDE 0.00012071720993844792 0.14935381710529327 0.14935381710529327 0.14969423413276672 0.03072480484843254 0.5444697141647339 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001816478749290953\n",
            "131 Data 0.0016950361471679573 PDE 0.00012042598245898262 0.14935478568077087 0.14935478568077087 0.1496949940919876 0.030754249542951584 0.5445103645324707 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00181546212962694\n",
            "132 Data 0.0016943385175115378 PDE 0.0001201359773403965 0.14935575425624847 0.14935575425624847 0.1496957540512085 0.03078366070985794 0.5445511341094971 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018144744948519343\n",
            "133 Data 0.0016936521364367969 PDE 0.00011984657612629235 0.14935672283172607 0.14935672283172607 0.1496964991092682 0.03081303834915161 0.5445919632911682 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018134987125630892\n",
            "134 Data 0.0016929594015984899 PDE 0.00011955769150517881 0.14935769140720367 0.14935769140720367 0.14969724416732788 0.030842380598187447 0.5446328520774841 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018125170931036687\n",
            "135 Data 0.001692247203290982 PDE 0.00011926930164918303 0.14935865998268127 0.14935865998268127 0.14969798922538757 0.03087168186903 0.5446738004684448 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001811516504940165\n",
            "136 Data 0.0016915110633221315 PDE 0.00011898155935341492 0.14935961365699768 0.14935961365699768 0.14969873428344727 0.03090093843638897 0.5447147488594055 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018104926226755465\n",
            "137 Data 0.0016907553121481267 PDE 0.00011869477748405188 0.1493605673313141 0.1493605673313141 0.14969946444034576 0.03093014657497406 0.544755756855011 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018094500896321786\n",
            "138 Data 0.0016899901493044201 PDE 0.00011840921797556803 0.1493615210056305 0.1493615210056305 0.14970019459724426 0.030959300696849823 0.5447967648506165 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018083993672799881\n",
            "139 Data 0.0016892279746246353 PDE 0.00011812503362307325 0.1493624746799469 0.1493624746799469 0.14970092475414276 0.03098839521408081 0.5448377728462219 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018073530082477085\n",
            "140 Data 0.0016884780582326786 PDE 0.00011784242815338075 0.1493634283542633 0.1493634283542633 0.14970165491104126 0.031017426401376724 0.5448787212371826 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018063204863860594\n",
            "141 Data 0.0016877447913001631 PDE 0.00011756137973861769 0.1493643820285797 0.1493643820285797 0.14970238506793976 0.031046390533447266 0.5449196696281433 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018053061710387808\n",
            "142 Data 0.001687026674117807 PDE 0.00011728171375580132 0.14936533570289612 0.14936533570289612 0.14970310032367706 0.031075283885002136 0.5449605584144592 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018043083878736084\n",
            "143 Data 0.0016863176667499746 PDE 0.00011700318282237276 0.14936628937721252 0.14936628937721252 0.14970381557941437 0.031104106456041336 0.5450013875961304 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018033208495723473\n",
            "144 Data 0.001685610352304747 PDE 0.0001167255177279003 0.14936724305152893 0.14936724305152893 0.14970453083515167 0.031132858246564865 0.5450421571731567 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018023358700326472\n",
            "145 Data 0.0016848984945080983 PDE 0.00011644844198599458 0.14936819672584534 0.14936819672584534 0.14970524609088898 0.031161537393927574 0.5450828671455383 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018013469364940929\n",
            "146 Data 0.0016841789165368967 PDE 0.00011617159907473251 0.14936915040016174 0.14936915040016174 0.14970596134662628 0.03119014762341976 0.5451235175132751 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0018003505156116292\n",
            "147 Data 0.0016834523938841746 PDE 0.0001158947852673009 0.14937010407447815 0.14937010407447815 0.1497066766023636 0.031218688935041428 0.5451641082763672 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017993471791514755\n",
            "148 Data 0.0016827222124974925 PDE 0.00011561793508008122 0.14937105774879456 0.14937105774879456 0.14970740675926208 0.03124716691672802 0.5452046394348145 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017983401475775737\n",
            "149 Data 0.0016819927907826496 PDE 0.00011534103396115825 0.14937201142311096 0.14937201142311096 0.14970813691616058 0.031275585293769836 0.5452451109886169 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017973338247438079\n",
            "150 Data 0.0016812679519664512 PDE 0.00011506416922202334 0.14937296509742737 0.14937296509742737 0.14970886707305908 0.03130394592881203 0.5452855229377747 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017963321211884745\n",
            "151 Data 0.0016805493327676176 PDE 0.00011478769010864198 0.14937390387058258 0.14937390387058258 0.14970959722995758 0.03133225440979004 0.5453259348869324 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017953370228762596\n",
            "152 Data 0.0016798364323268044 PDE 0.00011451182217570022 0.1493748426437378 0.1493748426437378 0.14971034228801727 0.03136051818728447 0.5453662872314453 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017943482545025047\n",
            "153 Data 0.0016791272256978203 PDE 0.00011423683463362977 0.149375781416893 0.149375781416893 0.14971108734607697 0.03138873726129532 0.5454066395759583 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00179336406033145\n",
            "154 Data 0.0016784188550883865 PDE 0.00011396308400435373 0.14937672019004822 0.14937672019004822 0.14971184730529785 0.03141691908240318 0.5454469919204712 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017923819390927402\n",
            "155 Data 0.0016777090337921038 PDE 0.00011369074491085485 0.14937765896320343 0.14937765896320343 0.14971260726451874 0.03144507110118866 0.5454874038696289 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017913997787029586\n",
            "156 Data 0.0016769964643359276 PDE 0.00011342004290781915 0.14937859773635864 0.14937859773635864 0.14971338212490082 0.03147319331765175 0.5455278158187866 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017904165072437468\n",
            "157 Data 0.0016762816411804349 PDE 0.00011315087613184005 0.14937953650951385 0.14937953650951385 0.1497141569852829 0.03150128945708275 0.5455682873725891 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001789432517312275\n",
            "158 Data 0.0016755658578691945 PDE 0.00011288321547908708 0.14938047528266907 0.14938047528266907 0.14971494674682617 0.03152936324477196 0.5456087589263916 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017884490733482816\n",
            "159 Data 0.001674850846523273 PDE 0.00011261701729381457 0.14938141405582428 0.14938141405582428 0.14971575140953064 0.031557418406009674 0.5456492900848389 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017874678638170876\n",
            "160 Data 0.0016741378232061942 PDE 0.00011235199781367555 0.1493823528289795 0.1493823528289795 0.1497165560722351 0.0315854549407959 0.5456898808479309 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017864898210198698\n",
            "161 Data 0.0016734277496996141 PDE 0.00011208790965611115 0.1493832916021347 0.1493832916021347 0.14971737563610077 0.03161347284913063 0.5457305312156677 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017855156593557253\n",
            "162 Data 0.0016727203997075127 PDE 0.00011182457092218101 0.14938423037528992 0.14938423037528992 0.14971821010112762 0.03164147585630417 0.5457712411880493 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017845449706296937\n",
            "163 Data 0.0016720152514678984 PDE 0.00011156170512549579 0.14938516914844513 0.14938516914844513 0.14971905946731567 0.031669460237026215 0.5458120107650757 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017835769565933942\n",
            "164 Data 0.0016713112993440374 PDE 0.00011129924678243697 0.14938610792160034 0.14938610792160034 0.14971990883350372 0.03169742599129677 0.5458528399467468 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017826105461264743\n",
            "165 Data 0.0016706081646386632 PDE 0.00011103698489023373 0.14938703179359436 0.14938703179359436 0.14972077310085297 0.03172537311911583 0.5458937287330627 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001781645149528897\n",
            "166 Data 0.0016699056170149352 PDE 0.00011077487579314038 0.14938795566558838 0.14938795566558838 0.1497216522693634 0.0317533016204834 0.5459346175193787 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017806804928080756\n",
            "167 Data 0.0016692039154258328 PDE 0.00011051291949115694 0.1493888795375824 0.1493888795375824 0.14972254633903503 0.03178120777010918 0.5459755659103394 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017797168349169897\n",
            "168 Data 0.0016685034928806926 PDE 0.000110251123260241 0.14938980340957642 0.14938980340957642 0.14972345530986786 0.031809087842702866 0.5460165739059448 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017787546161409336\n",
            "169 Data 0.0016678050098297766 PDE 0.00010998959623975679 0.14939072728157043 0.14939072728157043 0.14972436428070068 0.031836945563554764 0.5460575819015503 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017777946060695334\n",
            "170 Data 0.0016671087092284798 PDE 0.00010972846212098375 0.14939165115356445 0.14939165115356445 0.1497252881526947 0.03186477720737457 0.5460986495018005 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017768371713494636\n",
            "171 Data 0.0016664146778586134 PDE 0.00010946784459520131 0.14939257502555847 0.14939257502555847 0.14972622692584991 0.031892579048871994 0.5461397171020508 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017758825224538147\n",
            "172 Data 0.0016657227690547595 PDE 0.00010920783097390085 0.1493934988975525 0.1493934988975525 0.14972718060016632 0.03192035108804703 0.546180784702301 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017749306000286603\n",
            "173 Data 0.0016650322896077708 PDE 0.00010894864681176841 0.1493944227695465 0.1493944227695465 0.14972814917564392 0.03194809332489967 0.546221911907196 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017739809364195392\n",
            "174 Data 0.0016643429338423893 PDE 0.00010869027755688876 0.14939534664154053 0.14939534664154053 0.14972913265228271 0.03197580575942993 0.5462630391120911 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001773033211399278\n",
            "175 Data 0.0016636545106810926 PDE 0.00010843291966011748 0.14939627051353455 0.14939627051353455 0.1497301161289215 0.0320034883916378 0.5463041663169861 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00177208743034121\n",
            "176 Data 0.0016629666918135761 PDE 0.00010817658039741218 0.14939719438552856 0.14939719438552856 0.1497311145067215 0.032031137496232986 0.5463452935218811 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017711432722109883\n",
            "177 Data 0.0016622797097229443 PDE 0.00010792131070047617 0.14939811825752258 0.14939811825752258 0.14973212778568268 0.03205875679850578 0.5463864207267761 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017702010204234204\n",
            "178 Data 0.0016615936170518775 PDE 0.00010766716150101274 0.1493990272283554 0.1493990272283554 0.14973315596580505 0.032086342573165894 0.5464276075363159 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017692607785528903\n",
            "179 Data 0.001660908640202342 PDE 0.00010741412552306429 0.14939993619918823 0.14939993619918823 0.14973419904708862 0.032113898545503616 0.5464687943458557 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017683227657254064\n",
            "180 Data 0.001660224821663364 PDE 0.00010716217366280034 0.14940084517002106 0.14940084517002106 0.1497352570295334 0.03214142099022865 0.5465099811553955 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017673869953261643\n",
            "181 Data 0.0016595424059593594 PDE 0.00010691122588468716 0.14940175414085388 0.14940175414085388 0.14973632991313934 0.0321689136326313 0.5465511679649353 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017664536318440466\n",
            "182 Data 0.0016588613450250977 PDE 0.00010666118032531813 0.1494026631116867 0.1494026631116867 0.1497374027967453 0.03219637647271156 0.5465923547744751 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017655225253504158\n",
            "183 Data 0.001658181678801379 PDE 0.00010641200060490519 0.14940357208251953 0.14940357208251953 0.14973849058151245 0.03222380951046944 0.5466336011886597 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017645936794062841\n",
            "184 Data 0.0016575034036373402 PDE 0.00010616354848025367 0.14940448105335236 0.14940448105335236 0.1497395932674408 0.032251209020614624 0.5466748476028442 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017636669521175939\n",
            "185 Data 0.0016568267373339538 PDE 0.00010591565660433844 0.14940539002418518 0.14940539002418518 0.14974071085453033 0.032278578728437424 0.5467160940170288 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017627423939382922\n",
            "186 Data 0.0016561517434095648 PDE 0.00010566825949354097 0.149406298995018 0.149406298995018 0.14974184334278107 0.032305918633937836 0.5467574000358582 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017618200029031058\n",
            "187 Data 0.001655478483046044 PDE 0.00010542133531998843 0.14940720796585083 0.14940720796585083 0.149742990732193 0.03233322873711586 0.5467987060546875 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017608998183660325\n",
            "188 Data 0.0016548072249977159 PDE 0.00010517478949623182 0.14940811693668365 0.14940811693668365 0.1497441530227661 0.0323605090379715 0.5468400716781616 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017599820144939477\n",
            "189 Data 0.0016541377399089962 PDE 0.00010492858564248309 0.14940901100635529 0.14940901100635529 0.14974531531333923 0.032387759536504745 0.5468814373016357 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017590663255514793\n",
            "190 Data 0.001653470333189078 PDE 0.0001046827674144879 0.14940990507602692 0.14940990507602692 0.14974649250507355 0.03241497650742531 0.5469228029251099 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001758153100603566\n",
            "191 Data 0.001652804810561308 PDE 0.0001044374075718224 0.14941079914569855 0.14941079914569855 0.14974768459796906 0.03244216367602348 0.5469642281532288 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017572422181331305\n",
            "192 Data 0.0016521410650075887 PDE 0.00010419253521831706 0.14941169321537018 0.14941169321537018 0.14974889159202576 0.03246931731700897 0.5470056533813477 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017563336002259058\n",
            "193 Data 0.0016514790687865838 PDE 0.0001039482667692937 0.1494125872850418 0.1494125872850418 0.14975011348724365 0.032496437430381775 0.5470470786094666 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017554273355558775\n",
            "194 Data 0.0016508186269054508 PDE 0.00010370464588049799 0.14941348135471344 0.14941348135471344 0.14975135028362274 0.03252352401614189 0.5470885038375854 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017545232727859488\n",
            "195 Data 0.0016501597871796197 PDE 0.00010346182534703985 0.14941437542438507 0.14941437542438507 0.14975258708000183 0.03255057707428932 0.5471299886703491 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017536216125266595\n",
            "196 Data 0.001649502463750928 PDE 0.00010321976151317358 0.1494152694940567 0.1494152694940567 0.14975383877754211 0.032577596604824066 0.5471714735031128 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017527222252641016\n",
            "197 Data 0.001648846587485988 PDE 0.00010297860717400908 0.14941616356372833 0.14941616356372833 0.1497551053762436 0.032604578882455826 0.5472129583358765 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017518251946599971\n",
            "198 Data 0.0016481921301775533 PDE 0.00010273834050167352 0.14941704273223877 0.14941704273223877 0.14975638687610626 0.0326315239071846 0.5472544431686401 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017509304706792269\n",
            "199 Data 0.0016475392492431666 PDE 0.00010249891056446359 0.1494179219007492 0.1494179219007492 0.14975768327713013 0.03265843167901039 0.5472959280014038 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017500381598076302\n",
            "200 Data 0.0016468879342120906 PDE 0.0001022603755700402 0.14941880106925964 0.14941880106925964 0.14975899457931519 0.0326852984726429 0.5473374128341675 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017491483097821308\n",
            "201 Data 0.001646238156853567 PDE 0.0001020226045511663 0.14941968023777008 0.14941968023777008 0.14976030588150024 0.03271212428808212 0.5473788976669312 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017482607614047333\n",
            "202 Data 0.0016455900641261035 PDE 0.00010178553202422336 0.14942055940628052 0.14942055940628052 0.1497616320848465 0.032738909125328064 0.5474203824996948 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017473755961503268\n",
            "203 Data 0.0016449437891816853 PDE 0.00010154910705750808 0.14942143857479095 0.14942143857479095 0.14976297318935394 0.03276565298438072 0.5474618673324585 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017464928962391934\n",
            "204 Data 0.0016442992945644933 PDE 0.00010131327871931717 0.1494223177433014 0.1494223177433014 0.14976432919502258 0.0327923558652401 0.5475033521652222 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017456125732838105\n",
            "205 Data 0.00164365665023719 PDE 0.00010107799607794732 0.14942319691181183 0.14942319691181183 0.14976570010185242 0.03281901404261589 0.5475448369979858 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017447346463151372\n",
            "206 Data 0.0016430159285319066 PDE 0.00010084317182190716 0.14942407608032227 0.14942407608032227 0.14976707100868225 0.0328456312417984 0.5475863218307495 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017438591003538137\n",
            "207 Data 0.001642377120473811 PDE 0.00010060884233098477 0.1494249403476715 0.1494249403476715 0.14976845681667328 0.03287220373749733 0.5476278066635132 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017429859628047958\n",
            "208 Data 0.0016417401920996296 PDE 0.00010037502943305299 0.14942580461502075 0.14942580461502075 0.1497698575258255 0.03289873152971268 0.5476692318916321 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017421152215326826\n",
            "209 Data 0.0016411052636800694 PDE 0.0001001417258521542 0.14942666888237 0.14942666888237 0.14977127313613892 0.03292521461844444 0.547710657119751 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017412469895322236\n",
            "210 Data 0.0016404721225255423 PDE 9.990895341616124e-05 0.14942753314971924 0.14942753314971924 0.14977270364761353 0.03295165300369263 0.5477520823478699 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017403810759417035\n",
            "211 Data 0.0016398409672340828 PDE 9.967674850486219e-05 0.14942839741706848 0.14942839741706848 0.14977413415908813 0.03297804668545723 0.5477935075759888 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001739517715738945\n",
            "212 Data 0.001639211570997561 PDE 9.944519842974842e-05 0.14942926168441772 0.14942926168441772 0.14977557957172394 0.03300439566373825 0.5478349328041077 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017386567694273095\n",
            "213 Data 0.0016385838997168849 PDE 9.921433229465038e-05 0.14943012595176697 0.14943012595176697 0.14977703988552094 0.03303069993853569 0.5478763580322266 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017377982320115352\n",
            "214 Data 0.0016379581066328268 PDE 9.898417920339853e-05 0.1494309902191162 0.1494309902191162 0.14977851510047913 0.03305695950984955 0.5479177832603455 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017369422858362253\n",
            "215 Data 0.001637333936689483 PDE 9.875471732812002e-05 0.14943185448646545 0.14943185448646545 0.14977999031543732 0.033083174377679825 0.5479591488838196 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001736088654017603\n",
            "216 Data 0.001636711550958657 PDE 9.852605580817908e-05 0.1494327038526535 0.1494327038526535 0.1497814804315567 0.03310934081673622 0.5480005145072937 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017352376067668361\n",
            "217 Data 0.0016360908421626816 PDE 9.829811460804194e-05 0.14943355321884155 0.14943355321884155 0.14978298544883728 0.033135462552309036 0.5480418801307678 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017343889567707235\n",
            "218 Data 0.0016354718928339009 PDE 9.807087189983577e-05 0.1494344025850296 0.1494344025850296 0.14978450536727905 0.03316153585910797 0.5480832457542419 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017335427647337366\n",
            "219 Data 0.0016348547953997253 PDE 9.784431313164532e-05 0.14943525195121765 0.14943525195121765 0.14978604018688202 0.033187564462423325 0.5481246113777161 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017326991085313706\n",
            "220 Data 0.0016342393795712006 PDE 9.761843830347061e-05 0.1494361013174057 0.1494361013174057 0.14978757500648499 0.0332135446369648 0.5481659173965454 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017318578178746713\n",
            "221 Data 0.0016336259578303054 PDE 9.73931237240322e-05 0.14943695068359375 0.14943695068359375 0.14978912472724915 0.03323947638273239 0.5482072234153748 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017310190815543376\n",
            "222 Data 0.0016330143175904287 PDE 9.716845670482144e-05 0.1494378000497818 0.1494378000497818 0.1497906893491745 0.033265359699726105 0.5482485294342041 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017301827742952501\n",
            "223 Data 0.001632404703671454 PDE 9.694434993434697e-05 0.14943864941596985 0.14943864941596985 0.14979226887226105 0.03329119458794594 0.5482898354530334 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001729349053605801\n",
            "224 Data 0.0016317970763510402 PDE 9.67207524809055e-05 0.1494394838809967 0.1494394838809967 0.1497938483953476 0.03331698104739189 0.548331081867218 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017285178288319457\n",
            "225 Data 0.0016311913227799058 PDE 9.649767162045464e-05 0.14944031834602356 0.14944031834602356 0.14979544281959534 0.033342715352773666 0.5483723282814026 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017276889944003605\n",
            "226 Data 0.001630587483369447 PDE 9.62751655606553e-05 0.14944115281105042 0.14944115281105042 0.14979705214500427 0.03336840122938156 0.5484135746955872 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017268626489301024\n",
            "227 Data 0.0016299855998586044 PDE 9.605316154193133e-05 0.14944198727607727 0.14944198727607727 0.1497986614704132 0.03339403495192528 0.548454761505127 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017260387614005358\n",
            "228 Data 0.001629385716875328 PDE 9.58317395998165e-05 0.14944282174110413 0.14944282174110413 0.14980028569698334 0.033419616520404816 0.5484959483146667 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017252174564751446\n",
            "229 Data 0.0016287876337422369 PDE 9.561088518239558e-05 0.14944365620613098 0.14944365620613098 0.14980192482471466 0.033445145934820175 0.5485370755195618 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017243985189246325\n",
            "230 Data 0.001628191561199991 PDE 9.539064922137186e-05 0.14944449067115784 0.14944449067115784 0.14980357885360718 0.033470623195171356 0.5485782027244568 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017235822104213628\n",
            "231 Data 0.00162759731257024 PDE 9.517102444078773e-05 0.1494453251361847 0.1494453251361847 0.1498052328824997 0.03349604830145836 0.5486193299293518 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017227683370110278\n",
            "232 Data 0.0016270048988692495 PDE 9.495203994447365e-05 0.14944614470005035 0.14944614470005035 0.1498069018125534 0.03352142125368118 0.548660397529602 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017219569388137231\n",
            "233 Data 0.0016264144401804067 PDE 9.473365935264155e-05 0.14944696426391602 0.14944696426391602 0.1498085856437683 0.03354674205183983 0.5487014651298523 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017211480995330483\n",
            "234 Data 0.0016258256834954354 PDE 9.45159699767828e-05 0.14944778382778168 0.14944778382778168 0.1498102843761444 0.033572010695934296 0.5487424731254578 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017203416534722182\n",
            "235 Data 0.0016252388042909585 PDE 9.429895726498216e-05 0.14944860339164734 0.14944860339164734 0.1498119831085205 0.033597223460674286 0.5487834811210632 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017195377615559406\n",
            "236 Data 0.0016246538253909242 PDE 9.408253390574828e-05 0.149449422955513 0.149449422955513 0.1498136967420578 0.0336223840713501 0.5488244295120239 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017187363592966725\n",
            "237 Data 0.0016240707025060913 PDE 9.386675810674205e-05 0.14945024251937866 0.14945024251937866 0.1498154252767563 0.03364749252796173 0.5488653779029846 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017179374606128334\n",
            "238 Data 0.0016234894647334914 PDE 9.365157166030258e-05 0.14945106208324432 0.14945106208324432 0.14981715381145477 0.03367254510521889 0.5489062666893005 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001717141036393794\n",
            "239 Data 0.0016229101893603312 PDE 9.343698911834508e-05 0.14945188164710999 0.14945188164710999 0.14981889724731445 0.03369754180312157 0.5489471554756165 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017163471784786763\n",
            "240 Data 0.0016223328320418347 PDE 9.322294499725103e-05 0.14945268630981445 0.14945268630981445 0.14982065558433533 0.03372248634696007 0.5489879846572876 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017155557770390858\n",
            "241 Data 0.0016217574005448893 PDE 9.30094756768085e-05 0.14945349097251892 0.14945349097251892 0.1498224139213562 0.03374737501144409 0.5490288138389587 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017147668762216978\n",
            "242 Data 0.0016211839039981185 PDE 9.279652294935659e-05 0.1494542956352234 0.1494542956352234 0.14982418715953827 0.03377220779657364 0.5490695834159851 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017139804269474751\n",
            "243 Data 0.0016206123341557935 PDE 9.258410864276811e-05 0.14945510029792786 0.14945510029792786 0.14982597529888153 0.03379698470234871 0.5491103529930115 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017131964427985617\n",
            "244 Data 0.0016200428122859292 PDE 9.23722327570431e-05 0.14945590496063232 0.14945590496063232 0.1498277634382248 0.0338217057287693 0.5491510629653931 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017124150450429723\n",
            "245 Data 0.0016194752087477684 PDE 9.216086618835106e-05 0.1494567096233368 0.1494567096233368 0.14982956647872925 0.03384637087583542 0.5491917133331299 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017116360749361195\n",
            "246 Data 0.0016189095291156274 PDE 9.195005986839533e-05 0.14945751428604126 0.14945751428604126 0.1498313844203949 0.03387098014354706 0.5492323637008667 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017108595889840227\n",
            "247 Data 0.0016183458752008903 PDE 9.173978469334543e-05 0.14945830404758453 0.14945830404758453 0.14983320236206055 0.03389553353190422 0.5492729544639587 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017100856598942357\n",
            "248 Data 0.0016177840968009279 PDE 9.153006976703182e-05 0.1494590938091278 0.1494590938091278 0.1498350352048874 0.033920031040906906 0.5493135452270508 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017093141665679597\n",
            "249 Data 0.0016172242539374284 PDE 9.132089326158166e-05 0.14945988357067108 0.14945988357067108 0.14983688294887543 0.033944472670555115 0.549354076385498 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00170854514719901\n",
            "250 Data 0.0016166662754243933 PDE 9.111232066061348e-05 0.14946067333221436 0.14946067333221436 0.14983873069286346 0.033968858420848846 0.5493945479393005 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017077785960850067\n",
            "251 Data 0.001616110239957473 PDE 9.090425010072067e-05 0.14946146309375763 0.14946146309375763 0.1498405933380127 0.0339931845664978 0.549435019493103 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017070144900581936\n",
            "252 Data 0.0016155561042415527 PDE 9.069679799722508e-05 0.1494622528553009 0.1494622528553009 0.14984247088432312 0.03401745483279228 0.5494754314422607 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017062529022387777\n",
            "253 Data 0.0016150038775427776 PDE 9.048989886650816e-05 0.14946304261684418 0.14946304261684418 0.14984434843063354 0.034041665494441986 0.5495157837867737 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017054937764092858\n",
            "254 Data 0.0016144535923044248 PDE 9.028353088069707e-05 0.14946383237838745 0.14946383237838745 0.14984624087810516 0.03406582027673721 0.5495561361312866 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017047371231851218\n",
            "255 Data 0.0016139052215142292 PDE 9.007768676383421e-05 0.14946460723876953 0.14946460723876953 0.14984813332557678 0.034089915454387665 0.5495964288711548 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017039829082780634\n",
            "256 Data 0.0016133588665753974 PDE 8.98723810678348e-05 0.1494653820991516 0.1494653820991516 0.1498500406742096 0.03411395102739334 0.5496366620063782 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017032312476432322\n",
            "257 Data 0.001612814381772391 PDE 8.96675992407836e-05 0.1494661569595337 0.1494661569595337 0.1498519629240036 0.03413793072104454 0.5496768951416016 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017024819810131746\n",
            "258 Data 0.0016122718374808418 PDE 8.946334855863824e-05 0.14946693181991577 0.14946693181991577 0.1498538851737976 0.034161850810050964 0.5497170686721802 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00170173518603948\n",
            "259 Data 0.0016117312494425286 PDE 8.925958536565304e-05 0.14946770668029785 0.14946770668029785 0.1498558223247528 0.03418571129441261 0.549757182598114 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017009908348081817\n",
            "260 Data 0.00161119260496251 PDE 8.90563169377856e-05 0.14946848154067993 0.14946848154067993 0.149857759475708 0.034209512174129486 0.5497972369194031 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0017002489219002956\n",
            "261 Data 0.0016106559047615408 PDE 8.885357237886637e-05 0.149469256401062 0.149469256401062 0.1498597115278244 0.034233253449201584 0.5498372912406921 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016995094771404071\n",
            "262 Data 0.001610121223899418 PDE 8.865128620527685e-05 0.1494700163602829 0.1494700163602829 0.149861678481102 0.034256935119628906 0.5498772859573364 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001698772510104695\n",
            "263 Data 0.0016095884449065904 PDE 8.844952390063554e-05 0.14947077631950378 0.14947077631950378 0.14986364543437958 0.03428055718541145 0.5499172210693359 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001698037968807226\n",
            "264 Data 0.0016090576447684976 PDE 8.824822725728154e-05 0.14947153627872467 0.14947153627872467 0.14986562728881836 0.034304119646549225 0.5499570965766907 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016973058720257791\n",
            "265 Data 0.0016085287506343623 PDE 8.804743993096054e-05 0.14947229623794556 0.14947229623794556 0.14986760914325714 0.03432762250304222 0.5499969720840454 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016965761905653228\n",
            "266 Data 0.001608001791993589 PDE 8.784716919763014e-05 0.14947305619716644 0.14947305619716644 0.14986960589885712 0.03435106575489044 0.5500367879867554 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001695848961191219\n",
            "267 Data 0.0016074766858057553 PDE 8.764734957367182e-05 0.14947381615638733 0.14947381615638733 0.1498716175556183 0.03437444940209389 0.5500765442848206 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016951240353794271\n",
            "268 Data 0.0016069535905761194 PDE 8.744806109461933e-05 0.14947457611560822 0.14947457611560822 0.14987362921237946 0.03439777344465256 0.550116240978241 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016944016516707387\n",
            "269 Data 0.0016064324428116047 PDE 8.724925282876939e-05 0.1494753360748291 0.1494753360748291 0.14987565577030182 0.03442103788256645 0.5501558780670166 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016936816956403741\n",
            "270 Data 0.001605913172494987 PDE 8.705095387995243e-05 0.1494760811328888 0.1494760811328888 0.14987768232822418 0.03444423899054527 0.5501955151557922 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016929641263749395\n",
            "271 Data 0.0016053957996666729 PDE 8.685313514433801e-05 0.1494768261909485 0.1494768261909485 0.14987972378730774 0.03446738049387932 0.5502350926399231 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016922489348110109\n",
            "272 Data 0.0016048803625884254 PDE 8.665581844979897e-05 0.14947757124900818 0.14947757124900818 0.1498817652463913 0.03449046239256859 0.5502746105194092 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016915361810382243\n",
            "273 Data 0.0016043668766550301 PDE 8.645896014058962e-05 0.14947831630706787 0.14947831630706787 0.14988382160663605 0.034513480961322784 0.5503140687942505 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016908258367956197\n",
            "274 Data 0.001603855296004831 PDE 8.626254566479474e-05 0.14947906136512756 0.14947906136512756 0.149885892868042 0.034536439925432205 0.550353467464447 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016901178416696258\n",
            "275 Data 0.001603345647269186 PDE 8.606661867816001e-05 0.14947980642318726 0.14947980642318726 0.14988796412944794 0.03455933555960655 0.5503928065299988 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001689412265947346\n",
            "276 Data 0.0016028378987390545 PDE 8.587118645664304e-05 0.14948055148124695 0.14948055148124695 0.14989005029201508 0.034582171589136124 0.5504320859909058 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016887090851956975\n",
            "277 Data 0.0016023321317601973 PDE 8.567615441279486e-05 0.14948128163814545 0.14948128163814545 0.14989213645458221 0.03460494428873062 0.5504713654518127 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016880082861729922\n",
            "278 Data 0.0016018282429452167 PDE 8.548156620236114e-05 0.14948201179504395 0.14948201179504395 0.14989423751831055 0.034627657383680344 0.550510585308075 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016873098091475778\n",
            "279 Data 0.0016013263527427548 PDE 8.528744365321472e-05 0.14948274195194244 0.14948274195194244 0.14989633858203888 0.03465030714869499 0.5505497455596924 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016866137963959696\n",
            "280 Data 0.0016008263177206088 PDE 8.509375038556755e-05 0.14948347210884094 0.14948347210884094 0.1498984545469284 0.034672897309064865 0.550588846206665 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016859200681061763\n",
            "281 Data 0.0016003282376784234 PDE 8.490051550325006e-05 0.14948420226573944 0.14948420226573944 0.14990057051181793 0.034695424139499664 0.5506278872489929 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016852287531816735\n",
            "282 Data 0.001599832099303053 PDE 8.470777538605034e-05 0.14948493242263794 0.14948493242263794 0.14990270137786865 0.03471788763999939 0.550666868686676 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016845398746891034\n",
            "283 Data 0.001599337762019898 PDE 8.451541361864656e-05 0.14948566257953644 0.14948566257953644 0.14990483224391937 0.03474029153585434 0.5507057905197144 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016838531756385446\n",
            "284 Data 0.00159884537815938 PDE 8.432351023657247e-05 0.14948637783527374 0.14948637783527374 0.1499069780111313 0.034762632101774216 0.5507446527481079 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016831688883959524\n",
            "285 Data 0.0015983549784679607 PDE 8.413206523982808e-05 0.14948709309101105 0.14948709309101105 0.1499091237783432 0.03478490933775902 0.5507834553718567 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016824870437077888\n",
            "286 Data 0.0015978663451688634 PDE 8.394105680054054e-05 0.14948780834674835 0.14948780834674835 0.1499112844467163 0.034807123243808746 0.5508221983909607 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001681807401969404\n",
            "287 Data 0.0015973796842058841 PDE 8.375049219466746e-05 0.14948852360248566 0.14948852360248566 0.14991344511508942 0.0348292775452137 0.5508608818054199 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016811301764005516\n",
            "288 Data 0.0015968948909821147 PDE 8.356035687029362e-05 0.14948923885822296 0.14948923885822296 0.14991562068462372 0.03485136851668358 0.5508995652198792 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016804552478524083\n",
            "289 Data 0.0015964120321254511 PDE 8.337062172358856e-05 0.14948995411396027 0.14948995411396027 0.14991779625415802 0.034873396158218384 0.5509381890296936 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016797826538490397\n",
            "290 Data 0.0015959310443246116 PDE 8.318132313434035e-05 0.14949066936969757 0.14949066936969757 0.14991998672485352 0.034895360469818115 0.5509767532348633 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001679112367458952\n",
            "291 Data 0.001595451867051289 PDE 8.299245382659137e-05 0.14949138462543488 0.14949138462543488 0.149922177195549 0.03491726145148277 0.5510152578353882 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016784443208778803\n",
            "292 Data 0.0015949746239988793 PDE 8.280397742055357e-05 0.149492084980011 0.149492084980011 0.1499243825674057 0.03493909910321236 0.5510537028312683 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001677778601419433\n",
            "293 Data 0.0015944993227093407 PDE 8.261590846814215e-05 0.1494927853345871 0.1494927853345871 0.1499265879392624 0.034960873425006866 0.5510920882225037 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016771152311774828\n",
            "294 Data 0.0015940258777240783 PDE 8.242824696935713e-05 0.1494934856891632 0.1494934856891632 0.14992880821228027 0.0349825844168663 0.5511304140090942 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016764541246934354\n",
            "295 Data 0.001593554341606545 PDE 8.224101475207135e-05 0.14949418604373932 0.14949418604373932 0.14993102848529816 0.035004232078790665 0.55116868019104 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016757953563586163\n",
            "296 Data 0.0015930846022445273 PDE 8.205417543649673e-05 0.14949488639831543 0.14949488639831543 0.14993324875831604 0.03502581641077995 0.5512068867683411 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001675138777681024\n",
            "297 Data 0.0015926167649508354 PDE 8.186776540242136e-05 0.14949558675289154 0.14949558675289154 0.14993548393249512 0.03504733741283417 0.5512450337409973 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016744845303532567\n",
            "298 Data 0.0015921508413547804 PDE 8.168171916622669e-05 0.14949628710746765 0.14949628710746765 0.1499377191066742 0.03506879508495331 0.5512831211090088 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001673832560521007\n",
            "299 Data 0.0015916867273719519 PDE 8.14961240394041e-05 0.14949697256088257 0.14949697256088257 0.14993996918201447 0.035090189427137375 0.5513211488723755 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001673182851411356\n",
            "300 Data 0.001591224499331639 PDE 8.131089271046221e-05 0.14949765801429749 0.14949765801429749 0.14994221925735474 0.03511152043938637 0.5513591170310974 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016725353920421012\n",
            "301 Data 0.00159076412467637 PDE 8.112607611110434e-05 0.1494983434677124 0.1494983434677124 0.1499444842338562 0.03513278812170029 0.5513970255851746 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016718902007874744\n",
            "302 Data 0.001590305577173677 PDE 8.094161603366956e-05 0.14949902892112732 0.14949902892112732 0.14994674921035767 0.03515399247407913 0.5514348745346069 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016712471932073467\n",
            "303 Data 0.0015898488813439356 PDE 8.075759978964925e-05 0.14949971437454224 0.14949971437454224 0.14994902908802032 0.0351751334965229 0.5514726638793945 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016706064811335848\n",
            "304 Data 0.0015893939970920696 PDE 8.057391096372157e-05 0.14950039982795715 0.14950039982795715 0.14995130896568298 0.0351962111890316 0.5515103340148926 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016699679080557912\n",
            "305 Data 0.0015889410664813811 PDE 8.039061503950506e-05 0.14950108528137207 0.14950108528137207 0.14995358884334564 0.035217221826314926 0.5515479445457458 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016693316815208862\n",
            "306 Data 0.001588489946172403 PDE 8.020771929295734e-05 0.1495017558336258 0.1495017558336258 0.1499558836221695 0.03523816913366318 0.5515854954719543 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016686976654653604\n",
            "307 Data 0.0015880405591853516 PDE 8.002520917216316e-05 0.14950242638587952 0.14950242638587952 0.14995817840099335 0.035259053111076355 0.5516229867935181 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016680657683575148\n",
            "308 Data 0.0015875930363504823 PDE 7.98430701252073e-05 0.14950309693813324 0.14950309693813324 0.1499604880809784 0.03527987375855446 0.551660418510437 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016674361064756896\n",
            "309 Data 0.0015871473599550746 PDE 7.966127304825932e-05 0.14950376749038696 0.14950376749038696 0.14996279776096344 0.03530062735080719 0.5516977906227112 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016668086330033339\n",
            "310 Data 0.0015867035517436144 PDE 7.947985432110727e-05 0.14950443804264069 0.14950443804264069 0.1499651074409485 0.03532131761312485 0.5517351031303406 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016661834060647217\n",
            "311 Data 0.0015862615230357547 PDE 7.929884304758161e-05 0.1495051085948944 0.1495051085948944 0.14996743202209473 0.03534194454550743 0.5517723560333252 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016655603660833363\n",
            "312 Data 0.0015858213225948437 PDE 7.911818829597905e-05 0.14950577914714813 0.14950577914714813 0.14996975660324097 0.03536250814795494 0.551809549331665 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016649395108908227\n",
            "313 Data 0.0015853829356094772 PDE 7.893786096246913e-05 0.14950644969940186 0.14950644969940186 0.1499720960855484 0.03538300469517708 0.5518466830253601 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016643207965719464\n",
            "314 Data 0.001584946369011891 PDE 7.875791197875515e-05 0.14950710535049438 0.14950710535049438 0.14997443556785583 0.03540343791246414 0.5518837571144104 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016637042809906461\n",
            "315 Data 0.0015845115360995045 PDE 7.857835589675233e-05 0.14950776100158691 0.14950776100158691 0.14997677505016327 0.03542380779981613 0.5519207715988159 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016630898919962568\n",
            "316 Data 0.0015840784791905243 PDE 7.83991054049693e-05 0.14950841665267944 0.14950841665267944 0.1499791294336319 0.03544411063194275 0.5519577264785767 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016624775845954936\n",
            "317 Data 0.0015836473197941036 PDE 7.822021871106699e-05 0.14950907230377197 0.14950907230377197 0.14998148381710052 0.03546435013413429 0.5519945621490479 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016618675385051706\n",
            "318 Data 0.0015832178828850158 PDE 7.804168853908777e-05 0.1495097279548645 0.1495097279548645 0.14998383820056915 0.03548452630639076 0.5520313382148743 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016612595714241036\n",
            "319 Data 0.0015827902744285738 PDE 7.786350033711642e-05 0.14951038360595703 0.14951038360595703 0.14998620748519897 0.03550463542342186 0.5520680546760559 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016606537747656902\n",
            "320 Data 0.0015823643934830178 PDE 7.768566138111055e-05 0.14951103925704956 0.14951103925704956 0.1499885767698288 0.03552468121051788 0.5521047115325928 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016600500548641284\n",
            "321 Data 0.0015819403273687083 PDE 7.750817894702777e-05 0.1495116800069809 0.1495116800069809 0.14999094605445862 0.03554466366767883 0.5521413087844849 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001659448506315736\n",
            "322 Data 0.001581518021599661 PDE 7.733101665508002e-05 0.14951232075691223 0.14951232075691223 0.14999333024024963 0.03556457906961441 0.5521778464317322 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001658849038254741\n",
            "323 Data 0.0015810975131292546 PDE 7.715421088505536e-05 0.14951296150684357 0.14951296150684357 0.14999571442604065 0.035584431141614914 0.5522143244743347 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00165825172401431\n",
            "324 Data 0.0015806787415143116 PDE 7.697773980908096e-05 0.1495136022567749 0.1495136022567749 0.14999809861183167 0.035604216158390045 0.5522507429122925 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016576564813233926\n",
            "325 Data 0.0015802617009573717 PDE 7.68016034271568e-05 0.14951424300670624 0.14951424300670624 0.15000049769878387 0.0356239378452301 0.5522870421409607 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016570633043845285\n",
            "326 Data 0.001579846466621452 PDE 7.66258017392829e-05 0.14951488375663757 0.14951488375663757 0.15000289678573608 0.035643596202135086 0.5523232817649841 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016564722683607349\n",
            "327 Data 0.0015794329266066872 PDE 7.645033474545926e-05 0.1495155245065689 0.1495155245065689 0.1500052958726883 0.0356631875038147 0.5523594617843628 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016558832613521464\n",
            "328 Data 0.0015790211296203818 PDE 7.627518061781302e-05 0.14951615035533905 0.14951615035533905 0.1500077098608017 0.035682715475559235 0.5523955821990967 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016552963102381948\n",
            "329 Data 0.001578611107536865 PDE 7.610036118421704e-05 0.1495167762041092 0.1495167762041092 0.1500101238489151 0.0357021763920784 0.5524316430091858 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001654711468721082\n",
            "330 Data 0.0015782027533425003 PDE 7.592589099658653e-05 0.14951740205287933 0.14951740205287933 0.1500125378370285 0.03572157397866249 0.5524676442146301 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016541286443390868\n",
            "331 Data 0.0015777962236929706 PDE 7.575170457130298e-05 0.14951802790164948 0.14951802790164948 0.1500149667263031 0.03574090451002121 0.5525035858154297 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016535479282642736\n",
            "332 Data 0.0015773913486860677 PDE 7.557786739198491e-05 0.14951865375041962 0.14951865375041962 0.1500173956155777 0.035760171711444855 0.5525394082069397 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016529692160780526\n",
            "333 Data 0.0015769882094962494 PDE 7.540434307884425e-05 0.14951927959918976 0.14951927959918976 0.1500198245048523 0.035779375582933426 0.5525751709938049 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016523925525750937\n",
            "334 Data 0.0015765867726701174 PDE 7.523112435592338e-05 0.1495199054479599 0.1495199054479599 0.1500222533941269 0.035798512399196625 0.5526108741760254 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016518178970260408\n",
            "335 Data 0.0015761870073954343 PDE 7.505824760301039e-05 0.14952053129673004 0.14952053129673004 0.15002469718456268 0.03581758588552475 0.5526465177536011 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016512452549984446\n",
            "336 Data 0.0015757890408367645 PDE 7.488564733648673e-05 0.149521142244339 0.149521142244339 0.15002714097499847 0.0358365923166275 0.552682101726532 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016506746881732512\n",
            "337 Data 0.0015753927258427365 PDE 7.471339631592855e-05 0.14952175319194794 0.14952175319194794 0.15002958476543427 0.03585553541779518 0.5527176260948181 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001650106122158665\n",
            "338 Data 0.0015749980605081768 PDE 7.454144360963255e-05 0.14952236413955688 0.14952236413955688 0.15003204345703125 0.03587441146373749 0.5527530312538147 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016495395041178093\n",
            "339 Data 0.0015746051154215568 PDE 7.436979649355635e-05 0.14952297508716583 0.14952297508716583 0.15003450214862823 0.03589322417974472 0.5527883768081665 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016489749119151132\n",
            "340 Data 0.0015742137934572004 PDE 7.419844769174233e-05 0.14952358603477478 0.14952358603477478 0.15003696084022522 0.03591196984052658 0.5528236627578735 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016484122411489428\n",
            "341 Data 0.001573824221114331 PDE 7.402741903206334e-05 0.14952419698238373 0.14952419698238373 0.1500394195318222 0.03593065217137337 0.5528588891029358 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016478516401463943\n",
            "342 Data 0.0015734362309256325 PDE 7.385669596260414e-05 0.14952480792999268 0.14952480792999268 0.15004189312458038 0.03594926744699478 0.5528940558433533 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016472929268882366\n",
            "343 Data 0.0015730499532820733 PDE 7.36862639314495e-05 0.14952540397644043 0.14952540397644043 0.15004436671733856 0.03596781939268112 0.5529291033744812 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016467362172135228\n",
            "344 Data 0.0015726653877228165 PDE 7.351618114626035e-05 0.14952600002288818 0.14952600002288818 0.15004684031009674 0.03598630428314209 0.5529640913009644 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016461815688690769\n",
            "345 Data 0.0015722824479268106 PDE 7.334636029554531e-05 0.14952659606933594 0.14952659606933594 0.15004931390285492 0.036004725843667984 0.5529990196228027 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001645628808222356\n",
            "346 Data 0.0015719011548931316 PDE 7.3176808655262e-05 0.1495271921157837 0.1495271921157837 0.1500518023967743 0.036023080348968506 0.5530338883399963 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016450779635483936\n",
            "347 Data 0.0015715215094340596 PDE 7.300756260519847e-05 0.14952778816223145 0.14952778816223145 0.15005429089069366 0.036041371524333954 0.5530686974525452 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001644529072039258\n",
            "348 Data 0.0015711435199340495 PDE 7.283862942131236e-05 0.1495283842086792 0.1495283842086792 0.15005677938461304 0.03605959564447403 0.5531033873558044 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016439821493553618\n",
            "349 Data 0.0015707671482515233 PDE 7.266997272381559e-05 0.14952898025512695 0.14952898025512695 0.1500592678785324 0.03607775643467903 0.553138017654419 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016434371209753389\n",
            "350 Data 0.001570392350515788 PDE 7.250162161653861e-05 0.1495295763015747 0.1495295763015747 0.15006177127361298 0.03609585016965866 0.5531725883483887 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016428939721323265\n",
            "351 Data 0.0015700192502689438 PDE 7.23335615475662e-05 0.14953015744686127 0.14953015744686127 0.15006427466869354 0.03611388057470322 0.5532070994377136 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00164235281181651\n",
            "352 Data 0.0015696477154169834 PDE 7.216577796498314e-05 0.14953073859214783 0.14953073859214783 0.1500667780637741 0.0361318439245224 0.5532415509223938 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016418134933819666\n",
            "353 Data 0.0015692778799155963 PDE 7.199828542070463e-05 0.1495313197374344 0.1495313197374344 0.15006928145885468 0.03614974394440651 0.5532758831977844 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001641276165336301\n",
            "354 Data 0.001568909580010613 PDE 7.183107663877308e-05 0.14953190088272095 0.14953190088272095 0.15007178485393524 0.03616757690906525 0.5533101558685303 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001640740656649386\n",
            "355 Data 0.001568542885509796 PDE 7.16641588951461e-05 0.1495324820280075 0.1495324820280075 0.150074303150177 0.03618534654378891 0.5533443689346313 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001640207044404942\n",
            "356 Data 0.0015681777509891392 PDE 7.149753218982369e-05 0.14953306317329407 0.14953306317329407 0.15007682144641876 0.0362030491232872 0.5533785223960876 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016396752831789629\n",
            "357 Data 0.0015678142462699205 PDE 7.133118197089061e-05 0.14953364431858063 0.14953364431858063 0.15007933974266052 0.03622068837285042 0.5534125566482544 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016391454282408111\n",
            "358 Data 0.0015674522828160154 PDE 7.116508641047403e-05 0.149534210562706 0.149534210562706 0.15008185803890228 0.03623826056718826 0.5534465312957764 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016386173692264894\n",
            "359 Data 0.0015670919734621252 PDE 7.099930371623486e-05 0.14953477680683136 0.14953477680683136 0.15008437633514404 0.036255769431591034 0.5534804463386536 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00163809127717836\n",
            "360 Data 0.001566733189670721 PDE 7.083374657668173e-05 0.14953534305095673 0.14953534305095673 0.150086909532547 0.03627321124076843 0.553514301776886 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016375669362474028\n",
            "361 Data 0.0015663760181528767 PDE 7.06684950273484e-05 0.1495359092950821 0.1495359092950821 0.15008944272994995 0.03629058972001076 0.5535480380058289 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016370445131802251\n",
            "362 Data 0.0015660203031761031 PDE 7.050353451631963e-05 0.14953647553920746 0.14953647553920746 0.1500919759273529 0.03630790114402771 0.553581714630127 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016365238376924228\n",
            "363 Data 0.001565666209341973 PDE 7.033879228401929e-05 0.14953704178333282 0.14953704178333282 0.15009450912475586 0.03632514923810959 0.5536153316497803 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016360050016259924\n",
            "364 Data 0.0015653137059003055 PDE 7.017437746981159e-05 0.1495376080274582 0.1495376080274582 0.1500970423221588 0.036342330276966095 0.5536488890647888 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001635488083370117\n",
            "365 Data 0.0015649626711628302 PDE 7.001020276220515e-05 0.14953817427158356 0.14953817427158356 0.15009957551956177 0.03635944798588753 0.5536823868751526 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016349728739250354\n",
            "366 Data 0.0015646131672103574 PDE 6.984631181694567e-05 0.14953872561454773 0.14953872561454773 0.15010212361812592 0.03637649863958359 0.5537157654762268 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001634459479027303\n",
            "367 Data 0.0015642652709231205 PDE 6.968269735807553e-05 0.1495392769575119 0.1495392769575119 0.15010467171669006 0.036393485963344574 0.5537490844726562 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001633947968281196\n",
            "368 Data 0.0015639188723396253 PDE 6.951931572984904e-05 0.14953982830047607 0.14953982830047607 0.1501072198152542 0.03641040623188019 0.5537823438644409 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016334381880694744\n",
            "369 Data 0.0015635739755003378 PDE 6.935621058801189e-05 0.14954037964344025 0.14954037964344025 0.15010976791381836 0.03642726317048073 0.5538155436515808 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016329301860883497\n",
            "370 Data 0.0015632305779911225 PDE 6.919336010469124e-05 0.14954093098640442 0.14954093098640442 0.1501123160123825 0.036444053053855896 0.5538486242294312 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016324239380958137\n",
            "371 Data 0.001562888744790105 PDE 6.903079338371754e-05 0.1495414823293686 0.1495414823293686 0.15011486411094666 0.03646077960729599 0.5538816452026367 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016319195381738225\n",
            "372 Data 0.0015625483957148485 PDE 6.886848859721795e-05 0.14954203367233276 0.14954203367233276 0.1501174122095108 0.03647744283080101 0.5539146065711975 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016314168843120664\n",
            "373 Data 0.0015622094766313832 PDE 6.870643846923485e-05 0.14954258501529694 0.14954258501529694 0.15011997520923615 0.03649403899908066 0.5539475083351135 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001630915915100618\n",
            "374 Data 0.0015618720779146346 PDE 6.854465755168349e-05 0.14954312145709991 0.14954312145709991 0.1501225382089615 0.03651057183742523 0.55398029088974 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001630416735466318\n",
            "375 Data 0.0015615362270637247 PDE 6.838311674073339e-05 0.1495436578989029 0.1495436578989029 0.15012510120868683 0.036527037620544434 0.5540130138397217 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001629919343804458\n",
            "376 Data 0.001561201810488095 PDE 6.822185241617262e-05 0.14954419434070587 0.14954419434070587 0.15012766420841217 0.03654344007372856 0.5540456771850586 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016294236629042676\n",
            "377 Data 0.0015608688860595376 PDE 6.806084275012836e-05 0.14954473078250885 0.14954473078250885 0.1501302272081375 0.03655977547168732 0.5540782809257507 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001628929728809666\n",
            "378 Data 0.0015605373563633953 PDE 6.790008046664298e-05 0.14954526722431183 0.14954526722431183 0.15013279020786285 0.036576047539711 0.5541107654571533 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016284374368300382\n",
            "379 Data 0.001560207406856462 PDE 6.773959466954693e-05 0.1495458036661148 0.1495458036661148 0.1501353532075882 0.036592256277799606 0.5541431903839111 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001627947001526009\n",
            "380 Data 0.0015598788387428313 PDE 6.757933442713693e-05 0.14954634010791779 0.14954634010791779 0.15013791620731354 0.03660839796066284 0.5541755557060242 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016274581731699682\n",
            "381 Data 0.001559551688390017 PDE 6.74193215672858e-05 0.14954687654972076 0.14954687654972076 0.15014047920703888 0.036624476313591 0.5542078018188477 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016269710099573028\n",
            "382 Data 0.0015592260158111264 PDE 6.725957791786641e-05 0.14954739809036255 0.14954739809036255 0.15014305710792542 0.03664048761129379 0.5542399883270264 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016264855937289928\n",
            "383 Data 0.0015589017654893075 PDE 6.710005982313305e-05 0.14954791963100433 0.14954791963100433 0.15014563500881195 0.03665643557906151 0.5542721152305603 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016260018253124405\n",
            "384 Data 0.0015585789586833956 PDE 6.694081821478903e-05 0.14954844117164612 0.14954844117164612 0.1501482129096985 0.03667232021689415 0.5543041825294495 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016255197768981846\n",
            "385 Data 0.0015582576431116654 PDE 6.678183126496151e-05 0.1495489627122879 0.1495489627122879 0.15015079081058502 0.03668813779950142 0.5543361306190491 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001625039474376627\n",
            "386 Data 0.0015579376392335332 PDE 6.662306986982003e-05 0.1495494842529297 0.1495494842529297 0.15015336871147156 0.036703892052173615 0.5543680191040039 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016245607091033532\n",
            "387 Data 0.0015576191143554993 PDE 6.646453402936459e-05 0.14955000579357147 0.14955000579357147 0.1501559466123581 0.036719582974910736 0.554399847984314 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001624083648384864\n",
            "388 Data 0.0015573021050149457 PDE 6.630626739934087e-05 0.14955052733421326 0.14955052733421326 0.15015852451324463 0.036735206842422485 0.5544316172599792 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016236083724142866\n",
            "389 Data 0.0015569863242083156 PDE 6.614825542783365e-05 0.14955103397369385 0.14955103397369385 0.15016110241413116 0.03675076737999916 0.554463267326355 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016231345796361492\n",
            "390 Data 0.0015566720050126759 PDE 6.599048356292769e-05 0.14955154061317444 0.14955154061317444 0.1501636803150177 0.03676626458764076 0.5544948577880859 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016226624885756036\n",
            "391 Data 0.0015563591292726568 PDE 6.583292270079255e-05 0.14955204725265503 0.14955204725265503 0.15016625821590424 0.03678169474005699 0.5545263886451721 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016221920519734494\n",
            "392 Data 0.0015560474808897066 PDE 6.567564560100436e-05 0.14955255389213562 0.14955255389213562 0.15016883611679077 0.03679706156253815 0.5545578598976135 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001621723126490711\n",
            "393 Data 0.0015557374020202048 PDE 6.551860860781744e-05 0.1495530605316162 0.1495530605316162 0.1501714140176773 0.03681236505508423 0.5545892119407654 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016212560106280223\n",
            "394 Data 0.0015554285920375208 PDE 6.536180444527417e-05 0.1495535671710968 0.1495535671710968 0.15017399191856384 0.03682760149240494 0.5546205043792725 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001620790396482795\n",
            "395 Data 0.0015551210919810625 PDE 6.520528404507786e-05 0.1495540738105774 0.1495540738105774 0.15017658472061157 0.03684277459979057 0.5546517372131348 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016203263760261404\n",
            "396 Data 0.0015548151015344718 PDE 6.50489455438219e-05 0.14955458045005798 0.14955458045005798 0.1501791775226593 0.036857884377241135 0.5546829104423523 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016198640470782937\n",
            "397 Data 0.001554510322235334 PDE 6.489286897704005e-05 0.14955507218837738 0.14955507218837738 0.15018177032470703 0.03687293082475662 0.5547139644622803 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001619403191212374\n",
            "398 Data 0.0015542069757987064 PDE 6.473700341302902e-05 0.14955556392669678 0.14955556392669678 0.15018436312675476 0.03688791021704674 0.5547449588775635 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016189439792117354\n",
            "399 Data 0.001553904925776425 PDE 6.458142888732255e-05 0.14955605566501617 0.14955605566501617 0.1501869559288025 0.03690282627940178 0.5547758936882019 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016184863546637475\n",
            "400 Data 0.0015536042210116464 PDE 6.442607991630211e-05 0.14955654740333557 0.14955654740333557 0.15018954873085022 0.03691767901182175 0.5548067092895508 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016180303009279485\n",
            "401 Data 0.0015533049006714755 PDE 6.427094922401011e-05 0.14955703914165497 0.14955703914165497 0.15019214153289795 0.03693246841430664 0.5548374652862549 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016175758498954856\n",
            "402 Data 0.0015530068419029228 PDE 6.411604408640414e-05 0.14955753087997437 0.14955753087997437 0.15019473433494568 0.03694719076156616 0.5548681616783142 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001617122885989327\n",
            "403 Data 0.0015527100757063332 PDE 6.396142271114513e-05 0.14955802261829376 0.14955802261829376 0.1501973271369934 0.03696184977889061 0.5548987984657288 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016166714984174784\n",
            "404 Data 0.0015524147331150344 PDE 6.380701961461455e-05 0.14955851435661316 0.14955851435661316 0.15019991993904114 0.036976445466279984 0.5549293160438538 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001616221752729649\n",
            "405 Data 0.0015521206292793766 PDE 6.365284207277e-05 0.14955899119377136 0.14955899119377136 0.15020251274108887 0.03699097782373428 0.554959774017334 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016157734713521466\n",
            "406 Data 0.0015518278600035627 PDE 6.349891918944195e-05 0.14955946803092957 0.14955946803092957 0.1502051055431366 0.03700544685125351 0.5549901723861694 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016153267791930047\n",
            "407 Data 0.0015515363452652264 PDE 6.334518548101187e-05 0.14955994486808777 0.14955994486808777 0.15020769834518433 0.03701984882354736 0.5550205111503601 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016148815307462383\n",
            "408 Data 0.0015512461201962365 PDE 6.319174281088635e-05 0.14956042170524597 0.14956042170524597 0.15021029114723206 0.03703418746590614 0.5550507307052612 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016144378630071228\n",
            "409 Data 0.001550957255440285 PDE 6.303849659161642e-05 0.14956089854240417 0.14956089854240417 0.15021288394927979 0.03704846277832985 0.5550808906555176 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016139957520319014\n",
            "410 Data 0.001550669568493657 PDE 6.28854613751173e-05 0.14956137537956238 0.14956137537956238 0.15021547675132751 0.03706267476081848 0.5551109910011292 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016135550298687744\n",
            "411 Data 0.00155038315508795 PDE 6.273268809309229e-05 0.14956185221672058 0.14956185221672058 0.15021806955337524 0.03707682341337204 0.555141031742096 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016131158431810424\n",
            "412 Data 0.001550098095354974 PDE 6.258015491766855e-05 0.14956232905387878 0.14956232905387878 0.15022066235542297 0.037090908735990524 0.5551709532737732 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016126782502726427\n",
            "413 Data 0.001549814182635921 PDE 6.242784729693085e-05 0.1495627909898758 0.1495627909898758 0.1502232551574707 0.037104930728673935 0.5552008152008057 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016122420299328518\n",
            "414 Data 0.001549531630492964 PDE 6.227574340300635e-05 0.1495632529258728 0.1495632529258728 0.15022584795951843 0.03711888939142227 0.5552306175231934 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016118073738959704\n",
            "415 Data 0.001549250267152818 PDE 6.212389416759834e-05 0.1495637148618698 0.1495637148618698 0.15022844076156616 0.037132784724235535 0.5552603006362915 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016113741613204163\n",
            "416 Data 0.00154897012462566 PDE 6.197227048687637e-05 0.14956417679786682 0.14956417679786682 0.1502310335636139 0.037146616727113724 0.5552899241447449 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016109423951125364\n",
            "417 Data 0.0015486913454690672 PDE 6.182083598105237e-05 0.14956463873386383 0.14956463873386383 0.15023362636566162 0.03716038540005684 0.5553194880485535 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016105121814501196\n",
            "418 Data 0.0015484136242864715 PDE 6.166967796161771e-05 0.14956510066986084 0.14956510066986084 0.15023621916770935 0.03717409074306488 0.5553489923477173 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016100833022480892\n",
            "419 Data 0.0015481372071037376 PDE 6.151873822091147e-05 0.14956556260585785 0.14956556260585785 0.15023881196975708 0.03718772903084755 0.5553783774375916 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001609655945324649\n",
            "420 Data 0.0015478620447759834 PDE 6.136802403489128e-05 0.14956602454185486 0.14956602454185486 0.1502414047718048 0.037201303988695145 0.555407702922821 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016092300688108747\n",
            "421 Data 0.0015475880244572471 PDE 6.121753540355712e-05 0.14956647157669067 0.14956647157669067 0.15024399757385254 0.037214815616607666 0.5554369688034058 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016088055598608042\n",
            "422 Data 0.0015473152844714066 PDE 6.106727960286662e-05 0.1495669186115265 0.1495669186115265 0.15024659037590027 0.03722826763987541 0.5554661750793457 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016083825640742732\n",
            "423 Data 0.0015470436664823018 PDE 6.091727482271381e-05 0.1495673656463623 0.1495673656463623 0.150249183177948 0.037241656333208084 0.5554952621459961 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016079609413050156\n",
            "424 Data 0.0015467733299815883 PDE 6.0767470131395385e-05 0.14956781268119812 0.14956781268119812 0.15025177597999573 0.03725498169660568 0.5555242896080017 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016075408001129836\n",
            "425 Data 0.0015465041120893855 PDE 6.0617861890932545e-05 0.14956825971603394 0.14956825971603394 0.15025436878204346 0.03726824373006821 0.5555532574653625 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001607121973980318\n",
            "426 Data 0.0015462360736564526 PDE 6.0468515584943816e-05 0.14956870675086975 0.14956870675086975 0.1502569615840912 0.03728144243359566 0.5555821657180786 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016067045892413964\n",
            "427 Data 0.0015459692712167137 PDE 6.0319383919704705e-05 0.14956915378570557 0.14956915378570557 0.15025955438613892 0.037294577807188034 0.5556109547615051 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016062886551364184\n",
            "428 Data 0.0015457036357040064 PDE 6.017048508510925e-05 0.14956960082054138 0.14956960082054138 0.15026214718818665 0.03730764985084534 0.5556396842002869 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016058741207891157\n",
            "429 Data 0.0015454391567298613 PDE 6.0021808167221025e-05 0.1495700478553772 0.1495700478553772 0.15026472508907318 0.037320658564567566 0.5556683540344238 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016054609648970823\n",
            "430 Data 0.0015451757861700338 PDE 5.9873349528061226e-05 0.14957047998905182 0.14957047998905182 0.15026730298995972 0.03733360394835472 0.555696964263916 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001605049135698095\n",
            "431 Data 0.0015449135849733553 PDE 5.9725120081566274e-05 0.14957091212272644 0.14957091212272644 0.15026988089084625 0.0373464860022068 0.5557254552841187 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016046387050549216\n",
            "432 Data 0.0015446524919887124 PDE 5.9577138017630205e-05 0.14957134425640106 0.14957134425640106 0.1502724587917328 0.03735930472612381 0.5557538866996765 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016042296300063427\n",
            "433 Data 0.0015443926440832506 PDE 5.9429319662740454e-05 0.14957177639007568 0.14957177639007568 0.15027503669261932 0.03737206384539604 0.5557822585105896 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001603821963745991\n",
            "434 Data 0.0015441338929573021 PDE 5.928178143221885e-05 0.1495722085237503 0.1495722085237503 0.15027761459350586 0.0373847596347332 0.5558105111122131 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001603415674389521\n",
            "435 Data 0.0015438762650089167 PDE 5.913443965255283e-05 0.14957264065742493 0.14957264065742493 0.1502801924943924 0.037397392094135284 0.5558387041091919 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016030107046614696\n",
            "436 Data 0.0015436197766456649 PDE 5.8987337979488075e-05 0.14957307279109955 0.14957307279109955 0.15028277039527893 0.037409961223602295 0.5558668375015259 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001602607114625153\n",
            "437 Data 0.001543364424048013 PDE 5.8840443671215326e-05 0.14957350492477417 0.14957350492477417 0.15028534829616547 0.03742246702313423 0.5558949112892151 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016022048677192284\n",
            "438 Data 0.00154311019365694 PDE 5.8693749451776966e-05 0.1495739221572876 0.1495739221572876 0.150287926197052 0.03743491321802139 0.5559228658676147 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001601803943108717\n",
            "439 Data 0.0015428570824578955 PDE 5.8547320804791525e-05 0.14957433938980103 0.14957433938980103 0.15029050409793854 0.03744729608297348 0.5559507608413696 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001601404403262687\n",
            "440 Data 0.0015426050137715397 PDE 5.8401077694725245e-05 0.14957475662231445 0.14957475662231445 0.15029308199882507 0.037459615617990494 0.5559785962104797 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001601006091466265\n",
            "441 Data 0.0015423541111188627 PDE 5.8255071053281426e-05 0.14957517385482788 0.14957517385482788 0.1502956598997116 0.037471871823072433 0.5560063719749451 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0016006091821721441\n",
            "442 Data 0.0015421042396528584 PDE 5.810930815641768e-05 0.1495755910873413 0.1495755910873413 0.15029823780059814 0.0374840684235096 0.5560340285301208 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001600213547809276\n",
            "443 Data 0.0015418554295597746 PDE 5.796376717626117e-05 0.14957600831985474 0.14957600831985474 0.1503008008003235 0.03749620169401169 0.5560616254806519 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015998191967360358\n",
            "444 Data 0.001541607746462374 PDE 5.7818419008981436e-05 0.14957642555236816 0.14957642555236816 0.15030336380004883 0.037508271634578705 0.5560891628265381 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015994261654713555\n",
            "445 Data 0.0015413611251696138 PDE 5.7673292758408934e-05 0.1495768427848816 0.1495768427848816 0.15030592679977417 0.037520281970500946 0.5561166405677795 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015990344179280227\n",
            "446 Data 0.0015411155742794272 PDE 5.752840297645889e-05 0.14957724511623383 0.14957724511623383 0.1503084897994995 0.03753222897648811 0.5561440587043762 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001598643977255886\n",
            "447 Data 0.0015408711651211209 PDE 5.7383756939088926e-05 0.14957764744758606 0.14957764744758606 0.15031105279922485 0.03754411265254021 0.5561713576316833 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015982549220602098\n",
            "448 Data 0.0015406276716690985 PDE 5.7239303714595735e-05 0.1495780497789383 0.1495780497789383 0.1503136157989502 0.037555936723947525 0.5561985969543457 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015978669753836943\n",
            "449 Data 0.001540385337491565 PDE 5.709506513085216e-05 0.14957845211029053 0.14957845211029053 0.15031617879867554 0.03756769746541977 0.5562257766723633 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015974804026224173\n",
            "450 Data 0.0015401440653118005 PDE 5.6951044825837016e-05 0.14957885444164276 0.14957885444164276 0.15031874179840088 0.03757939487695694 0.5562528967857361 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015970951101376375\n",
            "451 Data 0.0015399037797773718 PDE 5.680726098944433e-05 0.149579256772995 0.149579256772995 0.15032130479812622 0.037591032683849335 0.5562798976898193 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015967110407668161\n",
            "452 Data 0.0015396645999188087 PDE 5.6663688155822456e-05 0.14957965910434723 0.14957965910434723 0.15032386779785156 0.037602607160806656 0.5563068389892578 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015963282880746311\n",
            "453 Data 0.0015394263533362066 PDE 5.6520359066780657e-05 0.14958006143569946 0.14958006143569946 0.1503264158964157 0.0376141220331192 0.5563337206840515 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015959467124029873\n",
            "454 Data 0.0015391891401136699 PDE 5.637722642859444e-05 0.1495804488658905 0.1495804488658905 0.15032896399497986 0.037625573575496674 0.5563605427742004 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015955663665422643\n",
            "455 Data 0.001538952981770191 PDE 5.623431206913665e-05 0.14958083629608154 0.14958083629608154 0.150331512093544 0.03763696551322937 0.5563872456550598 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015951872938393277\n",
            "456 Data 0.0015387179035754811 PDE 5.60916232643649e-05 0.14958122372627258 0.14958122372627258 0.15033406019210815 0.03764829412102699 0.5564138889312744 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001594809526839846\n",
            "457 Data 0.0015384837707988336 PDE 5.5949149100342765e-05 0.14958161115646362 0.14958161115646362 0.1503366082906723 0.03765956312417984 0.5564404726028442 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015944329198991764\n",
            "458 Data 0.0015382506905739057 PDE 5.5806896853027865e-05 0.14958199858665466 0.14958199858665466 0.15033915638923645 0.037670768797397614 0.5564669966697693 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015940575874269336\n",
            "459 Data 0.0015380185123763483 PDE 5.566488471231423e-05 0.1495823860168457 0.1495823860168457 0.1503417044878006 0.03768191486597061 0.5564934015274048 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015936833970886626\n",
            "460 Data 0.0015377874009232953 PDE 5.5523079936392605e-05 0.14958277344703674 0.14958277344703674 0.15034425258636475 0.037692997604608536 0.5565197467803955 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001593310480859688\n",
            "461 Data 0.0015375572114836808 PDE 5.538148252526298e-05 0.14958316087722778 0.14958316087722778 0.1503467857837677 0.037704020738601685 0.5565460324287415 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015929386940089438\n",
            "462 Data 0.0015373280824496256 PDE 5.524010703084059e-05 0.14958354830741882 0.14958354830741882 0.15034931898117065 0.03771498426795006 0.5565722584724426 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015925681894804662\n",
            "463 Data 0.0015370999339980112 PDE 5.509897891897708e-05 0.14958392083644867 0.14958392083644867 0.1503518521785736 0.03772588446736336 0.556598424911499 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015921989129169883\n",
            "464 Data 0.0015368726874965882 PDE 5.495807636179961e-05 0.14958429336547852 0.14958429336547852 0.15035438537597656 0.03773672506213188 0.5566244721412659 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015918307638583878\n",
            "465 Data 0.0015366464429564534 PDE 5.4817373893456534e-05 0.14958466589450836 0.14958466589450836 0.15035691857337952 0.03774750232696533 0.5566504597663879 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00159146381684991\n",
            "466 Data 0.001536421167424182 PDE 5.467689334182069e-05 0.1495850384235382 0.1495850384235382 0.15035945177078247 0.03775821998715401 0.5566763877868652 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015910980607660027\n",
            "467 Data 0.0015361968097577458 PDE 5.453664198284969e-05 0.14958541095256805 0.14958541095256805 0.15036198496818542 0.037768878042697906 0.5567022562026978 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015907334517405954\n",
            "468 Data 0.0015359734961794046 PDE 5.43966016266495e-05 0.1495857834815979 0.1495857834815979 0.15036450326442719 0.03777947276830673 0.5567280650138855 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001590370097806054\n",
            "469 Data 0.0015357510495611877 PDE 5.4256768635241315e-05 0.14958615601062775 0.14958615601062775 0.15036702156066895 0.03779000788927078 0.5567537546157837 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001590007818196429\n",
            "470 Data 0.001535529532127835 PDE 5.411718666437082e-05 0.1495865285396576 0.1495865285396576 0.1503695398569107 0.03780048340559006 0.5567793846130371 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015896467187922058\n",
            "471 Data 0.0015353090102883178 PDE 5.39777975063771e-05 0.14958690106868744 0.14958690106868744 0.15037205815315247 0.03781089931726456 0.5568049550056458 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001589286807794695\n",
            "472 Data 0.0015350893866031385 PDE 5.3838622989133e-05 0.1495872586965561 0.1495872586965561 0.15037457644939423 0.03782125189900398 0.5568304657936096 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015889280095922715\n",
            "473 Data 0.0015348706995183458 PDE 5.369964856072329e-05 0.14958761632442474 0.14958761632442474 0.150377094745636 0.03783154487609863 0.5568558573722839 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015885703480790691\n",
            "474 Data 0.0015346529422856608 PDE 5.3560943342745304e-05 0.1495879739522934 0.1495879739522934 0.15037961304187775 0.03784177824854851 0.5568811893463135 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015882138856284061\n",
            "475 Data 0.0015344360335689266 PDE 5.3422430937644094e-05 0.14958833158016205 0.14958833158016205 0.1503821164369583 0.03785195201635361 0.5569064617156982 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015878584645065707\n",
            "476 Data 0.0015342201228503167 PDE 5.3284144087228924e-05 0.1495886892080307 0.1495886892080307 0.15038461983203888 0.03786206617951393 0.5569316744804382 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015875042669375456\n",
            "477 Data 0.00153400505329538 PDE 5.3146082791499794e-05 0.14958904683589935 0.14958904683589935 0.15038712322711945 0.03787211701273918 0.5569568276405334 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015871511360868798\n",
            "478 Data 0.001533790971232248 PDE 5.30082470504567e-05 0.149589404463768 0.149589404463768 0.1503896266222 0.037882108241319656 0.5569818615913391 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015867992182827047\n",
            "479 Data 0.001533577693070498 PDE 5.287062231218442e-05 0.14958976209163666 0.14958976209163666 0.15039213001728058 0.037892039865255356 0.5570068359375 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015864483153826825\n",
            "480 Data 0.0015333653698691096 PDE 5.27331794728525e-05 0.14959010481834412 0.14959010481834412 0.15039463341236115 0.03790191188454628 0.5570317506790161 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001586098549341962\n",
            "481 Data 0.00153315390806688 PDE 5.259598765405826e-05 0.14959044754505157 0.14959044754505157 0.15039712190628052 0.03791172429919243 0.5570566058158875 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015857498957209382\n",
            "482 Data 0.001532943247525842 PDE 5.245905413175933e-05 0.14959079027175903 0.14959079027175903 0.1503996104001999 0.0379214771091938 0.557081401348114 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015854023016576013\n",
            "483 Data 0.001532733512484534 PDE 5.232230978435837e-05 0.1495911329984665 0.1495911329984665 0.15040209889411926 0.0379311703145504 0.5571061372756958 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015850558222688924\n",
            "484 Data 0.001532524704909341 PDE 5.218578007770702e-05 0.14959147572517395 0.14959147572517395 0.15040458738803864 0.03794080391526222 0.557130753993988 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001584710484987048\n",
            "485 Data 0.0015323166625645533 PDE 5.2049490477656946e-05 0.1495918184518814 0.1495918184518814 0.150407075881958 0.03795037791132927 0.5571553111076355 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015843661530422102\n",
            "486 Data 0.0015321095188972616 PDE 5.191341188037768e-05 0.14959216117858887 0.14959216117858887 0.1504095494747162 0.03795989230275154 0.5571798086166382 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015840229307776392\n",
            "487 Data 0.0015319032624183128 PDE 5.177754792384803e-05 0.14959250390529633 0.14959250390529633 0.15041202306747437 0.03796934708952904 0.5572042465209961 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015836808103421608\n",
            "488 Data 0.001531697939030188 PDE 5.164189133211039e-05 0.14959284663200378 0.14959284663200378 0.15041449666023254 0.03797874227166176 0.5572286248207092 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015833398303622985\n",
            "489 Data 0.0015314933497058787 PDE 5.150645301910117e-05 0.14959317445755005 0.14959317445755005 0.15041697025299072 0.037988077849149704 0.5572528839111328 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015829998027249799\n",
            "490 Data 0.0015312896093071257 PDE 5.1371251174714416e-05 0.1495935022830963 0.1495935022830963 0.1504194438457489 0.037997353821992874 0.5572770833969116 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015826608604818401\n",
            "491 Data 0.0015310867489940585 PDE 5.123627852299251e-05 0.14959383010864258 0.14959383010864258 0.15042191743850708 0.03800657019019127 0.5573012232780457 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001582323027517051\n",
            "492 Data 0.0015308847000745785 PDE 5.1101520512020215e-05 0.14959415793418884 0.14959415793418884 0.15042437613010406 0.03801572695374489 0.5573253035545349 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015819862205865988\n",
            "493 Data 0.001530683386598371 PDE 5.0966998969670385e-05 0.1495944857597351 0.1495944857597351 0.15042683482170105 0.03802482411265373 0.5573493242263794 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015816503855680415\n",
            "494 Data 0.001530482990596272 PDE 5.0832677516154945e-05 0.14959481358528137 0.14959481358528137 0.15042929351329803 0.0380338653922081 0.5573732852935791 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001581315668112427\n",
            "495 Data 0.001530283389074855 PDE 5.069858525530435e-05 0.14959514141082764 0.14959514141082764 0.15043175220489502 0.03804284706711769 0.5573971271514893 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015809819743301593\n",
            "496 Data 0.0015300846441176177 PDE 5.0564707635203376e-05 0.1495954692363739 0.1495954692363739 0.150434210896492 0.03805176913738251 0.5574209094047546 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001580649351752821\n",
            "497 Data 0.0015298866103879124 PDE 5.043104829383083e-05 0.14959579706192017 0.14959579706192017 0.1504366546869278 0.03806063160300255 0.5574446320533752 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015803176586817432\n",
            "498 Data 0.0015296894247007165 PDE 5.029761450714432e-05 0.14959610998630524 0.14959610998630524 0.1504390984773636 0.038069434463977814 0.5574682950973511 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015799870392078608\n",
            "499 Data 0.0015294930958520382 PDE 5.016441355110146e-05 0.1495964229106903 0.1495964229106903 0.15044154226779938 0.0380781814455986 0.5574918985366821 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015796575094031396\n",
            "500 Data 0.0015292974644065948 PDE 5.003141995985061e-05 0.14959673583507538 0.14959673583507538 0.15044398605823517 0.038086868822574615 0.5575154423713684 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015793288843664454\n",
            "501 Data 0.0015291026680779145 PDE 4.989864464732818e-05 0.14959704875946045 0.14959704875946045 0.15044642984867096 0.03809549659490585 0.5575388669967651 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015790013127252426\n",
            "502 Data 0.0015289086606523237 PDE 4.9766120355343446e-05 0.14959736168384552 0.14959736168384552 0.15044885873794556 0.038104068487882614 0.5575622320175171 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015786747810076671\n",
            "503 Data 0.0015287154475429198 PDE 4.963382161804475e-05 0.1495976746082306 0.1495976746082306 0.15045128762722015 0.0381125807762146 0.5575855374336243 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015783492691609645\n",
            "504 Data 0.00152852300043909 PDE 4.95017011417076e-05 0.14959798753261566 0.14959798753261566 0.15045371651649475 0.03812103345990181 0.5576087832450867 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015780247015807975\n",
            "505 Data 0.0015283312555259638 PDE 4.936982804792933e-05 0.14959830045700073 0.14959830045700073 0.15045614540576935 0.03812943026423454 0.5576319694519043 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015777010835738932\n",
            "506 Data 0.001528140357021521 PDE 4.923816959490068e-05 0.1495986133813858 0.1495986133813858 0.15045855939388275 0.0381377674639225 0.5576550960540771 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015773785266164216\n",
            "507 Data 0.0015279501650142379 PDE 4.9106722144642845e-05 0.14959891140460968 0.14959891140460968 0.15046097338199615 0.03814604505896568 0.5576781630516052 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015770568871588807\n",
            "508 Data 0.001527760796703532 PDE 4.897549297311343e-05 0.14959920942783356 0.14959920942783356 0.15046338737010956 0.03815426677465439 0.5577011108398438 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015767362896766455\n",
            "509 Data 0.001527572134861554 PDE 4.8844496632227674e-05 0.14959950745105743 0.14959950745105743 0.15046580135822296 0.03816242888569832 0.5577239990234375 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015764166314937817\n",
            "510 Data 0.0015273842886644865 PDE 4.871373675996438e-05 0.1495998054742813 0.1495998054742813 0.15046821534633636 0.03817053511738777 0.5577468276023865 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001576098025424451\n",
            "511 Data 0.001527197108192987 PDE 4.8583169700577855e-05 0.1496001034975052 0.1496001034975052 0.15047061443328857 0.03817858174443245 0.5577695965766907 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015757802778935649\n",
            "512 Data 0.0015270106747999029 PDE 4.8452824557898566e-05 0.14960040152072906 0.14960040152072906 0.15047301352024078 0.03818657249212265 0.5577923059463501 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015754634993578015\n",
            "513 Data 0.0015268249541269593 PDE 4.832273771171458e-05 0.14960069954395294 0.14960069954395294 0.150475412607193 0.038194503635168076 0.5578149557113647 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015751476918386739\n",
            "514 Data 0.001526640027611178 PDE 4.8192861868301407e-05 0.14960099756717682 0.14960099756717682 0.1504778116941452 0.038202378898859024 0.5578375458717346 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015748328894794794\n",
            "515 Data 0.0015264557938281635 PDE 4.8063193389680237e-05 0.1496012955904007 0.1496012955904007 0.15048019587993622 0.0382101945579052 0.5578600168228149 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015745189872178437\n",
            "516 Data 0.0015262722477641236 PDE 4.793375774170272e-05 0.14960157871246338 0.14960157871246338 0.15048258006572723 0.03821795433759689 0.5578824281692505 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015742060055058263\n",
            "517 Data 0.0015260894948342604 PDE 4.7804533096496016e-05 0.14960186183452606 0.14960186183452606 0.15048496425151825 0.038225654512643814 0.5579047799110413 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015738940279307564\n",
            "518 Data 0.001525907451005231 PDE 4.7675515816081315e-05 0.14960214495658875 0.14960214495658875 0.15048734843730927 0.03823329880833626 0.5579270720481873 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015735829668213123\n",
            "519 Data 0.0015257260894230402 PDE 4.754675683216192e-05 0.14960242807865143 0.14960242807865143 0.15048973262310028 0.038240887224674225 0.5579493045806885 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001573272846255202\n",
            "520 Data 0.0015255453709063025 PDE 4.741821248899214e-05 0.1496027112007141 0.1496027112007141 0.1504921019077301 0.038248416036367416 0.5579714775085449 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015729635833952947\n",
            "521 Data 0.0015253654057023572 PDE 4.728987914859317e-05 0.1496029943227768 0.1496029943227768 0.15049447119235992 0.03825588896870613 0.5579935908317566 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015726552848509504\n",
            "522 Data 0.001525186165971162 PDE 4.716177863883786e-05 0.14960327744483948 0.14960327744483948 0.15049684047698975 0.03826330602169037 0.5580156445503235 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015723479446099998\n",
            "523 Data 0.001525007566066245 PDE 4.703389640781097e-05 0.14960356056690216 0.14960356056690216 0.15049920976161957 0.03827066347002983 0.5580375790596008 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001572041462474056\n",
            "524 Data 0.001524829669024005 PDE 4.690624336944893e-05 0.14960384368896484 0.14960384368896484 0.1505015641450882 0.038277965039014816 0.5580594539642334 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015717359123934539\n",
            "525 Data 0.0015246524267297852 PDE 4.6778834075666964e-05 0.14960411190986633 0.14960411190986633 0.15050391852855682 0.038285210728645325 0.5580812692642212 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015714312608054522\n",
            "526 Data 0.0015244759230283078 PDE 4.665165033657104e-05 0.14960438013076782 0.14960438013076782 0.15050627291202545 0.038292400538921356 0.5581030249595642 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015711275733648789\n",
            "527 Data 0.0015243000402625783 PDE 4.652465941035189e-05 0.1496046483516693 0.1496046483516693 0.15050862729549408 0.03829953074455261 0.5581247210502625 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015708246996729302\n",
            "528 Data 0.00152412480014399 PDE 4.6397908590734005e-05 0.1496049165725708 0.1496049165725708 0.1505109667778015 0.03830660507082939 0.5581463575363159 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001570522708734724\n",
            "529 Data 0.0015239503109358886 PDE 4.627137604984455e-05 0.1496051847934723 0.1496051847934723 0.15051330626010895 0.038313623517751694 0.5581679344177246 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015702216869857332\n",
            "530 Data 0.0015237764241212018 PDE 4.614504723576829e-05 0.14960545301437378 0.14960545301437378 0.15051564574241638 0.03832058608531952 0.5581894516944885 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015699214713569701\n",
            "531 Data 0.0015236032859982714 PDE 4.6018983994144946e-05 0.14960572123527527 0.14960572123527527 0.15051798522472382 0.03832749277353287 0.5582109093666077 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015696222699924163\n",
            "532 Data 0.0015234307316083794 PDE 4.5893120841355994e-05 0.14960598945617676 0.14960598945617676 0.15052030980587006 0.03833434358239174 0.558232307434082 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015693238524497354\n",
            "533 Data 0.0015232588358502826 PDE 4.5767468691337854e-05 0.14960625767707825 0.14960625767707825 0.1505226343870163 0.03834113851189613 0.5582535862922668 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015690263045416205\n",
            "534 Data 0.0015230876040226388 PDE 4.56420675618574e-05 0.14960651099681854 0.14960651099681854 0.15052495896816254 0.03834787383675575 0.5582748055458069 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015687296715844962\n",
            "535 Data 0.001522916951076446 PDE 4.55168956250418e-05 0.14960676431655884 0.14960676431655884 0.15052728354930878 0.038354553282260895 0.5582959651947021 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015684338467014877\n",
            "536 Data 0.0015227470012191017 PDE 4.539192741503939e-05 0.14960701763629913 0.14960701763629913 0.15052959322929382 0.03836117684841156 0.5583170652389526 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015681389286341411\n",
            "537 Data 0.0015225777087089244 PDE 4.526719931163825e-05 0.14960727095603943 0.14960727095603943 0.15053190290927887 0.03836774453520775 0.5583381056785583 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015678449080205627\n",
            "538 Data 0.0015224089383832592 PDE 4.514268948696554e-05 0.14960752427577972 0.14960752427577972 0.15053421258926392 0.03837425634264946 0.5583590865135193 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015675516278702247\n",
            "539 Data 0.0015222409208795467 PDE 4.501840885495767e-05 0.14960777759552002 0.14960777759552002 0.15053652226924896 0.038380712270736694 0.5583800077438354 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015672593297345043\n",
            "540 Data 0.0015220734691483728 PDE 4.489436105359346e-05 0.14960803091526031 0.14960803091526031 0.15053881704807281 0.03838711231946945 0.5584008693695068 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015669678302019663\n",
            "541 Data 0.0015219066787139288 PDE 4.477052789297886e-05 0.1496082842350006 0.1496082842350006 0.15054111182689667 0.03839346021413803 0.5584216713905334 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015666772066069076\n",
            "542 Data 0.0015217404360805746 PDE 4.464692756300792e-05 0.1496085375547409 0.1496085375547409 0.15054340660572052 0.03839975222945213 0.5584424138069153 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015663873636435825\n",
            "543 Data 0.0015215748555025925 PDE 4.452355642570183e-05 0.1496087908744812 0.1496087908744812 0.15054570138454437 0.03840598836541176 0.5584630966186523 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015660984119282944\n",
            "544 Data 0.0015214097600997364 PDE 4.440039265318774e-05 0.1496090292930603 0.1496090292930603 0.15054798126220703 0.03841216862201691 0.5584837198257446 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015658101527529242\n",
            "545 Data 0.0015212454243279483 PDE 4.427745079738088e-05 0.1496092677116394 0.1496092677116394 0.1505502611398697 0.03841829299926758 0.5585042238235474 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015655228751253292\n",
            "546 Data 0.0015210816971512942 PDE 4.4154741772217676e-05 0.1496095061302185 0.1496095061302185 0.15055254101753235 0.03842436149716377 0.5585246682167053 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001565236438923512\n",
            "547 Data 0.001520918477550824 PDE 4.4032265577698126e-05 0.1496097445487976 0.1496097445487976 0.150554820895195 0.03843037411570549 0.5585450530052185 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001564950743128522\n",
            "548 Data 0.0015207558965289803 PDE 4.390998947201297e-05 0.1496099829673767 0.1496099829673767 0.15055708587169647 0.03843633458018303 0.5585653781890869 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015646658860009933\n",
            "549 Data 0.0015205938654142789 PDE 4.3787968024844304e-05 0.1496102213859558 0.1496102213859558 0.15055935084819794 0.03844223916530609 0.5585856437683105 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015643818334391232\n",
            "550 Data 0.0015204324578632774 PDE 4.366615758044645e-05 0.1496104598045349 0.1496104598045349 0.1505616158246994 0.038448087871074677 0.5586058497428894 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015640986154437239\n",
            "551 Data 0.0015202716990690048 PDE 4.354457269073464e-05 0.149610698223114 0.149610698223114 0.15056388080120087 0.038453880697488785 0.5586259961128235 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015638162717597395\n",
            "552 Data 0.0015201113671009332 PDE 4.34232315456029e-05 0.14961093664169312 0.14961093664169312 0.15056613087654114 0.038459621369838715 0.5586460828781128 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001563534598646536\n",
            "553 Data 0.0015199517016486809 PDE 4.330208685132675e-05 0.14961116015911102 0.14961116015911102 0.1505683809518814 0.03846530616283417 0.5586661100387573 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015632537885000076\n",
            "554 Data 0.0015197925709020989 PDE 4.318119681556709e-05 0.14961138367652893 0.14961138367652893 0.15057063102722168 0.03847093507647514 0.5586860775947571 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001562973767717666\n",
            "555 Data 0.0015196341039641443 PDE 4.306053233449347e-05 0.14961160719394684 0.14961160719394684 0.15057286620140076 0.03847651183605194 0.5587059855461121 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015626946362986378\n",
            "556 Data 0.0015194760926038498 PDE 4.2940082494169474e-05 0.14961183071136475 0.14961183071136475 0.15057510137557983 0.03848203271627426 0.5587258338928223 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015624161750980193\n",
            "557 Data 0.00151931863925205 PDE 4.281987276044674e-05 0.14961205422878265 0.14961205422878265 0.1505773365497589 0.038487501442432404 0.5587456226348877 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015621385120124967\n",
            "558 Data 0.0015191617466181464 PDE 4.269988858141005e-05 0.14961227774620056 0.14961227774620056 0.150579571723938 0.03849291428923607 0.5587653517723083 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015618616351995565\n",
            "559 Data 0.0015190055325058826 PDE 4.258012631908059e-05 0.14961250126361847 0.14961250126361847 0.15058179199695587 0.038498274981975555 0.5587850213050842 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015615856588249632\n",
            "560 Data 0.0015188496819204067 PDE 4.2460585973458365e-05 0.14961272478103638 0.14961272478103638 0.15058401226997375 0.038503579795360565 0.5588046312332153 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001561310267893865\n",
            "561 Data 0.0015186945064788466 PDE 4.2341300286352634e-05 0.14961294829845428 0.14961294829845428 0.15058623254299164 0.038508832454681396 0.5588241815567017 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015610358067651992\n",
            "562 Data 0.0015185398112150035 PDE 4.22222183260601e-05 0.1496131718158722 0.1496131718158722 0.15058845281600952 0.03851402923464775 0.5588436722755432 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015607620295410636\n",
            "563 Data 0.0015183856698039084 PDE 4.2103380110347643e-05 0.1496133804321289 0.1496133804321289 0.1505906581878662 0.03851917386054993 0.55886310338974 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001560489049914256\n",
            "564 Data 0.0015182320281617659 PDE 4.1984756535384804e-05 0.14961358904838562 0.14961358904838562 0.1505928635597229 0.038524262607097626 0.558882474899292 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015602167846971507\n",
            "565 Data 0.0015180789800236147 PDE 4.1866369429044425e-05 0.14961379766464233 0.14961379766464233 0.1505950689315796 0.038529299199581146 0.5589017868041992 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015599453494526592\n",
            "566 Data 0.0015179264755745242 PDE 4.174819332547486e-05 0.14961400628089905 0.14961400628089905 0.15059725940227509 0.03853427991271019 0.5589210391044617 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001559674668899999\n",
            "567 Data 0.00151777448938312 PDE 4.163023186265491e-05 0.14961421489715576 0.14961421489715576 0.15059944987297058 0.038539208471775055 0.5589402318000793 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015594047212457748\n",
            "568 Data 0.0015176228872646957 PDE 4.1512528696330264e-05 0.14961442351341248 0.14961442351341248 0.15060164034366608 0.03854408487677574 0.5589593648910522 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001559135415961026\n",
            "569 Data 0.0015174720304201184 PDE 4.139504744671285e-05 0.1496146321296692 0.1496146321296692 0.15060383081436157 0.03854890540242195 0.5589783787727356 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015588670778668313\n",
            "570 Data 0.0015173215595094861 PDE 4.127778083784506e-05 0.1496148407459259 0.1496148407459259 0.15060600638389587 0.03855367377400398 0.5589973330497742 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015585993403473312\n",
            "571 Data 0.0015171716005976277 PDE 4.116077980143018e-05 0.14961504936218262 0.14961504936218262 0.15060818195343018 0.038558389991521835 0.559016227722168 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001558332380399058\n",
            "572 Data 0.001517022153987679 PDE 4.104396793991327e-05 0.14961524307727814 0.14961524307727814 0.15061035752296448 0.03856305032968521 0.559035062789917 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015580661219275922\n",
            "573 Data 0.0015168732338556269 PDE 4.092739618499763e-05 0.14961543679237366 0.14961543679237366 0.15061253309249878 0.03856765851378441 0.5590538382530212 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015578006300406245\n",
            "574 Data 0.001516724911527898 PDE 4.0811031794874e-05 0.14961563050746918 0.14961563050746918 0.1506146937608719 0.03857221454381943 0.5590725541114807 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001557535943322772\n",
            "575 Data 0.0015165769623368705 PDE 4.069492206326686e-05 0.1496158242225647 0.1496158242225647 0.150616854429245 0.03857671841979027 0.5590912103652954 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015572718844001374\n",
            "576 Data 0.0015164294699479698 PDE 4.057903424836695e-05 0.14961601793766022 0.14961601793766022 0.1506190150976181 0.03858117014169693 0.5591098070144653 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015570085041963367\n",
            "577 Data 0.0015162825962359073 PDE 4.0463371988153085e-05 0.14961621165275574 0.14961621165275574 0.15062116086483002 0.03858556970953941 0.5591283440589905 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015567459682240604\n",
            "578 Data 0.0015161361051628153 PDE 4.0347953472519293e-05 0.14961640536785126 0.14961640536785126 0.15062330663204193 0.03858991339802742 0.5591468214988708 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015564840586353346\n",
            "579 Data 0.0015159901775097602 PDE 4.0232756873592734e-05 0.14961659908294678 0.14961659908294678 0.15062545239925385 0.03859420493245125 0.5591652393341064 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001556222934383353\n",
            "580 Data 0.001515844728138408 PDE 4.0117771277436987e-05 0.1496167927980423 0.1496167927980423 0.15062759816646576 0.0385984443128109 0.5591835975646973 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001555962499415845\n",
            "581 Data 0.0015156997094837165 PDE 4.0003003960009664e-05 0.14961698651313782 0.14961698651313782 0.15062972903251648 0.03860263153910637 0.5592018961906433 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015557027134437261\n",
            "582 Data 0.0015155551810933738 PDE 3.9888494939077646e-05 0.14961716532707214 0.14961716532707214 0.1506318598985672 0.03860676661133766 0.5592201352119446 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015554436760324515\n",
            "583 Data 0.0015154111498038676 PDE 3.977419692091644e-05 0.14961734414100647 0.14961734414100647 0.15063399076461792 0.038610849529504776 0.5592383146286011 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001555185346724784\n",
            "584 Data 0.001515267607567579 PDE 3.9660142647335306e-05 0.1496175229549408 0.1496175229549408 0.15063610672950745 0.03861488029360771 0.5592564344406128 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015549277502149143\n",
            "585 Data 0.0015151244651826095 PDE 3.9546288462588564e-05 0.14961770176887512 0.14961770176887512 0.15063822269439697 0.03861885890364647 0.5592745542526245 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001554670753645198\n",
            "586 Data 0.001514981890656043 PDE 3.9432681660400704e-05 0.14961788058280945 0.14961788058280945 0.1506403386592865 0.03862278535962105 0.5592926144599915 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015544145723164436\n",
            "587 Data 0.001514839684790475 PDE 3.9319322240771726e-05 0.14961805939674377 0.14961805939674377 0.15064245462417603 0.03862665966153145 0.5593106150627136 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015541590070312467\n",
            "588 Data 0.001514697947787478 PDE 3.9206181099871174e-05 0.1496182382106781 0.1496182382106781 0.15064455568790436 0.03863048180937767 0.559328556060791 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015539041288873492\n",
            "589 Data 0.001514556725035369 PDE 3.9093261875677854e-05 0.14961841702461243 0.14961841702461243 0.1506466567516327 0.03863425552845001 0.5593464374542236 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015536499869110469\n",
            "590 Data 0.0015144159648960676 PDE 3.8980579120106995e-05 0.14961859583854675 0.14961859583854675 0.15064875781536102 0.038637977093458176 0.5593642592430115 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015533965440161746\n",
            "591 Data 0.001514275619947294 PDE 3.886812919517979e-05 0.14961877465248108 0.14961877465248108 0.15065084397792816 0.03864164650440216 0.5593820214271545 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015531437491424739\n",
            "592 Data 0.0015141357509116603 PDE 3.875590118695982e-05 0.1496189385652542 0.1496189385652542 0.1506529301404953 0.03864526376128197 0.5593997240066528 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015528916520986201\n",
            "593 Data 0.0015139962528124115 PDE 3.864390237140469e-05 0.14961910247802734 0.14961910247802734 0.15065501630306244 0.038648828864097595 0.5594173669815063 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015526401551838162\n",
            "594 Data 0.00151385731896493 PDE 3.85321254725568e-05 0.14961926639080048 0.14961926639080048 0.15065710246562958 0.03865234553813934 0.5594349503517151 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015523894444374868\n",
            "595 Data 0.0015137186912440753 PDE 3.8420588680310175e-05 0.1496194303035736 0.1496194303035736 0.15065917372703552 0.03865581005811691 0.559452474117279 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015521392799243854\n",
            "596 Data 0.001513580533955573 PDE 3.830927744274959e-05 0.14961959421634674 0.14961959421634674 0.15066124498844147 0.038659222424030304 0.5594699382781982 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015518898113983225\n",
            "597 Data 0.0015134429458359753 PDE 3.819818812189624e-05 0.14961975812911987 0.14961975812911987 0.1506633162498474 0.03866258263587952 0.5594873428344727 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015516411339578716\n",
            "598 Data 0.0015133055682315733 PDE 3.808734254562296e-05 0.149619922041893 0.149619922041893 0.15066537261009216 0.03866589441895485 0.5595046877861023 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015513929107771962\n",
            "599 Data 0.001513168784350833 PDE 3.797670069616288e-05 0.14962008595466614 0.14962008595466614 0.15066742897033691 0.038669154047966 0.5595219731330872 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001551145485046996\n",
            "600 Data 0.001513032335325527 PDE 3.786629531532526e-05 0.14962024986743927 0.14962024986743927 0.15066948533058167 0.03867236524820328 0.5595391988754272 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015508986306408522\n",
            "601 Data 0.0015128963930139381 PDE 3.7756122765131295e-05 0.1496204137802124 0.1496204137802124 0.15067152678966522 0.03867552429437637 0.5595563650131226 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015506525157790694\n",
            "602 Data 0.001512760809811549 PDE 3.764618304558098e-05 0.14962057769298553 0.14962057769298553 0.15067356824874878 0.03867863118648529 0.5595734715461731 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00155040699285713\n",
            "603 Data 0.0015126256797735038 PDE 3.753646888071671e-05 0.14962072670459747 0.14962072670459747 0.15067560970783234 0.03868168964982033 0.5595905184745789 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015501621486542206\n",
            "604 Data 0.0015124909547420528 PDE 3.742698754649609e-05 0.1496208757162094 0.1496208757162094 0.1506776511669159 0.038684695959091187 0.5596075057983398 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015499179422885489\n",
            "605 Data 0.001512356657390944 PDE 3.7317731766961515e-05 0.14962102472782135 0.14962102472782135 0.15067967772483826 0.038687653839588165 0.559624433517456 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015496743891579056\n",
            "606 Data 0.001512222669422201 PDE 3.720870881807059e-05 0.1496211737394333 0.1496211737394333 0.15068170428276062 0.038690559566020966 0.5596413016319275 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015494313782402716\n",
            "607 Data 0.001512089214245978 PDE 3.709991506184451e-05 0.14962132275104523 0.14962132275104523 0.15068373084068298 0.038693416863679886 0.5596581101417542 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015491891293078226\n",
            "608 Data 0.0015119561385573263 PDE 3.69913614122197e-05 0.14962147176265717 0.14962147176265717 0.15068574249744415 0.03869622200727463 0.559674859046936 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001548947499969546\n",
            "609 Data 0.0015118233771051622 PDE 3.688302604132332e-05 0.1496216207742691 0.1496216207742691 0.15068775415420532 0.03869897872209549 0.5596916079521179 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015487064031464855\n",
            "610 Data 0.0015116911082500384 PDE 3.677491986309178e-05 0.14962176978588104 0.14962176978588104 0.1506897658109665 0.03870168700814247 0.559708297252655 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015484660281131302\n",
            "611 Data 0.0015115592206076567 PDE 3.666702832560986e-05 0.14962191879749298 0.14962191879749298 0.15069177746772766 0.038704343140125275 0.5597249269485474 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015482262489332666\n",
            "612 Data 0.0015114276815897277 PDE 3.655937689472921e-05 0.14962206780910492 0.14962206780910492 0.15069377422332764 0.0387069508433342 0.5597414970397949 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015479870584844569\n",
            "613 Data 0.0015112966415537221 PDE 3.6451932828640565e-05 0.14962220191955566 0.14962220191955566 0.1506957709789276 0.03870951011776924 0.5597580075263977 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015477485743823627\n",
            "614 Data 0.0015111658528769935 PDE 3.634475069702603e-05 0.1496223360300064 0.1496223360300064 0.1506977677345276 0.038712017238140106 0.5597744584083557 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015475106035740195\n",
            "615 Data 0.001511035569519186 PDE 3.6237765016267076e-05 0.14962247014045715 0.14962247014045715 0.15069974958896637 0.03871447592973709 0.559790849685669 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001547273334535453\n",
            "616 Data 0.0015109055850047622 PDE 3.613101944210939e-05 0.1496226042509079 0.1496226042509079 0.15070173144340515 0.038716886192560196 0.5598071813583374 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015470366044468716\n",
            "617 Data 0.0015107760839111928 PDE 3.6024499422637746e-05 0.14962273836135864 0.14962273836135864 0.15070371329784393 0.03871924802660942 0.5598234534263611 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015468005833338305\n",
            "618 Data 0.0015106468353223738 PDE 3.591820495785214e-05 0.1496228724718094 0.1496228724718094 0.15070568025112152 0.03872155770659447 0.55983966588974 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001546565040280226\n",
            "619 Data 0.0015105180772306591 PDE 3.581215423764661e-05 0.14962300658226013 0.14962300658226013 0.1507076472043991 0.038723818957805634 0.5598558187484741 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015463302314683057\n",
            "620 Data 0.0015103895947772567 PDE 3.570633634808473e-05 0.14962314069271088 0.14962314069271088 0.1507096141576767 0.03872603178024292 0.5598719120025635 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015460959311253415\n",
            "621 Data 0.001510261541611653 PDE 3.5600744013208896e-05 0.14962327480316162 0.14962327480316162 0.15071158111095428 0.038728196173906326 0.5598880052566528 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001545862285624862\n",
            "622 Data 0.0015101338021389155 PDE 3.549537359504029e-05 0.14962340891361237 0.14962340891361237 0.15071353316307068 0.03873031213879585 0.5599040389060974 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015456291757339558\n",
            "623 Data 0.0015100064922662 PDE 3.539023600751534e-05 0.1496235430240631 0.1496235430240631 0.15071548521518707 0.0387323796749115 0.5599200129508972 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015453967282737153\n",
            "624 Data 0.0015098795483618251 PDE 3.52853094227612e-05 0.14962366223335266 0.14962366223335266 0.15071743726730347 0.038734398782253265 0.5599359273910522 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015451648577845863\n",
            "625 Data 0.0015097529672712642 PDE 3.518061203067191e-05 0.1496237814426422 0.1496237814426422 0.15071937441825867 0.03873636946082115 0.5599517822265625 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015449335793019361\n",
            "626 Data 0.001509626656355195 PDE 3.5076169297099113e-05 0.14962390065193176 0.14962390065193176 0.15072131156921387 0.03873829171061516 0.559967577457428 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015447028256522942\n",
            "627 Data 0.0015095007990296506 PDE 3.497195211821236e-05 0.1496240198612213 0.1496240198612213 0.15072324872016907 0.038740165531635284 0.5599833130836487 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001544472751147863\n",
            "628 Data 0.0015093752784168623 PDE 3.486796049401164e-05 0.14962413907051086 0.14962413907051086 0.15072517096996307 0.03874199092388153 0.5599989891052246 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001544243238910874\n",
            "629 Data 0.0015092501238779786 PDE 3.476416532066651e-05 0.14962425827980042 0.14962425827980042 0.15072709321975708 0.0387437678873539 0.5600146055221558 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001544014289198645\n",
            "630 Data 0.0015091252838538593 PDE 3.466062480583787e-05 0.14962437748908997 0.14962437748908997 0.1507290154695511 0.03874549642205238 0.5600301623344421 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015437859086596972\n",
            "631 Data 0.0015090008461634255 PDE 3.4557306207716465e-05 0.14962449669837952 0.14962449669837952 0.1507309377193451 0.03874717652797699 0.5600457191467285 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001543558152371142\n",
            "632 Data 0.0015088767197097696 PDE 3.44542131642811e-05 0.14962461590766907 0.14962461590766907 0.1507328450679779 0.038748808205127716 0.5600612163543701 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015433309328740507\n",
            "633 Data 0.0015087529264233594 PDE 3.435134931351058e-05 0.14962473511695862 0.14962473511695862 0.15073475241661072 0.03875039145350456 0.5600766539573669 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00154310427573687\n",
            "634 Data 0.0015086295066495197 PDE 3.4248707379447296e-05 0.14962483942508698 0.14962483942508698 0.15073665976524353 0.03875192999839783 0.560092031955719 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001542878214028967\n",
            "635 Data 0.0015085063665686953 PDE 3.4146309189964086e-05 0.14962494373321533 0.14962494373321533 0.15073855221271515 0.03875342011451721 0.5601073503494263 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015426526757586594\n",
            "636 Data 0.0015083836426348519 PDE 3.4044136555166915e-05 0.1496250480413437 0.1496250480413437 0.15074044466018677 0.03875486180186272 0.5601226091384888 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015424277791900188\n",
            "637 Data 0.0015082612597757364 PDE 3.394217128516175e-05 0.14962515234947205 0.14962515234947205 0.1507423371076584 0.03875625506043434 0.5601378083229065 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015422034310608982\n",
            "638 Data 0.001508139165597363 PDE 3.3840453397715464e-05 0.1496252566576004 0.1496252566576004 0.1507442146539688 0.038757603615522385 0.5601529479026794 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015419796189950785\n",
            "639 Data 0.001508017411516312 PDE 3.373897561687045e-05 0.14962536096572876 0.14962536096572876 0.15074609220027924 0.03875890374183655 0.5601680874824524 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015417563871331825\n",
            "640 Data 0.0015078959300933918 PDE 3.3637719752732664e-05 0.14962546527385712 0.14962546527385712 0.15074796974658966 0.03876015543937683 0.5601831674575806 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015415336498461245\n",
            "641 Data 0.0015077748609543817 PDE 3.3536671253386885e-05 0.14962556958198547 0.14962556958198547 0.15074984729290009 0.03876136243343353 0.560198187828064 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015413115322077686\n",
            "642 Data 0.0015076540754232514 PDE 3.343587013659999e-05 0.14962567389011383 0.14962567389011383 0.15075170993804932 0.038762520998716354 0.5602131485939026 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015410899455598514\n",
            "643 Data 0.0015075336054515573 PDE 3.333528366056271e-05 0.1496257781982422 0.1496257781982422 0.15075357258319855 0.038763631135225296 0.5602280497550964 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00154086888911212\n",
            "644 Data 0.0015074135343130844 PDE 3.323493365314789e-05 0.14962588250637054 0.14962588250637054 0.15075543522834778 0.038764696568250656 0.5602428913116455 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015406484679662323\n",
            "645 Data 0.0015072936675261561 PDE 3.313480920041911e-05 0.1496259719133377 0.1496259719133377 0.15075728297233582 0.038765713572502136 0.5602576732635498 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015404284767265753\n",
            "646 Data 0.0015071742304281117 PDE 3.303491394035518e-05 0.14962606132030487 0.14962606132030487 0.15075913071632385 0.038766685873270035 0.5602723956108093 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001540209144368467\n",
            "647 Data 0.0015070550124809347 PDE 3.2935236959019676e-05 0.14962615072727203 0.14962615072727203 0.1507609784603119 0.03876760974526405 0.5602871179580688 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015399902494399543\n",
            "648 Data 0.0015069361318787145 PDE 3.283580008428544e-05 0.1496262401342392 0.1496262401342392 0.15076281130313873 0.03876848891377449 0.5603017807006836 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001539771931963\n",
            "649 Data 0.001506817570991074 PDE 3.273657421232201e-05 0.14962632954120636 0.14962632954120636 0.15076464414596558 0.038769323378801346 0.5603163838386536 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001539554145203396\n",
            "650 Data 0.0015066993254992927 PDE 3.263757753302343e-05 0.14962641894817352 0.14962641894817352 0.15076647698879242 0.03877010941505432 0.5603309273719788 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015393369030323161\n",
            "651 Data 0.001506581399609131 PDE 3.253879549447447e-05 0.14962650835514069 0.14962650835514069 0.15076830983161926 0.038770850747823715 0.5603454113006592 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015391201951036055\n",
            "652 Data 0.00150646374889902 PDE 3.244024628656916e-05 0.14962659776210785 0.14962659776210785 0.1507701277732849 0.03877154365181923 0.5603598356246948 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015389039951855892\n",
            "653 Data 0.0015063463976996113 PDE 3.234194082324393e-05 0.149626687169075 0.149626687169075 0.15077194571495056 0.03877219185233116 0.5603742599487305 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015386883385228552\n",
            "654 Data 0.001506229446305009 PDE 3.224385000066832e-05 0.14962677657604218 0.14962677657604218 0.1507737636566162 0.03877279534935951 0.5603886246681213 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015384732963056773\n",
            "655 Data 0.0015061126949765698 PDE 3.214599200873636e-05 0.14962686598300934 0.14962686598300934 0.15077556669712067 0.03877335041761398 0.5604029297828674 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015382586869853061\n",
            "656 Data 0.0015059961718640377 PDE 3.204837776138447e-05 0.1496269553899765 0.1496269553899765 0.15077736973762512 0.03877386078238487 0.5604171752929688 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015380445496254221\n",
            "657 Data 0.0015058800690685298 PDE 3.195097087882459e-05 0.14962702989578247 0.14962702989578247 0.15077917277812958 0.03877432644367218 0.5604313611984253 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015378310399473544\n",
            "658 Data 0.001505764242024577 PDE 3.1853771361056715e-05 0.14962710440158844 0.14962710440158844 0.15078096091747284 0.038774747401475906 0.5604454874992371 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015376180133856337\n",
            "659 Data 0.0015056486986949544 PDE 3.1756815587868914e-05 0.1496271789073944 0.1496271789073944 0.1507827490568161 0.03877512365579605 0.5604596138000488 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015374055142828233\n",
            "660 Data 0.0015055333753031088 PDE 3.166007809340954e-05 0.14962725341320038 0.14962725341320038 0.15078453719615936 0.038775451481342316 0.5604736804962158 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015371934533965183\n",
            "661 Data 0.001505418417812913 PDE 3.15635661536362e-05 0.14962732791900635 0.14962732791900635 0.15078632533550262 0.038775734603405 0.560487687587738 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015369819839665493\n",
            "662 Data 0.0015053037563575221 PDE 3.1467279768548906e-05 0.14962740242481232 0.14962740242481232 0.1507880985736847 0.0387759730219841 0.5605016350746155 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001536771036126071\n",
            "663 Data 0.0015051894225098295 PDE 3.137120802421123e-05 0.1496274769306183 0.1496274769306183 0.15078987181186676 0.03877616673707962 0.5605155229568481 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015365606305340407\n",
            "664 Data 0.0015050752725133702 PDE 3.1275369110517204e-05 0.14962755143642426 0.14962755143642426 0.15079164505004883 0.03877631574869156 0.560529351234436 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015363506416238874\n",
            "665 Data 0.0015049614116941772 PDE 3.117976666544564e-05 0.14962762594223022 0.14962762594223022 0.1507934033870697 0.038776420056819916 0.5605431795120239 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015361411783596228\n",
            "666 Data 0.0015048478659052582 PDE 3.108436430920847e-05 0.1496277004480362 0.1496277004480362 0.15079516172409058 0.03877647966146469 0.560556948184967 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015359322302144667\n",
            "667 Data 0.001504734621778579 PDE 3.0989212973508984e-05 0.14962777495384216 0.14962777495384216 0.15079692006111145 0.038776494562625885 0.5605706572532654 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015357238347520879\n",
            "668 Data 0.0015046216468880727 PDE 3.089424717472866e-05 0.14962783455848694 0.14962783455848694 0.15079867839813232 0.0387764647603035 0.560584306716919 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015355158940628013\n",
            "669 Data 0.0015045088915191917 PDE 3.079954694840126e-05 0.1496278941631317 0.1496278941631317 0.150800421833992 0.03877639025449753 0.5605978965759277 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001535308438467593\n",
            "670 Data 0.001504396453122228 PDE 3.0705043172929436e-05 0.1496279537677765 0.1496279537677765 0.15080216526985168 0.03877627104520798 0.5606114864349365 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015351014962951574\n",
            "671 Data 0.0015042842888932627 PDE 3.061078314203769e-05 0.14962801337242126 0.14962801337242126 0.15080390870571136 0.038776107132434845 0.5606250166893005 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015348950720353003\n",
            "672 Data 0.0015041723630409135 PDE 3.0516743208863772e-05 0.14962807297706604 0.14962807297706604 0.15080563724040985 0.03877589851617813 0.5606384873390198 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015346891062497773\n",
            "673 Data 0.001504060670402277 PDE 3.042292155441828e-05 0.14962813258171082 0.14962813258171082 0.15080736577510834 0.038775648921728134 0.5606518983840942 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015344835919566954\n",
            "674 Data 0.001503949357083827 PDE 3.0329307264764793e-05 0.1496281921863556 0.1496281921863556 0.15080909430980682 0.038775354623794556 0.5606652498245239 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015342786643485918\n",
            "675 Data 0.0015038382747874723 PDE 3.023593126272317e-05 0.14962825179100037 0.14962825179100037 0.15081080794334412 0.038775015622377396 0.5606786012649536 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015340742060501954\n",
            "676 Data 0.0015037273872037353 PDE 3.0142789910314605e-05 0.14962831139564514 0.14962831139564514 0.1508125215768814 0.038774631917476654 0.5606918931007385 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015338701771140499\n",
            "677 Data 0.001503616825508843 PDE 3.0049846827751026e-05 0.14962837100028992 0.14962837100028992 0.1508142352104187 0.03877420350909233 0.5607051253318787 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001533666672336594\n",
            "678 Data 0.0015035065397296856 PDE 2.9957163860672154e-05 0.1496284306049347 0.1496284306049347 0.150815948843956 0.038773734122514725 0.560718297958374 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015334637035903577\n",
            "679 Data 0.0015033964392480062 PDE 2.9864686439395882e-05 0.14962849020957947 0.14962849020957947 0.1508176475763321 0.03877322003245354 0.5607314109802246 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001533261125687402\n",
            "680 Data 0.0015032866579888346 PDE 2.977241456392221e-05 0.14962853491306305 0.14962853491306305 0.1508193463087082 0.03877266123890877 0.5607445240020752 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015330590725527568\n",
            "681 Data 0.0015031770882188117 PDE 2.968038825201802e-05 0.14962857961654663 0.14962857961654663 0.1508210450410843 0.038772061467170715 0.560757577419281 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015328574764708297\n",
            "682 Data 0.0015030677681316428 PDE 2.9588580218842253e-05 0.1496286243200302 0.1496286243200302 0.1508227288722992 0.03877141699194908 0.560770571231842 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001532656348350485\n",
            "683 Data 0.0015029586948848309 PDE 2.949697955045849e-05 0.1496286690235138 0.1496286690235138 0.1508244127035141 0.038770731538534164 0.5607835054397583 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015324556744352894\n",
            "684 Data 0.0015028499175601638 PDE 2.9405606255750172e-05 0.14962871372699738 0.14962871372699738 0.150826096534729 0.038770001381635666 0.5607963800430298 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001532255523815914\n",
            "685 Data 0.0015027413721900302 PDE 2.9314471248653717e-05 0.14962875843048096 0.14962875843048096 0.15082776546478271 0.038769226521253586 0.5608092546463013 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001532055843438684\n",
            "686 Data 0.0015026330432075154 PDE 2.9223541787359864e-05 0.14962880313396454 0.14962880313396454 0.15082943439483643 0.03876841068267822 0.560822069644928 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015318565849948753\n",
            "687 Data 0.0015025249407026931 PDE 2.913283788075205e-05 0.14962884783744812 0.14962884783744812 0.15083110332489014 0.03876755014061928 0.5608348250389099 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015316577785834452\n",
            "688 Data 0.0015024171864260387 PDE 2.904234133893624e-05 0.1496288925409317 0.1496288925409317 0.15083277225494385 0.03876664862036705 0.5608475208282471 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001531459527764975\n",
            "689 Data 0.00150230961859065 PDE 2.8952083084732294e-05 0.14962893724441528 0.14962893724441528 0.15083442628383636 0.03876570612192154 0.5608602166175842 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015312617016753822\n",
            "690 Data 0.0015022023193618534 PDE 2.886204129026737e-05 0.14962898194789886 0.14962898194789886 0.15083608031272888 0.03876471891999245 0.5608728528022766 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015310643606521207\n",
            "691 Data 0.0015020952109063516 PDE 2.8772214136552066e-05 0.14962902665138245 0.14962902665138245 0.1508377343416214 0.03876369073987007 0.5608854293823242 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015308674250429036\n",
            "692 Data 0.0015019883819630208 PDE 2.8682605261565186e-05 0.14962905645370483 0.14962905645370483 0.15083937346935272 0.038762617856264114 0.560897946357727 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001530670987224586\n",
            "693 Data 0.0015018817763348634 PDE 2.8593236493179575e-05 0.14962908625602722 0.14962908625602722 0.15084101259708405 0.038761503994464874 0.5609104037284851 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001530475012828043\n",
            "694 Data 0.0015017754144950228 PDE 2.8504064175649546e-05 0.1496291160583496 0.1496291160583496 0.15084265172481537 0.03876034915447235 0.5609228610992432 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015302794786706724\n",
            "695 Data 0.0015016692495314515 PDE 2.8415135602699593e-05 0.149629145860672 0.149629145860672 0.1508442908525467 0.038759153336286545 0.5609352588653564 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001530084385134151\n",
            "696 Data 0.001501563336885292 PDE 2.8326408937573433e-05 0.14962917566299438 0.14962917566299438 0.15084591507911682 0.03875791281461716 0.560947597026825 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015298897458228655\n",
            "697 Data 0.001501457687924241 PDE 2.8237898732186295e-05 0.14962920546531677 0.14962920546531677 0.15084753930568695 0.038756631314754486 0.5609598755836487 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015296955866564274\n",
            "698 Data 0.0015013522809756601 PDE 2.8149628633400425e-05 0.14962923526763916 0.14962923526763916 0.15084916353225708 0.03875530883669853 0.5609721541404724 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015295019096090606\n",
            "699 Data 0.0015012470314358672 PDE 2.806154043355491e-05 0.14962926506996155 0.14962926506996155 0.15085077285766602 0.038753945380449295 0.5609843730926514 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001529308571869422\n",
            "700 Data 0.0015011420868876648 PDE 2.7973688702331856e-05 0.14962929487228394 0.14962929487228394 0.15085238218307495 0.038752540946006775 0.5609965324401855 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015291157755899967\n",
            "701 Data 0.00150103731156505 PDE 2.7886067982763052e-05 0.14962932467460632 0.14962932467460632 0.1508539915084839 0.03875109180808067 0.561008632183075 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001528923379547813\n",
            "702 Data 0.0015009327723837158 PDE 2.779865280899685e-05 0.1496293544769287 0.1496293544769287 0.15085560083389282 0.03874960169196129 0.5610207319259644 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015287314251927126\n",
            "703 Data 0.0015008284198895017 PDE 2.7711454094969667e-05 0.1496293842792511 0.1496293842792511 0.15085719525814056 0.03874807059764862 0.561032772064209 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015285398739844714\n",
            "704 Data 0.0015007243359351978 PDE 2.7624490030575544e-05 0.1496293991804123 0.1496293991804123 0.1508587896823883 0.03874649852514267 0.5610447525978088 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015283488259657733\n",
            "705 Data 0.0015006205396017947 PDE 2.7537727874005213e-05 0.1496294140815735 0.1496294140815735 0.15086038410663605 0.038744885474443436 0.5610566735267639 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015281582674758\n",
            "706 Data 0.0015005168646968472 PDE 2.7451173082226887e-05 0.14962942898273468 0.14962942898273468 0.1508619636297226 0.03874323144555092 0.561068594455719 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015279680377790741\n",
            "707 Data 0.001500413441365564 PDE 2.736485839704983e-05 0.14962944388389587 0.14962944388389587 0.15086354315280914 0.03874153643846512 0.5610804557800293 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015277782997626139\n",
            "708 Data 0.0015003102322129664 PDE 2.7278760171611793e-05 0.14962945878505707 0.14962945878505707 0.1508651226758957 0.038739800453186035 0.5610922574996948 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015275889923845782\n",
            "709 Data 0.0015002071983453095 PDE 2.7192865672986954e-05 0.14962947368621826 0.14962947368621826 0.15086670219898224 0.03873802348971367 0.5611039996147156 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015274000640182965\n",
            "710 Data 0.0015001044877714928 PDE 2.7107196729048155e-05 0.14962948858737946 0.14962948858737946 0.1508682668209076 0.03873620927333832 0.5611157417297363 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001527211684500541\n",
            "711 Data 0.0015000018437346635 PDE 2.7021747882827185e-05 0.14962950348854065 0.14962950348854065 0.15086983144283295 0.038734354078769684 0.5611274242401123 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015270235916174906\n",
            "712 Data 0.0014998995434701225 PDE 2.6936519134324044e-05 0.14962951838970184 0.14962951838970184 0.1508713960647583 0.03873245790600777 0.5611390471458435 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015268360626044465\n",
            "713 Data 0.0014997974014140851 PDE 2.685147956071887e-05 0.14962953329086304 0.14962953329086304 0.15087294578552246 0.038730520755052567 0.5611506700515747 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001526648880974804\n",
            "714 Data 0.0014996954657821664 PDE 2.6766680093714967e-05 0.14962954819202423 0.14962954819202423 0.15087449550628662 0.03872854262590408 0.5611622333526611 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015264621458758814\n",
            "715 Data 0.0014995937465926638 PDE 2.6682075258577242e-05 0.14962956309318542 0.14962956309318542 0.15087604522705078 0.038726527243852615 0.5611737370491028 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001526275821851241\n",
            "716 Data 0.0014994922777347766 PDE 2.659769779711496e-05 0.14962957799434662 0.14962957799434662 0.15087759494781494 0.038724470883607864 0.5611851811408997 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015260899755318916\n",
            "717 Data 0.0014993909086168317 PDE 2.6513540433370508e-05 0.14962957799434662 0.14962957799434662 0.1508791297674179 0.03872237354516983 0.5611966252326965 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015259044490502022\n",
            "718 Data 0.0014992898434239472 PDE 2.6429606805322692e-05 0.14962957799434662 0.14962957799434662 0.15088066458702087 0.03872023522853851 0.5612080097198486 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00152571945022927\n",
            "719 Data 0.0014991889147017183 PDE 2.6345871447119862e-05 0.14962957799434662 0.14962957799434662 0.15088219940662384 0.03871805965900421 0.561219334602356 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015255347861488381\n",
            "720 Data 0.0014990882335284218 PDE 2.626235618663486e-05 0.14962957799434662 0.14962957799434662 0.1508837193250656 0.03871584311127663 0.5612306594848633 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015253505897150567\n",
            "721 Data 0.0014989877403442798 PDE 2.6179033739026636e-05 0.14962957799434662 0.14962957799434662 0.15088523924350739 0.03871358931064606 0.5612419247627258 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015251667740833065\n",
            "722 Data 0.0014988874545400052 PDE 2.6095953217009082e-05 0.14962957799434662 0.14962957799434662 0.15088675916194916 0.038711294531822205 0.5612531304359436 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015249834077570143\n",
            "723 Data 0.0014987873718860885 PDE 2.6013070964836515e-05 0.14962957799434662 0.14962957799434662 0.15088827908039093 0.03870895877480507 0.5612642765045166 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001524800442850925\n",
            "724 Data 0.0014986875115110974 PDE 2.5930423362297006e-05 0.14962957799434662 0.14962957799434662 0.1508897840976715 0.03870658576488495 0.5612754225730896 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015246179348733944\n",
            "725 Data 0.001498587806291474 PDE 2.58479831245495e-05 0.14962957799434662 0.14962957799434662 0.1508912891149521 0.038704171776771545 0.5612865090370178 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015244357894160234\n",
            "726 Data 0.0014984883475640772 PDE 2.5765750251594e-05 0.14962957799434662 0.14962957799434662 0.15089279413223267 0.03870172053575516 0.5612975358963013 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015242540978156712\n",
            "727 Data 0.0014983889975086035 PDE 2.5683726562419906e-05 0.14962957799434662 0.14962957799434662 0.15089428424835205 0.038699232041835785 0.5613085627555847 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015240727240710234\n",
            "728 Data 0.0014982898437120359 PDE 2.560191751399543e-05 0.14962957799434662 0.14962957799434662 0.15089577436447144 0.03869670256972313 0.5613195300102234 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015238917612260313\n",
            "729 Data 0.0014981909969711456 PDE 2.5520306735415943e-05 0.14962957799434662 0.14962957799434662 0.15089726448059082 0.03869413584470749 0.5613304376602173 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015237113037065615\n",
            "730 Data 0.0014980922158742101 PDE 2.543891423556488e-05 0.14962956309318542 0.14962956309318542 0.1508987545967102 0.038691528141498566 0.5613412857055664 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001523531130109775\n",
            "731 Data 0.0014979937472766763 PDE 2.5357741833431646e-05 0.14962954819202423 0.14962954819202423 0.1509002298116684 0.03868888318538666 0.5613521337509155 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001523351489110108\n",
            "732 Data 0.001497895397401499 PDE 2.5276787710026838e-05 0.14962953329086304 0.14962953329086304 0.1509017050266266 0.038686200976371765 0.5613629221916199 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015231721851115259\n",
            "733 Data 0.0014977972472582583 PDE 2.5196048227371648e-05 0.14962951838970184 0.14962951838970184 0.15090318024158478 0.03868347778916359 0.5613736510276794 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00152299329548563\n",
            "734 Data 0.0014976992840147996 PDE 2.511550519557204e-05 0.14962950348854065 0.14962950348854065 0.15090465545654297 0.03868071734905243 0.561384379863739 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015228147892103717\n",
            "735 Data 0.0014976015326751357 PDE 2.5035162252606824e-05 0.14962948858737946 0.14962948858737946 0.15090611577033997 0.038677919656038284 0.5613950490951538 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015226366949277425\n",
            "736 Data 0.0014975038956277463 PDE 2.4955048502306454e-05 0.14962947368621826 0.14962947368621826 0.15090757608413696 0.038675084710121155 0.5614056587219238 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015224589441300528\n",
            "737 Data 0.0014974064659887614 PDE 2.487515848770272e-05 0.14962945878505707 0.14962945878505707 0.15090903639793396 0.03867221251130104 0.5614162683486938 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015222816244764641\n",
            "738 Data 0.0014973092553369053 PDE 2.4795452191028744e-05 0.14962944388389587 0.14962944388389587 0.15091048181056976 0.03866929933428764 0.5614268183708191 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001522104707527934\n",
            "739 Data 0.0014972122239778648 PDE 2.4715969630051404e-05 0.14962942898273468 0.14962942898273468 0.15091192722320557 0.03866634890437126 0.5614373087882996 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015219281936079162\n",
            "740 Data 0.0014971153189731138 PDE 2.4636701709823683e-05 0.1496294140815735 0.1496294140815735 0.15091337263584137 0.038663361221551895 0.56144779920578 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015217520206829375\n",
            "741 Data 0.0014970186797332607 PDE 2.4557624783483334e-05 0.1496293991804123 0.1496293991804123 0.15091481804847717 0.038660336285829544 0.5614582300186157 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001521576304516744\n",
            "742 Data 0.001496922205164362 PDE 2.4478773411829025e-05 0.1496293842792511 0.1496293842792511 0.15091624855995178 0.03865727409720421 0.5614686012268066 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001521400978576191\n",
            "743 Data 0.0014968259065216113 PDE 2.4400116672040895e-05 0.1496293693780899 0.1496293693780899 0.1509176790714264 0.03865417465567589 0.5614789724349976 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015212260231936522\n",
            "744 Data 0.0014967296806882512 PDE 2.4321694581885822e-05 0.14962933957576752 0.14962933957576752 0.150919109582901 0.03865103796124458 0.5614892840385437 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001521051375270137\n",
            "745 Data 0.0014966337495731109 PDE 2.4243450752692297e-05 0.14962930977344513 0.14962930977344513 0.1509205400943756 0.038647864013910294 0.5614995360374451 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015208772003258032\n",
            "746 Data 0.0014965379718296418 PDE 2.4165434297174215e-05 0.14962927997112274 0.14962927997112274 0.15092195570468903 0.03864465281367302 0.5615097880363464 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001520703406126816\n",
            "747 Data 0.00149644237339523 PDE 2.408763066341635e-05 0.14962925016880035 0.14962925016880035 0.15092337131500244 0.03864140436053276 0.561519980430603 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015205300040586464\n",
            "748 Data 0.0014963469284926495 PDE 2.401002711849287e-05 0.14962922036647797 0.14962922036647797 0.15092478692531586 0.038638122379779816 0.5615301132202148 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015203569556111424\n",
            "749 Data 0.0014962516619826743 PDE 2.3932614567456767e-05 0.14962919056415558 0.14962919056415558 0.15092618763446808 0.038634803146123886 0.5615402460098267 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001520184276550131\n",
            "750 Data 0.0014961565806958922 PDE 2.3855429390096106e-05 0.1496291607618332 0.1496291607618332 0.1509275883436203 0.03863144665956497 0.5615503191947937 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015200120100859883\n",
            "751 Data 0.0014960616793756743 PDE 2.3778440663591027e-05 0.1496291309595108 0.1496291309595108 0.15092898905277252 0.03862805292010307 0.561560332775116 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015198401200392654\n",
            "752 Data 0.0014959669184304912 PDE 2.370166839682497e-05 0.14962910115718842 0.14962910115718842 0.15093038976192474 0.03862462192773819 0.5615703463554382 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015196685868273162\n",
            "753 Data 0.0014958723652031792 PDE 2.3625070753041655e-05 0.14962907135486603 0.14962907135486603 0.15093177556991577 0.03862115740776062 0.5615803003311157 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015194974359562209\n",
            "754 Data 0.0014957779529053072 PDE 2.3548705939901993e-05 0.14962904155254364 0.14962904155254364 0.1509331613779068 0.038617655634880066 0.5615901947021484 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015193266588452092\n",
            "755 Data 0.0014956836763385106 PDE 2.347253575862851e-05 0.14962901175022125 0.14962901175022125 0.15093454718589783 0.03861411660909653 0.5616000890731812 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015191562120971391\n",
            "756 Data 0.0014955896251702962 PDE 2.3396578399115242e-05 0.14962898194789886 0.14962898194789886 0.15093593299388885 0.038610540330410004 0.5616099238395691 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015189862035694115\n",
            "757 Data 0.0014954957512410265 PDE 2.3320812033489347e-05 0.14962895214557648 0.14962895214557648 0.1509373039007187 0.038606930524110794 0.5616196990013123 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015188165632745159\n",
            "758 Data 0.0014954019867278823 PDE 2.3245269403560087e-05 0.1496289074420929 0.1496289074420929 0.15093867480754852 0.0386032834649086 0.5616294741630554 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015186472561314424\n",
            "759 Data 0.0014953084546648987 PDE 2.31699177675182e-05 0.14962886273860931 0.14962886273860931 0.15094004571437836 0.03859960287809372 0.5616391897201538 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001518478372432417\n",
            "760 Data 0.0014952150522435045 PDE 2.3094773496268317e-05 0.14962881803512573 0.14962881803512573 0.150941401720047 0.038595885038375854 0.5616488456726074 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015183098257397728\n",
            "761 Data 0.0014951217741421446 PDE 2.3019845684757456e-05 0.14962877333164215 0.14962877333164215 0.15094275772571564 0.038592129945755005 0.561658501625061 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001518141619826902\n",
            "762 Data 0.0014950286938593233 PDE 2.2945108867133968e-05 0.14962872862815857 0.14962872862815857 0.15094411373138428 0.03858834132552147 0.5616680979728699 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015179738027264573\n",
            "763 Data 0.0014949358162128941 PDE 2.287057759531308e-05 0.149628683924675 0.149628683924675 0.15094546973705292 0.03858451917767525 0.5616776943206787 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015178063938082072\n",
            "764 Data 0.0014948430475046773 PDE 2.2796264602220617e-05 0.1496286392211914 0.1496286392211914 0.15094681084156036 0.03858065977692604 0.5616872310638428 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001517639312106898\n",
            "765 Data 0.001494750435144015 PDE 2.2722142603015527e-05 0.14962859451770782 0.14962859451770782 0.1509481519460678 0.03857676684856415 0.5616967082023621 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015174725777470305\n",
            "766 Data 0.0014946580313278941 PDE 2.2648218873655424e-05 0.14962854981422424 0.14962854981422424 0.15094949305057526 0.03857283666729927 0.5617061853408813 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015173062502015495\n",
            "767 Data 0.0014945657867601189 PDE 2.2574491595150903e-05 0.14962850511074066 0.14962850511074066 0.1509508341550827 0.03856887295842171 0.5617156028747559 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015171402783552698\n",
            "768 Data 0.0014944735972650827 PDE 2.2500980776385404e-05 0.14962846040725708 0.14962846040725708 0.15095216035842896 0.03856487572193146 0.5617249608039856 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015169745780414681\n",
            "769 Data 0.0014943816957677047 PDE 2.242765731352847e-05 0.1496284157037735 0.1496284157037735 0.1509534865617752 0.03856084123253822 0.5617343187332153 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015168093530812331\n",
            "770 Data 0.0014942898998028791 PDE 2.2354537577484734e-05 0.14962837100028992 0.14962837100028992 0.15095481276512146 0.0385567732155323 0.5617436170578003 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015166444373803638\n",
            "771 Data 0.0014941982709173406 PDE 2.2281632482190616e-05 0.14962832629680634 0.14962832629680634 0.1509561389684677 0.038552671670913696 0.5617529153823853 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015164799033995312\n",
            "772 Data 0.0014941067233388693 PDE 2.220892747573089e-05 0.14962828159332275 0.14962828159332275 0.15095745027065277 0.038548532873392105 0.5617621541023254 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015163156508146002\n",
            "773 Data 0.001494015404950256 PDE 2.2136420739116147e-05 0.14962822198867798 0.14962822198867798 0.15095876157283783 0.03854436054825783 0.5617713332176208 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015161518256893722\n",
            "774 Data 0.0014939242294309608 PDE 2.206410681537818e-05 0.1496281623840332 0.1496281623840332 0.1509600728750229 0.038540154695510864 0.5617805123329163 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001515988336246339\n",
            "775 Data 0.0014938331780545103 PDE 2.1991982066538185e-05 0.14962810277938843 0.14962810277938843 0.15096138417720795 0.038535915315151215 0.5617896318435669 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015158251601210485\n",
            "776 Data 0.0014937423168892461 PDE 2.1920075596426614e-05 0.14962804317474365 0.14962804317474365 0.1509626805782318 0.03853164240717888 0.5617986917495728 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015156623924856727\n",
            "777 Data 0.0014936516183184964 PDE 2.184835830121301e-05 0.14962798357009888 0.14962798357009888 0.15096397697925568 0.03852733597159386 0.5618077516555786 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015154999766197094\n",
            "778 Data 0.0014935610384902735 PDE 2.1776844732812606e-05 0.1496279239654541 0.1496279239654541 0.15096527338027954 0.03852299600839615 0.5618167519569397 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001515337883223086\n",
            "779 Data 0.0014934705531853196 PDE 2.1705527615267783e-05 0.14962786436080933 0.14962786436080933 0.1509665697813034 0.038518618792295456 0.5618257522583008 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015151760808005874\n",
            "780 Data 0.0014933803279960798 PDE 2.1634417862514965e-05 0.14962780475616455 0.14962780475616455 0.15096785128116608 0.03851420804858208 0.5618346929550171 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015150147458585948\n",
            "781 Data 0.0014932902139432685 PDE 2.156349910364952e-05 0.14962774515151978 0.14962774515151978 0.15096913278102875 0.03850976377725601 0.5618435740470886 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001514853713046918\n",
            "782 Data 0.001493200222105695 PDE 2.1492782252607867e-05 0.149627685546875 0.149627685546875 0.15097041428089142 0.03850528597831726 0.5618524551391602 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001514693004358303\n",
            "783 Data 0.001493110376975296 PDE 2.1422249119495973e-05 0.14962762594223022 0.14962762594223022 0.1509716957807541 0.03850077837705612 0.5618612766265869 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001514532626094792\n",
            "784 Data 0.0014930206629506474 PDE 2.1351934265112504e-05 0.14962756633758545 0.14962756633758545 0.15097296237945557 0.0384962372481823 0.5618700981140137 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015143725972157599\n",
            "785 Data 0.0014929311001964248 PDE 2.1281792214722373e-05 0.14962750673294067 0.14962750673294067 0.15097422897815704 0.038491662591695786 0.5618788599967957 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015142128924111472\n",
            "786 Data 0.0014928417618269164 PDE 2.1211868443060666e-05 0.1496274471282959 0.1496274471282959 0.15097549557685852 0.03848705440759659 0.5618875622749329 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001514053630269977\n",
            "787 Data 0.0014927525041729106 PDE 2.114213384629693e-05 0.14962738752365112 0.14962738752365112 0.15097676217556 0.038482412695884705 0.5618962645530701 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015138946380192076\n",
            "788 Data 0.0014926633150671102 PDE 2.1072592062409967e-05 0.14962731301784515 0.14962731301784515 0.15097801387310028 0.038477737456560135 0.5619049072265625 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015137359071295201\n",
            "789 Data 0.0014925744148867048 PDE 2.100324672937859e-05 0.14962723851203918 0.14962723851203918 0.15097926557064056 0.03847302868962288 0.5619135499000549 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015135776616160834\n",
            "790 Data 0.0014924855993777748 PDE 2.0934092390234582e-05 0.14962716400623322 0.14962716400623322 0.15098051726818085 0.038468290120363235 0.5619221329689026 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015134196917680094\n",
            "791 Data 0.0014923968915011192 PDE 2.0865127225988545e-05 0.14962708950042725 0.14962708950042725 0.15098176896572113 0.038463518023490906 0.5619306564331055 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015132620187271078\n",
            "792 Data 0.0014923083369024143 PDE 2.079638397844974e-05 0.14962701499462128 0.14962701499462128 0.15098300576210022 0.03845871239900589 0.5619391798973083 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001513104720880864\n",
            "793 Data 0.001492219942435667 PDE 2.072782262985129e-05 0.1496269404888153 0.1496269404888153 0.1509842425584793 0.038453876972198486 0.5619476437568665 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015129477650655183\n",
            "794 Data 0.0014921316997219343 PDE 2.0659448637161404e-05 0.14962686598300934 0.14962686598300934 0.1509854793548584 0.0384490080177784 0.5619561076164246 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015127911483590957\n",
            "795 Data 0.001492043567379294 PDE 2.0591269276337698e-05 0.14962679147720337 0.14962679147720337 0.1509867161512375 0.03844410553574562 0.5619645118713379 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015126348366556316\n",
            "796 Data 0.0014919556218644543 PDE 2.0523284547380172e-05 0.1496267169713974 0.1496267169713974 0.15098793804645538 0.03843917325139046 0.5619729161262512 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015124789064118345\n",
            "797 Data 0.0014918677421178821 PDE 2.045549263129942e-05 0.14962664246559143 0.14962664246559143 0.15098915994167328 0.03843420743942261 0.5619812607765198 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015123232347491816\n",
            "798 Data 0.0014917800899739938 PDE 2.0387891709106043e-05 0.14962656795978546 0.14962656795978546 0.15099038183689117 0.03842921182513237 0.5619895458221436 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015121679816830999\n",
            "799 Data 0.0014916925341747558 PDE 2.0320481780800037e-05 0.1496264934539795 0.1496264934539795 0.15099160373210907 0.038424182683229446 0.5619978308677673 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015120130159555559\n",
            "800 Data 0.001491605083161746 PDE 2.025326648436021e-05 0.14962641894817352 0.14962641894817352 0.15099281072616577 0.038419123739004135 0.5620060563087463 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015118583496461062\n",
            "801 Data 0.0014915178320438038 PDE 2.0186242181807756e-05 0.14962634444236755 0.14962634444236755 0.15099401772022247 0.03841403126716614 0.5620142817497253 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015117040742256116\n",
            "802 Data 0.0014914306649435039 PDE 2.011941614910029e-05 0.14962626993656158 0.14962626993656158 0.15099522471427917 0.03840890899300575 0.5620224475860596 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015115500810926042\n",
            "803 Data 0.001491343642943173 PDE 2.0052775653311983e-05 0.14962619543075562 0.14962619543075562 0.15099643170833588 0.03840375319123268 0.5620306134223938 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001511396418596485\n",
            "804 Data 0.001491256750709032 PDE 1.9986318875453435e-05 0.14962610602378845 0.14962610602378845 0.15099762380123138 0.03839856758713722 0.5620387196540833 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015112430695844855\n",
            "805 Data 0.001491170025665222 PDE 1.9920067643397488e-05 0.1496260166168213 0.1496260166168213 0.1509988158941269 0.038393352180719376 0.5620467662811279 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015110900933086194\n",
            "806 Data 0.0014910834124954038 PDE 1.9854003767250106e-05 0.14962592720985413 0.14962592720985413 0.1510000079870224 0.03838810324668884 0.5620548129081726 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015109374162626539\n",
            "807 Data 0.001490996920084769 PDE 1.978811815206427e-05 0.14962583780288696 0.14962583780288696 0.1510012000799179 0.03838282451033592 0.5620627999305725 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015107850382368334\n",
            "808 Data 0.0014909105831247486 PDE 1.9722421711776406e-05 0.1496257483959198 0.1496257483959198 0.15100237727165222 0.038377515971660614 0.5620707869529724 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001510633004836525\n",
            "809 Data 0.0014908243393874862 PDE 1.9656923541333526e-05 0.14962565898895264 0.14962565898895264 0.15100355446338654 0.03837217390537262 0.5620787143707275 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015104812629288197\n",
            "810 Data 0.0014907382897829835 PDE 1.9591614545788616e-05 0.14962556958198547 0.14962556958198547 0.15100473165512085 0.03836680203676224 0.5620866417884827 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015103299043287722\n",
            "811 Data 0.0014906523402566532 PDE 1.952648199221585e-05 0.1496254801750183 0.1496254801750183 0.15100590884685516 0.03836140036582947 0.562094509601593 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001510178822248869\n",
            "812 Data 0.0014905665159851542 PDE 1.9461538613541052e-05 0.14962539076805115 0.14962539076805115 0.15100708603858948 0.03835596889257431 0.5621023774147034 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015100280545986952\n",
            "813 Data 0.0014904808176045543 PDE 1.939679350471124e-05 0.14962530136108398 0.14962530136108398 0.1510082483291626 0.038350507616996765 0.562110185623169 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015098776111092656\n",
            "814 Data 0.001490395201622363 PDE 1.9332226656842977e-05 0.14962521195411682 0.14962521195411682 0.15100941061973572 0.03834501653909683 0.5621179342269897 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001509727428279206\n",
            "815 Data 0.0014903097717542497 PDE 1.9267854440840892e-05 0.14962512254714966 0.14962512254714966 0.15101057291030884 0.03833949565887451 0.5621256828308105 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015095776261950906\n",
            "816 Data 0.0014902244663584445 PDE 1.9203669580747373e-05 0.1496250331401825 0.1496250331401825 0.15101173520088196 0.0383339449763298 0.5621333718299866 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001509428135939192\n",
            "817 Data 0.0014901393213004 PDE 1.9139662981615402e-05 0.14962494373321533 0.14962494373321533 0.15101288259029388 0.03832836076617241 0.5621410608291626 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015092789842820154\n",
            "818 Data 0.0014900542600095653 PDE 1.907585101434961e-05 0.14962485432624817 0.14962485432624817 0.1510140299797058 0.03832274675369263 0.5621486902236938 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001509130111023915\n",
            "819 Data 0.001489969329512377 PDE 1.9012242773897015e-05 0.149624764919281 0.149624764919281 0.15101517736911774 0.038317106664180756 0.5621563196182251 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001508981572286274\n",
            "820 Data 0.0014898844787292844 PDE 1.8948800061480142e-05 0.14962467551231384 0.14962467551231384 0.15101632475852966 0.0383114367723465 0.5621638894081116 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015088332787907646\n",
            "821 Data 0.001489799848837036 PDE 1.8885544704971835e-05 0.14962457120418549 0.14962457120418549 0.1510174572467804 0.03830573707818985 0.562171459197998 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015086853935420079\n",
            "822 Data 0.0014897153018041094 PDE 1.8822467609425075e-05 0.14962446689605713 0.14962446689605713 0.15101858973503113 0.038300007581710815 0.5621789693832397 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015085377694135345\n",
            "823 Data 0.0014896307792279144 PDE 1.875958332675509e-05 0.14962436258792877 0.14962436258792877 0.15101972222328186 0.03829424828290939 0.5621864795684814 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015083903625546695\n",
            "824 Data 0.001489546547815799 PDE 1.8696866391110234e-05 0.14962425827980042 0.14962425827980042 0.1510208547115326 0.038288459181785583 0.5621939301490784 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015082434142069093\n",
            "825 Data 0.00148946243911253 PDE 1.8634342268342152e-05 0.14962415397167206 0.14962415397167206 0.15102198719978333 0.038282640278339386 0.5622013807296753 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015080967813808721\n",
            "826 Data 0.0014893783363030392 PDE 1.8572005501482636e-05 0.1496240496635437 0.1496240496635437 0.15102310478687286 0.0382767915725708 0.5622087717056274 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015079503418045218\n",
            "827 Data 0.0014892944190789048 PDE 1.8509852452552877e-05 0.14962394535541534 0.14962394535541534 0.1510242223739624 0.038270916789770126 0.5622161030769348 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015078042715314577\n",
            "828 Data 0.001489210613906426 PDE 1.844788494054228e-05 0.149623841047287 0.149623841047287 0.15102533996105194 0.038265012204647064 0.5622234344482422 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015076584988469683\n",
            "829 Data 0.0014891269319842486 PDE 1.8386108422419056e-05 0.14962373673915863 0.14962373673915863 0.15102645754814148 0.038259077817201614 0.5622307062149048 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015075130404066676\n",
            "830 Data 0.0014890433758173135 PDE 1.8324497432331555e-05 0.14962363243103027 0.14962363243103027 0.15102756023406982 0.038253117352724075 0.5622379779815674 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001507367873249645\n",
            "831 Data 0.0014889598834578746 PDE 1.826307016017381e-05 0.14962352812290192 0.14962352812290192 0.15102866291999817 0.03824712708592415 0.5622451901435852 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015072229536180484\n",
            "832 Data 0.0014888766010652597 PDE 1.8201817510998808e-05 0.14962342381477356 0.14962342381477356 0.1510297656059265 0.038241107016801834 0.562252402305603 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015070784185762585\n",
            "833 Data 0.0014887933626759266 PDE 1.814077768358402e-05 0.1496233195066452 0.1496233195066452 0.15103086829185486 0.03823506087064743 0.5622595548629761 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015069341403595106\n",
            "834 Data 0.0014887103802483721 PDE 1.8079877918353304e-05 0.14962321519851685 0.14962321519851685 0.1510319709777832 0.03822898492217064 0.5622667074203491 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015067902581667254\n",
            "835 Data 0.001488627354132346 PDE 1.801917824195698e-05 0.1496231108903885 0.1496231108903885 0.15103305876255035 0.03822288289666176 0.5622738003730774 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001506646532374303\n",
            "836 Data 0.0014885445726698576 PDE 1.7958658645511605e-05 0.14962300658226013 0.14962300658226013 0.1510341465473175 0.03821675106883049 0.5622808933258057 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015065032313153692\n",
            "837 Data 0.0014884617391682485 PDE 1.7898331861943007e-05 0.14962290227413177 0.14962290227413177 0.15103523433208466 0.03821059316396713 0.5622879266738892 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015063600710301915\n",
            "838 Data 0.0014883792132310673 PDE 1.7838166968431324e-05 0.14962279796600342 0.14962279796600342 0.1510363221168518 0.03820440545678139 0.5622949600219727 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015062173801994986\n",
            "839 Data 0.0014882967120421595 PDE 1.777817669790238e-05 0.14962267875671387 0.14962267875671387 0.15103739500045776 0.03819819167256355 0.5623019337654114 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015060748887400618\n",
            "840 Data 0.0014882143065778571 PDE 1.7718395611154847e-05 0.14962255954742432 0.14962255954742432 0.15103846788406372 0.03819194808602333 0.5623089075088501 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001505932702189012\n",
            "841 Data 0.0014881321062125094 PDE 1.765877823345363e-05 0.14962244033813477 0.14962244033813477 0.15103954076766968 0.03818567842245102 0.562315821647644 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001505790884445963\n",
            "842 Data 0.0014880499426390605 PDE 1.7599330021766946e-05 0.14962232112884521 0.14962232112884521 0.15104061365127563 0.03817938268184662 0.562322735786438 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015056492726608275\n",
            "843 Data 0.0014879679133826787 PDE 1.7540069165988825e-05 0.14962220191955566 0.14962220191955566 0.1510416865348816 0.03817305713891983 0.5623295903205872 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015055079825486675\n",
            "844 Data 0.0014878859987234616 PDE 1.7480973838246427e-05 0.1496220827102661 0.1496220827102661 0.15104274451732635 0.03816670551896095 0.5623364448547363 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001505366972561708\n",
            "845 Data 0.0014878042016438268 PDE 1.742205859045498e-05 0.14962196350097656 0.14962196350097656 0.15104380249977112 0.038160327821969986 0.5623432397842407 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015052262602342817\n",
            "846 Data 0.0014877225254273196 PDE 1.7363319784635678e-05 0.149621844291687 0.149621844291687 0.15104486048221588 0.03815392032265663 0.5623500347137451 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015050858452119553\n",
            "847 Data 0.001487640969875555 PDE 1.7304764696746133e-05 0.14962172508239746 0.14962172508239746 0.15104591846466064 0.03814748674631119 0.5623567700386047 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015049457345723011\n",
            "848 Data 0.0014875595118736666 PDE 1.7246376955881715e-05 0.1496216058731079 0.1496216058731079 0.1510469615459442 0.038141027092933655 0.5623635053634644 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015048058888295483\n",
            "849 Data 0.0014874781552874246 PDE 1.7188167475978844e-05 0.14962148666381836 0.14962148666381836 0.15104800462722778 0.03813454136252403 0.5623701810836792 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015046663227634035\n",
            "850 Data 0.0014873969959830929 PDE 1.7130138076026924e-05 0.1496213674545288 0.1496213674545288 0.15104904770851135 0.03812802955508232 0.562376856803894 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015045271340591198\n",
            "851 Data 0.0014873158663291685 PDE 1.7072277842089534e-05 0.14962124824523926 0.14962124824523926 0.15105009078979492 0.03812149167060852 0.5623834729194641 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001504388144171258\n",
            "852 Data 0.0014872348328670896 PDE 1.7014594050124288e-05 0.1496211290359497 0.1496211290359497 0.1510511338710785 0.03811492770910263 0.5623900890350342 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015042494269172139\n",
            "853 Data 0.0014871540280880155 PDE 1.6957083062152378e-05 0.14962100982666016 0.14962100982666016 0.15105216205120087 0.03810833767056465 0.5623966455459595 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015041111111501679\n",
            "854 Data 0.0014870732523533233 PDE 1.689974669716321e-05 0.1496208906173706 0.1496208906173706 0.15105319023132324 0.038101717829704285 0.5624032020568848 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015039729990504866\n",
            "855 Data 0.0014869926024842434 PDE 1.6842575860209763e-05 0.14962077140808105 0.14962077140808105 0.15105421841144562 0.03809507191181183 0.5624096989631653 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015038351783444532\n",
            "856 Data 0.001486912103316441 PDE 1.6785585103207268e-05 0.1496206521987915 0.1496206521987915 0.151055246591568 0.03808840364217758 0.5624161958694458 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015036976884196483\n",
            "857 Data 0.001486831557646557 PDE 1.6728770788176917e-05 0.14962053298950195 0.14962053298950195 0.15105627477169037 0.038081709295511246 0.5624226927757263 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001503560328434734\n",
            "858 Data 0.0014867512735174287 PDE 1.6672131096129306e-05 0.1496204137802124 0.1496204137802124 0.15105728805065155 0.03807498887181282 0.5624291300773621 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001503423404613558\n",
            "859 Data 0.0014866710141939173 PDE 1.6615651475149207e-05 0.14962027966976166 0.14962027966976166 0.15105830132961273 0.038068242371082306 0.5624355673789978 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015032866656690665\n",
            "860 Data 0.001486590879056278 PDE 1.6559353753109463e-05 0.1496201455593109 0.1496201455593109 0.1510593146085739 0.0380614697933197 0.5624419450759888 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015031502328093874\n",
            "861 Data 0.0014865109362461142 PDE 1.6503228835063055e-05 0.14962001144886017 0.14962001144886017 0.1510603278875351 0.03805467113852501 0.5624483227729797 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015030141650811772\n",
            "862 Data 0.0014864310504663315 PDE 1.6447278539999388e-05 0.14961987733840942 0.14961987733840942 0.15106134116649628 0.03804784640669823 0.5624546408653259 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015028783290063309\n",
            "863 Data 0.0014863512336049187 PDE 1.6391482859035023e-05 0.14961974322795868 0.14961974322795868 0.15106233954429626 0.038040995597839355 0.5624609589576721 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015027427164639538\n",
            "864 Data 0.0014862716246295763 PDE 1.6335861801053397e-05 0.14961960911750793 0.14961960911750793 0.15106333792209625 0.03803412243723869 0.5624672174453735 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015026074864306297\n",
            "865 Data 0.0014861920686406482 PDE 1.6280417185043916e-05 0.1496194750070572 0.1496194750070572 0.15106433629989624 0.03802722319960594 0.562473475933075 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001502472485825692\n",
            "866 Data 0.0014861126072779733 PDE 1.6225130821112543e-05 0.14961934089660645 0.14961934089660645 0.15106533467769623 0.0380202978849411 0.5624796748161316 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015023377380990858\n",
            "867 Data 0.0014860332648279646 PDE 1.6170009985216893e-05 0.1496192067861557 0.1496192067861557 0.15106633305549622 0.03801335021853447 0.5624858736991882 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015022032748131815\n",
            "868 Data 0.0014859540081710119 PDE 1.6115085600176826e-05 0.14961907267570496 0.14961907267570496 0.151067316532135 0.03800637647509575 0.5624920129776001 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015020690937711887\n",
            "869 Data 0.0014858748677369334 PDE 1.6060324924183078e-05 0.1496189385652542 0.1496189385652542 0.1510683000087738 0.03799937665462494 0.562498152256012 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015019351926611165\n",
            "870 Data 0.0014857958448118115 PDE 1.600572795723565e-05 0.14961880445480347 0.14961880445480347 0.1510692834854126 0.03799235448241234 0.562504231929779 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015018015727690471\n",
            "871 Data 0.0014857169276122962 PDE 1.5951287423376925e-05 0.14961867034435272 0.14961867034435272 0.1510702669620514 0.03798530623316765 0.5625103116035461 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015016682150356731\n",
            "872 Data 0.0014856381183829944 PDE 1.5897023331490345e-05 0.14961853623390198 0.14961853623390198 0.15107125043869019 0.03797823563218117 0.5625163912773132 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015015351417144848\n",
            "873 Data 0.0014855593811884619 PDE 1.5842917491681874e-05 0.14961840212345123 0.14961840212345123 0.15107221901416779 0.0379711389541626 0.5625224113464355 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015014022986801438\n",
            "874 Data 0.0014854807767502006 PDE 1.5789006283739582e-05 0.1496182680130005 0.1496182680130005 0.15107318758964539 0.03796401992440224 0.5625284314155579 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015012697830339402\n",
            "875 Data 0.0014854022534702626 PDE 1.573523877596017e-05 0.14961813390254974 0.14961813390254974 0.15107415616512299 0.03795687481760979 0.5625343918800354 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015011374922462228\n",
            "876 Data 0.001485323885133496 PDE 1.5681645891163498e-05 0.149617999792099 0.149617999792099 0.15107512474060059 0.037949707359075546 0.5625403523445129 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015010055310246595\n",
            "877 Data 0.0014852455903795556 PDE 1.5628214896423742e-05 0.14961786568164825 0.14961786568164825 0.15107609331607819 0.037942513823509216 0.5625462532043457 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015008738052759794\n",
            "878 Data 0.00148516738435851 PDE 1.5574956705677323e-05 0.1496177315711975 0.1496177315711975 0.1510770469903946 0.037935297936201096 0.5625521540641785 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015007423410641873\n",
            "879 Data 0.0014850893276560457 PDE 1.552185494801961e-05 0.14961759746074677 0.14961759746074677 0.151078000664711 0.037928055971860886 0.5625579953193665 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015006111826040653\n",
            "880 Data 0.0014850113663259713 PDE 1.546891508041881e-05 0.14961744844913483 0.14961744844913483 0.1510789543390274 0.037920791655778885 0.5625638365745544 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015004802814063902\n",
            "881 Data 0.0014849334573553991 PDE 1.5416149835800752e-05 0.1496172994375229 0.1496172994375229 0.1510799080133438 0.03791350498795509 0.5625696778297424 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015003496071911999\n",
            "882 Data 0.0014848557089141084 PDE 1.5363542843260802e-05 0.14961715042591095 0.14961715042591095 0.15108086168766022 0.03790619596838951 0.5625754594802856 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015002192517573692\n",
            "883 Data 0.0014847780061098344 PDE 1.5311099559767172e-05 0.149617001414299 0.149617001414299 0.15108180046081543 0.03789886087179184 0.5625812411308289 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0015000891056696016\n",
            "884 Data 0.0014847004428927594 PDE 1.525881998531986e-05 0.14961685240268707 0.14961685240268707 0.15108273923397064 0.03789150342345238 0.5625869631767273 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014999592628780793\n",
            "885 Data 0.0014846229845812736 PDE 1.520670502941357e-05 0.14961670339107513 0.14961670339107513 0.15108367800712585 0.037884123623371124 0.5625926852226257 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014998296896106872\n",
            "886 Data 0.0014845455787295808 PDE 1.5154755601543002e-05 0.1496165543794632 0.1496165543794632 0.15108461678028107 0.03787672147154808 0.5625983476638794 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014997003343311238\n",
            "887 Data 0.0014844683438502509 PDE 1.5102966244739946e-05 0.14961640536785126 0.14961640536785126 0.15108555555343628 0.037869296967983246 0.5626040101051331 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014995713100949908\n",
            "888 Data 0.001484391173828384 PDE 1.5051343325467315e-05 0.14961625635623932 0.14961625635623932 0.1510864794254303 0.03786184638738632 0.5626096725463867 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014994425171538513\n",
            "889 Data 0.0014843141310290396 PDE 1.49998832057463e-05 0.14961610734462738 0.14961610734462738 0.15108740329742432 0.03785437345504761 0.5626152753829956 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001499314014234786\n",
            "890 Data 0.0014842371367155285 PDE 1.4948579519113991e-05 0.14961595833301544 0.14961595833301544 0.15108832716941833 0.0378468781709671 0.5626208782196045 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014991857162346424\n",
            "891 Data 0.0014841603040292907 PDE 1.4897424989612773e-05 0.1496158093214035 0.1496158093214035 0.15108925104141235 0.037839360535144806 0.5626264214515686 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014990577290189034\n",
            "892 Data 0.0014840835055493044 PDE 1.48464478115784e-05 0.14961566030979156 0.14961566030979156 0.15109017491340637 0.03783182054758072 0.5626319646835327 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014989299533608828\n",
            "893 Data 0.0014840068721475577 PDE 1.479562070016982e-05 0.14961551129817963 0.14961551129817963 0.1510910987854004 0.03782425820827484 0.5626375079154968 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014988024928477275\n",
            "894 Data 0.0014839302380903314 PDE 1.4744960935786366e-05 0.1496153622865677 0.1496153622865677 0.15109200775623322 0.03781667351722717 0.5626429915428162 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014986751990261177\n",
            "895 Data 0.0014838537765489506 PDE 1.4694449419039302e-05 0.14961521327495575 0.14961521327495575 0.15109291672706604 0.037809066474437714 0.5626484751701355 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00149854822596799\n",
            "896 Data 0.0014837774683174494 PDE 1.4644111615780275e-05 0.1496150642633438 0.1496150642633438 0.15109382569789886 0.037801437079906464 0.5626538991928101 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014984215799332297\n",
            "897 Data 0.0014837011370282343 PDE 1.4593930245609954e-05 0.14961491525173187 0.14961491525173187 0.1510947346687317 0.03779378905892372 0.5626593232154846 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014982950672738442\n",
            "898 Data 0.0014836250098329566 PDE 1.4543899851560127e-05 0.14961476624011993 0.14961476624011993 0.15109564363956451 0.03778611868619919 0.5626647472381592 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014981689096845168\n",
            "899 Data 0.001483548925623166 PDE 1.4494034985546023e-05 0.149614617228508 0.149614617228508 0.15109653770923615 0.037778425961732864 0.562670111656189 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001498042960608712\n",
            "900 Data 0.001483472951153073 PDE 1.4444322914641816e-05 0.14961446821689606 0.14961446821689606 0.15109743177890778 0.03777071088552475 0.5626754760742188 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014979172740677148\n",
            "901 Data 0.001483397061322454 PDE 1.4394759091373999e-05 0.14961431920528412 0.14961431920528412 0.1510983258485794 0.037762973457574844 0.5626807808876038 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001497791820413828\n",
            "902 Data 0.0014833213331481476 PDE 1.434536352462601e-05 0.14961417019367218 0.14961417019367218 0.15109921991825104 0.03775521740317345 0.5626860857009888 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014976666966727737\n",
            "903 Data 0.0014832456157375803 PDE 1.4296116205514409e-05 0.14961400628089905 0.14961400628089905 0.15110011398792267 0.03774743899703026 0.5626913905143738 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014975417319430947\n",
            "904 Data 0.0014831700416695194 PDE 1.4247026228986215e-05 0.14961384236812592 0.14961384236812592 0.1511010080575943 0.03773963823914528 0.562696635723114 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014974170678985056\n",
            "905 Data 0.0014830945303938128 PDE 1.4198087228578515e-05 0.14961367845535278 0.14961367845535278 0.15110188722610474 0.03773181512951851 0.5627018809318542 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014972926176223913\n",
            "906 Data 0.001483019223414955 PDE 1.4149307389743626e-05 0.14961351454257965 0.14961351454257965 0.15110276639461517 0.03772397339344025 0.5627070665359497 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014971685308046987\n",
            "907 Data 0.001482943867107165 PDE 1.4100697626417968e-05 0.14961335062980652 0.14961335062980652 0.1511036455631256 0.037716109305620193 0.5627122521400452 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001497044564733583\n",
            "908 Data 0.001482868667296778 PDE 1.4052227925276384e-05 0.1496131867170334 0.1496131867170334 0.15110452473163605 0.03770822659134865 0.5627174377441406 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014969208952220544\n",
            "909 Data 0.0014827936442975164 PDE 1.40039128382341e-05 0.14961302280426025 0.14961302280426025 0.15110540390014648 0.03770032152533531 0.5627225637435913 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014967975571357505\n",
            "910 Data 0.001482718611727917 PDE 1.3955756912764627e-05 0.14961285889148712 0.14961285889148712 0.15110628306865692 0.037692394107580185 0.562727689743042 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014966743686406817\n",
            "911 Data 0.001482643704455184 PDE 1.390776287735207e-05 0.149612694978714 0.149612694978714 0.15110714733600616 0.037684448063373566 0.5627327561378479 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001496551467332536\n",
            "912 Data 0.001482568911081537 PDE 1.3859905266144779e-05 0.14961253106594086 0.14961253106594086 0.1511080116033554 0.037676479667425156 0.5627378225326538 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014964288163476817\n",
            "913 Data 0.0014824941547806573 PDE 1.3812207726004999e-05 0.14961236715316772 0.14961236715316772 0.15110887587070465 0.03766849264502525 0.5627428889274597 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014963063625066623\n",
            "914 Data 0.0014824195527139321 PDE 1.3764670256932732e-05 0.1496122032403946 0.1496122032403946 0.1511097401380539 0.03766048699617386 0.5627478957176208 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014961842229708649\n",
            "915 Data 0.0014823450319020005 PDE 1.3717274669033941e-05 0.14961203932762146 0.14961203932762146 0.15111060440540314 0.03765245899558067 0.562752902507782 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014960623065710344\n",
            "916 Data 0.0014822705872887098 PDE 1.3670034604729153e-05 0.14961187541484833 0.14961187541484833 0.15111146867275238 0.037644412368535995 0.5627578496932983 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001495940621893439\n",
            "917 Data 0.0014821962184812258 PDE 1.3622939150081947e-05 0.1496117115020752 0.1496117115020752 0.15111231803894043 0.03763634338974953 0.5627627968788147 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014958191576313078\n",
            "918 Data 0.001482121972516312 PDE 1.3576001038018148e-05 0.14961154758930206 0.14961154758930206 0.15111316740512848 0.037628255784511566 0.562767744064331 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00149569797355433\n",
            "919 Data 0.0014820478539046957 PDE 1.3529220268537756e-05 0.14961138367652893 0.14961138367652893 0.15111401677131653 0.03762014955282211 0.5627726316452026 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014955770741732334\n",
            "920 Data 0.001481973776954789 PDE 1.3482579561241437e-05 0.1496112197637558 0.1496112197637558 0.15111486613750458 0.03761202096939087 0.5627775192260742 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014954563565160305\n",
            "921 Data 0.0014818998337198601 PDE 1.3436103472486138e-05 0.14961105585098267 0.14961105585098267 0.15111571550369263 0.03760387375950813 0.5627824068069458 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014953359371923463\n",
            "922 Data 0.0014818259638535944 PDE 1.3389762898441404e-05 0.14961089193820953 0.14961089193820953 0.15111656486988068 0.037595707923173904 0.5627872347831726 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014952157267520358\n",
            "923 Data 0.0014817520966811017 PDE 1.3343578757485375e-05 0.1496107280254364 0.1496107280254364 0.15111739933490753 0.037587523460388184 0.5627920627593994 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014950956754385871\n",
            "924 Data 0.001481678488462278 PDE 1.3297546502144542e-05 0.14961056411266327 0.14961056411266327 0.1511182337999344 0.03757932037115097 0.5627968907356262 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014949760349644225\n",
            "925 Data 0.0014816048395749155 PDE 1.3251659765955992e-05 0.14961040019989014 0.14961040019989014 0.15111906826496124 0.03757109493017197 0.5628016591072083 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014948564993408715\n",
            "926 Data 0.0014815313675083802 PDE 1.320592582487734e-05 0.149610236287117 0.149610236287117 0.1511199027299881 0.03756285086274147 0.5628064274787903 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014947372933332575\n",
            "927 Data 0.001481457926421605 PDE 1.3160332855477463e-05 0.14961007237434387 0.14961007237434387 0.15112073719501495 0.03755458816885948 0.5628111362457275 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014946182592770825\n",
            "928 Data 0.001481384597521595 PDE 1.3114897228660993e-05 0.14960990846157074 0.14960990846157074 0.1511215716600418 0.037546306848526 0.5628158450126648 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001494499494750256\n",
            "929 Data 0.0014813113589749675 PDE 1.306959893554449e-05 0.1496097445487976 0.1496097445487976 0.15112239122390747 0.03753800690174103 0.562820553779602 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001494380957910512\n",
            "930 Data 0.0014812381850113584 PDE 1.3024449799559079e-05 0.14960956573486328 0.14960956573486328 0.15112321078777313 0.03752968832850456 0.5628252029418945 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014942626348109175\n",
            "931 Data 0.0014811651273013593 PDE 1.2979454368178267e-05 0.14960938692092896 0.14960938692092896 0.1511240303516388 0.037521351128816605 0.562829852104187 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014941445816695376\n",
            "932 Data 0.0014810921736560238 PDE 1.2934598089486826e-05 0.14960920810699463 0.14960920810699463 0.15112484991550446 0.037512995302677155 0.5628345012664795 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014940267717455107\n",
            "933 Data 0.00148101934914036 PDE 1.2889889148937073e-05 0.1496090292930603 0.1496090292930603 0.15112566947937012 0.03750462085008621 0.5628390908241272 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001493909238289297\n",
            "934 Data 0.0014809465074887356 PDE 1.284532936551841e-05 0.14960885047912598 0.14960885047912598 0.15112648904323578 0.03749622777104378 0.5628436803817749 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001493791836854254\n",
            "935 Data 0.0014808737898340573 PDE 1.2800917829736136e-05 0.14960867166519165 0.14960867166519165 0.15112729370594025 0.03748781606554985 0.5628482699394226 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014936747076637934\n",
            "936 Data 0.0014808012319524088 PDE 1.2756648175127339e-05 0.14960849285125732 0.14960849285125732 0.15112809836864471 0.03747938573360443 0.5628527998924255 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014935578801275361\n",
            "937 Data 0.0014807287036463668 PDE 1.2712518582702614e-05 0.149608314037323 0.149608314037323 0.15112890303134918 0.03747094050049782 0.5628573298454285 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014934412222290694\n",
            "938 Data 0.0014806562695501874 PDE 1.2668535418924876e-05 0.14960813522338867 0.14960813522338867 0.15112970769405365 0.03746247664093971 0.5628618597984314 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014933248049691123\n",
            "939 Data 0.0014805839146384335 PDE 1.2624704140762333e-05 0.14960795640945435 0.14960795640945435 0.15113051235675812 0.037453994154930115 0.5628663301467896 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014932086187791958\n",
            "940 Data 0.0014805116834385452 PDE 1.2581009286805056e-05 0.14960777759552002 0.14960777759552002 0.15113131701946259 0.037445493042469025 0.5628708004951477 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014930926927253503\n",
            "941 Data 0.0014804395334662513 PDE 1.2537455404526554e-05 0.1496075987815857 0.1496075987815857 0.15113212168216705 0.03743697702884674 0.5628752708435059 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014929769888707778\n",
            "942 Data 0.0014803674121374873 PDE 1.2494051588873845e-05 0.14960741996765137 0.14960741996765137 0.15113291144371033 0.037428442388772964 0.5628796815872192 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014928614637263612\n",
            "943 Data 0.001480295455828979 PDE 1.2450792382878717e-05 0.14960724115371704 0.14960724115371704 0.1511337012052536 0.037419889122247696 0.5628840923309326 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014927462482118576\n",
            "944 Data 0.0014802235173697058 PDE 1.2407670510583557e-05 0.14960706233978271 0.14960706233978271 0.15113449096679688 0.037411317229270935 0.562888503074646 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014926311878802894\n",
            "945 Data 0.001480151771570739 PDE 1.2364687790977769e-05 0.1496068835258484 0.1496068835258484 0.15113528072834015 0.03740273043513298 0.5628928542137146 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014925164593617168\n",
            "946 Data 0.0014800800089658317 PDE 1.232184331456665e-05 0.14960670471191406 0.14960670471191406 0.15113607048988342 0.03739412501454353 0.5628972053527832 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014924018522803984\n",
            "947 Data 0.0014800083977531946 PDE 1.2279155271244235e-05 0.14960652589797974 0.14960652589797974 0.1511368602514267 0.03738550469279289 0.5629015564918518 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014922875530244389\n",
            "948 Data 0.0014799368012784308 PDE 1.2236597285664175e-05 0.1496063470840454 0.1496063470840454 0.15113765001296997 0.03737686574459076 0.5629058480262756 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001492173398564095\n",
            "949 Data 0.001479865385814244 PDE 1.219416571984766e-05 0.14960616827011108 0.14960616827011108 0.15113842487335205 0.03736821189522743 0.5629101395606995 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014920595515340916\n",
            "950 Data 0.0014797940143796034 PDE 1.2151887858635746e-05 0.14960598945617676 0.14960598945617676 0.15113919973373413 0.03735953941941261 0.5629144310951233 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014919459022382391\n",
            "951 Data 0.0014797227340138494 PDE 1.2109740964660887e-05 0.14960581064224243 0.14960581064224243 0.1511399745941162 0.0373508520424366 0.5629186630249023 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014918324749785103\n",
            "952 Data 0.0014796515105750044 PDE 1.2067752322764136e-05 0.1496056318283081 0.1496056318283081 0.1511407494544983 0.037342146039009094 0.5629228949546814 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014917192628977685\n",
            "953 Data 0.0014795804215575262 PDE 1.2025891010125633e-05 0.14960545301437378 0.14960545301437378 0.15114152431488037 0.037333425134420395 0.5629271268844604 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014916063125676518\n",
            "954 Data 0.0014795094108098442 PDE 1.1984169759671204e-05 0.14960527420043945 0.14960527420043945 0.15114229917526245 0.0373246856033802 0.5629312992095947 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014914935805695154\n",
            "955 Data 0.0014794384791285136 PDE 1.1942580385948531e-05 0.14960509538650513 0.14960509538650513 0.15114307403564453 0.03731593117117882 0.562935471534729 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014913810595144621\n",
            "956 Data 0.0014793676311413422 PDE 1.1901143807335757e-05 0.1496049165725708 0.1496049165725708 0.15114383399486542 0.03730715811252594 0.5629396438598633 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001491268774948678\n",
            "957 Data 0.0014792968465573567 PDE 1.1859843652928248e-05 0.14960473775863647 0.14960473775863647 0.1511445939540863 0.03729837015271187 0.5629437565803528 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001491156690210285\n",
            "958 Data 0.001479226157717053 PDE 1.1818665370810777e-05 0.14960455894470215 0.14960455894470215 0.1511453539133072 0.0372895672917366 0.5629478693008423 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014910448230878637\n",
            "959 Data 0.0014791556054712318 PDE 1.1777618965425063e-05 0.14960438013076782 0.14960438013076782 0.15114611387252808 0.03728074952960014 0.5629519820213318 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014909332244366568\n",
            "960 Data 0.001479085012224606 PDE 1.1736726264643949e-05 0.1496041864156723 0.1496041864156723 0.15114687383174896 0.03727191314101219 0.5629560947418213 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00149082173848925\n",
            "961 Data 0.0014790146273833157 PDE 1.1695953617163468e-05 0.14960399270057678 0.14960399270057678 0.15114763379096985 0.037263061851263046 0.562960147857666 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014907105810004792\n",
            "962 Data 0.001478944342842027 PDE 1.1655321941361763e-05 0.14960379898548126 0.14960379898548126 0.15114839375019073 0.03725419566035271 0.5629642009735107 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014905996647833887\n",
            "963 Data 0.0014788740348579419 PDE 1.161483032774413e-05 0.14960360527038574 0.14960360527038574 0.15114913880825043 0.037245314568281174 0.5629682540893555 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001490488865185686\n",
            "964 Data 0.0014788038727097145 PDE 1.1574481504794676e-05 0.14960341155529022 0.14960341155529022 0.15114988386631012 0.03723641484975815 0.5629722476005554 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014903783542145092\n",
            "965 Data 0.0014787338010650319 PDE 1.153425455413526e-05 0.1496032178401947 0.1496032178401947 0.1511506289243698 0.03722750023007393 0.5629762411117554 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014902680556191671\n",
            "966 Data 0.0014786637785264463 PDE 1.1494168575154617e-05 0.14960302412509918 0.14960302412509918 0.1511513739824295 0.037218570709228516 0.5629802346229553 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001490157947101601\n",
            "967 Data 0.0014785938781854827 PDE 1.145421811088454e-05 0.14960283041000366 0.14960283041000366 0.1511521190404892 0.03720962628722191 0.5629841685295105 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014900480962963673\n",
            "968 Data 0.001478523991261367 PDE 1.1414407708798535e-05 0.14960263669490814 0.14960263669490814 0.1511528640985489 0.03720066696405411 0.5629881024360657 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014899383989701656\n",
            "969 Data 0.001478454247469181 PDE 1.1374723726476077e-05 0.14960244297981262 0.14960244297981262 0.15115360915660858 0.03719169273972511 0.5629920363426208 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001489828971195657\n",
            "970 Data 0.0014783845976553496 PDE 1.1335177077853587e-05 0.1496022492647171 0.1496022492647171 0.15115433931350708 0.037182703614234924 0.562995970249176 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014897197747332032\n",
            "971 Data 0.0014783150047597271 PDE 1.1295765034446958e-05 0.14960205554962158 0.14960205554962158 0.15115506947040558 0.03717369958758354 0.5629998445510864 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001489610769794174\n",
            "972 Data 0.0014782455282023512 PDE 1.1256482139287982e-05 0.14960186183452606 0.14960186183452606 0.15115579962730408 0.037164680659770966 0.5630037188529968 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014895020103416392\n",
            "973 Data 0.0014781760886005246 PDE 1.1217323844903149e-05 0.14960166811943054 0.14960166811943054 0.15115652978420258 0.037155646830797195 0.5630075931549072 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014893934124454277\n",
            "974 Data 0.0014781067616329686 PDE 1.1178293789271265e-05 0.14960147440433502 0.14960147440433502 0.15115725994110107 0.03714659810066223 0.5630114078521729 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014892850554222398\n",
            "975 Data 0.001478037506528406 PDE 1.1139410162286367e-05 0.1496012806892395 0.1496012806892395 0.15115799009799957 0.037137534469366074 0.5630152225494385 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014891769166906924\n",
            "976 Data 0.0014779683488289838 PDE 1.1100642950623296e-05 0.14960108697414398 0.14960108697414398 0.15115872025489807 0.03712845966219902 0.5630190372467041 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001489068991779607\n",
            "977 Data 0.0014778992563409894 PDE 1.106200670619728e-05 0.14960089325904846 0.14960089325904846 0.15115945041179657 0.03711936995387077 0.5630228519439697 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014889612630471867\n",
            "978 Data 0.0014778302069791542 PDE 1.1023520528397057e-05 0.14960069954395294 0.14960069954395294 0.15116016566753387 0.03711026534438133 0.5630266070365906 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014888537275075512\n",
            "979 Data 0.0014777613216055236 PDE 1.098514530895045e-05 0.14960050582885742 0.14960050582885742 0.15116088092327118 0.0371011458337307 0.5630303621292114 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001488746466914474\n",
            "980 Data 0.001477692466965063 PDE 1.094691742764553e-05 0.1496003121137619 0.1496003121137619 0.15116159617900848 0.03709201142191887 0.5630341172218323 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014886393843927086\n",
            "981 Data 0.0014776237057754747 PDE 1.0908811418630648e-05 0.14960011839866638 0.14960011839866638 0.1511623114347458 0.037082865834236145 0.5630378723144531 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014885325171941053\n",
            "982 Data 0.0014775550077695356 PDE 1.0870827281905804e-05 0.14959992468357086 0.14959992468357086 0.1511630266904831 0.03707370534539223 0.5630415678024292 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014884258350514414\n",
            "983 Data 0.0014774864333218895 PDE 1.0832977750396822e-05 0.14959973096847534 0.14959973096847534 0.1511637419462204 0.037064529955387115 0.5630452632904053 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014883194110722863\n",
            "984 Data 0.0014774179081213361 PDE 1.0795262824103702e-05 0.14959953725337982 0.14959953725337982 0.1511644572019577 0.03705534338951111 0.5630489587783813 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014882131709454398\n",
            "985 Data 0.0014773494363087719 PDE 1.0757670679595321e-05 0.1495993435382843 0.1495993435382843 0.151165172457695 0.03704614192247391 0.5630526542663574 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014881071069883672\n",
            "986 Data 0.0014772810782826959 PDE 1.0720212230808102e-05 0.14959914982318878 0.14959914982318878 0.15116587281227112 0.03703692555427551 0.5630562901496887 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001488001290513504\n",
            "987 Data 0.0014772128112111624 PDE 1.0682859283406287e-05 0.14959895610809326 0.14959895610809326 0.15116657316684723 0.03702769801020622 0.56305992603302 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014878956704945686\n",
            "988 Data 0.0014771445597329296 PDE 1.064565822161967e-05 0.14959876239299774 0.14959876239299774 0.15116727352142334 0.03701845556497574 0.5630635619163513 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014877902179545493\n",
            "989 Data 0.0014770765355291535 PDE 1.0608561751723755e-05 0.14959856867790222 0.14959856867790222 0.15116797387599945 0.03700920194387436 0.5630671381950378 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014876850972808772\n",
            "990 Data 0.0014770084589123156 PDE 1.0571598977549002e-05 0.1495983749628067 0.1495983749628067 0.15116867423057556 0.036999933421611786 0.5630707144737244 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014875800578898646\n",
            "991 Data 0.0014769405186831534 PDE 1.0534756256674882e-05 0.14959818124771118 0.14959818124771118 0.15116937458515167 0.03699065372347832 0.5630742907524109 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014874752749398283\n",
            "992 Data 0.0014768726421409508 PDE 1.049804541253252e-05 0.14959798753261566 0.14959798753261566 0.15117007493972778 0.036981359124183655 0.5630778670310974 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014873706875534833\n",
            "993 Data 0.0014768048313797254 PDE 1.046146917360602e-05 0.14959779381752014 0.14959779381752014 0.1511707752943039 0.0369720533490181 0.5630813837051392 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014872663005533315\n",
            "994 Data 0.0014767371182334767 PDE 1.0425010259496048e-05 0.14959760010242462 0.14959760010242462 0.15117147564888 0.036962732672691345 0.5630849003791809 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014871621284929728\n",
            "995 Data 0.0014766694794895864 PDE 1.0388678674644325e-05 0.1495974063873291 0.1495974063873291 0.15117216110229492 0.0369534008204937 0.5630884170532227 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014870581581642307\n",
            "996 Data 0.0014766018557097818 PDE 1.0352476238040254e-05 0.14959721267223358 0.14959721267223358 0.15117284655570984 0.03694405406713486 0.5630919337272644 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001486954331947822\n",
            "997 Data 0.0014765344514665686 PDE 1.0316379302821588e-05 0.14959701895713806 0.14959701895713806 0.15117353200912476 0.03693469613790512 0.5630953907966614 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014868508307693902\n",
            "998 Data 0.0014764669888452883 PDE 1.0280417882313486e-05 0.14959682524204254 0.14959682524204254 0.15117421746253967 0.03692532703280449 0.5630988478660583 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014867474067276018\n",
            "999 Data 0.0014763997120015994 PDE 1.024457742460072e-05 0.14959661662578583 0.14959661662578583 0.1511749029159546 0.036915943026542664 0.5631023049354553 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014866442894262\n",
            "1000 Data 0.0014763325031598971 PDE 1.0208861567662098e-05 0.1495964080095291 0.1495964080095291 0.1511755883693695 0.03690654784440994 0.5631057620048523 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014865413647275592\n",
            "1001 Data 0.0014762653037733225 PDE 1.0173270311497618e-05 0.1495961993932724 0.1495961993932724 0.15117627382278442 0.036897141486406326 0.5631092190742493 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014864385740848201\n",
            "1002 Data 0.0014761981704417024 PDE 1.0137801837117877e-05 0.14959599077701569 0.14959599077701569 0.15117695927619934 0.036887723952531815 0.5631126165390015 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014863359722788203\n",
            "1003 Data 0.0014761311765702045 PDE 1.010245341603877e-05 0.14959578216075897 0.14959578216075897 0.15117764472961426 0.03687829524278641 0.5631160140037537 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014862336299862433\n",
            "1004 Data 0.0014760642822360213 PDE 1.0067227776744403e-05 0.14959557354450226 0.14959557354450226 0.15117831528186798 0.036868851631879807 0.5631194114685059 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014861315100127657\n",
            "1005 Data 0.0014759974211520886 PDE 1.0032121281255968e-05 0.14959536492824554 0.14959536492824554 0.1511789858341217 0.03685939684510231 0.5631228089332581 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014860295424333446\n",
            "1006 Data 0.001475930581801985 PDE 9.99713665805757e-06 0.14959515631198883 0.14959515631198883 0.15117965638637543 0.03684993088245392 0.5631261467933655 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014859277184600425\n",
            "1007 Data 0.0014758639247899692 PDE 9.962268450181e-06 0.14959494769573212 0.14959494769573212 0.15118032693862915 0.03684045374393463 0.5631294846534729 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014858261932401502\n",
            "1008 Data 0.001475797326754429 PDE 9.927529390552081e-06 0.1495947390794754 0.1495947390794754 0.15118099749088287 0.03683096542954445 0.5631328225135803 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001485724856144981\n",
            "1009 Data 0.0014757307643364038 PDE 9.892904927255586e-06 0.1495945304632187 0.1495945304632187 0.1511816680431366 0.03682146593928337 0.5631361603736877 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014856236692636594\n",
            "1010 Data 0.0014756642514905313 PDE 9.858406883722637e-06 0.14959432184696198 0.14959432184696198 0.15118233859539032 0.0368119552731514 0.5631394386291504 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001485522658374254\n",
            "1011 Data 0.0014755978332694126 PDE 9.824010703596286e-06 0.14959411323070526 0.14959411323070526 0.15118300914764404 0.03680243343114853 0.563142716884613 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014854218439730089\n",
            "1012 Data 0.0014755315291727089 PDE 9.789742762222886e-06 0.14959390461444855 0.14959390461444855 0.15118367969989777 0.036792900413274765 0.5631459951400757 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014853212719349318\n",
            "1013 Data 0.0014754653210643062 PDE 9.755595783644821e-06 0.14959369599819183 0.14959369599819183 0.1511843353509903 0.036783356219530106 0.5631492733955383 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001485220916847951\n",
            "1014 Data 0.00147539916675787 PDE 9.721564310893882e-06 0.14959348738193512 0.14959348738193512 0.15118499100208282 0.03677380084991455 0.563152551651001 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014851207310687638\n",
            "1015 Data 0.0014753330829267099 PDE 9.687646524980664e-06 0.1495932787656784 0.1495932787656784 0.15118564665317535 0.0367642343044281 0.5631557703018188 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014850207294516905\n",
            "1016 Data 0.0014752670432518714 PDE 9.65384879236808e-06 0.1495930701494217 0.1495930701494217 0.15118630230426788 0.036754656583070755 0.5631589889526367 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014849208920442395\n",
            "1017 Data 0.0014752011145410836 PDE 9.620176570024341e-06 0.14959286153316498 0.14959286153316498 0.1511869579553604 0.036745067685842514 0.5631622076034546 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001484821291111108\n",
            "1018 Data 0.0014751352512271808 PDE 9.586613487044815e-06 0.14959265291690826 0.14959265291690826 0.15118761360645294 0.03673546761274338 0.5631654262542725 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014847218647142256\n",
            "1019 Data 0.0014750694863160342 PDE 9.55316409090301e-06 0.14959244430065155 0.14959244430065155 0.15118826925754547 0.036725856363773346 0.5631686449050903 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014846226504069372\n",
            "1020 Data 0.0014750037438322253 PDE 9.519838386040647e-06 0.14959223568439484 0.14959223568439484 0.151188924908638 0.03671623766422272 0.5631718039512634 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001484523582218266\n",
            "1021 Data 0.0014749381093441167 PDE 9.486631824984215e-06 0.14959202706813812 0.14959202706813812 0.15118958055973053 0.03670660778880119 0.5631749629974365 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001484424741169101\n",
            "1022 Data 0.0014748725747491447 PDE 9.45352076087147e-06 0.1495918184518814 0.1495918184518814 0.15119023621082306 0.036696966737508774 0.5631781220436096 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014843260955100161\n",
            "1023 Data 0.001474807126151002 PDE 9.420549758942798e-06 0.1495916098356247 0.1495916098356247 0.1511908769607544 0.03668731451034546 0.5631812810897827 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014842276759099448\n",
            "1024 Data 0.0014747416968399149 PDE 9.387693353346549e-06 0.14959140121936798 0.14959140121936798 0.15119151771068573 0.03667765483260155 0.5631844401359558 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014841293901932614\n",
            "1025 Data 0.0014746763708907795 PDE 9.354940630146302e-06 0.14959119260311127 0.14959119260311127 0.15119215846061707 0.03666798397898674 0.5631875395774841 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014840313115209258\n",
            "1026 Data 0.0014746110889016044 PDE 9.322300684289075e-06 0.14959098398685455 0.14959098398685455 0.1511927992105484 0.03665830194950104 0.5631906390190125 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014839333895858935\n",
            "1027 Data 0.0014745458955498602 PDE 9.289771696785465e-06 0.14959077537059784 0.14959077537059784 0.15119343996047974 0.03664861246943474 0.5631937384605408 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014838356672466457\n",
            "1028 Data 0.0014744808489426037 PDE 9.257366400561295e-06 0.14959056675434113 0.14959056675434113 0.15119408071041107 0.03663891181349754 0.5631968379020691 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001483738215343165\n",
            "1029 Data 0.0014744158238441014 PDE 9.22507570066955e-06 0.1495903581380844 0.1495903581380844 0.1511947214603424 0.03662919998168945 0.5631999373435974 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001483640899544771\n",
            "1030 Data 0.0014743507866084408 PDE 9.192887773679104e-06 0.1495901495218277 0.1495901495218277 0.15119536221027374 0.036619480699300766 0.563202977180481 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00148354367438212\n",
            "1031 Data 0.0014742859326650295 PDE 9.16081353352638e-06 0.14958994090557098 0.14958994090557098 0.15119600296020508 0.036609750241041183 0.5632060170173645 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014834467461985559\n",
            "1032 Data 0.0014742211416644915 PDE 9.128855708695482e-06 0.14958973228931427 0.14958973228931427 0.1511966437101364 0.036600012332201004 0.563209056854248 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001483349997373187\n",
            "1033 Data 0.0014741564221025464 PDE 9.0970079327235e-06 0.14958952367305756 0.14958952367305756 0.15119728446006775 0.03659026324748993 0.5632120966911316 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00148325343003527\n",
            "1034 Data 0.0014740917484703901 PDE 9.06526929611573e-06 0.14958931505680084 0.14958931505680084 0.1511979103088379 0.03658050671219826 0.5632151365280151 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014831570177665059\n",
            "1035 Data 0.001474027114299186 PDE 9.03364889381919e-06 0.14958910644054413 0.14958910644054413 0.15119853615760803 0.03657073900103569 0.5632181167602539 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014830607631930051\n",
            "1036 Data 0.0014739625663745004 PDE 9.002139449876267e-06 0.14958889782428741 0.14958889782428741 0.15119916200637817 0.036560963839292526 0.5632210969924927 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014829647058243766\n",
            "1037 Data 0.0014738981313922673 PDE 8.970746421255171e-06 0.1495886892080307 0.1495886892080307 0.15119978785514832 0.036551181226968765 0.5632240772247314 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014828688778135224\n",
            "1038 Data 0.0014738337727447646 PDE 8.939452527556568e-06 0.149588480591774 0.149588480591774 0.15120041370391846 0.03654138743877411 0.5632270574569702 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014827732252723211\n",
            "1039 Data 0.0014737694326426585 PDE 8.908270501706284e-06 0.14958827197551727 0.14958827197551727 0.1512010395526886 0.036531586199998856 0.563230037689209 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014826777031443647\n",
            "1040 Data 0.001473705249433718 PDE 8.877204891177826e-06 0.14958806335926056 0.14958806335926056 0.15120166540145874 0.03652177378535271 0.5632330179214478 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001482582454324896\n",
            "1041 Data 0.0014736410846075756 PDE 8.84624296304537e-06 0.14958785474300385 0.14958785474300385 0.15120229125022888 0.03651195392012596 0.5632359385490417 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001482487327570621\n",
            "1042 Data 0.0014735769926517497 PDE 8.815389264782425e-06 0.14958764612674713 0.14958764612674713 0.15120291709899902 0.03650212660431862 0.5632388591766357 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014823923819165322\n",
            "1043 Data 0.0014735129536067293 PDE 8.784639248915482e-06 0.14958743751049042 0.14958743751049042 0.15120354294776917 0.03649229183793068 0.5632417798042297 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014822975928556448\n",
            "1044 Data 0.001473449044893165 PDE 8.754006557865068e-06 0.1495872288942337 0.1495872288942337 0.1512041687965393 0.036482445895671844 0.5632447004318237 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014822030514510301\n",
            "1045 Data 0.001473385157500166 PDE 8.723482096684165e-06 0.149587020277977 0.149587020277977 0.15120479464530945 0.03647259250283241 0.5632476210594177 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00148210863959685\n",
            "1046 Data 0.0014733213601984478 PDE 8.69306950335158e-06 0.14958681166172028 0.14958681166172028 0.1512054055929184 0.036462731659412384 0.5632505416870117 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014820144297017993\n",
            "1047 Data 0.0014732576819410604 PDE 8.662755135446787e-06 0.14958660304546356 0.14958660304546356 0.15120601654052734 0.03645286336541176 0.5632534027099609 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014819204370765072\n",
            "1048 Data 0.0014731940635969122 PDE 8.63255718286382e-06 0.14958639442920685 0.14958639442920685 0.1512066274881363 0.03644298389554024 0.5632562637329102 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001481826620779776\n",
            "1049 Data 0.0014731304331455663 PDE 8.602461093687452e-06 0.14958618581295013 0.14958618581295013 0.15120723843574524 0.03643309697508812 0.5632591247558594 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014817328942392538\n",
            "1050 Data 0.0014730669247202744 PDE 8.572476872359402e-06 0.14958597719669342 0.14958597719669342 0.1512078493833542 0.036423202604055405 0.5632619857788086 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014816394015926338\n",
            "1051 Data 0.0014730035008599 PDE 8.542579053028021e-06 0.1495857685804367 0.1495857685804367 0.15120846033096313 0.03641330078244209 0.5632648468017578 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001481546079912928\n",
            "1052 Data 0.0014729401490331307 PDE 8.512801286997274e-06 0.14958555996418 0.14958555996418 0.15120907127857208 0.036403391510248184 0.563267707824707 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001481452950320128\n",
            "1053 Data 0.0014728768453429167 PDE 8.483129022351932e-06 0.14958535134792328 0.14958535134792328 0.15120968222618103 0.03639347478747368 0.5632705092430115 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014813599743652686\n",
            "1054 Data 0.0014728136105916084 PDE 8.453557711618487e-06 0.14958512783050537 0.14958512783050537 0.15121029317378998 0.036383550614118576 0.5632733106613159 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001481267168303227\n",
            "1055 Data 0.0014727504888392988 PDE 8.424090083281044e-06 0.14958490431308746 0.14958490431308746 0.15121090412139893 0.03637361899018288 0.5632761120796204 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014811745789225798\n",
            "1056 Data 0.0014726874409133875 PDE 8.394731594307814e-06 0.14958468079566956 0.14958468079566956 0.15121151506900787 0.03636367991566658 0.5632789134979248 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014810821725076953\n",
            "1057 Data 0.0014726244473216754 PDE 8.365479516214691e-06 0.14958445727825165 0.14958445727825165 0.15121212601661682 0.03635373339056969 0.5632817149162292 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014809899268378901\n",
            "1058 Data 0.0014725614622348483 PDE 8.336329301528167e-06 0.14958423376083374 0.14958423376083374 0.15121273696422577 0.0363437794148922 0.5632845163345337 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014808977915363765\n",
            "1059 Data 0.0014724986187473036 PDE 8.307282769237645e-06 0.14958401024341583 0.14958401024341583 0.15121333301067352 0.03633381798863411 0.5632872581481934 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014808059015165413\n",
            "1060 Data 0.0014724358013380437 PDE 8.278340828837827e-06 0.14958378672599792 0.14958378672599792 0.15121392905712128 0.036323849111795425 0.563289999961853 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014807141421668815\n",
            "1061 Data 0.0014723730590412679 PDE 8.249506208812818e-06 0.14958356320858002 0.14958356320858002 0.15121452510356903 0.036313872784376144 0.5632927417755127 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014806225652500807\n",
            "1062 Data 0.0014723104414747794 PDE 8.220767085731495e-06 0.1495833396911621 0.1495833396911621 0.15121512115001678 0.036303889006376266 0.5632954835891724 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014805312085605109\n",
            "1063 Data 0.0014722478418032433 PDE 8.192141649487894e-06 0.1495831161737442 0.1495831161737442 0.15121571719646454 0.03629390150308609 0.563298225402832 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014804399834527312\n",
            "1064 Data 0.0014721852981255338 PDE 8.163606253219768e-06 0.1495828926563263 0.1495828926563263 0.1512163132429123 0.03628390654921532 0.5633009672164917 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014803489043787536\n",
            "1065 Data 0.0014721228437393696 PDE 8.135186362778768e-06 0.1495826691389084 0.1495826691389084 0.15121690928936005 0.03627390414476395 0.5633037090301514 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014802580301021483\n",
            "1066 Data 0.0014720604048203115 PDE 8.10686560726026e-06 0.14958244562149048 0.14958244562149048 0.1512175053358078 0.03626389428973198 0.563306450843811 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014801672704275717\n",
            "1067 Data 0.0014719981425595123 PDE 8.078642167674843e-06 0.14958222210407257 0.14958222210407257 0.15121810138225555 0.036253876984119415 0.5633091330528259 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014800767847271872\n",
            "1068 Data 0.001471935918386849 PDE 8.05051331553841e-06 0.14958199858665466 0.14958199858665466 0.1512186974287033 0.03624385595321655 0.5633118152618408 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014799864317023874\n",
            "1069 Data 0.0014718737303345342 PDE 8.022491783776786e-06 0.14958177506923676 0.14958177506923676 0.15121929347515106 0.03623382747173309 0.5633144974708557 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001479896222118311\n",
            "1070 Data 0.0014718116410485525 PDE 7.994570296432357e-06 0.14958155155181885 0.14958155155181885 0.15121988952159882 0.03622379153966904 0.5633171796798706 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014798062113449849\n",
            "1071 Data 0.001471749577204548 PDE 7.966750672494527e-06 0.14958132803440094 0.14958132803440094 0.15122048556804657 0.03621375188231468 0.5633198618888855 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014797163278770426\n",
            "1072 Data 0.0014716876073661614 PDE 7.93903018347919e-06 0.14958110451698303 0.14958110451698303 0.15122108161449432 0.03620370477437973 0.5633225440979004 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014796266375496406\n",
            "1073 Data 0.0014716257376456105 PDE 7.911398824944627e-06 0.14958088099956512 0.14958088099956512 0.15122166275978088 0.03619365021586418 0.5633252263069153 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001479537136470555\n",
            "1074 Data 0.0014715638822007057 PDE 7.883877515268978e-06 0.14958065748214722 0.14958065748214722 0.15122224390506744 0.036183591932058334 0.5633278489112854 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014794477597159746\n",
            "1075 Data 0.0014715021278141737 PDE 7.85645443102112e-06 0.1495804339647293 0.1495804339647293 0.151222825050354 0.03617352619767189 0.5633304715156555 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014793585822451948\n",
            "1076 Data 0.001471440395179707 PDE 7.829128662706353e-06 0.1495802104473114 0.1495802104473114 0.15122340619564056 0.03616345673799515 0.5633330941200256 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014792695238424133\n",
            "1077 Data 0.0014713787983704784 PDE 7.80189293436706e-06 0.1495799869298935 0.1495799869298935 0.15122398734092712 0.03615337982773781 0.5633357167243958 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014791806913048455\n",
            "1078 Data 0.001471317202482015 PDE 7.774769073876087e-06 0.14957976341247559 0.14957976341247559 0.15122456848621368 0.03614329919219017 0.5633383393287659 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014790919715558912\n",
            "1079 Data 0.0014712557597237085 PDE 7.747731615381781e-06 0.14957953989505768 0.14957953989505768 0.15122514963150024 0.036133211106061935 0.563340961933136 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014790034913390903\n",
            "1080 Data 0.0014711943023790002 PDE 7.72079874877818e-06 0.14957931637763977 0.14957931637763977 0.1512257307767868 0.0361231192946434 0.5633435845375061 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014789151011277784\n",
            "1081 Data 0.0014711329528124177 PDE 7.69395410316065e-06 0.14957909286022186 0.14957909286022186 0.15122631192207336 0.03611302003264427 0.5633462071418762 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014788269069155784\n",
            "1082 Data 0.0014710717114024248 PDE 7.667220415896736e-06 0.14957886934280396 0.14957886934280396 0.15122689306735992 0.03610291704535484 0.5633488297462463 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014787389318183216\n",
            "1083 Data 0.0014710104590308533 PDE 7.640576768608298e-06 0.14957864582538605 0.14957864582538605 0.15122747421264648 0.03609280660748482 0.5633513927459717 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014786510357994616\n",
            "1084 Data 0.0014709492518086467 PDE 7.614034529979108e-06 0.14957842230796814 0.14957842230796814 0.15122805535793304 0.03608269244432449 0.563353955745697 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014785632863386258\n",
            "1085 Data 0.0014708882154780133 PDE 7.58758051233599e-06 0.14957819879055023 0.14957819879055023 0.1512286365032196 0.03607257455587387 0.5633565187454224 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014784757959903493\n",
            "1086 Data 0.0014708271745183879 PDE 7.561220172647154e-06 0.14957797527313232 0.14957797527313232 0.15122921764850616 0.03606244921684265 0.5633590817451477 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001478388394691035\n",
            "1087 Data 0.0014707662209602545 PDE 7.534959877375513e-06 0.14957775175571442 0.14957775175571442 0.15122979879379272 0.03605232015252113 0.563361644744873 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00147830118083763\n",
            "1088 Data 0.0014707053225123895 PDE 7.508791441068752e-06 0.1495775282382965 0.1495775282382965 0.15123037993907928 0.03604218736290932 0.5633642077445984 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014782141139534583\n",
            "1089 Data 0.0014706444755665734 PDE 7.482722139684483e-06 0.1495773047208786 0.1495773047208786 0.15123096108436584 0.036032047122716904 0.5633667707443237 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001478127197706258\n",
            "1090 Data 0.0014705837201653522 PDE 7.456748335243901e-06 0.1495770812034607 0.1495770812034607 0.1512315422296524 0.03602190315723419 0.5633693337440491 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001478040468500596\n",
            "1091 Data 0.0014705230235785565 PDE 7.430865935020847e-06 0.14957685768604279 0.14957685768604279 0.15123210847377777 0.03601175546646118 0.5633718967437744 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014779538895135773\n",
            "1092 Data 0.0014704623995837778 PDE 7.40507948648883e-06 0.14957663416862488 0.14957663416862488 0.15123267471790314 0.03600160405039787 0.5633744597434998 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014778674790702666\n",
            "1093 Data 0.0014704018422581667 PDE 7.379385806416394e-06 0.14957641065120697 0.14957641065120697 0.1512332409620285 0.03599144518375397 0.5633769631385803 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014777812280645831\n",
            "1094 Data 0.001470341339038233 PDE 7.3537826210667845e-06 0.14957618713378906 0.14957618713378906 0.15123380720615387 0.03598128259181976 0.5633794665336609 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014776951216592998\n",
            "1095 Data 0.001470280954041934 PDE 7.3282685661979485e-06 0.14957596361637115 0.14957596361637115 0.15123437345027924 0.03597111627459526 0.5633819699287415 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001477609222608132\n",
            "1096 Data 0.0014702205910805816 PDE 7.3028431870625354e-06 0.14957574009895325 0.14957574009895325 0.1512349396944046 0.03596094623208046 0.563384473323822 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001477523434267644\n",
            "1097 Data 0.0014701602430557809 PDE 7.277523764059879e-06 0.14957551658153534 0.14957551658153534 0.15123550593852997 0.03595077246427536 0.5633869767189026 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014774377668198408\n",
            "1098 Data 0.0014701000880479854 PDE 7.252281193359522e-06 0.14957529306411743 0.14957529306411743 0.15123607218265533 0.03594059497117996 0.5633894801139832 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014773523692413449\n",
            "1099 Data 0.0014700399024343432 PDE 7.2271386670763604e-06 0.14957506954669952 0.14957506954669952 0.1512366384267807 0.035930413752794266 0.5633919835090637 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014772670411014196\n",
            "1100 Data 0.0014699797856252133 PDE 7.202091182989534e-06 0.14957484602928162 0.14957484602928162 0.15123720467090607 0.03592022880911827 0.5633944869041443 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014771818768082028\n",
            "1101 Data 0.0014699197610498697 PDE 7.177131010394078e-06 0.1495746225118637 0.1495746225118637 0.15123777091503143 0.03591004014015198 0.5633969902992249 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014770968920602638\n",
            "1102 Data 0.0014698597551346677 PDE 7.152261787268799e-06 0.1495743989944458 0.1495743989944458 0.1512383371591568 0.035899847745895386 0.5633994936943054 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014770120169219365\n",
            "1103 Data 0.0014697998984600832 PDE 7.127484423108399e-06 0.1495741754770279 0.1495741754770279 0.15123890340328217 0.035889651626348495 0.563401997089386 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014769273828831916\n",
            "1104 Data 0.0014697399829084394 PDE 7.102801646396983e-06 0.14957395195960999 0.14957395195960999 0.15123946964740753 0.03587945178151131 0.5634045004844666 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014768427845548364\n",
            "1105 Data 0.0014696802947005386 PDE 7.078203907440184e-06 0.14957372844219208 0.14957372844219208 0.1512400358915329 0.03586924821138382 0.5634069442749023 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014767584986079788\n",
            "1106 Data 0.0014696205369809402 PDE 7.0537043939111754e-06 0.14957350492477417 0.14957350492477417 0.15124060213565826 0.035859040915966034 0.5634093880653381 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014766742413748514\n",
            "1107 Data 0.0014695608908000047 PDE 7.0292835516738705e-06 0.14957328140735626 0.14957328140735626 0.15124116837978363 0.03584882989525795 0.5634118318557739 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014765901743516786\n",
            "1108 Data 0.001469501304637607 PDE 7.004960480117006e-06 0.14957305788993835 0.14957305788993835 0.151241734623909 0.03583861514925957 0.5634142756462097 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001476506265117724\n",
            "1109 Data 0.0014694417949315823 PDE 6.9807192630833015e-06 0.14957283437252045 0.14957283437252045 0.15124230086803436 0.035828396677970886 0.5634167194366455 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014764225141946656\n",
            "1110 Data 0.0014693823049312534 PDE 6.956574452487985e-06 0.14957261085510254 0.14957261085510254 0.15124286711215973 0.03581817448139191 0.5634191632270813 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014763388793837414\n",
            "1111 Data 0.001469322918655423 PDE 6.9325074036896694e-06 0.14957238733768463 0.14957238733768463 0.1512434333562851 0.03580794855952263 0.5634216070175171 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014762554260591126\n",
            "1112 Data 0.0014692635452348778 PDE 6.908536761329742e-06 0.14957216382026672 0.14957216382026672 0.15124398469924927 0.03579771891236305 0.5634240508079529 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014761720819962076\n",
            "1113 Data 0.0014692043134137687 PDE 6.884655249450589e-06 0.14957194030284882 0.14957194030284882 0.15124453604221344 0.035787489265203476 0.5634264945983887 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014760889686632193\n",
            "1114 Data 0.0014691450789975243 PDE 6.8608642322942615e-06 0.1495717167854309 0.1495717167854309 0.1512450873851776 0.0357772558927536 0.5634289383888245 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014760059432298185\n",
            "1115 Data 0.0014690859282870084 PDE 6.8371487031981815e-06 0.149571493268013 0.149571493268013 0.15124563872814178 0.03576701879501343 0.5634313821792603 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014759230769902066\n",
            "1116 Data 0.0014690268258229354 PDE 6.813531854277244e-06 0.1495712697505951 0.1495712697505951 0.15124619007110596 0.035756777971982956 0.563433825969696 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014758403576772127\n",
            "1117 Data 0.0014689678031886066 PDE 6.7899936766480096e-06 0.14957104623317719 0.14957104623317719 0.15124674141407013 0.035746537148952484 0.5634362697601318 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014757577968652546\n",
            "1118 Data 0.0014689088950644387 PDE 6.766541901015444e-06 0.14957082271575928 0.14957082271575928 0.1512472927570343 0.035736292600631714 0.5634387135505676 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014756754369654541\n",
            "1119 Data 0.0014688500058160948 PDE 6.743181529600406e-06 0.14957059919834137 0.14957059919834137 0.15124784409999847 0.035726044327020645 0.5634411573410034 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014755931873456953\n",
            "1120 Data 0.001468791102246707 PDE 6.719905286445282e-06 0.14957037568092346 0.14957037568092346 0.15124839544296265 0.035715796053409576 0.5634436011314392 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014755110075331524\n",
            "1121 Data 0.0014687323991321622 PDE 6.69671680952888e-06 0.14957015216350555 0.14957015216350555 0.15124894678592682 0.03570554405450821 0.5634459853172302 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001475429115941691\n",
            "1122 Data 0.0014686736781690027 PDE 6.6736115513776895e-06 0.14956992864608765 0.14956992864608765 0.151249498128891 0.035695288330316544 0.5634483695030212 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014753472897203804\n",
            "1123 Data 0.0014686150232381856 PDE 6.6505931499705184e-06 0.14956970512866974 0.14956970512866974 0.15125004947185516 0.03568503260612488 0.5634507536888123 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014752656163881561\n",
            "1124 Data 0.0014685564638501861 PDE 6.627655238844454e-06 0.14956948161125183 0.14956948161125183 0.15125060081481934 0.035674773156642914 0.5634531378746033 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014751841190890306\n",
            "1125 Data 0.0014684979066934929 PDE 6.604806912946515e-06 0.14956925809383392 0.14956925809383392 0.1512511521577835 0.03566451370716095 0.5634555220603943 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014751027136064394\n",
            "1126 Data 0.0014684394723829565 PDE 6.582040896319086e-06 0.14956903457641602 0.14956903457641602 0.15125170350074768 0.03565425053238869 0.5634579062461853 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014750215132792756\n",
            "1127 Data 0.0014683810535533364 PDE 6.559360826940974e-06 0.1495688110589981 0.1495688110589981 0.15125225484371185 0.035643983632326126 0.5634602904319763 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014749404143802774\n",
            "1128 Data 0.00146832274315353 PDE 6.536763066833373e-06 0.1495685875415802 0.1495685875415802 0.15125280618667603 0.035633716732263565 0.5634626746177673 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014748595062203634\n",
            "1129 Data 0.0014682644636096654 PDE 6.514248980238335e-06 0.1495683640241623 0.1495683640241623 0.1512533575296402 0.035623446106910706 0.5634650588035583 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014747787125899037\n",
            "1130 Data 0.0014682062470267301 PDE 6.491817202913808e-06 0.14956814050674438 0.14956814050674438 0.15125390887260437 0.035613175481557846 0.5634674429893494 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001474698064229644\n",
            "1131 Data 0.0014681480751014345 PDE 6.469462732638931e-06 0.14956791698932648 0.14956791698932648 0.15125446021556854 0.03560290485620499 0.5634698271751404 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014746175378340734\n",
            "1132 Data 0.00146809000294723 PDE 6.447197847592179e-06 0.14956769347190857 0.14956769347190857 0.15125501155853271 0.03559263050556183 0.5634722113609314 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014745372007948221\n",
            "1133 Data 0.0014680319561343662 PDE 6.425010269595077e-06 0.14956746995449066 0.14956746995449066 0.1512555629014969 0.03558235615491867 0.5634745955467224 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014744569664039613\n",
            "1134 Data 0.0014679740085430293 PDE 6.402902727131732e-06 0.14956724643707275 0.14956724643707275 0.15125611424446106 0.035572078078985214 0.5634769797325134 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001474376911270161\n",
            "1135 Data 0.0014679161111256548 PDE 6.3808793129283e-06 0.14956702291965485 0.14956702291965485 0.15125666558742523 0.03556180000305176 0.5634793639183044 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001474296990438583\n",
            "1136 Data 0.0014678582381627181 PDE 6.358945029205643e-06 0.14956679940223694 0.14956679940223694 0.1512572169303894 0.0355515219271183 0.5634817481040955 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014742171831919238\n",
            "1137 Data 0.001467800425315983 PDE 6.33708532404853e-06 0.14956657588481903 0.14956657588481903 0.15125776827335358 0.035541240125894547 0.5634841322898865 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014741375106400315\n",
            "1138 Data 0.0014677427143360985 PDE 6.315310201898683e-06 0.14956635236740112 0.14956635236740112 0.15125831961631775 0.03553095832467079 0.5634865164756775 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014740580245379972\n",
            "1139 Data 0.0014676850410842313 PDE 6.293612841545837e-06 0.14956612884998322 0.14956612884998322 0.15125887095928192 0.03552067652344704 0.5634889006614685 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014739786539257772\n",
            "1140 Data 0.0014676274836817494 PDE 6.271993697737344e-06 0.1495659053325653 0.1495659053325653 0.1512594223022461 0.03551039099693298 0.5634912848472595 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014738994773794867\n",
            "1141 Data 0.0014675698573443311 PDE 6.250460501178168e-06 0.1495656818151474 0.1495656818151474 0.15125997364521027 0.03550010547041893 0.5634936690330505 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014738203178455093\n",
            "1142 Data 0.0014675124153051258 PDE 6.228997790458379e-06 0.1495654582977295 0.1495654582977295 0.15126052498817444 0.03548981994390488 0.5634960532188416 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014737414130955842\n",
            "1143 Data 0.0014674549506640802 PDE 6.20762693870347e-06 0.14956523478031158 0.14956523478031158 0.1512610763311386 0.03547953441739082 0.5634984374046326 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014736625776027837\n",
            "1144 Data 0.00146739765078637 PDE 6.186324753798544e-06 0.14956501126289368 0.14956501126289368 0.15126162767410278 0.03546924516558647 0.5635008215904236 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014735839755401685\n",
            "1145 Data 0.001467340283549585 PDE 6.165114427858498e-06 0.14956478774547577 0.14956478774547577 0.15126216411590576 0.03545895591378212 0.5635032057762146 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014735053979774435\n",
            "1146 Data 0.0014672830352855917 PDE 6.143979589978699e-06 0.14956456422805786 0.14956456422805786 0.15126270055770874 0.03544866666197777 0.5635055899620056 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014734270148755704\n",
            "1147 Data 0.0014672258509107734 PDE 6.122913418948883e-06 0.14956434071063995 0.14956434071063995 0.15126323699951172 0.035438377410173416 0.5635079741477966 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014733487643297223\n",
            "1148 Data 0.0014671686869356603 PDE 6.1019245549687184e-06 0.14956411719322205 0.14956411719322205 0.1512637734413147 0.035428088158369064 0.5635103583335876 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001473270611490629\n",
            "1149 Data 0.0014671116384211418 PDE 6.081025276216678e-06 0.14956389367580414 0.14956389367580414 0.15126430988311768 0.03541779890656471 0.5635127425193787 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014731926636973585\n",
            "1150 Data 0.0014670546147783705 PDE 6.060206487745745e-06 0.14956367015838623 0.14956367015838623 0.15126484632492065 0.03540750592947006 0.5635151267051697 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014731148212661163\n",
            "1151 Data 0.0014669976621576296 PDE 6.039464096829761e-06 0.14956344664096832 0.14956344664096832 0.15126538276672363 0.03539721295237541 0.5635175108909607 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014730371262544593\n",
            "1152 Data 0.0014669407989991435 PDE 6.018790827511111e-06 0.14956322312355042 0.14956322312355042 0.1512659192085266 0.03538691997528076 0.5635198950767517 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014729595898266546\n",
            "1153 Data 0.0014668839381860456 PDE 5.998205324431183e-06 0.1495629996061325 0.1495629996061325 0.1512664556503296 0.03537662699818611 0.5635222792625427 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014728821435104767\n",
            "1154 Data 0.001466827113584756 PDE 5.977688488201238e-06 0.1495627760887146 0.1495627760887146 0.15126699209213257 0.03536633402109146 0.5635246634483337 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014728048020729572\n",
            "1155 Data 0.0014667703932902878 PDE 5.9572439568000846e-06 0.1495625525712967 0.1495625525712967 0.15126752853393555 0.03535604104399681 0.5635270476341248 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014727276372470878\n",
            "1156 Data 0.0014667137606088805 PDE 5.936879460932687e-06 0.14956232905387878 0.14956232905387878 0.15126806497573853 0.03534574806690216 0.5635294318199158 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014726506400698132\n",
            "1157 Data 0.0014666571474730558 PDE 5.916589088883484e-06 0.14956210553646088 0.14956210553646088 0.1512686014175415 0.03533545508980751 0.5635318160057068 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014725737365619393\n",
            "1158 Data 0.001466600607502343 PDE 5.896376478631282e-06 0.14956188201904297 0.14956188201904297 0.15126913785934448 0.03532516211271286 0.5635342001914978 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014724969839809742\n",
            "1159 Data 0.0014665440651435346 PDE 5.876247996638995e-06 0.14956165850162506 0.14956165850162506 0.15126967430114746 0.03531486913561821 0.5635365843772888 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014724203131401736\n",
            "1160 Data 0.0014664876522700813 PDE 5.856186817254638e-06 0.14956143498420715 0.14956143498420715 0.15127021074295044 0.03530457615852356 0.5635389685630798 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001472343839087336\n",
            "1161 Data 0.0014664312753003951 PDE 5.836210220877547e-06 0.14956121146678925 0.14956121146678925 0.15127074718475342 0.03529428318142891 0.5635413527488708 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014722674855212727\n",
            "1162 Data 0.0014663749216720244 PDE 5.816298653371632e-06 0.14956098794937134 0.14956098794937134 0.1512712836265564 0.03528399392962456 0.5635437369346619 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001472191220325396\n",
            "1163 Data 0.0014663186842293334 PDE 5.79646030018921e-06 0.14956076443195343 0.14956076443195343 0.15127182006835938 0.035273704677820206 0.5635461211204529 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014721151445295227\n",
            "1164 Data 0.0014662624758004605 PDE 5.776702892035246e-06 0.14956054091453552 0.14956054091453552 0.15127235651016235 0.035263415426015854 0.5635485053062439 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014720391786924958\n",
            "1165 Data 0.0014662062995696515 PDE 5.757024609920336e-06 0.14956031739711761 0.14956031739711761 0.15127289295196533 0.0352531261742115 0.5635508894920349 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014719633241795718\n",
            "1166 Data 0.0014661501889756976 PDE 5.737407263950445e-06 0.1495600938796997 0.1495600938796997 0.1512734293937683 0.03524283692240715 0.5635532736778259 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001471887596239648\n",
            "1167 Data 0.0014660942089603614 PDE 5.7178663155355025e-06 0.1495598703622818 0.1495598703622818 0.1512739658355713 0.0352325476706028 0.5635556578636169 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001471812075275897\n",
            "1168 Data 0.0014660382120739734 PDE 5.6984004004334565e-06 0.1495596468448639 0.1495596468448639 0.15127450227737427 0.035222262144088745 0.563558042049408 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014717366124744069\n",
            "1169 Data 0.00146598228208353 PDE 5.67900087844464e-06 0.14955942332744598 0.14955942332744598 0.15127503871917725 0.03521197661757469 0.563560426235199 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014716612829619746\n",
            "1170 Data 0.0014659264313468309 PDE 5.659673661284614e-06 0.14955919981002808 0.14955919981002808 0.15127557516098022 0.03520169109106064 0.56356281042099 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014715861050081155\n",
            "1171 Data 0.0014658705934146891 PDE 5.640425115416292e-06 0.14955897629261017 0.14955897629261017 0.1512761116027832 0.035191405564546585 0.563565194606781 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014715110185301054\n",
            "1172 Data 0.00146581482443293 PDE 5.621245691145305e-06 0.14955875277519226 0.14955875277519226 0.15127664804458618 0.03518112376332283 0.563567578792572 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014714360701240754\n",
            "1173 Data 0.001465759186298811 PDE 5.602142209681915e-06 0.14955852925777435 0.14955852925777435 0.15127718448638916 0.035170841962099075 0.563569962978363 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001471361328508493\n",
            "1174 Data 0.001465703498566722 PDE 5.58310784981586e-06 0.14955830574035645 0.14955830574035645 0.15127772092819214 0.03516056016087532 0.563572347164154 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001471286606416538\n",
            "1175 Data 0.0014656479188477367 PDE 5.564144430536544e-06 0.14955808222293854 0.14955808222293854 0.15127825736999512 0.035150278359651566 0.5635747313499451 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014712120632782732\n",
            "1176 Data 0.0014655923784532346 PDE 5.545260137296282e-06 0.14955785870552063 0.14955785870552063 0.1512787938117981 0.03514000028371811 0.5635771155357361 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001471137638590531\n",
            "1177 Data 0.001465536928323745 PDE 5.52643768969574e-06 0.14955763518810272 0.14955763518810272 0.15127933025360107 0.03512972220778465 0.5635794997215271 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014710633660134408\n",
            "1178 Data 0.0014654814749806976 PDE 5.5076952776289545e-06 0.14955741167068481 0.14955741167068481 0.15127986669540405 0.035119447857141495 0.5635818839073181 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014709891702583266\n",
            "1179 Data 0.0014654261109214087 PDE 5.4890183491806965e-06 0.1495571881532669 0.1495571881532669 0.15128040313720703 0.03510917350649834 0.5635842680931091 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014709151292705894\n",
            "1180 Data 0.001465370825678275 PDE 5.47040872334037e-06 0.149556964635849 0.149556964635849 0.15128093957901 0.03509889915585518 0.5635866522789001 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014708412344016154\n",
            "1181 Data 0.0014653155571430164 PDE 5.451863671623869e-06 0.1495567411184311 0.1495567411184311 0.151281476020813 0.03508862853050232 0.5635890364646912 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014707674208146403\n",
            "1182 Data 0.0014652603642721733 PDE 5.433394562714966e-06 0.14955651760101318 0.14955651760101318 0.15128201246261597 0.03507835790514946 0.5635914206504822 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014706937588348883\n",
            "1183 Data 0.00146520523498329 PDE 5.415001396613661e-06 0.14955629408359528 0.14955629408359528 0.15128254890441895 0.0350680910050869 0.5635938048362732 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014706202363799037\n",
            "1184 Data 0.0014651501133381218 PDE 5.396672349888831e-06 0.14955607056617737 0.14955607056617737 0.15128308534622192 0.03505782410502434 0.5635961890220642 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014705467856880106\n",
            "1185 Data 0.0014650951087312823 PDE 5.378416062740143e-06 0.14955584704875946 0.14955584704875946 0.1512836217880249 0.035047560930252075 0.5635985732078552 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014704735247940224\n",
            "1186 Data 0.0014650401595673978 PDE 5.3602275329467375e-06 0.14955562353134155 0.14955562353134155 0.15128415822982788 0.03503729775547981 0.5636009573936462 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014704003871003446\n",
            "1187 Data 0.0014649851861958762 PDE 5.342105396266561e-06 0.14955540001392365 0.14955540001392365 0.15128469467163086 0.03502703830599785 0.5636033415794373 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014703272915921428\n",
            "1188 Data 0.0014649303035027667 PDE 5.324048288457561e-06 0.14955517649650574 0.14955517649650574 0.15128523111343384 0.035016778856515884 0.5636057257652283 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014702543517912242\n",
            "1189 Data 0.0014648755085027133 PDE 5.306065759214107e-06 0.14955495297908783 0.14955495297908783 0.15128576755523682 0.03500652313232422 0.5636081099510193 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014701815742619274\n",
            "1190 Data 0.0014648207255109204 PDE 5.288151442073286e-06 0.14955472946166992 0.14955472946166992 0.1512863039970398 0.03499626740813255 0.5636104941368103 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014701088769529936\n",
            "1191 Data 0.0014647660061630193 PDE 5.27030670127715e-06 0.14955450594425201 0.14955450594425201 0.15128684043884277 0.034986015409231186 0.5636128783226013 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014700363128642964\n",
            "1192 Data 0.0014647113748695443 PDE 5.252530627330998e-06 0.1495542824268341 0.1495542824268341 0.15128737688064575 0.03497576713562012 0.5636152625083923 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014699639054968753\n",
            "1193 Data 0.00146465679888093 PDE 5.234817308519268e-06 0.1495540589094162 0.1495540589094162 0.15128791332244873 0.03496551886200905 0.5636177062988281 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014698916161894494\n",
            "1194 Data 0.0014646022055983842 PDE 5.2171812967571896e-06 0.1495538353919983 0.1495538353919983 0.1512884497642517 0.03495527431368828 0.5636201500892639 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014698193868951414\n",
            "1195 Data 0.0014645477385270345 PDE 5.199602583161322e-06 0.14955361187458038 0.14955361187458038 0.1512889862060547 0.034945033490657806 0.5636225938796997 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014697473411101958\n",
            "1196 Data 0.001464493271807153 PDE 5.182083896215772e-06 0.14955338835716248 0.14955338835716248 0.15128952264785767 0.034934792667627335 0.5636250376701355 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014696753557033688\n",
            "1197 Data 0.0014644388701629688 PDE 5.164640242583118e-06 0.14955316483974457 0.14955316483974457 0.15129005908966064 0.03492455556988716 0.5636274814605713 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001469603510405552\n",
            "1198 Data 0.001464384560189798 PDE 5.147262527316343e-06 0.14955294132232666 0.14955294132232666 0.15129059553146362 0.034914322197437286 0.5636299252510071 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014695318227171143\n",
            "1199 Data 0.0014643302688158808 PDE 5.129944383952534e-06 0.14955271780490875 0.14955271780490875 0.1512911319732666 0.03490408882498741 0.5636323690414429 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014694602131998333\n",
            "1200 Data 0.0014642760588676376 PDE 5.112693088449305e-06 0.14955249428749084 0.14955249428749084 0.15129166841506958 0.034893859177827835 0.5636348128318787 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014693887519560869\n",
            "1201 Data 0.0014642218766042076 PDE 5.095502274343744e-06 0.14955227077007294 0.14955227077007294 0.15129220485687256 0.03488363325595856 0.5636372566223145 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014693173788785513\n",
            "1202 Data 0.0014641677842183551 PDE 5.078390131529886e-06 0.14955204725265503 0.14955204725265503 0.15129274129867554 0.03487341105937958 0.5636397004127502 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001469246174349885\n",
            "1203 Data 0.0014641136528602597 PDE 5.0613343773875386e-06 0.14955182373523712 0.14955182373523712 0.15129327774047852 0.0348631888628006 0.563642144203186 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014691749872376473\n",
            "1204 Data 0.0014640596821337416 PDE 5.044338195148157e-06 0.1495516002178192 0.1495516002178192 0.1512938141822815 0.03485297039151192 0.5636445879936218 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014691040203288898\n",
            "1205 Data 0.0014640056937898183 PDE 5.0274134082428645e-06 0.1495513767004013 0.1495513767004013 0.15129435062408447 0.034842755645513535 0.5636470317840576 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014690331071980612\n",
            "1206 Data 0.0014639518025733672 PDE 5.010556378692854e-06 0.1495511531829834 0.1495511531829834 0.15129488706588745 0.03483254462480545 0.5636494755744934 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00146896235895206\n",
            "1207 Data 0.001463897878478084 PDE 4.993755737814354e-06 0.1495509296655655 0.1495509296655655 0.15129542350769043 0.034822337329387665 0.5636519193649292 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014688916342158983\n",
            "1208 Data 0.0014638441157501536 PDE 4.977018306817627e-06 0.14955070614814758 0.14955070614814758 0.1512959599494934 0.03481213375926018 0.563654363155365 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014688211340569712\n",
            "1209 Data 0.0014637903387106296 PDE 4.96035318064969e-06 0.14955048263072968 0.14955048263072968 0.1512964963912964 0.03480193018913269 0.5636568665504456 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014687506918912793\n",
            "1210 Data 0.001463736673609168 PDE 4.943754902342334e-06 0.14955025911331177 0.14955025911331177 0.15129703283309937 0.0347917303442955 0.5636593699455261 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014686804285115103\n",
            "1211 Data 0.0014636830066823178 PDE 4.92720891998033e-06 0.14955003559589386 0.14955003559589386 0.15129756927490234 0.03478153422474861 0.5636618733406067 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014686102156022981\n",
            "1212 Data 0.0014636293549552485 PDE 4.910733878205065e-06 0.14954981207847595 0.14954981207847595 0.15129810571670532 0.03477134183049202 0.5636643767356873 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014685400888334536\n",
            "1213 Data 0.0014635758508500785 PDE 4.894311132375151e-06 0.14954958856105804 0.14954958856105804 0.1512986421585083 0.034761153161525726 0.5636668801307678 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014684701619824536\n",
            "1214 Data 0.001463522331549477 PDE 4.877958417637274e-06 0.14954936504364014 0.14954936504364014 0.15129917860031128 0.03475096821784973 0.5636693835258484 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014684002899671143\n",
            "1215 Data 0.0014634688849193174 PDE 4.861662091570906e-06 0.14954914152622223 0.14954914152622223 0.15129971504211426 0.034740786999464035 0.563671886920929 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014683305470108883\n",
            "1216 Data 0.0014634155158352207 PDE 4.845425792154856e-06 0.14954891800880432 0.14954891800880432 0.15130025148391724 0.03473060950636864 0.5636743903160095 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014682609416273756\n",
            "1217 Data 0.0014633621365262174 PDE 4.829260888072895e-06 0.1495486944913864 0.1495486944913864 0.15130078792572021 0.03472043573856354 0.5636768937110901 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014681913974142903\n",
            "1218 Data 0.0014633088655454521 PDE 4.813158284378005e-06 0.1495484709739685 0.1495484709739685 0.1513013243675232 0.03471026569604874 0.5636793971061707 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014681220238298301\n",
            "1219 Data 0.0014632556027584587 PDE 4.797106612386415e-06 0.1495482474565506 0.1495482474565506 0.15130186080932617 0.034700099378824234 0.5636819005012512 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014680527093708451\n",
            "1220 Data 0.001463202372946525 PDE 4.781117695529247e-06 0.1495480239391327 0.1495480239391327 0.15130239725112915 0.03468993678689003 0.5636844038963318 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014679834906420543\n",
            "1221 Data 0.0014631493096565044 PDE 4.765199264511466e-06 0.14954780042171478 0.14954780042171478 0.15130294859409332 0.034679777920246124 0.5636869668960571 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014679145089210159\n",
            "1222 Data 0.0014630961635421348 PDE 4.749339041154599e-06 0.14954757690429688 0.14954757690429688 0.1513034999370575 0.03466962277889252 0.5636895298957825 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014678455025832894\n",
            "1223 Data 0.001463043093328132 PDE 4.733535661216592e-06 0.14954735338687897 0.14954735338687897 0.15130405128002167 0.03465947136282921 0.5636920928955078 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014677766289893485\n",
            "1224 Data 0.001462990169058219 PDE 4.717792762676254e-06 0.14954712986946106 0.14954712986946106 0.15130460262298584 0.0346493236720562 0.5636946558952332 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014677079618208952\n",
            "1225 Data 0.001462937178301739 PDE 4.702116711996496e-06 0.14954690635204315 0.14954690635204315 0.15130515396595 0.034639183431863785 0.5636972188949585 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014676392950137354\n",
            "1226 Data 0.0014628843155375052 PDE 4.686492047767388e-06 0.14954668283462524 0.14954668283462524 0.15130570530891418 0.03462904691696167 0.5636997818946838 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014675708075852726\n",
            "1227 Data 0.0014628314246774938 PDE 4.670927410188597e-06 0.14954645931720734 0.14954645931720734 0.15130625665187836 0.034618914127349854 0.5637023448944092 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014675023520876824\n",
            "1228 Data 0.0014627786447592333 PDE 4.655421889765421e-06 0.14954623579978943 0.14954623579978943 0.15130680799484253 0.034608785063028336 0.5637049078941345 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014674340666489987\n",
            "1229 Data 0.0014627259220353372 PDE 4.639982307708124e-06 0.14954601228237152 0.14954601228237152 0.1513073593378067 0.034598659723997116 0.5637074708938599 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014673659043430453\n",
            "1230 Data 0.0014626731952138193 PDE 4.6246027523011435e-06 0.1495457887649536 0.1495457887649536 0.15130791068077087 0.034588538110256195 0.5637100338935852 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014672977979661205\n",
            "1231 Data 0.0014626205516586705 PDE 4.609280949807726e-06 0.1495455652475357 0.1495455652475357 0.15130846202373505 0.03457842394709587 0.5637125968933105 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014672298326084783\n",
            "1232 Data 0.001462567947626795 PDE 4.5940100790176075e-06 0.1495453417301178 0.1495453417301178 0.15130901336669922 0.034568313509225845 0.5637152194976807 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014671619577058127\n",
            "1233 Data 0.001462515404333625 PDE 4.578802418109262e-06 0.1495451182126999 0.1495451182126999 0.1513095647096634 0.03455820679664612 0.5637178421020508 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014670942067517342\n",
            "1234 Data 0.0014624628807273525 PDE 4.563648417388322e-06 0.14954489469528198 0.14954489469528198 0.15131011605262756 0.03454810380935669 0.5637204647064209 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014670265291447408\n",
            "1235 Data 0.0014624104294833488 PDE 4.548554443317698e-06 0.14954467117786407 0.14954467117786407 0.15131066739559174 0.03453800454735756 0.563723087310791 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014669589839266665\n",
            "1236 Data 0.0014623580866273684 PDE 4.5335177674132865e-06 0.14954444766044617 0.14954444766044617 0.1513112187385559 0.034527912735939026 0.5637257099151611 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014668916043947817\n",
            "1237 Data 0.00146230571873544 PDE 4.51854020866449e-06 0.14954422414302826 0.14954422414302826 0.15131177008152008 0.03451782464981079 0.5637283325195312 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014668242589441045\n",
            "1238 Data 0.0014622533891980637 PDE 4.503619038587203e-06 0.14954400062561035 0.14954400062561035 0.15131232142448425 0.034507740288972855 0.5637309551239014 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001466757008236651\n",
            "1239 Data 0.0014622011339363728 PDE 4.488755166676128e-06 0.14954377710819244 0.14954377710819244 0.15131287276744843 0.034497663378715515 0.5637335777282715 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001466689889103049\n",
            "1240 Data 0.0014621489446240404 PDE 4.473943590710405e-06 0.14954355359077454 0.14954355359077454 0.1513134241104126 0.034487590193748474 0.5637362003326416 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014666228882147508\n",
            "1241 Data 0.0014620967852649897 PDE 4.459196588868508e-06 0.14954333007335663 0.14954333007335663 0.15131397545337677 0.03447752073407173 0.5637388825416565 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014665559818538582\n",
            "1242 Data 0.001462044681843688 PDE 4.444504611456068e-06 0.14954310655593872 0.14954310655593872 0.15131452679634094 0.034467458724975586 0.5637415647506714 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014664891864551441\n",
            "1243 Data 0.0014619926254508635 PDE 4.429862201504875e-06 0.1495428830385208 0.1495428830385208 0.15131507813930511 0.03445740044116974 0.5637442469596863 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014664224876523684\n",
            "1244 Data 0.001461940585222623 PDE 4.41528027295135e-06 0.1495426595211029 0.1495426595211029 0.1513156294822693 0.03444734588265419 0.5637469291687012 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014663558654955744\n",
            "1245 Data 0.0014618886404138762 PDE 4.400754733069334e-06 0.149542436003685 0.149542436003685 0.15131618082523346 0.03443729877471924 0.5637496113777161 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014662893951469455\n",
            "1246 Data 0.0014618367252086858 PDE 4.386287400848232e-06 0.1495422124862671 0.1495422124862671 0.15131673216819763 0.034427255392074585 0.563752293586731 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001466223012609534\n",
            "1247 Data 0.0014617847941349109 PDE 4.3718710003304295e-06 0.14954198896884918 0.14954198896884918 0.1513172835111618 0.03441721946001053 0.5637549757957458 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014661566651352413\n",
            "1248 Data 0.0014617330242473919 PDE 4.3575082600000314e-06 0.14954176545143127 0.14954176545143127 0.15131783485412598 0.03440718725323677 0.5637576580047607 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014660905325073919\n",
            "1249 Data 0.0014616812663109922 PDE 4.343206001067301e-06 0.14954154193401337 0.14954154193401337 0.15131838619709015 0.03439716249704361 0.5637603998184204 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014660244723120595\n",
            "1250 Data 0.0014616295556138326 PDE 4.328951945353765e-06 0.14954131841659546 0.14954131841659546 0.15131893754005432 0.03438714146614075 0.5637631416320801 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014659585075591864\n",
            "1251 Data 0.001461577859097835 PDE 4.314757006795844e-06 0.14954109489917755 0.14954109489917755 0.1513194888830185 0.03437712788581848 0.5637658834457397 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014658926161046307\n",
            "1252 Data 0.001461526218563865 PDE 4.300618456909433e-06 0.14954087138175964 0.14954087138175964 0.15132004022598267 0.034367118030786514 0.5637686252593994 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014658268370207745\n",
            "1253 Data 0.0014614745890548936 PDE 4.286532657715725e-06 0.14954066276550293 0.14954066276550293 0.15132059156894684 0.034357115626335144 0.5637713670730591 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014657611217126093\n",
            "1254 Data 0.0014614230311423512 PDE 4.2724991544673685e-06 0.14954045414924622 0.14954045414924622 0.151321142911911 0.03434711694717407 0.5637741088867188 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014656955302968185\n",
            "1255 Data 0.0014613715741572247 PDE 4.258522039890522e-06 0.1495402455329895 0.1495402455329895 0.15132169425487518 0.0343371257185936 0.5637768507003784 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014656300961971153\n",
            "1256 Data 0.001461320137907938 PDE 4.244591309543466e-06 0.1495400369167328 0.1495400369167328 0.15132224559783936 0.03432713821530342 0.5637795925140381 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014655647292174815\n",
            "1257 Data 0.0014612687352518338 PDE 4.230720151099376e-06 0.14953982830047607 0.14953982830047607 0.15132281184196472 0.03431715816259384 0.5637823939323425 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014654994554029331\n",
            "1258 Data 0.0014612173418696174 PDE 4.2169053813267965e-06 0.14953961968421936 0.14953961968421936 0.1513233780860901 0.03430718183517456 0.563785195350647 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014654342472509442\n",
            "1259 Data 0.0014611660570019284 PDE 4.203141543257516e-06 0.14953941106796265 0.14953941106796265 0.15132394433021545 0.034297212958335876 0.5637879967689514 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001465369198545186\n",
            "1260 Data 0.0014611148032198232 PDE 4.189427727396833e-06 0.14953920245170593 0.14953920245170593 0.15132451057434082 0.03428724780678749 0.5637907981872559 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00146530423094722\n",
            "1261 Data 0.0014610635257957863 PDE 4.17577575717587e-06 0.14953899383544922 0.14953899383544922 0.1513250768184662 0.0342772901058197 0.5637935996055603 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014652393015529622\n",
            "1262 Data 0.0014610123911467953 PDE 4.162169261689996e-06 0.1495387852191925 0.1495387852191925 0.15132564306259155 0.03426733985543251 0.5637964010238647 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014651745604084853\n",
            "1263 Data 0.001460961262218305 PDE 4.148610969423316e-06 0.1495385766029358 0.1495385766029358 0.15132620930671692 0.03425739333033562 0.5637992024421692 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014651098731877283\n",
            "1264 Data 0.001460910215458785 PDE 4.135109065828146e-06 0.14953836798667908 0.14953836798667908 0.15132677555084229 0.03424745425581932 0.5638020634651184 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014650453245246132\n",
            "1265 Data 0.00146085914239655 PDE 4.121654455957469e-06 0.14953815937042236 0.14953815937042236 0.15132734179496765 0.03423752263188362 0.5638049244880676 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014649807968525074\n",
            "1266 Data 0.0014608082212653056 PDE 4.108259872737108e-06 0.14953795075416565 0.14953795075416565 0.15132790803909302 0.03422759473323822 0.5638077855110168 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014649164811380427\n",
            "1267 Data 0.0014607572275099896 PDE 4.094908945262432e-06 0.14953774213790894 0.14953774213790894 0.15132847428321838 0.034217674285173416 0.5638106465339661 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001464852136455252\n",
            "1268 Data 0.0014607063576804694 PDE 4.081612587469863e-06 0.14953753352165222 0.14953753352165222 0.15132904052734375 0.03420776128768921 0.5638135075569153 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014647879702679393\n",
            "1269 Data 0.0014606554961160026 PDE 4.0683707993594e-06 0.1495373249053955 0.1495373249053955 0.15132960677146912 0.0341978520154953 0.5638163685798645 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001464723866915362\n",
            "1270 Data 0.0014606046758244263 PDE 4.055177669215482e-06 0.1495371162891388 0.1495371162891388 0.15133017301559448 0.03418795019388199 0.5638192296028137 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014646598534936417\n",
            "1271 Data 0.001460553936895338 PDE 4.042035925522214e-06 0.14953690767288208 0.14953690767288208 0.15133073925971985 0.034178055822849274 0.5638221502304077 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014645959728208603\n",
            "1272 Data 0.00146050320689364 PDE 4.028938747069333e-06 0.14953669905662537 0.14953669905662537 0.15133130550384521 0.034168168902397156 0.5638250708580017 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014645321456407093\n",
            "1273 Data 0.0014604525507348669 PDE 4.015891136077698e-06 0.14953649044036865 0.14953649044036865 0.15133187174797058 0.034158285707235336 0.5638279914855957 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014644684418709446\n",
            "1274 Data 0.0014604019120436297 PDE 4.002902187494328e-06 0.14953628182411194 0.14953628182411194 0.15133243799209595 0.034148409962654114 0.5638309121131897 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001464404814231124\n",
            "1275 Data 0.0014603513457493338 PDE 3.989959168393398e-06 0.14953607320785522 0.14953607320785522 0.1513330042362213 0.03413854166865349 0.5638338327407837 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014643413049177272\n",
            "1276 Data 0.0014603007778850384 PDE 3.9770707189745735e-06 0.1495358645915985 0.1495358645915985 0.15133357048034668 0.03412868082523346 0.5638367533683777 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001464277848604013\n",
            "1277 Data 0.0014602502866612607 PDE 3.964225015806733e-06 0.1495356559753418 0.1495356559753418 0.15133413672447205 0.03411882370710373 0.5638397336006165 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014642145116770674\n",
            "1278 Data 0.0014601998500981205 PDE 3.951432518078946e-06 0.14953544735908508 0.14953544735908508 0.1513347029685974 0.034108974039554596 0.5638427138328552 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014641512826161994\n",
            "1279 Data 0.001460149430348416 PDE 3.938688678317703e-06 0.14953523874282837 0.14953523874282837 0.15133526921272278 0.03409913182258606 0.563845694065094 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014640881190267337\n",
            "1280 Data 0.001460099065902752 PDE 3.92599577025976e-06 0.14953503012657166 0.14953503012657166 0.15133585035800934 0.03408929705619812 0.5638486742973328 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014640250616730118\n",
            "1281 Data 0.0014600487752504095 PDE 3.913352429663064e-06 0.14953482151031494 0.14953482151031494 0.1513364315032959 0.03407946974039078 0.5638516545295715 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014639621276800725\n",
            "1282 Data 0.0014599985094042116 PDE 3.900751835317351e-06 0.14953461289405823 0.14953461289405823 0.15133701264858246 0.03406964987516403 0.5638546347618103 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001463899261239529\n",
            "1283 Data 0.0014599482950435636 PDE 3.888197625201428e-06 0.1495344042778015 0.1495344042778015 0.15133759379386902 0.03405983746051788 0.5638576149940491 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001463836492668765\n",
            "1284 Data 0.0014598980694958397 PDE 3.875698894262314e-06 0.1495341956615448 0.1495341956615448 0.15133817493915558 0.03405002877116203 0.5638606548309326 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001463773768390102\n",
            "1285 Data 0.0014598479475582628 PDE 3.863249276037095e-06 0.14953398704528809 0.14953398704528809 0.15133875608444214 0.03404022753238678 0.5638636946678162 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014637111968342999\n",
            "1286 Data 0.001459797860901306 PDE 3.85084240406286e-06 0.14953377842903137 0.14953377842903137 0.1513393372297287 0.03403043374419212 0.5638667345046997 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001463648703305369\n",
            "1287 Data 0.0014597478244377238 PDE 3.838480552076362e-06 0.14953356981277466 0.14953356981277466 0.15133991837501526 0.034020647406578064 0.5638697743415833 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014635863049898001\n",
            "1288 Data 0.0014596977955527127 PDE 3.826170996035216e-06 0.14953336119651794 0.14953336119651794 0.15134049952030182 0.0340108685195446 0.5638728141784668 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001463523966548748\n",
            "1289 Data 0.0014596478022420366 PDE 3.8139091884659138e-06 0.14953315258026123 0.14953315258026123 0.15134108066558838 0.034001097083091736 0.5638759136199951 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014634617114305025\n",
            "1290 Data 0.0014595978593516248 PDE 3.8016994494682876e-06 0.14953294396400452 0.14953294396400452 0.15134166181087494 0.03399133309721947 0.5638790130615234 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001463399558801093\n",
            "1291 Data 0.0014595480044072972 PDE 3.789529046116513e-06 0.1495327353477478 0.1495327353477478 0.1513422429561615 0.033981576561927795 0.5638821125030518 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014633375334534137\n",
            "1292 Data 0.0014594981785511097 PDE 3.777407073357608e-06 0.1495325267314911 0.1495325267314911 0.15134282410144806 0.03397182747721672 0.5638852119445801 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014632755856244673\n",
            "1293 Data 0.0014594483171251782 PDE 3.765333303817897e-06 0.14953231811523438 0.14953231811523438 0.15134340524673462 0.03396208584308624 0.5638883113861084 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014632136504289961\n",
            "1294 Data 0.0014593986373340128 PDE 3.7533002341660904e-06 0.14953210949897766 0.14953210949897766 0.15134398639202118 0.03395235165953636 0.5638914108276367 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001463151937568179\n",
            "1295 Data 0.0014593489513672192 PDE 3.7413178688439075e-06 0.14953190088272095 0.14953190088272095 0.15134456753730774 0.03394262492656708 0.5638945698738098 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014630902692360631\n",
            "1296 Data 0.0014592992354826022 PDE 3.7293796140147606e-06 0.14953169226646423 0.14953169226646423 0.1513451486825943 0.03393290564417839 0.5638977289199829 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001463028615096617\n",
            "1297 Data 0.001459249626101793 PDE 3.7174859244260006e-06 0.14953148365020752 0.14953148365020752 0.15134572982788086 0.0339231938123703 0.563900887966156 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001462967112026219\n",
            "1298 Data 0.0014592000648041162 PDE 3.70564066543011e-06 0.1495312750339508 0.1495312750339508 0.15134631097316742 0.03391348943114281 0.5639040470123291 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014629057054695463\n",
            "1299 Data 0.0014591504992476126 PDE 3.6938438370270887e-06 0.1495310664176941 0.1495310664176941 0.15134689211845398 0.03390379250049591 0.5639072060585022 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014628443430846397\n",
            "1300 Data 0.001459101005375394 PDE 3.6820940749748843e-06 0.14953085780143738 0.14953085780143738 0.15134748816490173 0.03389410302042961 0.5639103651046753 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014627830994503688\n",
            "1301 Data 0.001459051582730363 PDE 3.6703868317999877e-06 0.14953064918518066 0.14953064918518066 0.1513480842113495 0.03388442099094391 0.5639135837554932 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001462721969562163\n",
            "1302 Data 0.0014590021397364041 PDE 3.6587271097232588e-06 0.14953044056892395 0.14953044056892395 0.15134868025779724 0.0338747501373291 0.563916802406311 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014626608668461274\n",
            "1303 Data 0.0014589527750185549 PDE 3.6471112707658904e-06 0.14953023195266724 0.14953023195266724 0.151349276304245 0.03386508673429489 0.5639200210571289 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014625998862893208\n",
            "1304 Data 0.0014589034205455464 PDE 3.6355429529066896e-06 0.14953002333641052 0.14953002333641052 0.15134987235069275 0.03385543078184128 0.5639232397079468 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001462538963498453\n",
            "1305 Data 0.0014588541571205268 PDE 3.6240128338249633e-06 0.1495298147201538 0.1495298147201538 0.1513504683971405 0.03384578227996826 0.5639264583587646 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014624781699543517\n",
            "1306 Data 0.0014588048961486687 PDE 3.612530917962431e-06 0.1495296061038971 0.1495296061038971 0.15135106444358826 0.03383614122867584 0.5639297366142273 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014624174270666311\n",
            "1307 Data 0.001458755693109618 PDE 3.6010899293614784e-06 0.14952939748764038 0.14952939748764038 0.151351660490036 0.03382650762796402 0.5639330148696899 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014623567830389795\n",
            "1308 Data 0.001458706536785385 PDE 3.5896896406484302e-06 0.14952918887138367 0.14952918887138367 0.15135225653648376 0.033816881477832794 0.5639362931251526 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014622962264260334\n",
            "1309 Data 0.0014586573978921987 PDE 3.578334599296795e-06 0.14952898025512695 0.14952898025512695 0.15135285258293152 0.033807266503572464 0.5639395713806152 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014622357324914955\n",
            "1310 Data 0.0014586083098520849 PDE 3.567027079043328e-06 0.14952877163887024 0.14952877163887024 0.15135344862937927 0.03379765897989273 0.5639428496360779 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014621753369311282\n",
            "1311 Data 0.0014585592518510544 PDE 3.555758212314686e-06 0.14952856302261353 0.14952856302261353 0.15135404467582703 0.033788058906793594 0.5639461874961853 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014621150100633691\n",
            "1312 Data 0.0014585102158809963 PDE 3.5445364119368605e-06 0.1495283544063568 0.1495283544063568 0.15135464072227478 0.033778466284275055 0.5639495253562927 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014620547522929332\n",
            "1313 Data 0.0014584612821603215 PDE 3.533364179020282e-06 0.1495281457901001 0.1495281457901001 0.15135523676872253 0.03376888111233711 0.5639528632164001 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014619946463393417\n",
            "1314 Data 0.001458412375412446 PDE 3.522231281749555e-06 0.14952793717384338 0.14952793717384338 0.1513558328151703 0.033759307116270065 0.5639562010765076 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014619346066941956\n",
            "1315 Data 0.001458363440449518 PDE 3.5111459055769956e-06 0.14952772855758667 0.14952772855758667 0.15135642886161804 0.033749740570783615 0.563959538936615 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001461874586355095\n",
            "1316 Data 0.0014583146436069599 PDE 3.5000934985873755e-06 0.14952751994132996 0.14952751994132996 0.1513570249080658 0.03374018147587776 0.5639629364013672 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014618147371055473\n",
            "1317 Data 0.0014582658148590856 PDE 3.489090659059002e-06 0.14952731132507324 0.14952731132507324 0.15135763585567474 0.033730629831552505 0.5639663338661194 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014617549055181446\n",
            "1318 Data 0.0014582170716633403 PDE 3.478128974165884e-06 0.14952710270881653 0.14952710270881653 0.1513582468032837 0.033721089363098145 0.5639697313308716 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014616952006375062\n",
            "1319 Data 0.0014581683537767476 PDE 3.467206397544942e-06 0.14952689409255981 0.14952689409255981 0.15135885775089264 0.03371155634522438 0.5639731287956238 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014616355601742926\n",
            "1320 Data 0.0014581196198170843 PDE 3.456327021922334e-06 0.1495266854763031 0.1495266854763031 0.1513594686985016 0.03370203077793121 0.563976526260376 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014615759468390066\n",
            "1321 Data 0.001458071040870718 PDE 3.445484935582499e-06 0.1495264768600464 0.1495264768600464 0.15136007964611053 0.03369251266121864 0.563979983329773 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014615165258063006\n",
            "1322 Data 0.0014580224724827385 PDE 3.4346858228673227e-06 0.14952626824378967 0.14952626824378967 0.15136069059371948 0.03368300572037697 0.5639834403991699 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014614571583056058\n",
            "1323 Data 0.0014579738943864826 PDE 3.423936732360744e-06 0.14952605962753296 0.14952605962753296 0.15136130154132843 0.03367350623011589 0.5639868974685669 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014613978311188434\n",
            "1324 Data 0.001457925373913265 PDE 3.4132176551793236e-06 0.14952585101127625 0.14952585101127625 0.15136191248893738 0.03366401419043541 0.5639903545379639 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014613385915684444\n",
            "1325 Data 0.0014578769012732248 PDE 3.40254814545915e-06 0.14952564239501953 0.14952564239501953 0.15136252343654633 0.033654533326625824 0.5639938712120056 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001461279449418684\n",
            "1326 Data 0.001457828434356343 PDE 3.391919108253205e-06 0.14952543377876282 0.14952543377876282 0.15136313438415527 0.033645059913396835 0.5639973878860474 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014612203534645962\n",
            "1327 Data 0.0014577800329743879 PDE 3.381331453056191e-06 0.1495252400636673 0.1495252400636673 0.15136374533176422 0.033635593950748444 0.5640009045600891 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001461161364427444\n",
            "1328 Data 0.0014577316671225498 PDE 3.370785861989134e-06 0.14952504634857178 0.14952504634857178 0.15136435627937317 0.03362613916397095 0.5640044212341309 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001461102452984539\n",
            "1329 Data 0.0014576833494870211 PDE 3.3602809708099812e-06 0.14952485263347626 0.14952485263347626 0.15136496722698212 0.03361669182777405 0.5640079379081726 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001461043630457831\n",
            "1330 Data 0.0014576350385605984 PDE 3.349808594066417e-06 0.14952465891838074 0.14952465891838074 0.15136557817459106 0.033607255667448044 0.5640115141868591 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014609848471546648\n",
            "1331 Data 0.0014575867917011739 PDE 3.339380555189564e-06 0.14952446520328522 0.14952446520328522 0.1513661891222 0.03359782695770264 0.5640150904655457 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014609261722563634\n",
            "1332 Data 0.0014575386011963507 PDE 3.328992079332238e-06 0.1495242714881897 0.1495242714881897 0.15136681497097015 0.03358840569853783 0.5640186667442322 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001460867593275683\n",
            "1333 Data 0.0014574904294143462 PDE 3.318644985483843e-06 0.14952407777309418 0.14952407777309418 0.1513674408197403 0.03357899561524391 0.5640222430229187 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00146080907439983\n",
            "1334 Data 0.0014574423125083345 PDE 3.3083363177865976e-06 0.14952388405799866 0.14952388405799866 0.15136806666851044 0.033569592982530594 0.56402587890625 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014607506488261211\n",
            "1335 Data 0.001457394210779797 PDE 3.298067895229906e-06 0.14952369034290314 0.14952369034290314 0.15136869251728058 0.03356020152568817 0.5640295147895813 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014606922786750268\n",
            "1336 Data 0.0014573461984516518 PDE 3.287839490440092e-06 0.14952349662780762 0.14952349662780762 0.15136931836605072 0.033550817519426346 0.5640331506729126 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001460634037942092\n",
            "1337 Data 0.001457298160064028 PDE 3.2776531497802353e-06 0.1495233029127121 0.1495233029127121 0.15136994421482086 0.033541444689035416 0.5640367865562439 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014605758132138083\n",
            "1338 Data 0.0014572501996101995 PDE 3.2675056900188792e-06 0.14952310919761658 0.14952310919761658 0.151370570063591 0.03353207930922508 0.5640404224395752 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014605177053002183\n",
            "1339 Data 0.0014572022250381803 PDE 3.257400067013805e-06 0.14952291548252106 0.14952291548252106 0.15137119591236115 0.033522725105285645 0.5640441179275513 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001460459625105194\n",
            "1340 Data 0.0014571543682128454 PDE 3.2473303690494504e-06 0.14952272176742554 0.14952272176742554 0.1513718217611313 0.033513378351926804 0.5640478134155273 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014604016985818949\n",
            "1341 Data 0.001457106488822802 PDE 3.2373013709730003e-06 0.14952252805233002 0.14952252805233002 0.15137244760990143 0.03350403904914856 0.5640515089035034 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001460343790193775\n",
            "1342 Data 0.0014570586533380152 PDE 3.2273078431899194e-06 0.1495223343372345 0.1495223343372345 0.15137307345867157 0.03349471092224121 0.5640552043914795 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014602859611812051\n",
            "1343 Data 0.0014570108593889232 PDE 3.217348421458155e-06 0.14952214062213898 0.14952214062213898 0.1513736993074417 0.03348539397120476 0.5640589594841003 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014602282078103814\n",
            "1344 Data 0.0014569631095093813 PDE 3.207430381735321e-06 0.14952194690704346 0.14952194690704346 0.15137432515621185 0.0334760844707489 0.5640627145767212 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014601705398911166\n",
            "1345 Data 0.0014569154233877784 PDE 3.1975487218005583e-06 0.14952175319194794 0.14952175319194794 0.151374951004982 0.03346678614616394 0.564066469669342 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001460112972109579\n",
            "1346 Data 0.0014568677606345655 PDE 3.1877086712484015e-06 0.14952155947685242 0.14952155947685242 0.15137559175491333 0.033457495272159576 0.5640702247619629 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014600554693058139\n",
            "1347 Data 0.0014568201557685068 PDE 3.1779059099790175e-06 0.1495213657617569 0.1495213657617569 0.15137623250484467 0.03344821557402611 0.5640740394592285 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014599980616784858\n",
            "1348 Data 0.0014567725564050237 PDE 3.1681422569818096e-06 0.14952117204666138 0.14952117204666138 0.151376873254776 0.033438943326473236 0.5640778541564941 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014599406986620055\n",
            "1349 Data 0.0014567249970026306 PDE 3.158413164783269e-06 0.14952097833156586 0.14952097833156586 0.15137751400470734 0.03342968225479126 0.5640816688537598 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014598834101674138\n",
            "1350 Data 0.0014566774461589188 PDE 3.148723635604256e-06 0.14952078461647034 0.14952078461647034 0.15137815475463867 0.03342042863368988 0.5640854835510254 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001459826169794523\n",
            "1351 Data 0.0014566299766829705 PDE 3.139073896818445e-06 0.14952059090137482 0.14952059090137482 0.15137879550457 0.033411186188459396 0.5640893578529358 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001459769050579789\n",
            "1352 Data 0.0014565825304097776 PDE 3.1294582640839508e-06 0.1495203971862793 0.1495203971862793 0.15137943625450134 0.03340195491909981 0.5640932321548462 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014597119886738616\n",
            "1353 Data 0.001456535041358005 PDE 3.1198806027532555e-06 0.14952020347118378 0.14952020347118378 0.15138007700443268 0.033392731100320816 0.5640971064567566 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014596549219607583\n",
            "1354 Data 0.0014564876992492735 PDE 3.110333636868745e-06 0.14952000975608826 0.14952000975608826 0.151380717754364 0.03338351845741272 0.564100980758667 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014595980328861423\n",
            "1355 Data 0.0014564404093148397 PDE 3.100830554103595e-06 0.14951981604099274 0.14951981604099274 0.15138135850429535 0.03337431326508522 0.5641049146652222 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014595412398689432\n",
            "1356 Data 0.0014563930781029018 PDE 3.091361122642411e-06 0.14951962232589722 0.14951962232589722 0.15138199925422668 0.033365119248628616 0.5641088485717773 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014594844392255442\n",
            "1357 Data 0.0014563458658767147 PDE 3.0819194307696307e-06 0.1495194286108017 0.1495194286108017 0.15138264000415802 0.03335593640804291 0.5641127824783325 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014594277853074843\n",
            "1358 Data 0.0014562986122112214 PDE 3.072523213631939e-06 0.14951923489570618 0.14951923489570618 0.15138328075408936 0.033346761018037796 0.5641167163848877 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014593711354248533\n",
            "1359 Data 0.0014562514205700742 PDE 3.063161557292915e-06 0.14951904118061066 0.14951904118061066 0.1513839215040207 0.03333759680390358 0.5641207098960876 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014593145821273671\n",
            "1360 Data 0.0014562042292786603 PDE 3.0538383271050407e-06 0.14951884746551514 0.14951884746551514 0.15138457715511322 0.03332844376564026 0.5641247034072876 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014592580676057653\n",
            "1361 Data 0.0014561571343364803 PDE 3.044546247110702e-06 0.14951865375041962 0.14951865375041962 0.15138523280620575 0.033319298177957535 0.5641286969184875 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001459201680583591\n",
            "1362 Data 0.0014561101009816502 PDE 3.0352941848832415e-06 0.1495184600353241 0.1495184600353241 0.15138588845729828 0.033310163766145706 0.5641326904296875 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014591453951665334\n",
            "1363 Data 0.001456063001086604 PDE 3.0260746370913694e-06 0.14951826632022858 0.14951826632022858 0.1513865441083908 0.03330104053020477 0.5641367435455322 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014590890757236953\n",
            "1364 Data 0.001456015993274918 PDE 3.016896016561077e-06 0.14951807260513306 0.14951807260513306 0.15138719975948334 0.03329192474484444 0.564140796661377 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001459032889291479\n",
            "1365 Data 0.0014559691042869746 PDE 3.0077492283453466e-06 0.14951787889003754 0.14951787889003754 0.15138785541057587 0.033282820135354996 0.5641448497772217 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.00145897685351532\n",
            "1366 Data 0.0014559221010835871 PDE 2.9986410936544416e-06 0.14951768517494202 0.14951768517494202 0.1513885110616684 0.03327372670173645 0.5641489028930664 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014589207421772416\n",
            "1367 Data 0.001455875191011518 PDE 2.9895677471358795e-06 0.1495174914598465 0.1495174914598465 0.15138916671276093 0.0332646407186985 0.5641530156135559 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014588647587586538\n",
            "1368 Data 0.001455828297053142 PDE 2.980526460305555e-06 0.14951729774475098 0.14951729774475098 0.15138982236385345 0.03325556591153145 0.5641571283340454 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014588088235134476\n",
            "1369 Data 0.0014557814707714183 PDE 2.971519506900222e-06 0.14951710402965546 0.14951710402965546 0.15139047801494598 0.03324650228023529 0.5641612410545349 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014587529902783186\n",
            "1370 Data 0.001455734713193725 PDE 2.9625468869198812e-06 0.14951691031455994 0.14951691031455994 0.1513911336660385 0.03323744982481003 0.5641653537750244 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014586972600806449\n",
            "1371 Data 0.0014556879280243082 PDE 2.9536067813751288e-06 0.14951671659946442 0.14951671659946442 0.15139178931713104 0.03322840481996536 0.5641695261001587 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014586415348056833\n",
            "1372 Data 0.0014556412517427902 PDE 2.9447032829921227e-06 0.1495165228843689 0.1495165228843689 0.15139245986938477 0.03321937099099159 0.564173698425293 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014585859550257823\n",
            "1373 Data 0.0014555945470044666 PDE 2.935830707428977e-06 0.14951632916927338 0.14951632916927338 0.1513931304216385 0.03321034833788872 0.5641778707504272 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014585303777118956\n",
            "1374 Data 0.0014555478676583926 PDE 2.9269976948853582e-06 0.14951613545417786 0.14951613545417786 0.1513938009738922 0.03320133686065674 0.5641820430755615 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001458474865353278\n",
            "1375 Data 0.0014555012387748574 PDE 2.918195150414249e-06 0.14951594173908234 0.14951594173908234 0.15139447152614594 0.033192332834005356 0.5641862750053406 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014584194339252717\n",
            "1376 Data 0.0014554546614570748 PDE 2.9094273941154825e-06 0.14951574802398682 0.14951574802398682 0.15139514207839966 0.03318333998322487 0.5641905069351196 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014583640888511903\n",
            "1377 Data 0.001455408124471901 PDE 2.900691470131278e-06 0.1495155543088913 0.1495155543088913 0.15139581263065338 0.03317435830831528 0.5641947388648987 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014583088159420322\n",
            "1378 Data 0.0014553615946345173 PDE 2.8919894248247147e-06 0.14951536059379578 0.14951536059379578 0.1513964831829071 0.03316538780927658 0.5641989707946777 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001458253584059342\n",
            "1379 Data 0.0014553150961818177 PDE 2.883324214053573e-06 0.14951516687870026 0.14951516687870026 0.15139715373516083 0.03315642848610878 0.5642032623291016 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014581984203958713\n",
            "1380 Data 0.0014552686497695982 PDE 2.8746876523655374e-06 0.14951497316360474 0.14951497316360474 0.15139782428741455 0.033147480338811874 0.5642075538635254 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014581433374219637\n",
            "1381 Data 0.0014552222340960996 PDE 2.866095428544213e-06 0.14951477944850922 0.14951477944850922 0.15139849483966827 0.033138539642095566 0.5642118453979492 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014580883295246439\n",
            "1382 Data 0.001455175841845319 PDE 2.8575298074429156e-06 0.1495145857334137 0.1495145857334137 0.151399165391922 0.03312961012125015 0.5642161965370178 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001458033371652762\n",
            "1383 Data 0.0014551295190801123 PDE 2.8489962460298557e-06 0.14951439201831818 0.14951439201831818 0.15139983594417572 0.033120691776275635 0.5642205476760864 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014579785153261422\n",
            "1384 Data 0.0014550832085361914 PDE 2.8404908789525507e-06 0.14951419830322266 0.14951419830322266 0.15140052139759064 0.03311178460717201 0.564224898815155 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001457923699415144\n",
            "1385 Data 0.0014550369569114295 PDE 2.8320177989371587e-06 0.14951401948928833 0.14951401948928833 0.15140120685100555 0.033102888613939285 0.5642292499542236 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014578689747103666\n",
            "1386 Data 0.0014549907511370757 PDE 2.8235763238626532e-06 0.149513840675354 0.149513840675354 0.15140189230442047 0.033094003796577454 0.564233660697937 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014578143274609383\n",
            "1387 Data 0.001454944533134853 PDE 2.8151619062555255e-06 0.14951366186141968 0.14951366186141968 0.1514025777578354 0.03308513015508652 0.5642380714416504 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014577596950411085\n",
            "1388 Data 0.0014548983285750852 PDE 2.806786596920574e-06 0.14951348304748535 0.14951348304748535 0.1514032632112503 0.03307626396417618 0.5642424821853638 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014577051151720057\n",
            "1389 Data 0.0014548522098284966 PDE 2.79844334727386e-06 0.14951330423355103 0.14951330423355103 0.15140394866466522 0.033067408949136734 0.5642469525337219 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014576506531757704\n",
            "1390 Data 0.0014548061106608723 PDE 2.790136932162568e-06 0.1495131254196167 0.1495131254196167 0.15140463411808014 0.033058565109968185 0.5642514228820801 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014575962475930349\n",
            "1391 Data 0.0014547600477851153 PDE 2.7818621219921624e-06 0.14951294660568237 0.14951294660568237 0.15140531957149506 0.03304973244667053 0.5642558932304382 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014575419099071075\n",
            "1392 Data 0.001454713940600994 PDE 2.7736225547414506e-06 0.14951276779174805 0.14951276779174805 0.15140600502490997 0.033040910959243774 0.5642603635787964 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014574875631557355\n",
            "1393 Data 0.0014546679571752713 PDE 2.7654089080897393e-06 0.14951258897781372 0.14951258897781372 0.1514066904783249 0.03303210064768791 0.5642648935317993 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001457433366083361\n",
            "1394 Data 0.0014546220423103085 PDE 2.7572293674893444e-06 0.1495124101638794 0.1495124101638794 0.1514073759317398 0.033023301512002945 0.5642694234848022 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014573792716777978\n",
            "1395 Data 0.0014545760204859007 PDE 2.7490793854667572e-06 0.14951223134994507 0.14951223134994507 0.15140806138515472 0.03301451355218887 0.5642739534378052 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014573250998713675\n",
            "1396 Data 0.0014545301573220821 PDE 2.7409603262640303e-06 0.14951205253601074 0.14951205253601074 0.15140876173973083 0.0330057367682457 0.5642785429954529 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014572711176483461\n",
            "1397 Data 0.0014544842912733054 PDE 2.7328680971550057e-06 0.14951187372207642 0.14951187372207642 0.15140946209430695 0.032996971160173416 0.5642831325531006 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014572171593704604\n",
            "1398 Data 0.0014544384573529612 PDE 2.724806563492166e-06 0.1495116949081421 0.1495116949081421 0.15141016244888306 0.03298821672797203 0.5642877221107483 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014571632639164534\n",
            "1399 Data 0.0014543926133705068 PDE 2.716777316891239e-06 0.14951151609420776 0.14951151609420776 0.15141086280345917 0.03297947347164154 0.564292311668396 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001457109390687398\n",
            "1400 Data 0.0014543468722361382 PDE 2.708775582505041e-06 0.14951133728027344 0.14951133728027344 0.15141156315803528 0.032970741391181946 0.5642969608306885 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014570556478186432\n",
            "1401 Data 0.0014543011284743974 PDE 2.7008070446754573e-06 0.1495111584663391 0.1495111584663391 0.1514122635126114 0.032962020486593246 0.564301609992981 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014570019355190729\n",
            "1402 Data 0.0014542554762480544 PDE 2.692865791686927e-06 0.14951097965240479 0.14951097965240479 0.1514129638671875 0.03295331075787544 0.5643062591552734 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014569483420397414\n",
            "1403 Data 0.0014542097590692718 PDE 2.684958644749713e-06 0.14951080083847046 0.14951080083847046 0.1514136642217636 0.032944612205028534 0.5643109679222107 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014568947177140215\n",
            "1404 Data 0.0014541641295897057 PDE 2.6770828753797105e-06 0.14951062202453613 0.14951062202453613 0.15141436457633972 0.03293592482805252 0.564315676689148 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014568412124650854\n",
            "1405 Data 0.001454118532742363 PDE 2.669234845598112e-06 0.1495104432106018 0.1495104432106018 0.15141506493091583 0.0329272486269474 0.5643203854560852 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001456787767587961\n",
            "1406 Data 0.0014540729387691991 PDE 2.661420012373128e-06 0.14951026439666748 0.14951026439666748 0.15141578018665314 0.03291858360171318 0.5643251538276672 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014567343587815723\n",
            "1407 Data 0.0014540273898405198 PDE 2.6536361019680044e-06 0.14951008558273315 0.14951008558273315 0.15141649544239044 0.032909929752349854 0.5643299221992493 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014566810259424878\n",
            "1408 Data 0.001453981845134151 PDE 2.6458853881194955e-06 0.14950990676879883 0.14950990676879883 0.15141721069812775 0.03290128707885742 0.5643346905708313 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014566277305222705\n",
            "1409 Data 0.0014539364032960488 PDE 2.638161049617338e-06 0.1495097279548645 0.1495097279548645 0.15141792595386505 0.032892655581235886 0.5643394589424133 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001456574564345666\n",
            "1410 Data 0.0014538909417734262 PDE 2.6304617222194793e-06 0.14950954914093018 0.14950954914093018 0.15141864120960236 0.032884035259485245 0.5643442869186401 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014565214034956457\n",
            "1411 Data 0.001453845519157335 PDE 2.6227910439047264e-06 0.14950937032699585 0.14950937032699585 0.15141935646533966 0.0328754261136055 0.5643491148948669 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014564683102012397\n",
            "1412 Data 0.001453800153676552 PDE 2.6151510610361584e-06 0.14950919151306152 0.14950919151306152 0.15142007172107697 0.03286682814359665 0.5643539428710938 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014564153047375882\n",
            "1413 Data 0.0014537547639690443 PDE 2.607537680887617e-06 0.1495090126991272 0.1495090126991272 0.15142078697681427 0.032858241349458694 0.5643588304519653 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001456362301649932\n",
            "1414 Data 0.0014537094643301781 PDE 2.599959771032445e-06 0.14950883388519287 0.14950883388519287 0.15142150223255157 0.032849665731191635 0.5643637180328369 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014563094241012106\n",
            "1415 Data 0.0014536641950160924 PDE 2.592403916423791e-06 0.14950865507125854 0.14950865507125854 0.15142221748828888 0.03284110128879547 0.5643686056137085 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014562565989325162\n",
            "1416 Data 0.0014536189016783178 PDE 2.584875801403541e-06 0.14950847625732422 0.14950847625732422 0.15142293274402618 0.0328325480222702 0.5643735527992249 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014562037774797213\n",
            "1417 Data 0.001453573746026926 PDE 2.577374743850669e-06 0.1495082974433899 0.1495082974433899 0.15142366290092468 0.03282400593161583 0.5643784999847412 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014561511207707768\n",
            "1418 Data 0.0014535284999311797 PDE 2.5699059733597096e-06 0.14950811862945557 0.14950811862945557 0.15142439305782318 0.03281547501683235 0.5643834471702576 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014560984059045394\n",
            "1419 Data 0.0014534833696003779 PDE 2.562469262556988e-06 0.14950793981552124 0.14950793981552124 0.15142512321472168 0.03280695527791977 0.5643884539604187 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014560458388629348\n",
            "1420 Data 0.0014534382565045904 PDE 2.5550573354848893e-06 0.14950776100158691 0.14950776100158691 0.15142585337162018 0.03279844671487808 0.5643934607505798 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014559933138400753\n",
            "1421 Data 0.0014533931505909812 PDE 2.5476765586063266e-06 0.1495075821876526 0.1495075821876526 0.15142658352851868 0.03278995305299759 0.564398467540741 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014559408271495875\n",
            "1422 Data 0.0014533480337681618 PDE 2.5403264771739487e-06 0.14950740337371826 0.14950740337371826 0.15142731368541718 0.03278147056698799 0.5644034743309021 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014558883602453357\n",
            "1423 Data 0.0014533030322170125 PDE 2.5330014068458695e-06 0.14950722455978394 0.14950722455978394 0.15142804384231567 0.03277299925684929 0.564408540725708 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014558360336238583\n",
            "1424 Data 0.0014532579937814982 PDE 2.5257077140850015e-06 0.1495070457458496 0.1495070457458496 0.15142877399921417 0.03276453912258148 0.5644136071205139 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014557837014955832\n",
            "1425 Data 0.001453213035279423 PDE 2.5184385776810814e-06 0.14950686693191528 0.14950686693191528 0.15142950415611267 0.03275609016418457 0.5644186735153198 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001455731473857104\n",
            "1426 Data 0.0014531680389236786 PDE 2.5111935428867582e-06 0.14950668811798096 0.14950668811798096 0.15143023431301117 0.032747652381658554 0.5644237995147705 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014556792324665654\n",
            "1427 Data 0.0014531231751913876 PDE 2.503978294043918e-06 0.14950650930404663 0.14950650930404663 0.15143097937107086 0.03273922577500343 0.5644289255142212 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014556271534854315\n",
            "1428 Data 0.0014530782954755579 PDE 2.4967848730739206e-06 0.1495063304901123 0.1495063304901123 0.15143172442913055 0.03273081034421921 0.5644340515136719 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014555750803486318\n",
            "1429 Data 0.001453033413464407 PDE 2.4896239665395115e-06 0.14950615167617798 0.14950615167617798 0.15143246948719025 0.032722409814596176 0.5644392371177673 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014555230374309466\n",
            "1430 Data 0.0014529886028071962 PDE 2.4824873889883747e-06 0.14950597286224365 0.14950597286224365 0.15143321454524994 0.03271402046084404 0.5644444227218628 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014554710901961846\n",
            "1431 Data 0.0014529438407672568 PDE 2.4753812795097474e-06 0.14950579404830933 0.14950579404830933 0.15143395960330963 0.0327056422829628 0.5644496083259583 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014554192220467665\n",
            "1432 Data 0.0014528990497406932 PDE 2.4683004085090943e-06 0.149505615234375 0.149505615234375 0.15143470466136932 0.032697275280952454 0.5644548535346985 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014553673501492023\n",
            "1433 Data 0.0014528543420308294 PDE 2.4612427296233363e-06 0.14950543642044067 0.14950543642044067 0.15143544971942902 0.032688919454813004 0.5644600987434387 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014553155847604527\n",
            "1434 Data 0.0014528096689203811 PDE 2.454213017699658e-06 0.14950525760650635 0.14950525760650635 0.1514361947774887 0.03268057480454445 0.564465343952179 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014552638819380808\n",
            "1435 Data 0.0014527650004123934 PDE 2.4472144559695153e-06 0.14950507879257202 0.14950507879257202 0.1514369398355484 0.03267224505543709 0.564470648765564 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001455212214868363\n",
            "1436 Data 0.0014527203578037669 PDE 2.440238631606917e-06 0.1495048999786377 0.1495048999786377 0.1514376848936081 0.03266392648220062 0.564475953578949 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014551605964353738\n",
            "1437 Data 0.0014526757188716248 PDE 2.43328918259067e-06 0.14950472116470337 0.14950472116470337 0.15143844485282898 0.03265561908483505 0.564481258392334 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014551090080542155\n",
            "1438 Data 0.0014526311637465732 PDE 2.426361788820941e-06 0.14950455725193024 0.14950455725193024 0.15143920481204987 0.03264732286334038 0.5644866228103638 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014550575255353941\n",
            "1439 Data 0.0014525866071739074 PDE 2.419468728476204e-06 0.1495043933391571 0.1495043933391571 0.15143996477127075 0.0326390415430069 0.5644919872283936 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014550060759023836\n",
            "1440 Data 0.0014525420886494047 PDE 2.412600906609441e-06 0.14950422942638397 0.14950422942638397 0.15144072473049164 0.03263077139854431 0.5644973516464233 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014549546895560141\n",
            "1441 Data 0.0014524975976551698 PDE 2.4057583232206525e-06 0.14950406551361084 0.14950406551361084 0.15144148468971252 0.03262251242995262 0.5645027756690979 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014549033559783904\n",
            "1442 Data 0.0014524531090343838 PDE 2.398935293967952e-06 0.1495039016008377 0.1495039016008377 0.1514422446489334 0.03261426463723183 0.5645081996917725 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014548520443283517\n",
            "1443 Data 0.0014524087413556733 PDE 2.3921450065245153e-06 0.14950373768806458 0.14950373768806458 0.1514430046081543 0.03260602802038193 0.564513623714447 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014548008863621978\n",
            "1444 Data 0.001452364347369466 PDE 2.385375864832895e-06 0.14950357377529144 0.14950357377529144 0.15144376456737518 0.03259780630469322 0.5645191073417664 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014547497232342988\n",
            "1445 Data 0.001452319942454714 PDE 2.3786312794982223e-06 0.1495034098625183 0.1495034098625183 0.15144452452659607 0.03258959576487541 0.5645245909690857 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014546985737342122\n",
            "1446 Data 0.0014522755552879144 PDE 2.3719173896097345e-06 0.14950324594974518 0.14950324594974518 0.15144528448581696 0.0325813964009285 0.564530074596405 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014546474726775242\n",
            "1447 Data 0.0014522313241781904 PDE 2.3652253275940893e-06 0.14950308203697205 0.14950308203697205 0.15144605934619904 0.03257320821285248 0.5645356178283691 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014545965495057845\n",
            "1448 Data 0.001452186988273969 PDE 2.3585616872878745e-06 0.1495029181241989 0.1495029181241989 0.15144683420658112 0.03256503492593765 0.5645411610603333 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014545455499612569\n",
            "1449 Data 0.0014521427380768904 PDE 2.3519185106124496e-06 0.14950275421142578 0.14950275421142578 0.1514476090669632 0.03255687281489372 0.5645467042922974 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014544946565875029\n",
            "1450 Data 0.0014520985208774514 PDE 2.3453010271623498e-06 0.14950259029865265 0.14950259029865265 0.15144838392734528 0.03254872187972069 0.5645523071289062 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014544438219046137\n",
            "1451 Data 0.001452054341722462 PDE 2.3387094643112505e-06 0.14950242638587952 0.14950242638587952 0.15144915878772736 0.03254058212041855 0.5645579099655151 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014543930511867734\n",
            "1452 Data 0.0014520101311245232 PDE 2.3321435946854763e-06 0.14950226247310638 0.14950226247310638 0.15144993364810944 0.0325324572622776 0.564563512802124 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014543422747192087\n",
            "1453 Data 0.0014519659942198518 PDE 2.32559978030622e-06 0.14950209856033325 0.14950209856033325 0.15145070850849152 0.03252434358000755 0.5645691752433777 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001454291594000158\n",
            "1454 Data 0.0014519218985471047 PDE 2.3190773390524555e-06 0.14950193464756012 0.14950193464756012 0.1514514833688736 0.0325162410736084 0.5645748376846313 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014542409758861571\n",
            "1455 Data 0.0014518778294381467 PDE 2.3125778625399107e-06 0.149501770734787 0.149501770734787 0.15145225822925568 0.03250815346837044 0.5645805597305298 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014541904073006866\n",
            "1456 Data 0.001451833766283072 PDE 2.3061068077367963e-06 0.14950160682201385 0.14950160682201385 0.15145304799079895 0.03250007703900337 0.5645862817764282 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014541398730908088\n",
            "1457 Data 0.0014517897739313942 PDE 2.2996578081802e-06 0.14950144290924072 0.14950144290924072 0.15145383775234222 0.0324920117855072 0.5645920038223267 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014540894317395744\n",
            "1458 Data 0.0014517457802591218 PDE 2.2932329102332005e-06 0.1495012789964676 0.1495012789964676 0.1514546275138855 0.032483961433172226 0.5645977854728699 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001454039013169355\n",
            "1459 Data 0.0014517018207976943 PDE 2.2868339328852016e-06 0.14950111508369446 0.14950111508369446 0.15145541727542877 0.032475922256708145 0.5646035671234131 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014539886547305795\n",
            "1460 Data 0.001451657898805362 PDE 2.2804590571467998e-06 0.14950095117092133 0.14950095117092133 0.15145620703697205 0.03246789425611496 0.5646093487739563 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014539383578625088\n",
            "1461 Data 0.001451613928853843 PDE 2.2741091925126966e-06 0.1495007872581482 0.1495007872581482 0.15145699679851532 0.03245988115668297 0.5646151900291443 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014538880380463557\n",
            "1462 Data 0.001451570116211115 PDE 2.2677813831251115e-06 0.14950062334537506 0.14950062334537506 0.1514577865600586 0.03245187923312187 0.5646210312843323 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014538378975942402\n",
            "1463 Data 0.0014515262402412074 PDE 2.261479039589176e-06 0.14950045943260193 0.14950045943260193 0.15145857632160187 0.03244388848543167 0.5646268725395203 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014537877192807966\n",
            "1464 Data 0.0014514823876879998 PDE 2.2552039808942936e-06 0.1495002955198288 0.1495002955198288 0.15145936608314514 0.032435912638902664 0.564632773399353 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001453737591668894\n",
            "1465 Data 0.0014514386210940108 PDE 2.248949840577552e-06 0.14950013160705566 0.14950013160705566 0.15146015584468842 0.03242794796824455 0.5646386742591858 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014536875709345884\n",
            "1466 Data 0.0014513948621456823 PDE 2.24272116611246e-06 0.14949996769428253 0.14949996769428253 0.15146096050739288 0.032419994473457336 0.5646445751190186 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014536375833117947\n",
            "1467 Data 0.0014513511077647305 PDE 2.2365168206306407e-06 0.1494998037815094 0.1494998037815094 0.15146176517009735 0.032412055879831314 0.5646505355834961 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014535876245853612\n",
            "1468 Data 0.0014513073885415381 PDE 2.230327709185076e-06 0.14949963986873627 0.14949963986873627 0.15146256983280182 0.03240412846207619 0.5646564960479736 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014535377162507232\n",
            "1469 Data 0.0014512636799915348 PDE 2.2241645183385117e-06 0.14949947595596313 0.14949947595596313 0.1514633744955063 0.032396212220191956 0.5646624565124512 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014534878445098733\n",
            "1470 Data 0.0014512200433678922 PDE 2.2180258838488953e-06 0.14949931204319 0.14949931204319 0.15146417915821075 0.03238831087946892 0.5646684765815735 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001453438069251741\n",
            "1471 Data 0.0014511764680862987 PDE 2.2119054392533144e-06 0.14949914813041687 0.14949914813041687 0.15146498382091522 0.032380420714616776 0.5646744966506958 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001453388373525552\n",
            "1472 Data 0.001451132785968097 PDE 2.2058143258618657e-06 0.14949898421764374 0.14949898421764374 0.1514657884836197 0.03237254172563553 0.5646805763244629 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014533386002939588\n",
            "1473 Data 0.00145108925886964 PDE 2.1997359453962417e-06 0.1494988203048706 0.1494988203048706 0.15146659314632416 0.032364677637815475 0.56468665599823 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014532889948150362\n",
            "1474 Data 0.0014510457174784382 PDE 2.1936841676506447e-06 0.14949865639209747 0.14949865639209747 0.15146739780902863 0.03235682472586632 0.5646927356719971 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014532394016460888\n",
            "1475 Data 0.001451002144198242 PDE 2.187657628383022e-06 0.14949849247932434 0.14949849247932434 0.1514682173728943 0.032348986715078354 0.5646988749504089 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001453189801826625\n",
            "1476 Data 0.0014509586888901283 PDE 2.181656782340724e-06 0.1494983285665512 0.1494983285665512 0.15146903693675995 0.032341159880161285 0.5647050142288208 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001453140345672469\n",
            "1477 Data 0.0014509152152560453 PDE 2.1756745809398126e-06 0.14949816465377808 0.14949816465377808 0.1514698565006256 0.03233334422111511 0.5647111535072327 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014530908898369851\n",
            "1478 Data 0.001450871790204709 PDE 2.1697151169064455e-06 0.14949800074100494 0.14949800074100494 0.15147067606449127 0.03232554346323013 0.5647173523902893 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014530415053216155\n",
            "1479 Data 0.0014508283536224266 PDE 2.163783619835158e-06 0.1494978368282318 0.1494978368282318 0.15147149562835693 0.03231775388121605 0.564723551273346 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014529921372422618\n",
            "1480 Data 0.001450784934275639 PDE 2.157867811547476e-06 0.14949767291545868 0.14949767291545868 0.1514723151922226 0.03230997920036316 0.5647297501564026 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014529428020871864\n",
            "1481 Data 0.0014507416158677656 PDE 2.1519704205275048e-06 0.14949750900268555 0.14949750900268555 0.15147313475608826 0.032302215695381165 0.564736008644104 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014528935862882931\n",
            "1482 Data 0.0014506983003027718 PDE 2.146095312127727e-06 0.14949734508991241 0.14949734508991241 0.15147395431995392 0.032294463366270065 0.5647422671318054 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014528443956148996\n",
            "1483 Data 0.0014506550237513042 PDE 2.140242486348143e-06 0.14949718117713928 0.14949718117713928 0.15147477388381958 0.03228672593832016 0.5647485852241516 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014527952662376524\n",
            "1484 Data 0.0014506117405114316 PDE 2.1344133074308047e-06 0.14949701726436615 0.14949701726436615 0.15147560834884644 0.03227899968624115 0.5647549033164978 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014527461538188624\n",
            "1485 Data 0.0014505684841511007 PDE 2.1286057290126337e-06 0.14949685335159302 0.14949685335159302 0.1514764428138733 0.032271288335323334 0.564761221408844 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014526970898801133\n",
            "1486 Data 0.0014505252356811903 PDE 2.1228202058409806e-06 0.14949668943881989 0.14949668943881989 0.15147727727890015 0.03226358816027641 0.564767599105835 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014526480558870313\n",
            "1487 Data 0.001450482067330717 PDE 2.1170501440792577e-06 0.14949652552604675 0.14949652552604675 0.151478111743927 0.032255902886390686 0.5647739768028259 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014525991174747963\n",
            "1488 Data 0.001450438867348173 PDE 2.1113105503900442e-06 0.14949637651443481 0.14949637651443481 0.15147894620895386 0.032248228788375854 0.5647803544998169 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001452550177898563\n",
            "1489 Data 0.0014503957474826738 PDE 2.105588009726489e-06 0.14949622750282288 0.14949622750282288 0.1514797806739807 0.03224056586623192 0.5647867918014526 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014525013354924003\n",
            "1490 Data 0.0014503526597524038 PDE 2.0998904801672325e-06 0.14949607849121094 0.14949607849121094 0.15148061513900757 0.032232917845249176 0.5647932291030884 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001452452550232571\n",
            "1491 Data 0.0014503095317534727 PDE 2.0942156879755203e-06 0.149495929479599 0.149495929479599 0.15148144960403442 0.03222528100013733 0.5647997260093689 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014524037474414483\n",
            "1492 Data 0.0014502664847369595 PDE 2.0885565845674137e-06 0.14949578046798706 0.14949578046798706 0.15148228406906128 0.032217659056186676 0.5648062229156494 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001452355041321527\n",
            "1493 Data 0.001450223461382742 PDE 2.0829188542847987e-06 0.14949563145637512 0.14949563145637512 0.15148313343524933 0.03221004828810692 0.5648127198219299 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014523063802370268\n",
            "1494 Data 0.0014501804349654629 PDE 2.077305907732807e-06 0.14949548244476318 0.14949548244476318 0.15148398280143738 0.032202452421188354 0.5648192763328552 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014522577408731957\n",
            "1495 Data 0.0014501374762835432 PDE 2.0717106963275e-06 0.14949533343315125 0.14949533343315125 0.15148483216762543 0.032194867730140686 0.5648258328437805 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014522091869798707\n",
            "1496 Data 0.0014500944752040702 PDE 2.066137540168711e-06 0.1494951844215393 0.1494951844215393 0.15148568153381348 0.03218729794025421 0.5648324489593506 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001452160612744239\n",
            "1497 Data 0.0014500515458966286 PDE 2.06058143703558e-06 0.14949503540992737 0.14949503540992737 0.15148653090000153 0.03217973932623863 0.5648390650749207 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014521121273336642\n",
            "1498 Data 0.001450008648344681 PDE 2.055045570159564e-06 0.14949488639831543 0.14949488639831543 0.15148738026618958 0.03217219561338425 0.5648456811904907 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014520636939148405\n",
            "1499 Data 0.0014499657897126558 PDE 2.0495272110565566e-06 0.1494947373867035 0.1494947373867035 0.15148822963237762 0.03216466307640076 0.5648523569107056 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014520153169237124\n",
            "1500 Data 0.00144992296343612 PDE 2.0440345451788744e-06 0.14949458837509155 0.14949458837509155 0.15148907899856567 0.03215714544057846 0.5648590326309204 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014519669979812988\n",
            "1501 Data 0.001449880070601494 PDE 2.0385625703056576e-06 0.14949443936347961 0.14949443936347961 0.15148994326591492 0.03214963898062706 0.5648657083511353 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014519186331717997\n",
            "1502 Data 0.0014498373175008885 PDE 2.033111741184257e-06 0.14949429035186768 0.14949429035186768 0.15149080753326416 0.03214214742183685 0.5648724436759949 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014518704292420727\n",
            "1503 Data 0.0014497945261696078 PDE 2.0276779650885146e-06 0.14949414134025574 0.14949414134025574 0.1514916718006134 0.03213466703891754 0.5648791790008545 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014518222041346963\n",
            "1504 Data 0.0014497517720868285 PDE 2.0222628336341586e-06 0.1494939923286438 0.1494939923286438 0.15149253606796265 0.032127201557159424 0.5648859739303589 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014517740349204626\n",
            "1505 Data 0.0014497090591878358 PDE 2.0168574792478466e-06 0.14949384331703186 0.14949384331703186 0.1514934003353119 0.0321197472512722 0.5648927688598633 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014517259166670837\n",
            "1506 Data 0.0014496663659007126 PDE 2.0114782728342107e-06 0.14949369430541992 0.14949369430541992 0.15149426460266113 0.03211230784654617 0.5648995637893677 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014516778441735468\n",
            "1507 Data 0.0014496236632735176 PDE 2.00611975742504e-06 0.14949354529380798 0.14949354529380798 0.15149512887001038 0.03210487961769104 0.5649064183235168 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014516297830309427\n",
            "1508 Data 0.0014495810246962974 PDE 2.0007844341307646e-06 0.14949339628219604 0.14949339628219604 0.15149599313735962 0.0320974662899971 0.564913272857666 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014515818091304282\n",
            "1509 Data 0.0014495384232910055 PDE 1.9954668459831737e-06 0.1494932472705841 0.1494932472705841 0.15149685740470886 0.03209006413817406 0.56492018699646 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014515338901369887\n",
            "1510 Data 0.0014494957966145845 PDE 1.9901722225768026e-06 0.14949309825897217 0.14949309825897217 0.1514977365732193 0.03208267688751221 0.5649271011352539 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014514859688371613\n",
            "1511 Data 0.0014494532038670154 PDE 1.984896243811818e-06 0.14949294924736023 0.14949294924736023 0.15149861574172974 0.03207530081272125 0.5649340152740479 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014514381001108272\n",
            "1512 Data 0.0014494107019844887 PDE 1.9796377728198422e-06 0.1494928002357483 0.1494928002357483 0.15149949491024017 0.03206793963909149 0.5649409890174866 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014513903397573085\n",
            "1513 Data 0.0014493681854833927 PDE 1.974395900106174e-06 0.14949265122413635 0.14949265122413635 0.1515003740787506 0.032060589641332626 0.5649479627609253 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014513425813834988\n",
            "1514 Data 0.0014493256672183548 PDE 1.9691733541549183e-06 0.14949250221252441 0.14949250221252441 0.15150125324726105 0.032053254544734955 0.5649549961090088 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014512948405725097\n",
            "1515 Data 0.0014492832111844976 PDE 1.963970362339751e-06 0.14949235320091248 0.14949235320091248 0.15150213241577148 0.03204593062400818 0.5649620294570923 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014512471815468374\n",
            "1516 Data 0.0014492407197926062 PDE 1.9587869246606715e-06 0.14949220418930054 0.14949220418930054 0.15150301158428192 0.032038621604442596 0.5649690628051758 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001451199506717267\n",
            "1517 Data 0.0014491983216573467 PDE 1.9536209947546013e-06 0.1494920551776886 0.1494920551776886 0.15150389075279236 0.03203132376074791 0.564976155757904 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014511519426521013\n",
            "1518 Data 0.0014491559542299433 PDE 1.9484732547425665e-06 0.14949190616607666 0.14949190616607666 0.151504784822464 0.032024040818214417 0.5649832487106323 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014511044274846858\n",
            "1519 Data 0.001449113544700478 PDE 1.9433402940194355e-06 0.14949175715446472 0.14949175715446472 0.15150567889213562 0.03201676905155182 0.5649904012680054 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014510568849944974\n",
            "1520 Data 0.0014490712241644467 PDE 1.9382318896532524e-06 0.14949160814285278 0.14949160814285278 0.15150657296180725 0.032009512186050415 0.5649975538253784 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014510094560541\n",
            "1521 Data 0.0014490289104994055 PDE 1.9331380372022977e-06 0.14949145913124084 0.14949145913124084 0.15150746703147888 0.03200226649641991 0.5650047063827515 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014509620485366078\n",
            "1522 Data 0.0014489865813399915 PDE 1.9280685137346154e-06 0.1494913101196289 0.1494913101196289 0.1515083611011505 0.03199503570795059 0.5650119185447693 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014509146498537261\n",
            "1523 Data 0.0014489443404083454 PDE 1.9230151337978896e-06 0.14949116110801697 0.14949116110801697 0.15150925517082214 0.03198781982064247 0.5650191307067871 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014508673555421433\n",
            "1524 Data 0.0014489020240306201 PDE 1.9179817627446027e-06 0.14949101209640503 0.14949101209640503 0.15151014924049377 0.031980615109205246 0.5650264024734497 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014508200057933647\n",
            "1525 Data 0.001448859808238664 PDE 1.912965899464325e-06 0.1494908630847931 0.1494908630847931 0.1515110433101654 0.031973425298929214 0.5650336742401123 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014507727741381283\n",
            "1526 Data 0.001448817657890519 PDE 1.907967771330732e-06 0.14949071407318115 0.14949071407318115 0.15151195228099823 0.03196624666452408 0.5650409460067749 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014507256256618497\n",
            "1527 Data 0.001448775478720473 PDE 1.902987492030661e-06 0.1494905650615692 0.1494905650615692 0.15151286125183105 0.031959082931280136 0.5650482773780823 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014506784662125037\n",
            "1528 Data 0.0014487333032278601 PDE 1.898026880553516e-06 0.14949041604995728 0.14949041604995728 0.15151377022266388 0.03195193037390709 0.5650556087493896 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014506313301084137\n",
            "1529 Data 0.0014486911796268345 PDE 1.893085482151946e-06 0.14949026703834534 0.14949026703834534 0.1515146791934967 0.031944792717695236 0.5650629997253418 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014505842651089864\n",
            "1530 Data 0.0014486490501539153 PDE 1.8881628420786e-06 0.1494901180267334 0.1494901180267334 0.15151558816432953 0.03193766996264458 0.565070390701294 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001450537212995994\n",
            "1531 Data 0.0014486070081971031 PDE 1.8832510022548377e-06 0.14948996901512146 0.14948996901512146 0.15151649713516235 0.03193055838346481 0.5650778412818909 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001450490259199358\n",
            "1532 Data 0.0014485649417412645 PDE 1.8783596260618651e-06 0.14948982000350952 0.14948982000350952 0.15151740610599518 0.03192346170544624 0.5650852918624878 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014504433013673263\n",
            "1533 Data 0.0014485228905336954 PDE 1.8734830291577964e-06 0.14948967099189758 0.14948967099189758 0.151518315076828 0.03191637620329857 0.5650927424430847 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014503963735628532\n",
            "1534 Data 0.0014484808894148257 PDE 1.868627123258193e-06 0.14948952198028564 0.14948952198028564 0.15151923894882202 0.03190930560231209 0.5651002526283264 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014503495165380839\n",
            "1535 Data 0.0014484389010588337 PDE 1.8637854282133048e-06 0.1494893729686737 0.1494893729686737 0.15152016282081604 0.0319022499024868 0.5651077628135681 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001450302686487047\n",
            "1536 Data 0.0014483969159304496 PDE 1.8589678347780136e-06 0.14948922395706177 0.14948922395706177 0.15152108669281006 0.03189520537853241 0.5651153326034546 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014502558837652276\n",
            "1537 Data 0.001448354987685202 PDE 1.854163087955385e-06 0.14948908984661102 0.14948908984661102 0.15152201056480408 0.03188817575573921 0.5651229023933411 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014502091507731575\n",
            "1538 Data 0.0014483130599533469 PDE 1.8493801690055989e-06 0.14948895573616028 0.14948895573616028 0.1515229344367981 0.03188115730881691 0.5651304721832275 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014501624401223525\n",
            "1539 Data 0.0014482711847530292 PDE 1.8446093008606113e-06 0.14948882162570953 0.14948882162570953 0.15152385830879211 0.0318741537630558 0.5651381015777588 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014501157940538898\n",
            "1540 Data 0.0014482293641239227 PDE 1.839859919527953e-06 0.1494886875152588 0.1494886875152588 0.15152478218078613 0.03186716511845589 0.56514573097229 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014500692240434507\n",
            "1541 Data 0.0014481874552699215 PDE 1.8351252037973609e-06 0.14948855340480804 0.14948855340480804 0.15152570605278015 0.03186018764972687 0.5651534199714661 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014500225804737188\n",
            "1542 Data 0.001448145684701093 PDE 1.8304049262951594e-06 0.1494884192943573 0.1494884192943573 0.15152664482593536 0.03185322508215904 0.5651611089706421 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014499760896273881\n",
            "1543 Data 0.0014481038350289896 PDE 1.8257061356052873e-06 0.14948828518390656 0.14948828518390656 0.15152758359909058 0.03184627369046211 0.5651688575744629 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014499295411645949\n",
            "1544 Data 0.001448062092472985 PDE 1.8210187135991873e-06 0.1494881510734558 0.1494881510734558 0.1515285223722458 0.031839337199926376 0.5651766061782837 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014498831111865841\n",
            "1545 Data 0.0014480203389956674 PDE 1.816346184568829e-06 0.14948801696300507 0.14948801696300507 0.151529461145401 0.031832415610551834 0.5651843547821045 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014498366851802363\n",
            "1546 Data 0.0014479786298772397 PDE 1.8116882074536989e-06 0.14948788285255432 0.14948788285255432 0.1515303999185562 0.03182550519704819 0.5651921629905701 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014497903180846934\n",
            "1547 Data 0.001447936899910328 PDE 1.8070527403324377e-06 0.14948774874210358 0.14948774874210358 0.15153133869171143 0.031818609684705734 0.5651999711990356 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014497439526506605\n",
            "1548 Data 0.0014478952355263424 PDE 1.8024331893684575e-06 0.14948761463165283 0.14948761463165283 0.15153227746486664 0.03181172534823418 0.565207839012146 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014496976687157109\n",
            "1549 Data 0.001447853578323818 PDE 1.7978265987039777e-06 0.1494874805212021 0.1494874805212021 0.15153321623802185 0.03180485591292381 0.5652157068252563 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014496514049225219\n",
            "1550 Data 0.0014478119206983936 PDE 1.7932361515704542e-06 0.14948734641075134 0.14948734641075134 0.15153416991233826 0.03179800137877464 0.5652236342430115 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001449605156849964\n",
            "1551 Data 0.0014477703356949785 PDE 1.7886637806441286e-06 0.1494872123003006 0.1494872123003006 0.15153512358665466 0.03179115802049637 0.5652315616607666 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014495589994756226\n",
            "1552 Data 0.0014477287382908654 PDE 1.7841098269855138e-06 0.14948707818984985 0.14948707818984985 0.15153607726097107 0.03178432956337929 0.5652394890785217 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001449512848117851\n",
            "1553 Data 0.001447687233353223 PDE 1.7795686062527238e-06 0.1494869440793991 0.1494869440793991 0.15153703093528748 0.0317775160074234 0.5652474761009216 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014494668019594758\n",
            "1554 Data 0.0014476456378990372 PDE 1.775045689100807e-06 0.14948680996894836 0.14948680996894836 0.15153798460960388 0.03177071362733841 0.5652554631233215 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001449420683588138\n",
            "1555 Data 0.0014476041262971265 PDE 1.7705431218928425e-06 0.14948667585849762 0.14948667585849762 0.1515389382839203 0.03176392614841461 0.5652635097503662 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014493746694190194\n",
            "1556 Data 0.001447562634751001 PDE 1.7660526054896764e-06 0.14948654174804688 0.14948654174804688 0.1515398919582367 0.03175715357065201 0.5652715563774109 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014493286873564907\n",
            "1557 Data 0.0014475211966025387 PDE 1.7615743672649842e-06 0.14948640763759613 0.14948640763759613 0.1515408456325531 0.0317503921687603 0.5652796626091003 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014492827709698037\n",
            "1558 Data 0.001447479757188038 PDE 1.7571136368133011e-06 0.14948627352714539 0.14948627352714539 0.1515418142080307 0.031743645668029785 0.5652877688407898 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014492368708248513\n",
            "1559 Data 0.0014474383015474406 PDE 1.7526707551951404e-06 0.14948613941669464 0.14948613941669464 0.1515427827835083 0.031736914068460464 0.565295934677124 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014491909723026357\n",
            "1560 Data 0.0014473969067840598 PDE 1.7482415159975062e-06 0.1494860053062439 0.1494860053062439 0.1515437513589859 0.03173019364476204 0.5653041005134583 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014491451483000573\n",
            "1561 Data 0.0014473555222751845 PDE 1.743826601341425e-06 0.14948587119579315 0.14948587119579315 0.1515447199344635 0.03172348812222481 0.5653122663497925 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001449099348876526\n",
            "1562 Data 0.0014473141806855886 PDE 1.7394272617821116e-06 0.1494857370853424 0.1494857370853424 0.1515456885099411 0.03171679377555847 0.5653204917907715 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014490536079473707\n",
            "1563 Data 0.0014472728385812588 PDE 1.7350448615616187e-06 0.14948560297489166 0.14948560297489166 0.1515466570854187 0.03171011433005333 0.5653287172317505 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014490078834428204\n",
            "1564 Data 0.0014472315201914646 PDE 1.73067871855892e-06 0.14948546886444092 0.14948546886444092 0.1515476256608963 0.03170344978570938 0.5653370022773743 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014489621989100235\n",
            "1565 Data 0.0014471902586315776 PDE 1.726328264339827e-06 0.14948533475399017 0.14948533475399017 0.1515485942363739 0.03169679641723633 0.565345287322998 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014489165868959175\n",
            "1566 Data 0.0014471489174944696 PDE 1.7219939536516904e-06 0.14948520064353943 0.14948520064353943 0.1515495777130127 0.03169015794992447 0.5653536319732666 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014488709114481213\n",
            "1567 Data 0.001447107701802981 PDE 1.7176743085656199e-06 0.14948506653308868 0.14948506653308868 0.1515505611896515 0.031683534383773804 0.5653619766235352 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014488253761115466\n",
            "1568 Data 0.0014470664370660501 PDE 1.713368646960589e-06 0.14948493242263794 0.14948493242263794 0.15155154466629028 0.031676921993494034 0.5653703808784485 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014487798057130107\n",
            "1569 Data 0.001447025278205584 PDE 1.7090762867155718e-06 0.1494847983121872 0.1494847983121872 0.15155252814292908 0.03167032450437546 0.5653787851333618 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014487343544922996\n",
            "1570 Data 0.0014469840883097602 PDE 1.7048032532329671e-06 0.14948466420173645 0.14948466420173645 0.15155351161956787 0.031663741916418076 0.5653871893882751 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014486888915629932\n",
            "1571 Data 0.001446942905804948 PDE 1.700545112726104e-06 0.1494845300912857 0.1494845300912857 0.15155449509620667 0.03165717050433159 0.5653956532478333 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014486434509176742\n",
            "1572 Data 0.001446901810918709 PDE 1.6963010693871183e-06 0.14948439598083496 0.14948439598083496 0.15155547857284546 0.031650613993406296 0.5654041171073914 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001448598111988096\n",
            "1573 Data 0.0014468606898097888 PDE 1.692072828518576e-06 0.14948426187038422 0.14948426187038422 0.15155646204948425 0.0316440723836422 0.5654126405715942 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014485527626383074\n",
            "1574 Data 0.001446819547723599 PDE 1.6878618680493673e-06 0.14948412775993347 0.14948412775993347 0.15155746042728424 0.03163754194974899 0.5654211640357971 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014485074095916483\n",
            "1575 Data 0.001446778487270625 PDE 1.683666255303251e-06 0.14948399364948273 0.14948399364948273 0.15155845880508423 0.03163102641701698 0.5654297471046448 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014484621535259283\n",
            "1576 Data 0.0014467374674358956 PDE 1.6794826933619333e-06 0.14948385953903198 0.14948385953903198 0.15155945718288422 0.03162452578544617 0.5654383301734924 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014484169501292575\n",
            "1577 Data 0.0014466964506594673 PDE 1.675313683335844e-06 0.14948372542858124 0.14948372542858124 0.1515604555606842 0.031618040055036545 0.5654469728469849 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014483717643428032\n",
            "1578 Data 0.001446655401506186 PDE 1.6711575199224171e-06 0.1494835913181305 0.1494835913181305 0.1515614539384842 0.03161156550049782 0.5654556155204773 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014483265590261084\n",
            "1579 Data 0.0014466144003480585 PDE 1.667019887463539e-06 0.14948345720767975 0.14948345720767975 0.15156245231628418 0.031605105847120285 0.5654642581939697 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001448281420235522\n",
            "1580 Data 0.0014465734178289317 PDE 1.6628929415674065e-06 0.149483323097229 0.149483323097229 0.15156345069408417 0.031598661094903946 0.5654729604721069 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001448236310770499\n",
            "1581 Data 0.0014465325392046117 PDE 1.6587786149102612e-06 0.14948318898677826 0.14948318898677826 0.15156446397304535 0.0315922275185585 0.5654816627502441 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001448191317819522\n",
            "1582 Data 0.0014464915723489842 PDE 1.6546812275919365e-06 0.14948305487632751 0.14948305487632751 0.15156547725200653 0.03158580884337425 0.5654904246330261 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014481462535765762\n",
            "1583 Data 0.0014464507240596408 PDE 1.650600438551919e-06 0.14948292076587677 0.14948292076587677 0.1515664905309677 0.031579405069351196 0.5654991865158081 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014481013244981927\n",
            "1584 Data 0.0014464097964299915 PDE 1.6465317003167002e-06 0.14948278665542603 0.14948278665542603 0.1515675038099289 0.031573012471199036 0.5655080080032349 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014480563281303082\n",
            "1585 Data 0.0014463689227283269 PDE 1.6424774003098719e-06 0.14948265254497528 0.14948265254497528 0.15156851708889008 0.03156663477420807 0.5655168294906616 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014480114001286367\n",
            "1586 Data 0.0014463281168718928 PDE 1.6384366290367325e-06 0.14948251843452454 0.14948251843452454 0.15156953036785126 0.031560271978378296 0.5655257105827332 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014479665535009295\n",
            "1587 Data 0.0014462873247360485 PDE 1.6344079085683916e-06 0.14948239922523499 0.14948239922523499 0.15157054364681244 0.03155392035841942 0.5655345916748047 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014479217326446169\n",
            "1588 Data 0.0014462465088295184 PDE 1.6303936263284413e-06 0.14948228001594543 0.14948228001594543 0.15157155692577362 0.031547583639621735 0.565543532371521 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014478769024558469\n",
            "1589 Data 0.0014462057083651093 PDE 1.6263901443380746e-06 0.14948216080665588 0.14948216080665588 0.151572585105896 0.031541261821985245 0.5655524730682373 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014478320985094474\n",
            "1590 Data 0.0014461649616774493 PDE 1.6224028058786644e-06 0.14948204159736633 0.14948204159736633 0.15157361328601837 0.03153495490550995 0.5655614733695984 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001447787364483328\n",
            "1591 Data 0.0014461242770462382 PDE 1.6184275182240526e-06 0.14948192238807678 0.14948192238807678 0.15157464146614075 0.03152865916490555 0.5655704736709595 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014477427045644622\n",
            "1592 Data 0.0014460835438584512 PDE 1.6144665551109938e-06 0.14948180317878723 0.14948180317878723 0.15157566964626312 0.03152237832546234 0.5655795335769653 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014476980104135622\n",
            "1593 Data 0.0014460428675898563 PDE 1.6105187796711107e-06 0.14948168396949768 0.14948168396949768 0.1515766978263855 0.03151611238718033 0.5655885934829712 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014476533863695274\n",
            "1594 Data 0.0014460021944994999 PDE 1.606589648872614e-06 0.14948156476020813 0.14948156476020813 0.15157772600650787 0.03150985762476921 0.565597653388977 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014476087841483725\n",
            "1595 Data 0.0014459615615223286 PDE 1.6026689309001085e-06 0.14948144555091858 0.14948144555091858 0.15157875418663025 0.03150361776351929 0.5656067728996277 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014475642304532287\n",
            "1596 Data 0.00144592091905513 PDE 1.5987644701453974e-06 0.14948132634162903 0.14948132634162903 0.15157979726791382 0.03149739280343056 0.5656158924102783 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014475196835252754\n",
            "1597 Data 0.0014458803358402014 PDE 1.594875584487454e-06 0.14948120713233948 0.14948120713233948 0.1515808403491974 0.03149117901921272 0.5656250715255737 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014474752114246889\n",
            "1598 Data 0.0014458397267335876 PDE 1.5909997728158487e-06 0.14948108792304993 0.14948108792304993 0.15158188343048096 0.03148498013615608 0.5656342506408691 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014474307265064034\n",
            "1599 Data 0.0014457991815612899 PDE 1.5871329424044234e-06 0.14948096871376038 0.14948096871376038 0.15158292651176453 0.031478796154260635 0.5656434893608093 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014473863145036943\n",
            "1600 Data 0.0014457586300621638 PDE 1.5832825965844677e-06 0.14948084950447083 0.14948084950447083 0.1515839695930481 0.03147262707352638 0.5656527280807495 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014473419126587483\n",
            "1601 Data 0.0014457181123460022 PDE 1.5794406635905034e-06 0.14948073029518127 0.14948073029518127 0.15158501267433167 0.031466469168663025 0.5656620264053345 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014472975530095927\n",
            "1602 Data 0.0014456776272735285 PDE 1.575615897309035e-06 0.14948061108589172 0.14948061108589172 0.15158605575561523 0.03146032616496086 0.5656713247299194 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014472532431708375\n",
            "1603 Data 0.0014456371679581474 PDE 1.5718039776402293e-06 0.14948049187660217 0.14948049187660217 0.1515870988368988 0.03145419806241989 0.5656806826591492 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014472089719357877\n",
            "1604 Data 0.001445596658787895 PDE 1.5680037677157088e-06 0.14948037266731262 0.14948037266731262 0.15158815681934357 0.03144808113574982 0.5656900405883789 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014471646625556108\n",
            "1605 Data 0.0014455562512068708 PDE 1.5642169728380395e-06 0.14948025345802307 0.14948025345802307 0.15158921480178833 0.031441979110240936 0.5656994581222534 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014471204681797089\n",
            "1606 Data 0.0014455158098128325 PDE 1.560447799420217e-06 0.14948013424873352 0.14948013424873352 0.1515902727842331 0.03143589198589325 0.5657088756561279 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014470762576122528\n",
            "1607 Data 0.0014454755018878523 PDE 1.5566880620099255e-06 0.14948001503944397 0.14948001503944397 0.15159133076667786 0.03142981976270676 0.5657183527946472 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014470321899498623\n",
            "1608 Data 0.0014454350639553652 PDE 1.552943786009564e-06 0.14947989583015442 0.14947989583015442 0.15159238874912262 0.03142375871539116 0.5657278299331665 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014469880077413748\n",
            "1609 Data 0.0014453946645068185 PDE 1.5492100828851108e-06 0.14947977662086487 0.14947977662086487 0.15159344673156738 0.031417712569236755 0.5657373666763306 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014469438745897036\n",
            "1610 Data 0.0014453543434976368 PDE 1.5454860431418638e-06 0.14947965741157532 0.14947965741157532 0.15159450471401215 0.031411681324243546 0.5657469034194946 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014468998295407787\n",
            "1611 Data 0.001445314073184218 PDE 1.5417757595059811e-06 0.14947953820228577 0.14947953820228577 0.1515955775976181 0.03140566125512123 0.5657564401626587 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001446855848943724\n",
            "1612 Data 0.0014452737427471682 PDE 1.538079573037976e-06 0.14947941899299622 0.14947941899299622 0.15159665048122406 0.03139965608716011 0.5657660365104675 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014468118223202062\n",
            "1613 Data 0.0014452334663416907 PDE 1.5343954373747692e-06 0.14947929978370667 0.14947929978370667 0.15159772336483002 0.031393665820360184 0.5657756328582764 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014467678617790655\n",
            "1614 Data 0.0014451932429313118 PDE 1.5307252851926023e-06 0.14947918057441711 0.14947918057441711 0.15159879624843597 0.03138769045472145 0.56578528881073 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014467239682165044\n",
            "1615 Data 0.001445153013350048 PDE 1.527067524875747e-06 0.14947906136512756 0.14947906136512756 0.15159986913204193 0.03138172626495361 0.5657949447631836 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014466800808749238\n",
            "1616 Data 0.0014451128189353476 PDE 1.5234279544529272e-06 0.149478942155838 0.149478942155838 0.1516009420156479 0.03137577697634697 0.565804660320282 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014466362468898005\n",
            "1617 Data 0.0014450725872933645 PDE 1.5197962284219102e-06 0.14947882294654846 0.14947882294654846 0.15160201489925385 0.03136984258890152 0.5658143758773804 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014465923835217864\n",
            "1618 Data 0.001445032420795472 PDE 1.5161770079430426e-06 0.1494787037372589 0.1494787037372589 0.151603102684021 0.031363919377326965 0.5658241510391235 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001446548597803415\n",
            "1619 Data 0.0014449922427491697 PDE 1.512566313977004e-06 0.14947858452796936 0.14947858452796936 0.15160419046878815 0.031358011066913605 0.5658339262008667 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014465048090631467\n",
            "1620 Data 0.0014449521327597522 PDE 1.5089663065737113e-06 0.1494784653186798 0.1494784653186798 0.1516052782535553 0.03135211765766144 0.5658437609672546 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001446461099066326\n",
            "1621 Data 0.0014449120010745521 PDE 1.5053774404805154e-06 0.14947834610939026 0.14947834610939026 0.15160636603832245 0.031346239149570465 0.5658535957336426 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014464173785150327\n",
            "1622 Data 0.0014448719140873064 PDE 1.5018035810498986e-06 0.1494782269001007 0.1494782269001007 0.1516074538230896 0.03134037181735039 0.5658634901046753 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014463737176683563\n",
            "1623 Data 0.00144483184147288 PDE 1.4982404081820277e-06 0.14947810769081116 0.14947810769081116 0.15160854160785675 0.031334519386291504 0.565873384475708 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001446330081881062\n",
            "1624 Data 0.0014447918022108862 PDE 1.4946923556635738e-06 0.1494779884815216 0.1494779884815216 0.1516096293926239 0.031328681856393814 0.5658833384513855 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014462864945665498\n",
            "1625 Data 0.0014447517735372657 PDE 1.491152829657949e-06 0.14947786927223206 0.14947786927223206 0.15161071717739105 0.03132285922765732 0.565893292427063 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014462429263669236\n",
            "1626 Data 0.0014447117539209868 PDE 1.4876250133966096e-06 0.1494777500629425 0.1494777500629425 0.1516118198633194 0.03131704777479172 0.5659033060073853 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014461993789343834\n",
            "1627 Data 0.0014446717817638798 PDE 1.4841064057691256e-06 0.14947763085365295 0.14947763085365295 0.15161292254924774 0.03131125122308731 0.5659133195877075 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001446155888169649\n",
            "1628 Data 0.001444631798293419 PDE 1.4806021226831945e-06 0.1494775116443634 0.1494775116443634 0.1516140252351761 0.0313054695725441 0.5659233927726746 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014461124004161022\n",
            "1629 Data 0.0014445918189014283 PDE 1.4771068208574434e-06 0.14947739243507385 0.14947739243507385 0.15161512792110443 0.03129969909787178 0.5659334659576416 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014460689257222857\n",
            "1630 Data 0.0014445519093173624 PDE 1.4736290268047014e-06 0.1494772732257843 0.1494772732257843 0.15161623060703278 0.03129394352436066 0.5659435987472534 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001446025538344167\n",
            "1631 Data 0.001444512034292914 PDE 1.4701633972435957e-06 0.14947715401649475 0.14947715401649475 0.15161733329296112 0.03128820285201073 0.5659537315368652 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014459821976901576\n",
            "1632 Data 0.0014444720901257155 PDE 1.4667100458609639e-06 0.1494770348072052 0.1494770348072052 0.15161843597888947 0.03128247708082199 0.5659639239311218 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014459388001715765\n",
            "1633 Data 0.0014444322170556914 PDE 1.4632663578595384e-06 0.14947691559791565 0.14947691559791565 0.151619553565979 0.03127676248550415 0.5659741163253784 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001445895483413551\n",
            "1634 Data 0.001444392384856084 PDE 1.4598315374314552e-06 0.1494767963886261 0.1494767963886261 0.15162067115306854 0.031271062791347504 0.5659843683242798 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014458522163935154\n",
            "1635 Data 0.0014443525252818472 PDE 1.4564062666977406e-06 0.14947667717933655 0.14947667717933655 0.15162178874015808 0.03126537799835205 0.5659946203231812 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001445808931548545\n",
            "1636 Data 0.0014443127144841768 PDE 1.4529956615660922e-06 0.149476557970047 0.149476557970047 0.15162290632724762 0.03125970810651779 0.5660049319267273 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014457657101457429\n",
            "1637 Data 0.0014442729064762306 PDE 1.4495952882498386e-06 0.14947643876075745 0.14947643876075745 0.15162402391433716 0.03125404939055443 0.5660152435302734 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014457225017644804\n",
            "1638 Data 0.0014442331901213588 PDE 1.4462067383647081e-06 0.1494763195514679 0.1494763195514679 0.1516251415014267 0.03124840557575226 0.5660256147384644 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014456793968597235\n",
            "1639 Data 0.0014441933831219802 PDE 1.4428281929212972e-06 0.14947621524333954 0.14947621524333954 0.15162625908851624 0.031242776662111282 0.5660359859466553 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014456362113149015\n",
            "1640 Data 0.001444153636953913 PDE 1.439465222574654e-06 0.14947611093521118 0.14947611093521118 0.15162739157676697 0.03123716078698635 0.566046416759491 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014455931021764875\n",
            "1641 Data 0.0014441139099281952 PDE 1.4361111198013532e-06 0.14947600662708282 0.14947600662708282 0.1516285240650177 0.031231557950377464 0.5660568475723267 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014455500210479966\n",
            "1642 Data 0.0014440742092284417 PDE 1.4327725921248202e-06 0.14947590231895447 0.14947590231895447 0.15162965655326843 0.03122597001492977 0.5660673379898071 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014455069818205665\n",
            "1643 Data 0.0014440345482159664 PDE 1.4294430457084673e-06 0.1494757980108261 0.1494757980108261 0.15163078904151917 0.031220395117998123 0.5660778284072876 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014454639912616749\n",
            "1644 Data 0.0014439948845908642 PDE 1.426124640602211e-06 0.14947569370269775 0.14947569370269775 0.1516319215297699 0.03121483512222767 0.5660883784294128 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014454210092314664\n",
            "1645 Data 0.0014439552748236643 PDE 1.4228160125639988e-06 0.1494755893945694 0.1494755893945694 0.15163305401802063 0.03120928816497326 0.5660989284515381 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014453780908362283\n",
            "1646 Data 0.0014439156577917495 PDE 1.419519890077936e-06 0.14947548508644104 0.14947548508644104 0.15163418650627136 0.031203756108880043 0.5661095380783081 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014453351776818274\n",
            "1647 Data 0.0014438760464762696 PDE 1.416235704709834e-06 0.14947538077831268 0.14947538077831268 0.1516353338956833 0.03119823709130287 0.5661201477050781 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014452922821809795\n",
            "1648 Data 0.0014438364081777613 PDE 1.4129569763099425e-06 0.14947527647018433 0.14947527647018433 0.15163648128509521 0.031192732974886894 0.5661308169364929 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014452493651540713\n",
            "1649 Data 0.0014437968594080658 PDE 1.409688479725446e-06 0.14947517216205597 0.14947517216205597 0.15163762867450714 0.03118724189698696 0.5661414861679077 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014452065478877912\n",
            "1650 Data 0.001443757309883566 PDE 1.4064306697036955e-06 0.1494750678539276 0.1494750678539276 0.15163877606391907 0.031181765720248222 0.5661522150039673 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014451637405532697\n",
            "1651 Data 0.0014437177668925895 PDE 1.4031820683158003e-06 0.14947496354579926 0.14947496354579926 0.151639923453331 0.031176302582025528 0.5661629438400269 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014451209489609053\n",
            "1652 Data 0.0014436782621251936 PDE 1.3999443808643264e-06 0.1494748592376709 0.1494748592376709 0.15164107084274292 0.031170854344964027 0.5661737322807312 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001445078206506058\n",
            "1653 Data 0.00144363878597539 PDE 1.3967249969937257e-06 0.14947475492954254 0.14947475492954254 0.15164221823215485 0.03116541914641857 0.5661845207214355 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014450355109723837\n",
            "1654 Data 0.0014435992742942722 PDE 1.3935142533227918e-06 0.14947465062141418 0.14947465062141418 0.15164338052272797 0.03115999884903431 0.5661953687667847 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001444992788547595\n",
            "1655 Data 0.00144355984376395 PDE 1.3903135140935774e-06 0.14947454631328583 0.14947454631328583 0.1516445428133011 0.031154591590166092 0.5662062168121338 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014449501572780436\n",
            "1656 Data 0.0014435203829858978 PDE 1.3871281225874554e-06 0.14947444200515747 0.14947444200515747 0.1516457051038742 0.03114919923245907 0.5662171244621277 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014449075111084852\n",
            "1657 Data 0.0014434809753231448 PDE 1.3839455732522765e-06 0.1494743376970291 0.1494743376970291 0.15164686739444733 0.03114381991326809 0.5662280321121216 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001444864920896397\n",
            "1658 Data 0.001443441555757105 PDE 1.3807732557324925e-06 0.14947423338890076 0.14947423338890076 0.15164802968502045 0.031138455495238304 0.5662389993667603 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014448223290128375\n",
            "1659 Data 0.0014434022211495623 PDE 1.377613443764858e-06 0.1494741290807724 0.1494741290807724 0.15164919197559357 0.031133104115724564 0.5662499666213989 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014447798345933272\n",
            "1660 Data 0.0014433628014618065 PDE 1.374461135128513e-06 0.14947402477264404 0.14947402477264404 0.1516503542661667 0.031127765774726868 0.5662609934806824 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001444737262596935\n",
            "1661 Data 0.0014433235066315193 PDE 1.3713173530049971e-06 0.14947392046451569 0.14947392046451569 0.151651531457901 0.031122442334890366 0.5662720799446106 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014446948239845243\n",
            "1662 Data 0.001443284116903581 PDE 1.3681856216862798e-06 0.14947381615638733 0.14947381615638733 0.15165270864963531 0.031117131933569908 0.5662831664085388 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014446523025252673\n",
            "1663 Data 0.0014432448696835738 PDE 1.365059347335773e-06 0.14947371184825897 0.14947371184825897 0.15165388584136963 0.031111836433410645 0.5662943124771118 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014446099290309096\n",
            "1664 Data 0.00144320557270309 PDE 1.3619520586871658e-06 0.14947360754013062 0.14947360754013062 0.15165506303310394 0.031106553971767426 0.5663054585456848 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014445675247617771\n",
            "1665 Data 0.0014431662790352425 PDE 1.3588505680672824e-06 0.14947350323200226 0.14947350323200226 0.15165624022483826 0.0311012864112854 0.5663166642189026 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014445251296033097\n",
            "1666 Data 0.0014431270784898826 PDE 1.3557614693127107e-06 0.1494733989238739 0.1494733989238739 0.15165741741657257 0.03109603188931942 0.5663278698921204 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014444828399591953\n",
            "1667 Data 0.0014430878111446703 PDE 1.3526820339393453e-06 0.14947329461574554 0.14947329461574554 0.15165859460830688 0.031090792268514633 0.5663391351699829 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014444404931786097\n",
            "1668 Data 0.0014430485945326507 PDE 1.349614763057616e-06 0.1494731903076172 0.1494731903076172 0.1516597867012024 0.03108556568622589 0.5663504004478455 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014443982092957083\n",
            "1669 Data 0.0014430094418321552 PDE 1.34655681449658e-06 0.14947308599948883 0.14947308599948883 0.1516609787940979 0.031080354005098343 0.5663617253303528 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014443559986466517\n",
            "1670 Data 0.0014429702356982836 PDE 1.343509893558803e-06 0.14947298169136047 0.14947298169136047 0.1516621708869934 0.03107515536248684 0.5663730502128601 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014443137455918424\n",
            "1671 Data 0.0014429310719188723 PDE 1.3404684295892366e-06 0.14947287738323212 0.14947287738323212 0.15166336297988892 0.03106997162103653 0.5663844347000122 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014442715403484615\n",
            "1672 Data 0.0014428918971949541 PDE 1.3374393574849819e-06 0.14947277307510376 0.14947277307510376 0.15166455507278442 0.031064800918102264 0.5663958191871643 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014442293365524391\n",
            "1673 Data 0.0014428527926446315 PDE 1.334416083409451e-06 0.1494726687669754 0.1494726687669754 0.15166574716567993 0.031059643253684044 0.5664072632789612 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001444187208728041\n",
            "1674 Data 0.0014428136942005142 PDE 1.3314041780176922e-06 0.14947256445884705 0.14947256445884705 0.15166693925857544 0.031054500490427017 0.5664187073707581 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014441450983785319\n",
            "1675 Data 0.0014427746711589825 PDE 1.328401026512438e-06 0.1494724601507187 0.1494724601507187 0.15166814625263214 0.031049370765686035 0.5664302110671997 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001444103072185495\n",
            "1676 Data 0.0014427355361484717 PDE 1.3254089026304428e-06 0.14947235584259033 0.14947235584259033 0.15166935324668884 0.031044255942106247 0.5664417147636414 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001444060945051102\n",
            "1677 Data 0.0014426964964375927 PDE 1.3224276926848688e-06 0.14947225153446198 0.14947225153446198 0.15167056024074554 0.031039154157042503 0.5664532780647278 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014440189241302775\n",
            "1678 Data 0.00144265740303444 PDE 1.3194604662203346e-06 0.14947214722633362 0.14947214722633362 0.15167176723480225 0.031034067273139954 0.5664648413658142 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014439768635006603\n",
            "1679 Data 0.0014426184243422656 PDE 1.3164997199055506e-06 0.14947204291820526 0.14947204291820526 0.15167297422885895 0.03102899342775345 0.5664764642715454 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014439349240621712\n",
            "1680 Data 0.001442579421259828 PDE 1.3135496601535124e-06 0.1494719386100769 0.1494719386100769 0.15167418122291565 0.031023934483528137 0.5664880871772766 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014438929709199816\n",
            "1681 Data 0.001442540463958784 PDE 1.3106055121170357e-06 0.14947183430194855 0.14947183430194855 0.15167538821697235 0.03101888857781887 0.5664997696876526 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001443851069470901\n",
            "1682 Data 0.001442501479748357 PDE 1.3076740970063838e-06 0.1494717299938202 0.1494717299938202 0.15167661011219025 0.031013857573270798 0.5665115118026733 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014438091538453634\n",
            "1683 Data 0.0014424625659062227 PDE 1.3047518905295874e-06 0.14947162568569183 0.14947162568569183 0.15167783200740814 0.03100883960723877 0.5665232539176941 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014437673177967523\n",
            "1684 Data 0.0014424236547276976 PDE 1.3018418485444272e-06 0.14947152137756348 0.14947152137756348 0.15167905390262604 0.031003834679722786 0.5665350556373596 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001443725496576242\n",
            "1685 Data 0.0014423847382733746 PDE 1.298941583627311e-06 0.14947141706943512 0.14947141706943512 0.15168027579784393 0.030998844653367996 0.5665468573570251 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001443683679857002\n",
            "1686 Data 0.00144234582267474 PDE 1.29604904941516e-06 0.14947131276130676 0.14947131276130676 0.15168149769306183 0.03099386766552925 0.5665587186813354 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014436418717241552\n",
            "1687 Data 0.001442306975343526 PDE 1.2931608353028423e-06 0.1494712084531784 0.1494712084531784 0.15168271958827972 0.0309889055788517 0.5665705800056458 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014436001361788288\n",
            "1688 Data 0.0014422681065838882 PDE 1.2902828530059196e-06 0.14947110414505005 0.14947110414505005 0.15168394148349762 0.030983956530690193 0.5665825009346008 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014435583894368941\n",
            "1689 Data 0.0014422292866493617 PDE 1.2874089634351549e-06 0.1494709998369217 0.1494709998369217 0.1516851782798767 0.03097902238368988 0.5665944218635559 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001443516695612797\n",
            "1690 Data 0.0014421904422977105 PDE 1.2845499668401317e-06 0.14947089552879333 0.14947089552879333 0.1516864150762558 0.030974101275205612 0.5666064023971558 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014434749922645506\n",
            "1691 Data 0.0014421516431319214 PDE 1.2816967682738323e-06 0.14947079122066498 0.14947079122066498 0.1516876518726349 0.030969195067882538 0.5666183829307556 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014434333399001953\n",
            "1692 Data 0.0014421128853204757 PDE 1.278854938391305e-06 0.14947068691253662 0.14947068691253662 0.15168888866901398 0.030964301899075508 0.5666304230690002 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001443391740258867\n",
            "1693 Data 0.0014420740783224401 PDE 1.2760258414346026e-06 0.14947058260440826 0.14947058260440826 0.15169012546539307 0.030959423631429672 0.5666425228118896 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014433501041638747\n",
            "1694 Data 0.001442035346986939 PDE 1.2732073173538083e-06 0.1494704782962799 0.1494704782962799 0.15169136226177216 0.03095455840229988 0.566654622554779 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014433085543042927\n",
            "1695 Data 0.0014419966733667385 PDE 1.270394932362251e-06 0.14947038888931274 0.14947038888931274 0.15169259905815125 0.030949708074331284 0.5666667819023132 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014432670682991008\n",
            "1696 Data 0.0014419579056072407 PDE 1.2675944844886544e-06 0.14947029948234558 0.14947029948234558 0.15169385075569153 0.03094487078487873 0.5666789412498474 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014432255000917293\n",
            "1697 Data 0.0014419192043015087 PDE 1.264800062017457e-06 0.14947021007537842 0.14947021007537842 0.1516951024532318 0.030940046533942223 0.5666911602020264 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014431840043635262\n",
            "1698 Data 0.0014418805054668927 PDE 1.262014507119602e-06 0.14947012066841125 0.14947012066841125 0.1516963541507721 0.030935237184166908 0.5667033791542053 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014431425199740123\n",
            "1699 Data 0.0014418418472908695 PDE 1.2592367966135498e-06 0.1494700312614441 0.1494700312614441 0.15169760584831238 0.03093044087290764 0.566715657711029 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001443101084087483\n",
            "1700 Data 0.001441803229981386 PDE 1.2564615872179274e-06 0.14946994185447693 0.14946994185447693 0.15169885754585266 0.030925659462809563 0.5667279362678528 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014430596915686038\n",
            "1701 Data 0.0014417646322154177 PDE 1.253699451808643e-06 0.14946985244750977 0.14946985244750977 0.15170010924339294 0.03092089109122753 0.5667402744293213 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014430183316672264\n",
            "1702 Data 0.0014417259906179074 PDE 1.2509443649832974e-06 0.1494697630405426 0.1494697630405426 0.15170137584209442 0.030916137620806694 0.5667526125907898 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014429769349828907\n",
            "1703 Data 0.0014416874516666464 PDE 1.2481967814892414e-06 0.14946967363357544 0.14946967363357544 0.1517026424407959 0.0309113971889019 0.5667650103569031 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014429356484481357\n",
            "1704 Data 0.001441648844902044 PDE 1.2454555644580978e-06 0.14946958422660828 0.14946958422660828 0.15170390903949738 0.030906671658158302 0.5667774677276611 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001442894300466502\n",
            "1705 Data 0.0014416103343083083 PDE 1.2427240108081605e-06 0.1494694948196411 0.1494694948196411 0.15170517563819885 0.030901959165930748 0.5667899250984192 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014428530583191165\n",
            "1706 Data 0.0014415717970258728 PDE 1.2400024615999428e-06 0.14946940541267395 0.14946940541267395 0.15170644223690033 0.03089725971221924 0.566802442073822 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014428117994874728\n",
            "1707 Data 0.00144153327944009 PDE 1.2372889841572032e-06 0.1494693160057068 0.1494693160057068 0.1517077088356018 0.030892575159668922 0.5668149590492249 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014427705684242472\n",
            "1708 Data 0.0014414947516875605 PDE 1.2345850564088323e-06 0.14946922659873962 0.14946922659873962 0.15170897543430328 0.03088790364563465 0.5668275356292725 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014427293367439693\n",
            "1709 Data 0.0014414562507145746 PDE 1.2318915878495318e-06 0.14946913719177246 0.14946913719177246 0.15171025693416595 0.030883247032761574 0.5668401122093201 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001442688142302424\n",
            "1710 Data 0.00144141778380062 PDE 1.2292057363083586e-06 0.1494690477848053 0.1494690477848053 0.15171153843402863 0.03087860345840454 0.5668527483940125 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014426469895369284\n",
            "1711 Data 0.0014413793724179043 PDE 1.226529434461554e-06 0.14946895837783813 0.14946895837783813 0.1517128199338913 0.030873974785208702 0.5668653845787048 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014426059018523658\n",
            "1712 Data 0.0014413409076622515 PDE 1.2238608633197146e-06 0.14946886897087097 0.14946886897087097 0.15171410143375397 0.030869359150528908 0.566878080368042 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014425647685255712\n",
            "1713 Data 0.0014413025066245578 PDE 1.2211997955091647e-06 0.1494687795639038 0.1494687795639038 0.15171538293361664 0.030864756554365158 0.5668908357620239 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001442523706420067\n",
            "1714 Data 0.0014412641099272767 PDE 1.218549300574523e-06 0.14946869015693665 0.14946869015693665 0.1517166644334793 0.030860168859362602 0.5669035911560059 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014424826592278513\n",
            "1715 Data 0.0014412257182258303 PDE 1.2159050584159559e-06 0.14946860074996948 0.14946860074996948 0.15171794593334198 0.03085559420287609 0.5669164061546326 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014424416232842463\n",
            "1716 Data 0.001441187342420249 PDE 1.213263999488845e-06 0.14946851134300232 0.14946851134300232 0.15171924233436584 0.030851034447550774 0.5669292211532593 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001442400606419738\n",
            "1717 Data 0.0014411490699479123 PDE 1.2106370377296116e-06 0.14946842193603516 0.14946842193603516 0.1517205387353897 0.0308464877307415 0.5669420957565308 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001442359706985642\n",
            "1718 Data 0.0014411106969941265 PDE 1.2080195119779091e-06 0.149468332529068 0.149468332529068 0.15172183513641357 0.030841955915093422 0.5669549703598022 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014423187165061044\n",
            "1719 Data 0.0014410724103464869 PDE 1.2054101716785226e-06 0.14946824312210083 0.14946824312210083 0.15172313153743744 0.030837437137961388 0.5669679045677185 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014422778205181654\n",
            "1720 Data 0.0014410340575349227 PDE 1.2028117453155573e-06 0.14946815371513367 0.14946815371513367 0.1517244279384613 0.030832931399345398 0.5669808387756348 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014422368692802382\n",
            "1721 Data 0.0014409958350434762 PDE 1.2002200264760177e-06 0.1494680643081665 0.1494680643081665 0.15172572433948517 0.030828440561890602 0.5669938325881958 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014421960550699523\n",
            "1722 Data 0.0014409575514260128 PDE 1.1976402447544388e-06 0.14946797490119934 0.14946797490119934 0.15172702074050903 0.03082396276295185 0.5670068860054016 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014421551916707672\n",
            "1723 Data 0.0014409192969879853 PDE 1.1950630778301274e-06 0.14946788549423218 0.14946788549423218 0.1517283320426941 0.030819499865174294 0.5670199394226074 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014421143600658155\n",
            "1724 Data 0.0014408810701956325 PDE 1.1924960290343734e-06 0.14946779608726501 0.14946779608726501 0.15172964334487915 0.03081505000591278 0.567033052444458 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014420735662246669\n",
            "1725 Data 0.0014408428389291635 PDE 1.1899330729647772e-06 0.14946770668029785 0.14946770668029785 0.1517309546470642 0.030810613185167313 0.5670461654663086 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014420327720021282\n",
            "1726 Data 0.0014408046768016089 PDE 1.1873810308316024e-06 0.1494676172733307 0.1494676172733307 0.15173226594924927 0.03080619126558304 0.567059338092804 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014419920578324405\n",
            "1727 Data 0.0014407665083327236 PDE 1.1848344456666382e-06 0.14946752786636353 0.14946752786636353 0.15173357725143433 0.03080178238451481 0.5670725107192993 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014419513427783902\n",
            "1728 Data 0.0014407283079689674 PDE 1.182298888124933e-06 0.14946743845939636 0.14946743845939636 0.15173488855361938 0.030797388404607773 0.5670857429504395 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014419106068570924\n",
            "1729 Data 0.001440690166607551 PDE 1.1797679917435744e-06 0.1494673490524292 0.1494673490524292 0.15173621475696564 0.03079300746321678 0.5670990347862244 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014418699345992946\n",
            "1730 Data 0.001440652041596842 PDE 1.1772461903092335e-06 0.14946725964546204 0.14946725964546204 0.1517375409603119 0.030788639560341835 0.5671123266220093 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014418292877871513\n",
            "1731 Data 0.0014406139327066087 PDE 1.1747324606403708e-06 0.14946717023849487 0.14946717023849487 0.15173886716365814 0.030784286558628082 0.567125678062439 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001441788665167249\n",
            "1732 Data 0.001440575816193541 PDE 1.1722250974344206e-06 0.1494670808315277 0.1494670808315277 0.1517401933670044 0.030779946595430374 0.5671390295028687 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014417480412909755\n",
            "1733 Data 0.0014405377203543512 PDE 1.16972523755976e-06 0.14946699142456055 0.14946699142456055 0.15174151957035065 0.03077561967074871 0.5671524405479431 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.001441707445591911\n",
            "1734 Data 0.0014404997143645062 PDE 1.1672319715216872e-06 0.14946690201759338 0.14946690201759338 0.1517428457736969 0.03077130764722824 0.5671658515930176 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014416669463360279\n",
            "1735 Data 0.0014404616398793044 PDE 1.16474791411747e-06 0.14946681261062622 0.14946681261062622 0.15174417197704315 0.030767008662223816 0.5671793222427368 0.9523944854736328 0.6128332614898682 \tTraning Loss: 0.0014416263877934218\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d1bb324f2036>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#Condiciones de la física sobre la misma grilla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#Parte real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdxr\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdxxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdxr\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0mx_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdxr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdtr\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    395\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pinn.a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUx5fBRge-DQ",
        "outputId": "f053fa4e-d97c-41ee-fb55-bf2eea4ddb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0.6598], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = yh[:,:,0].detach().numpy()**2 + yh[:,:,1].detach().numpy()**2\n",
        "valo = y_data[:,:,0].detach().numpy()**2 + y_data[:,:,1].detach().numpy()**2"
      ],
      "metadata": {
        "id": "qTPM1ZMPp0U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW7DGT2FrDb4",
        "outputId": "e93066bd-916c-4fda-8fbb-6a21ca79e851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(val[0,:])\n",
        "plt.plot(valo[0,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "s21CdZMVqgr7",
        "outputId": "f658631d-a1ee-4a0d-cd00-4c99c3147617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7a6c767a69e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(val-valo)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "9enWnX-aov-S",
        "outputId": "1ad6ebe8-bdc0-4e52-cbcc-2f7cc66ecfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7a6c76d97910>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(valo)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "8VSQJWIXpg_N",
        "outputId": "98801963-4f8f-4425-805c-30dd55c5e8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7a6c76b12fb0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(val)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "QEl-yqHkp2_d",
        "outputId": "de01af74-dc0b-4164-9a8a-b2276052776d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7a6c769268f0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkTFrSptqb0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VERSION VIEJA DE CARGAR LOS DATOS, SIN DATALOADER\n",
        "interval = 100\n",
        "real_cutted     = rout\n",
        "imag_cutted     = iout\n",
        "real_data = torch.from_numpy(real_cutted[:,::interval])[:,:,None]\n",
        "imag_data = torch.from_numpy(imag_cutted[:,::interval])[:,:,None]\n",
        "y_data = torch.cat((real_data,imag_data),dim = -1)\n",
        "\n",
        "t_data = (interval*dt*torch.arange(steps//interval+1)).view(-1)\n",
        "x_data  = torch.linspace(0,2*torch.pi,N)\n",
        "x_grid, t_grid = torch.meshgrid(x_data, t_data, indexing='ij')\n",
        "x_grid = x_grid[:,:,None].requires_grad_(True) #Agregamos una dimensión al final para que pueda ser input de la red\n",
        "t_grid = t_grid[:,:,None].requires_grad_(True) #Agregamos una dimensión al final para que pueda ser input de la red\n",
        "input_data = torch.cat((x_grid, t_grid), dim=-1)"
      ],
      "metadata": {
        "id": "NtxV8W5-vfhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}